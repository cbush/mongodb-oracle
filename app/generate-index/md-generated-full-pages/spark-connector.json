{
  "https://www.mongodb.com/docs/spark-connector/current/api-docs/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # API Documentation ← [Release Notes](https://mongodb.com/docs/spark-connector/current/release-notes/ \"Previous Section\")",
  "https://www.mongodb.com/docs/spark-connector/current/write-to-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Write to MongoDB ## Important If your write operation includes a field with a `null` value, the connector writes the field name and `null` value to MongoDB. ← [Getting Started](https://mongodb.com/docs/spark-connector/current/getting-started/ \"Previous Section\")[Read from MongoDB](https://mongodb.com/docs/spark-connector/current/read-from-mongodb/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # MongoDB Connector for Spark The [MongoDB Connector for Spark](https://www.mongodb.com/products/spark-connector) provides integration between MongoDB and Apache Spark. ## Note Version 10.x of the MongoDB Connector for Spark is an all-new connector based on the latest Spark API. Install and migrate to version 10.x to take advantage of new capabilities, such as tighter integration with[Spark Structured Streaming.](https://mongodb.com/docs/spark-connector/current/structured-streaming/#std-label-spark-structured-streaming) Version 10.x uses the new namespace`com.mongodb.spark.sql.connector.MongoTableProvider`. This allows you to use old versions of the connector (versions 3.x and earlier) in parallel with version 10.x. To learn more about the new connector and its advantages, see the[MongoDB announcement blog post.](https://www.mongodb.com/blog/post/new-mongodb-spark-connector) With the connector, you have access to all Spark libraries for use with MongoDB datasets: Datasets for analysis with SQL (benefiting from automatic schema inference), streaming, machine learning, and graph APIs. You can also use the connector with the Spark Shell. The MongoDB Connector for Spark is compatible with the following versions of Apache Spark and MongoDB: | MongoDB Connector for Spark | Spark Version    | MongoDB Version  |\n| --------------------------- | ---------------- | ---------------- |\n| **10.1.1**                  | **3.1 or later** | **4.0 or later** | [Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/release-notes/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Release Notes ## MongoDB Connector for Spark 10.1.1 * Corrected a bug in which aggregations including the `$collStats` pipeline stage did not return a count field for Time Series collections. ## MongoDB Connector for Spark 10.1.0 * Support for Scala 2.13.\n* Support for micro-batch mode with Spark Structured Streaming.\n* Support for BSON data types.\n* Improved partitioner support for empty collections.\n* Option to disable automatic upsert on write operations.\n* Improved schema inference for empty arrays.\n* Support for null values in arrays and lists. The Connector now writes these values to MongoDB instead of throwing an exception. See [this post](https://www.mongodb.com/blog/post/introducing-mongodb-spark-connector-version-10-1)on the MongoDB blog for more information. ## MongoDB Connector for Spark 10.0.0 * Support for Spark Structured Streaming. ← [FAQ](https://mongodb.com/docs/spark-connector/current/faq/ \"Previous Section\")[API Documentation](https://mongodb.com/docs/spark-connector/current/api-docs/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/getting-started/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Getting Started On this page * [Prerequisites](#prerequisites)\n* [Getting Started](#getting-started-1)\n* [Tutorials](#tutorials) ## Prerequisites * Basic working knowledge of MongoDB and Apache Spark. Refer to the[MongoDB documentation](https://www.mongodb.com/docs/manual/) and [Spark documentation](https://spark.apache.org/docs/latest/) for more details.\n* Running MongoDB instance (version 4.0 or later).\n* Spark version 3.1 or later.\n* Java 8 or later. ## Getting Started ## Tutorials * [Write to MongoDB](https://mongodb.com/docs/spark-connector/current/write-to-mongodb/)\n* [Read from MongoDB](https://mongodb.com/docs/spark-connector/current/read-from-mongodb/)\n* [Structured Streaming with MongoDB](https://mongodb.com/docs/spark-connector/current/structured-streaming/) ← [Read Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/read/ \"Previous Section\")[Write to MongoDB](https://mongodb.com/docs/spark-connector/current/write-to-mongodb/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/tutorials/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Tutorials [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/faq/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # FAQ ## How can I achieve data locality? For any MongoDB deployment, the Mongo Spark Connector sets the preferred location for a DataFrame or Dataset to be where the data is: * For a non sharded system, it sets the preferred location to be the hostname(s) of the standalone or the replica set.\n* For a sharded system, it sets the preferred location to be the hostname(s) of the shards. To promote data locality, * Ensure there is a Spark Worker on one of the hosts for non-sharded system or one per shard for sharded systems.\n* Use a [nearest](https://www.mongodb.com/docs/manual/core/read-preference/#mongodb-readmode-nearest) read preference to read from the local[mongod.](https://www.mongodb.com/docs/manual/reference/program/mongod/#mongodb-binary-bin.mongod)\n* For a sharded cluster, you should have a [mongos](https://www.mongodb.com/docs/manual/reference/program/mongos/#mongodb-binary-bin.mongos) on the same nodes and use [localThreshold](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-input-conf)configuration to connect to the nearest [mongos](https://www.mongodb.com/docs/manual/reference/program/mongos/#mongodb-binary-bin.mongos). To partition the data by shard use the[ShardedPartitioner Configuration.](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-conf-shardedpartitioner) ## How do I resolve `Unrecognized pipeline stage name` Error? In MongoDB deployments with mixed versions of [mongod](https://www.mongodb.com/docs/manual/reference/program/mongod/#mongodb-binary-bin.mongod), it is possible to get an `Unrecognized pipeline stage name: '$sample'`error. To mitigate this situation, explicitly configure the partitioner to use and define the Schema when using DataFrames. ← [Structured Streaming with MongoDB](https://mongodb.com/docs/spark-connector/current/structured-streaming/ \"Previous Section\")[Release Notes](https://mongodb.com/docs/spark-connector/current/release-notes/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Read from MongoDB On this page * [Overview](#overview)\n* [SQL Queries](#sql-queries) ## Overview ## SQL Queries ← [Write to MongoDB](https://mongodb.com/docs/spark-connector/current/write-to-mongodb/ \"Previous Section\")[Structured Streaming with MongoDB](https://mongodb.com/docs/spark-connector/current/structured-streaming/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/python/write-to-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) To create a DataFrame, first create a [SparkSession object](https://mongodb.com/docs/spark-connector/current/getting-started/#std-label-python-basics), then use the object's `createDataFrame()` function. In the following example, `createDataFrame()` takes a list of tuples containing names and ages, and a list of column names: `| people = spark.createDataFrame([(\"Bilbo Baggins\",  50), (\"Gandalf\", 1000), (\"Thorin\", 195), (\"Balin\", 178), (\"Kili\", 77), |\n| ------------------------------------------------------------------------------------------------------------------------- |\n| (\"Dwalin\", 169), (\"Oin\", 167), (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", None)], [\"name\", \"age\"])                          | ` Write the `people` DataFrame to the MongoDB database and collection specified in the [spark.mongodb.write.connection.uri](https://mongodb.com/docs/spark-connector/current/getting-started/#std-label-pyspark-shell) option by using the `write` method: ```python people.write.format(\"mongodb\").mode(\"append\").save() \n``` The above operation writes to the MongoDB database and collection specified in the [spark.mongodb.write.connection.uri](https://mongodb.com/docs/spark-connector/current/getting-started/#std-label-pyspark-shell) option when you connect to the `pyspark` shell. To read the contents of the DataFrame, use the `show()` method. ```python people.show() \n``` In the `pyspark` shell, the operation prints the following output: ``` +-------------+----+ |         name| age| +-------------+----+ |Bilbo Baggins|  50| |      Gandalf|1000| |       Thorin| 195| |        Balin| 178| |         Kili|  77| |       Dwalin| 169| |          Oin| 167| |        Gloin| 158| |         Fili|  82| |       Bombur|null| +-------------+----+ \n``` The `printSchema()` method prints out the DataFrame's schema: ```python people.printSchema() \n``` In the `pyspark` shell, the operation prints the following output: ``` root  |-- _id: struct (nullable = true)  |    |-- oid: string (nullable = true)  |-- age: long (nullable = true)  |-- name: string (nullable = true) \n``` If you need to write to a different MongoDB collection, use the `.option()` method with `.write()`. To write to a collection called `contacts` in a database called`people`, specify the collection and database with `.option()`: ```python people.write.format(\"mongodb\").mode(\"append\").option(\"database\", \"people\").option(\"collection\", \"contacts\").save() \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/python/api/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) ## Important In version 10.0.0 and later of the Connector, use the format`mongodb` to read from and write to MongoDB: `df = spark.read.format(\"mongodb\").load()` # Python Spark Shell This tutorial uses the `pyspark` shell, but the code works with self-contained Python applications as well. When starting the `pyspark` shell, you can specify: * the `--packages` option to download the MongoDB Spark Connector package. The following package is available:  \n   * `mongo-spark-connector`\n* the `--conf` option to configure the MongoDB Spark Connnector. These settings configure the `SparkConf` object.  \n## Note  \nWhen specifying the Connector configuration via `SparkConf`, you must prefix the settings appropriately. For details and other available MongoDB Spark Connector options, see the[Configuration Options.](https://mongodb.com/docs/spark-connector/current/configuration/) The following example starts the `pyspark` shell from the command line: `| ./bin/pyspark --conf \"spark.mongodb.read.connection.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\ |\n| -------------------------------------------------------------------------------------------------------------------------------- |\n| --conf \"spark.mongodb.write.connection.uri=mongodb://127.0.0.1/test.myCollection\" \\                                              |\n| --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1                                                                   | ` * The [spark.mongodb.read.connection.uri](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-input-conf) specifies the MongoDB server address (`127.0.0.1`), the database to connect (`test`), and the collection (`myCollection`) from which to read data, and the read preference.\n* The [spark.mongodb.write.connection.uri](https://mongodb.com/docs/spark-connector/current/configuration/write/#std-label-spark-output-conf) specifies the MongoDB server address (`127.0.0.1`), the database to connect (`test`), and the collection (`myCollection`) to which to write data. Connects to port `27017` by default.\n* The `packages` option specifies the Spark Connector's Maven coordinates, in the format `groupId:artifactId:version`. The examples in this tutorial will use this database and collection. # Create a `SparkSession` Object ## Note When you start `pyspark` you get a `SparkSession` object called`spark` by default. In a standalone Python application, you need to create your `SparkSession` object explicitly, as show below. If you specified the `spark.mongodb.read.connection.uri`and `spark.mongodb.write.connection.uri` configuration options when you started `pyspark`, the default `SparkSession` object uses them. If you'd rather create your own `SparkSession` object from within`pyspark`, you can use `SparkSession.builder` and specify different configuration options. ```python from pyspark.sql import SparkSession my_spark = SparkSession \\     .builder \\     .appName(\"myApp\") \\     .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1/test.coll\") \\     .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1/test.coll\") \\\n\n \n``` You can use a `SparkSession` object to write data to MongoDB, read data from MongoDB, create DataFrames, and perform SQL operations. [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/python/filters/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Filters When using filters with DataFrames or Datasets, the underlying MongoDB Connector code constructs an [aggregation pipeline](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/) to filter the data in MongoDB before sending it to Spark. This improves Spark performance by retrieving and processing only the data you need. MongoDB Spark Connector turns the following filters into aggregation pipeline stages: * And\n* EqualNullSafe\n* EqualTo\n* GreaterThan\n* GreaterThanOrEqual\n* In\n* IsNull\n* LessThan\n* LessThanOrEqual\n* Not\n* Or\n* StringContains\n* StringEndsWith\n* StringStartsWith Use `filter()` to read a subset of data from your MongoDB collection. Consider a collection named `fruit` that contains the following documents: `| { \"_id\" : 1, \"type\" : \"apple\", \"qty\" : 5 }   |\n| -------------------------------------------- |\n| { \"_id\" : 2, \"type\" : \"orange\", \"qty\" : 10 } |\n| { \"_id\" : 3, \"type\" : \"banana\", \"qty\" : 15 } | ` First, set up a DataFrame to connect with your default MongoDB data source: ```python df = spark.read.format(\"mongodb\").load() \n``` The following example includes only records in which the `qty` field is greater than or equal to `10`. ```python df.filter(df['qty'] >= 10).show() \n``` The operation prints the following output: ``` +---+----+------+ |_id| qty|  type| +---+----+------+ |2.0|10.0|orange| |3.0|15.0|banana| +---+----+------+ \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/python/sql/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) Before you can run SQL queries against your DataFrame, you need to register a temporary table. The following example registers a temporary table called `temp`, then uses SQL to query for records in which the `type` field contains the letter `e`: `| df.createOrReplaceTempView(\"temp\")                                         |\n| -------------------------------------------------------------------------- |\n| some_fruit = spark.sql(\"SELECT type, qty FROM temp WHERE type LIKE '%e%'\") |\n| some_fruit.show()                                                          | ` In the `pyspark` shell, the operation prints the following output: ``` +------+----+ |  type| qty| +------+----+ | apple| 5.0| |orange|10.0| +------+----+ \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/python/read-from-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) You can create a Spark DataFrame to hold data from the MongoDB collection specified in the[spark.mongodb.read.connection.uri](https://mongodb.com/docs/spark-connector/current/getting-started/#std-label-pyspark-shell) option which your`SparkSession` option is using. Consider a collection named `fruit` that contains the following documents: `| { \"_id\" : 1, \"type\" : \"apple\", \"qty\" : 5 }   |\n| -------------------------------------------- |\n| { \"_id\" : 2, \"type\" : \"orange\", \"qty\" : 10 } |\n| { \"_id\" : 3, \"type\" : \"banana\", \"qty\" : 15 } | ` Assign the collection to a DataFrame with `spark.read()`from within the `pyspark` shell. ```python df = spark.read.format(\"mongodb\").load() \n``` Spark samples the records to infer the schema of the collection. ```python df.printSchema() \n``` The above operation produces the following shell output: ``` root  |-- _id: double (nullable = true)  |-- qty: double (nullable = true)  |-- type: string (nullable = true) \n``` If you need to read from a different MongoDB collection, use the `.option` method when reading data into a DataFrame. To read from a collection called `contacts` in a database called`people`, specify `people.contacts` in the input URI option. ```python df = spark.read.format(\"mongodb\").option(\"uri\", \"mongodb://127.0.0.1/people.contacts\").load() \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/configuration/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Configuration Options On this page * [Specify Configuration](#specify-configuration)\n* [Cache Configuration](#cache-configuration)\n* [ConfigExceptions](#configexceptions) Various configuration options are available for the MongoDB Spark Connector. To learn more about the options you can set, see[Write Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/write/#std-label-spark-write-conf) and [Read Configuration Options.](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-read-conf) ## Specify Configuration ### Using `SparkConf` You can specify configuration options with `SparkConf` using any of the following approaches: * The `--conf` flag at runtime. To learn more, see[Dynamically Loading Spark Properties](https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties) in the Spark documentation.\n* The `$SPARK_HOME/conf/spark-default.conf` file. The MongoDB Spark Connector will use the settings in `SparkConf` as defaults. ## Important When setting configurations with `SparkConf`, you must prefix the configuration options. Refer to [Write Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/write/#std-label-spark-write-conf) and[Read Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-read-conf) for the specific prefixes. ### Using an Options Map In the Spark API, the DataFrameReader and DataFrameWriter methods accept options in the form of a `Map[String, String]`. Options specified this way override any corresponding settings in `SparkConf`. #### Short-Form Syntax Options maps support short-form syntax. You may omit the prefix when specifying an option key string. ## Example The following syntaxes are equivalent to one another: * `dfw.option(\"spark.mongodb.write.collection\", \"myCollection\").save()`\n* `dfw.option(\"spark.mongodb.collection\", \"myCollection\").save()`\n* `dfw.option(\"collection\", \"myCollection\").save()` ### Using a System Property The connector provides a cache for `MongoClients` which can only be configured with a System Property. See [Cache Configuration.](#std-label-cache-configuration) ## Cache Configuration The MongoConnector includes a cache for MongoClients, so workers can share the MongoClient across threads. ## Important As the cache is setup before the Spark Configuration is available, the cache can only be configured with a System Property. | System Property name    | Description                                                                      |\n| ----------------------- | -------------------------------------------------------------------------------- |\n| mongodb.keep\\_alive\\_ms | The length of time to keep a MongoClient available for sharing.**Default:** 5000 | ## `ConfigException`s A configuration error throws a `ConfigException`. Confirm that any of the following methods of configuration that you use are configured properly: * [SparkConf](#std-label-spark-conf)\n* [Options maps](#std-label-options-map) ← [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Previous Section\")[Write Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/write/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/configuration/write/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Write Configuration Options On this page * [Write Configuration](#write-configuration)\n* [connection.uri Configuration Setting](#connection.uri-configuration-setting) ## Write Configuration The following options for writing to MongoDB are available: ## Note If you use `SparkConf` to set the connector's write configurations, prefix `spark.mongodb.write.` to each property.\n\n ## `connection.uri` Configuration Setting You can set all [Write Configuration](#std-label-spark-output-conf) via the write `connection.uri`. ## Note If you use `SparkConf` to set the connector's write configurations, prefix `spark.mongodb.write.` to the setting. ``` spark.mongodb.write.connection.uri=mongodb://127.0.0.1/test.myCollection \n``` The configuration corresponds to the following separate configuration settings: ``` spark.mongodb.write.connection.uri=mongodb://127.0.0.1/  spark.mongodb.write.database=test  spark.mongodb.write.collection=myCollection \n``` If you specify a setting both in the `connection.uri` and in a separate configuration, the `connection.uri` setting overrides the separate setting. For example, given the following configuration, the database for the connection is `foobar`: ``` spark.mongodb.write.connection.uri=mongodb://127.0.0.1/foobar  spark.mongodb.write.database=bar \n``` ← [Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/ \"Previous Section\")[Read Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/read/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/java/write-to-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) The following example creates a DataFrame from a `json` file and saves it to the MongoDB collection specified in `SparkConf`: `| Dataset<Row> df = spark.read().format(\"json\").load(\"example.json\"); |\n| ------------------------------------------------------------------- |\n| df.write().format(\"mongodb\").mode(\"overwrite\").save();              | ` The MongoDB Connector for Spark supports the following save modes: * `append`\n* `overwrite` To learn more about save modes, see the [Spark SQL Guide.](https://spark.apache.org/docs/3.2.0/sql-data-sources-load-save-functions.html#save-modes) [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/java/sql/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) Before running SQL queries on your dataset, you must register a temporary view for the dataset. The following operation registers a`characters` table and then queries it to find all characters that are 100 or older: `| implicitDS.createOrReplaceTempView(\"characters\");                                           |\n| ------------------------------------------------------------------------------------------- |\n| Dataset<Row> centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\"); |\n| centenarians.show();                                                                        | ` `centenarians.show()` outputs the following: ```shell +-------+----+ |   name| age| +-------+----+ |Gandalf|1000| | Thorin| 195| |  Balin| 178| | Dwalin| 169| |    Óin| 167| |  Glóin| 158| +-------+----+ \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/java/api/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) ## Important In version 10.0.0 and later of the Connector, use the format`mongodb` to read from and write to MongoDB: `df = spark.read.format(\"mongodb\").load()` # Dependency Management Provide the Spark Core, Spark SQL, and MongoDB Spark Connector dependencies to your dependency management tool. Beginning in version 3.2.0, Apache Spark supports both Scala 2.12 and 2.13\\. Spark 3.1.3 and previous versions support only Scala 2.12\\. To provide support for both Scala versions, version 10.1.1 of the Spark Connector produces two artifacts: * `org.mongodb.spark:mongo-spark-connector_2.12:10.1.1` is compiled against Scala 2.12, and supports Spark 3.1.x and above.\n* `org.mongodb.spark:mongo-spark-connector_2.13:10.1.1` is compiled against Scala 2.13, and supports Spark 3.2.x and above. ## Important Use the Spark Connector artifact that's compatible with your versions of Scala and Spark. The following excerpt from a Maven `pom.xml` file shows how to include dependencies compatible with Scala 2.12: `| <dependencies>                                      |\n| --------------------------------------------------- |\n| <dependency>                                        |\n| <groupId>org.mongodb.spark</groupId>                |\n| <artifactId>mongo-spark-connector_2.12</artifactId> |\n| <version>10.1.1</version>                           |\n| </dependency>                                       |\n| <dependency>                                        |\n| <groupId>org.apache.spark</groupId>                 |\n| <artifactId>spark-core_2.12</artifactId>            |\n| <version>3.3.1</version>                            |\n| </dependency>                                       |\n| <dependency>                                        |\n| <groupId>org.apache.spark</groupId>                 |\n| <artifactId>spark-sql_2.12</artifactId>             |\n| <version>3.3.1</version>                            |\n| </dependency>                                       |\n| </dependencies>                                     | ` # Configuration\n\n ```java package com.mongodb.spark_examples; import org.apache.spark.sql.SparkSession; public final class GettingStarted {   public static void main(final String[] args) throws InterruptedException {     /* Create the SparkSession.      * If config arguments are passed from the command line using --conf,      * parse args for the values to set.      */     SparkSession spark = SparkSession.builder()       .master(\"local\")       .appName(\"MongoSparkConnectorIntro\")       .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1/test.myCollection\")       .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1/test.myCollection\")       .getOrCreate();     // Application logic   } } \n``` * The [spark.mongodb.read.connection.uri](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-input-conf) specifies the MongoDB server address(`127.0.0.1`), the database to connect (`test`), and the collection (`myCollection`) from which to read data, and the read preference.\n* The [spark.mongodb.write.connection.uri](https://mongodb.com/docs/spark-connector/current/configuration/write/#std-label-spark-output-conf) specifies the MongoDB server address(`127.0.0.1`), the database to connect (`test`), and the collection (`myCollection`) to which to write data. You can use a `SparkSession` object to write data to MongoDB, read data from MongoDB, create Datasets, and perform SQL operations. [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/scala/write-to-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) The following example creates a DataFrame from a `json` file and saves it to the MongoDB collection specified in `SparkConf`: `| val df = spark.read.format(\"json\").load(\"example.json\") |\n| ------------------------------------------------------- |\n| df.write.format(\"mongodb\").mode(\"overwrite\").save()     | ` The MongoDB Connector for Spark supports the following save modes: * `append`\n* `overwrite` To learn more about save modes, see the [Spark SQL Guide.](https://spark.apache.org/docs/3.2.0/sql-data-sources-load-save-functions.html#save-modes) [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/java/read-from-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) Use your local SparkSession's `read` method to create a DataFrame representing a collection. ## Note `DataFrame` does not exist as a class in the Java API. Use`Dataset<Row>` to reference a DataFrame. The following example loads the collection specified in the`SparkConf`: `| Dataset<Row> df = spark.read().format(\"mongodb\").load(); // Uses the SparkConf for configuration |\n| ------------------------------------------------------------------------------------------------ | ` To specify a different collection, database, and other [read configuration settings](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-input-conf), use the `option` method: ```java Dataset<Row> df = spark.read().format(\"mongodb\").option(\"database\", \"<example-database>\").option(\"collection\", \"<example-collection>\").load(); \n``` # Schema Inference When you load a Dataset or DataFrame without a schema, Spark samples the records to infer the schema of the collection. Consider a collection named `characters`: ```javascript { \"_id\" : ObjectId(\"585024d558bef808ed84fc3e\"), \"name\" : \"Bilbo Baggins\", \"age\" : 50 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc3f\"), \"name\" : \"Gandalf\", \"age\" : 1000 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc40\"), \"name\" : \"Thorin\", \"age\" : 195 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc41\"), \"name\" : \"Balin\", \"age\" : 178 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc42\"), \"name\" : \"Kíli\", \"age\" : 77 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc43\"), \"name\" : \"Dwalin\", \"age\" : 169 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc44\"), \"name\" : \"Óin\", \"age\" : 167 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc45\"), \"name\" : \"Glóin\", \"age\" : 158 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc46\"), \"name\" : \"Fíli\", \"age\" : 82 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc47\"), \"name\" : \"Bombur\" } \n``` The following operation loads data from the MongoDB collection specified in `SparkConf` and infers the schema: ```java Dataset<Row> implicitDS = spark.read().format(\"mongodb\").load(); implicitDS.printSchema(); implicitDS.show(); \n``` `implicitDS.printSchema()` outputs the following schema to the console: ```shell root  |-- _id: struct (nullable = true)  |    |-- oid: string (nullable = true)  |-- age: integer (nullable = true)  |-- name: string (nullable = true) \n``` `implicitDS.show()` outputs the following to the console: ```shell +--------------------+----+-------------+ |                 _id| age|         name| +--------------------+----+-------------+ |[585024d558bef808...|  50|Bilbo Baggins| |[585024d558bef808...|1000|      Gandalf| |[585024d558bef808...| 195|       Thorin| |[585024d558bef808...| 178|        Balin| |[585024d558bef808...|  77|         Kíli| |[585024d558bef808...| 169|       Dwalin|\n\n |[585024d558bef808...| 158|        Glóin| |[585024d558bef808...|  82|         Fíli| |[585024d558bef808...|null|       Bombur| +--------------------+----+-------------+ \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/scala/api/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) ## Important In version 10.0.0 and later of the Connector, use the format`mongodb` to read from and write to MongoDB: `df = spark.read.format(\"mongodb\").load()` # Spark Shell When starting the Spark shell, specify: * the `--packages` option to download the MongoDB Spark Connector package. The following package is available:  \n   * `mongo-spark-connector`\n* the `--conf` option to configure the MongoDB Spark Connnector. These settings configure the `SparkConf` object.  \n## Note  \nWhen specifying the Connector configuration via `SparkConf`, you must prefix the settings appropriately. For details and other available MongoDB Spark Connector options, see the[Configuration Options.](https://mongodb.com/docs/spark-connector/current/configuration/) For example, `| ./bin/spark-shell --conf \"spark.mongodb.read.connection.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\ |\n| ------------------------------------------------------------------------------------------------------------------------------------ |\n| --conf \"spark.mongodb.write.connection.uri=mongodb://127.0.0.1/test.myCollection\" \\                                                  |\n| --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1                                                                       | ` * The [spark.mongodb.read.connection.uri](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-input-conf) specifies the MongoDB server address (`127.0.0.1`), the database to connect (`test`), and the collection (`myCollection`) from which to read data, and the read preference.\n* The [spark.mongodb.write.connection.uri](https://mongodb.com/docs/spark-connector/current/configuration/write/#std-label-spark-output-conf) specifies the MongoDB server address (`127.0.0.1`), the database to connect (`test`), and the collection (`myCollection`) to which to write data. Connects to port `27017` by default.\n* The `packages` option specifies the Spark Connector's Maven coordinates, in the format `groupId:artifactId:version`. ## Import the MongoDB Connector Package Enable MongoDB Connector specific functions and implicits for your`SparkSession` and Datasets by importing the following package in the Spark shell: ``` import com.mongodb.spark._ \n``` ## Connect to MongoDB Connection to MongoDB happens automatically when a Dataset action requires a [read](https://mongodb.com/docs/spark-connector/current/read-from-mongodb/#std-label-scala-read) from MongoDB or a[write](https://mongodb.com/docs/spark-connector/current/write-to-mongodb/#std-label-scala-write) to MongoDB. # Self-Contained Scala Application ## Dependency Management Provide the Spark Core, Spark SQL, and MongoDB Spark Connector dependencies to your dependency management tool. The following excerpt demonstrates how to include these dependencies in a [SBT](http://www.scala-sbt.org/documentation.html) `build.scala` file: ``` scalaVersion := \"2.12\", libraryDependencies ++= Seq(   \"org.mongodb.spark\" %% \"mongo-spark-connector_2.12\" % \"10.1.1\",   \"org.apache.spark\" %% \"spark-core\" % \"3.3.1\",\n\n ) \n``` ## Configuration When specifying the Connector configuration via `SparkSession`, you must prefix the settings appropriately. For details and other available MongoDB Spark Connector options, see the[Configuration Options.](https://mongodb.com/docs/spark-connector/current/configuration/) ``` package com.mongodb object GettingStarted {   def main(args: Array[String]): Unit = {     /* Create the SparkSession.      * If config arguments are passed from the command line using --conf,      * parse args for the values to set.      */     import org.apache.spark.sql.SparkSession     val spark = SparkSession.builder()       .master(\"local\")       .appName(\"MongoSparkConnectorIntro\")       .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1/test.myCollection\")       .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1/test.myCollection\")       .getOrCreate()   } } \n``` # Troubleshooting If you get a `java.net.BindException: Can't assign requested address`, * Check to ensure that you do not have another Spark shell already running.\n* Try setting the `SPARK_LOCAL_IP` environment variable; e.g.  \n```shell  \nexport SPARK_LOCAL_IP=127.0.0.1  \n```\n* Try including the following option when starting the Spark shell:  \n```shell  \n--driver-java-options \"-Djava.net.preferIPv4Stack=true\"  \n``` If you have errors running the examples in this tutorial, you may need to clear your local ivy cache (`~/.ivy2/cache/org.mongodb.spark` and`~/.ivy2/jars`). [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/structured-streaming/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Structured Streaming with MongoDB On this page * [Overview](#overview)\n* [Configuring a Write Stream to MongoDB](#configuring-a-write-stream-to-mongodb)\n* [Configuring a Read Stream from MongoDB](#configuring-a-read-stream-from-mongodb)\n* [Examples](#examples)\n* [Stream to MongoDB from a CSV File](#stream-to-mongodb-from-a-csv-file)\n* [Stream to your Console from MongoDB](#stream-to-your-console-from-mongodb) ## Overview Spark Structured Streaming is a data stream processing engine you can use through the Dataset or DataFrame API. The MongoDB Spark Connector enables you to stream to and from MongoDB using Spark Structured Streaming. ## Important [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) and [Spark Streaming with DStreams](https://spark.apache.org/docs/latest/streaming-programming-guide.html) are different. To learn more about Structured Streaming, see the[Spark Programming Guide.](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) ## Configuring a Write Stream to MongoDB ## Configuring a Read Stream from MongoDB When reading a stream from a MongoDB database, the MongoDB Spark Connector supports both_micro-batch processing_ and_continuous processing_. Micro-batch processing is the default processing engine, while continuous processing is an experimental feature introduced in Spark version 2.3\\. To learn more about continuous processing, see the [Spark documentation.](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing) ## Note The connector reads from your MongoDB deployment's change stream. To generate change events on the change stream, perform update operations on your database. To learn more about change streams, see[Change Streams](https://www.mongodb.com/docs/manual/changeStreams/) in the MongoDB manual. ## Examples The following examples show Spark Structured Streaming configurations for streaming to and from MongoDB. ### Stream to MongoDB from a CSV File To stream data from a CSV file to MongoDB: ### Stream to your Console from MongoDB To stream data from MongoDB to your console: ← [Read from MongoDB](https://mongodb.com/docs/spark-connector/current/read-from-mongodb/ \"Previous Section\")[FAQ](https://mongodb.com/docs/spark-connector/current/faq/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/scala/filters/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Filters When using filters with DataFrames or Datasets, the underlying MongoDB Connector code constructs an [aggregation pipeline](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/) to filter the data in MongoDB before sending it to Spark. This improves Spark performance by retrieving and processing only the data you need. MongoDB Spark Connector turns the following filters into aggregation pipeline stages: * And\n* EqualNullSafe\n* EqualTo\n* GreaterThan\n* GreaterThanOrEqual\n* In\n* IsNull\n* LessThan\n* LessThanOrEqual\n* Not\n* Or\n* StringContains\n* StringEndsWith\n* StringStartsWith The following example filters and output the characters with ages under 100: `| df.filter(df(\"age\") < 100).show() |\n| --------------------------------- | ` The operation outputs the following: ``` +--------------------+---+-------------+ |                 _id|age|         name| +--------------------+---+-------------+ |[5755d7b4566878c9...| 50|Bilbo Baggins| |[5755d7b4566878c9...| 82|         Fíli| |[5755d7b4566878c9...| 77|         Kíli| +--------------------+---+-------------+ \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/scala/sql/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) Before running SQL queries on your dataset, you must register a temporary view for the dataset. The following operation registers a`characters` table and then queries it to find all characters that are 100 or older: `| val characters = spark.read.format(\"mongodb\").as[Character]                       |\n| --------------------------------------------------------------------------------- |\n| characters.createOrReplaceTempView(\"characters\")                                  |\n| val centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\") |\n| centenarians.show()                                                               | ` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/scala/streaming/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) ## Important [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) and [Spark Streaming with DStreams](https://spark.apache.org/docs/latest/streaming-programming-guide.html) are different. Spark Streaming allows on-the-fly analysis of live data streams with MongoDB. See the [Apache documentation](http://spark.apache.org/docs/latest/streaming-programming-guide.html)for a detailed description of Spark Streaming functionality. This tutorial uses the Spark Shell. For more information about starting the Spark Shell and configuring it for use with MongoDB, see[Getting Started.](https://mongodb.com/docs/spark-connector/current/getting-started/#std-label-scala-getting-started) This tutorial demonstrates how to use Spark Streaming to analyze input data from a TCP port. It uses Netcat, a lightweight network utility, to send text inputs to a local port, then uses Scala to determine how many times each word occurs in each line and write the results to a MongoDB collection. Start Netcat from the command line: `| $ nc -lk 9999 |\n| ------------- | ` Start the Spark Shell at another terminal prompt. ``` import com.mongodb.spark.sql._ import org.apache.spark.streaming._ \n``` Create a new `StreamingContext` object and assign it to `ssc`.`sc` is a SparkContext object that is automatically created when you start the Spark Shell. The second argument specifies how often to check for new input data. ``` val ssc = new StreamingContext(sc, Seconds(1)) \n``` Use the `socketTextStream` method to create a connection to Netcat on port 9999: ``` val lines = ssc.socketTextStream(\"localhost\", 9999) \n``` Determine how many times each word occurs in each line: ``` val words = lines.flatMap(_.split(\" \")) val pairs = words.map(word => (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) \n``` Create a data structure to hold the results: ``` case class WordCount(word: String, count: Int) \n``` Use a `foreachRDD` loop to collect results and write to the MongoDB collection specified in the Spark Connector[configuration](https://mongodb.com/docs/spark-connector/current/getting-started/#std-label-scala-getting-started). The `append`mode causes data to be appended to the collection, whereas`overwrite` mode replaces the existing data. ``` wordCounts.foreachRDD({ rdd =>   import spark.implicits._   val wordCounts = rdd.map({ case (word: String, count: Int)           => WordCount(word, count) }).toDF()   wordCounts.write.mode(\"append\").mongo() }) \n``` Start listening: ``` ssc.start() \n``` To give your program something to listen to, go back to the terminal prompt where you started Netcat and start typing. ```shell hello world cats cats dogs dogs dogs \n``` In your MongoDB collection you'll find something similar to the following: ```javascript { \"_id\" : ObjectId(\"588a539927c22bd43214131f\"), \"word\" : \"hello\", \"count\" : 1 } { \"_id\" : ObjectId(\"588a539927c22bd432141320\"), \"word\" : \"world\", \"count\" : 1 } { \"_id\" : ObjectId(\"588a53b227c22bd432141322\"), \"word\" : \"cats\", \"count\" : 2 } { \"_id\" : ObjectId(\"588a53b227c22bd432141323\"), \"word\" : \"dogs\", \"count\" : 3 } \n``` To end your Netcat process, use `ctrl-c`. To end your Spark Shell session, use `System.exit(0)`. [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/scala/read-from-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) Use your local SparkSession's `read` method to create a DataFrame representing a collection. ## Note A `DataFrame` is represented by a `Dataset` of`Rows`. It is an alias of `Dataset[Row]`. The following example loads the collection specified in the`SparkConf`: `| val df = spark.read.format(\"mongodb\").load() // Uses the SparkConf for configuration |\n| ------------------------------------------------------------------------------------ | ` To specify a different collection, database, and other [read configuration settings](https://mongodb.com/docs/spark-connector/current/configuration/read/#std-label-spark-input-conf), use the `option` method: ``` val df = spark.read.format(\"mongodb\").option(\"database\", \"<example-database>\").option(\"collection\", \"<example-collection>\").load() \n``` # Schema Inference When you load a Dataset or DataFrame without a schema, Spark samples the records to infer the schema of the collection. Consider a collection named `characters`: ```javascript { \"_id\" : ObjectId(\"585024d558bef808ed84fc3e\"), \"name\" : \"Bilbo Baggins\", \"age\" : 50 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc3f\"), \"name\" : \"Gandalf\", \"age\" : 1000 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc40\"), \"name\" : \"Thorin\", \"age\" : 195 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc41\"), \"name\" : \"Balin\", \"age\" : 178 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc42\"), \"name\" : \"Kíli\", \"age\" : 77 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc43\"), \"name\" : \"Dwalin\", \"age\" : 169 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc44\"), \"name\" : \"Óin\", \"age\" : 167 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc45\"), \"name\" : \"Glóin\", \"age\" : 158 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc46\"), \"name\" : \"Fíli\", \"age\" : 82 } { \"_id\" : ObjectId(\"585024d558bef808ed84fc47\"), \"name\" : \"Bombur\" } \n``` The following operation loads data from the MongoDB collection specified in `SparkConf` and infers the schema: ``` val df = MongoSpark.load(spark)  // Uses the SparkSession df.printSchema()                 // Prints DataFrame schema \n``` `df.printSchema()` outputs the following schema to the console: ```shell root  |-- _id: struct (nullable = true)  |    |-- oid: string (nullable = true)  |-- age: integer (nullable = true)  |-- name: string (nullable = true) \n``` [MongoDB Connector for Spark](https://mongodb.com/docs/spark-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/spark-connector/current/configuration/read/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Spark Connector](https://mongodb.com/docs/spark-connector/current/) # Read Configuration Options On this page * [Read Configuration](#read-configuration)\n* [Change Streams](#change-streams)\n* [connection.uri Configuration Setting](#connection.uri-configuration-setting) ## Read Configuration You can configure the following properties to read from MongoDB: ## Note If you use `SparkConf` to set the connector's read configurations, prefix `spark.mongodb.read.` to each property.\n\n ### Partitioner Configurations This section contains configuration information for the following partitioners: * [SamplePartitioner](#std-label-conf-samplepartitioner)\n* [ShardedPartitioner](#std-label-conf-shardedpartitioner)\n* [PaginateBySizePartitioner](#std-label-conf-paginatebysizepartitioner)\n* [PaginateIntoPartitionsPartitioner](#std-label-conf-paginateintopartitionspartitioner)\n* [SinglePartitionPartitioner](#std-label-conf-singlepartitionpartitioner) #### `SamplePartitioner` Configuration ## Note If you use `SparkConf` to set the connector's read configurations, prefix each property with `spark.mongodb.read.partitioner.options.` instead of`partitioner.options.`. You must specify this partitioner using the full classname:`com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner`. | Property name                             | Description                                                                                                                                                           |\n| ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| partitioner.options.partition.field       | The field to use for partitioning, which must be a unique field.**Default:** \\_id                                                                                     |\n| partitioner.options.partition.size        | The size (in MB) for each partition. Smaller partition sizes create more partitions containing fewer documents.**Default:** 64                                        |\n| partitioner.options.samples.per.partition | The number of samples to take per partition. The total number of samples taken is:samples per partiion \\* ( count / number of documents per partition)**Default:** 10 | ## Example For a collection with 640 documents with an average document size of 0.5 MB, the default SamplePartitioner configuration values creates 5 partitions with 128 documents per partition. The MongoDB Spark Connector samples 50 documents (the default 10 per intended partition) and defines 5 partitions by selecting partition field ranges from the sampled documents. #### `ShardedPartitioner` Configuration The `ShardedPartitioner` automatically determines the partitions to use based on your shard configuration. You must specify this partitioner using the full classname:`com.mongodb.spark.sql.connector.read.partitioner.ShardedPartitioner`. ## Warning This partitioner is not compatible with hashed shard keys. #### `PaginateBySizePartitioner` Configuration ## Note If you use `SparkConf` to set the connector's read configurations, prefix each property with `spark.mongodb.read.partitioner.options.` instead of`partitioner.options.`. You must specify this partitioner using the full classname:`com.mongodb.spark.sql.connector.read.partitioner.PaginateBySizePartitioner`.\n\n #### `PaginateIntoPartitionsPartitioner` Configuration ## Note If you use `SparkConf` to set the connector's read configurations, prefix each property with `spark.mongodb.read.partitioner.options.` instead of`partitioner.options.`. You must specify this partitioner using the full classname:`com.mongodb.spark.sql.connector.read.partitioner.PaginateIntoPartitionsPartitioner`. | Property name                             | Description                                                                       |\n| ----------------------------------------- | --------------------------------------------------------------------------------- |\n| partitioner.options.partition.field       | The field to use for partitioning, which must be a unique field.**Default:** \\_id |\n| partitioner.options.maxNumberOfPartitions | The number of partitions to create.**Default:** 64                                | #### `SinglePartitionPartitioner` Configuration ## Note If you use `SparkConf` to set the connector's read configurations, prefix each property with `spark.mongodb.read.partitioner.options.` instead of`partitioner.options.`. You must specify this partitioner using the full classname:`com.mongodb.spark.sql.connector.read.partitioner.SinglePartitionPartitioner`. This partitioner creates a single partition. ## Change Streams ## Note If you use `SparkConf` to set the connector's change stream configurations, prefix `spark.mongodb.change.stream.` to each property.\n\n ## `connection.uri` Configuration Setting You can set all [Read Configuration](#std-label-spark-input-conf) via the read `connection.uri` setting. For example, consider the following example which sets the read`connection.uri` setting: ## Note If you use `SparkConf` to set the connector's read configurations, prefix `spark.mongodb.read.` to the setting. ``` spark.mongodb.read.connection.uri=mongodb://127.0.0.1/databaseName.collectionName?readPreference=primaryPreferred \n``` The configuration corresponds to the following separate configuration settings: ``` spark.mongodb.read.connection.uri=mongodb://127.0.0.1/ spark.mongodb.read.database=databaseName spark.mongodb.read.collection=collectionName spark.mongodb.read.readPreference.name=primaryPreferred \n``` If you specify a setting both in the `connection.uri` and in a separate configuration, the `connection.uri` setting overrides the separate setting. For example, given the following configuration, the database for the connection is `foobar`: ``` spark.mongodb.read.connection.uri=mongodb://127.0.0.1/foobar spark.mongodb.read.database=bar \n``` ← [Write Configuration Options](https://mongodb.com/docs/spark-connector/current/configuration/write/ \"Previous Section\")[Getting Started](https://mongodb.com/docs/spark-connector/current/getting-started/ \"Next Section\") →"
}