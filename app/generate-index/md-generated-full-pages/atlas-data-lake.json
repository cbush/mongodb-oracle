{
  "https://www.mongodb.com/docs/datalake/limitations/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Data Lake Limitations **Looking for documentation for what used to be called \"Atlas Data Lake\"?** Atlas Data Lake is now called Atlas Data Federation. To learn more about the renamed federated query engine service, see [Atlas Data Federation](https://www.mongodb.com/docs/atlas/data-federation/). * Atlas Data Lake doesn't support `M0`, `M2`, or `M5` clusters. It supports `M10` or higher clusters only.\n* Atlas Data Lake doesn't support sharded clusters.\n* Atlas Data Lake provides optimized storage in the following AWS regions only:  \n| Data Lake Regions  | AWS Regions    |  \n| ------------------ | -------------- |  \n| Virginia, USA      | us-east-1      |  \n| Oregon, USA        | us-west-2      |  \n| Sao Paulo, Brazil  | sa-east-1      |  \n| Ireland            | eu-west-1      |  \n| London, England    | eu-west-2      |  \n| Frankfurt, Germany | eu-central-1   |  \n| Mumbai, India      | ap-south-1     |  \n| Singapore          | ap-southeast-1 |  \n| Sydney, Australia  | ap-southeast-2 | ← [Delete Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/administration/delete-datalake-pipeline/ \"Previous Section\")[Release Notes](https://mongodb.com/docs/datalake/release-notes/data-lake/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/get-started/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Get Started On this page * [Overview](#overview) **Looking for documentation for what used to be called \"Atlas Data Lake\"?** Atlas Data Lake is now called Atlas Data Federation. To learn more about the renamed federated query engine service, see [Atlas Data Federation](https://www.mongodb.com/docs/atlas/data-federation/). ## Overview Atlas Data Lake is MongoDB's solution for storing and querying historical data from your Atlas cluster in a performant analytic storage service. The following pages guide you through the process of creating and connecting to a sample Data Lake in Atlas and running MongoDB Query Language (MQL) operations against the data in your Data Lake. * **Step 1:** [Create an Atlas Data Lake Pipeline.](https://mongodb.com/docs/datalake/tutorial/add-dataset-pipeline/#std-label-adl-add-pipeline)\n* **Step 2:** [Set Up a Federated Database Instance for Your Dataset.](https://mongodb.com/docs/datalake/tutorial/set-up-federated-database/#std-label-adl-create-federated-db)\n* **Step 3:** [Connect to Your Federated Database Instance.](https://mongodb.com/docs/datalake/tutorial/adl-connect-federated-db-instance/#std-label-adl-connect-federated-db-instance)\n* **Step 4:** [Run Queries Against Your Data Lake Dataset.](https://mongodb.com/docs/datalake/tutorial/adl-run-sample-queries/#std-label-adl-run-queries)",
  "https://www.mongodb.com/docs/datalake/manage-adl-dataset-pipeline/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Manage Atlas Data Lake Pipeline You can perform the following actions on your Data Lake pipeline: * View and edit your Data Lake pipelines.\n* Manually trigger ingestion of data from your snapshot.\n* Pause and resume ingestion of snapshot data from the Atlascluster to Atlas Data Lake datasets. To learn more, see[Pause Data Ingestion for Your Data Lake Pipeline.](https://mongodb.com/docs/datalake/administration/pause-resume-data-extraction/#std-label-pause-resume-data-ingestion)\n* Delete your Data Lake pipeline at any time. To learn more, see[Delete Atlas Data Lake Pipeline.](https://mongodb.com/docs/datalake/administration/delete-datalake-pipeline/#std-label-delete-pipeline) ← [Run Queries Against Your Data Lake Dataset](https://mongodb.com/docs/datalake/tutorial/adl-run-sample-queries/ \"Previous Section\")[View Atlas Data Lake Pipelines](https://mongodb.com/docs/datalake/administration/view-datalake-pipelines/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Atlas Data Lake On this page * [About Atlas Data Lake](#about-atlas-data-lake)\n* [Sample Uses](#sample-uses)\n* [Atlas Data Lake Regions](#atlas-data-lake-regions)\n* [Billing](#billing) **Looking for documentation for what used to be called \"Atlas Data Lake\"?** Atlas Data Lake is now called Atlas Data Federation. To learn more about the renamed federated query engine service, see [Atlas Data Federation](https://www.mongodb.com/docs/atlas/data-federation/). ## About Atlas Data Lake MongoDB Atlas Data Lake is now an analytic-optimized object storage service for extracted data. Atlas Data Lake provides an analytic storage service optimized for flat or nested data with low latency query performance. ### Prerequisites Atlas Data Lake requires an `M10` or higher backup-enabled Atlas cluster with cloud backup jobs running on a specified cadence. To learn more about cloud backups, see [Back Up Your Database Deployment.](https://www.mongodb.com/docs/atlas/backup/cloud-backup/overview/) ### Supported Types of Data Source Atlas Data Lake supports collection snapshots from Atlas clusters as a data source for extracted data. Atlas Data Lake automatically ingests data from the snapshots, and partitions and stores data in an analytics-optimized format. ## Sample Uses You can use Atlas Data Lake to: * Isolate analytical workloads from your operational cluster.\n* Provide a consistent view of cluster data from a snapshot for long running aggregations using `$out`.\n* Query and compare across versions of your cluster data at different points in time. ## Atlas Data Lake Regions Atlas Data Lake provides optimized storage in the following AWS regions: | Data Lake Regions  | AWS Regions    |\n| ------------------ | -------------- |\n| Virginia, USA      | us-east-1      |\n| Oregon, USA        | us-west-2      |\n| Sao Paulo, Brazil  | sa-east-1      |\n| Ireland            | eu-west-1      |\n| London, England    | eu-west-2      |\n| Frankfurt, Germany | eu-central-1   |\n| Mumbai, India      | ap-south-1     |\n| Singapore          | ap-southeast-1 |\n| Sydney, Australia  | ap-southeast-2 | Atlas Data Lake automatically selects the region closest to your Atlascluster for storing ingested data. ## Billing You incur Atlas Data Lake charges per GB per month based on the AWS region where the ingested data is stored. You incur Atlas Data Lake costs for the following items: * Ingestion of data from your data source\n* Storage on the cloud object storage ### Extraction Costs Atlas Data Lake charges you for the resources utilized to extract, upload, and transfer data. Atlas Data Lake charges for the [snapshot export](https://www.mongodb.com/docs/atlas/backup/cloud-backup/export/) operations is based on the following: * Cost per GB for snapshot extraction\n* Cost per hour on the AWS server for snapshot export download\n* Cost per GB per hour for snapshot export restore storage\n* Cost per IOPS per hour for snapshot export storage IOPS ### Storage Costs Atlas Data Lake charges for storing and accessing stored data is based on the following: * Cost per GB per day\n* Cost for every one thousand storage access requests when querying Data Lake datasets using Atlas Data Federation. Each access request corresponds to a partition of data from a Data Lake dataset that Atlas Data Federation fetches to process for a query.  \n## Note  \nYou can now set limits on the amount of data that Atlas Data Federation processes for your queries to control costs. To learn more, see[Manage Atlas Data Federation Query Limits.](https://www.mongodb.com/docs/atlas/data-federation/query/manage-query-limits/) To learn more, see the Atlas pricing page. [Get Started](https://mongodb.com/docs/datalake/get-started/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/tutorial/adl-run-sample-queries/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Run Queries Against Your Data Lake Dataset On this page * [Prerequisites](#prerequisites)\n* [Procedure](#procedure) This page guides you through the steps for running queries in[mongosh](https://www.mongodb.com/docs/mongodb-shell/#mongodb-binary-bin.mongosh) against Data Lake dataset for the `sample_mflix.movies`collection. ## Prerequisites Before you run the queries, you must complete the following using the examples shown in the procedures: * [Create an Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/tutorial/add-dataset-pipeline/#std-label-adl-add-pipeline) for the `sample_mflix.movies`collection\n* (For On Demand schedule only) Manually trigger[Ingestion of data](https://mongodb.com/docs/datalake/administration/ingest-data-on-demand/#std-label-ingest-on-demand) from your snapshot\n* [Set Up a Federated Database Instance for Your Dataset](https://mongodb.com/docs/datalake/tutorial/set-up-federated-database/#std-label-adl-create-federated-db) for the Data Lake dataset that is a snapshot of data in the `sample_mflix.movies` collection\n* [Connect to Your Federated Database Instance](https://mongodb.com/docs/datalake/tutorial/adl-connect-federated-db-instance/#std-label-adl-connect-federated-db-instance) to run the queries ## Procedure 1 ### Switch to `Database0` database: `| use Database0 |\n| ------------- | ` 2 ### Copy, paste, and run the following queries in your terminal. The following queries use the `sample_mflix.movies` collection for which you [created](https://mongodb.com/docs/datalake/tutorial/add-dataset-pipeline/#std-label-adl-add-pipeline) the pipeline. Find movie with the title `The Frozen Ground` released between 2010 and 2015: ```shell db.Collection0.find({ \"year\": {$gt: 2010, $lt: 2015}, \"title\": \"The Frozen Ground\" }, {\"title\": 1, \"year\": 1, \"genres\": 1 }) \n``` Find all movies whose title includes the word `Ground` and limit the number of documents returned to `10`. ```shell db.Collection0.find({ \"year\": {$gt: 2010, $lt: 2015}, title: /Ground/ }, {\"title\": 1, \"year\": 1, \"genres\": 1 }).limit(10) \n``` ← [Connect to Your Federated Database Instance](https://mongodb.com/docs/datalake/tutorial/adl-connect-federated-db-instance/ \"Previous Section\")[Manage Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/manage-adl-dataset-pipeline/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/tutorial/adl-connect-federated-db-instance/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Connect to Your Federated Database Instance On this page * [Prerequisites](#prerequisites)\n* [Procedure](#procedure)\n* [Next Steps](#next-steps) This page guides you through the steps for connecting to the federated database instance to query your Data Lake dataset. ## Prerequisites Before you begin, you must have the following: * An IP address added to the access list of the project that contains the cluster. To learn more, see [Add Your Connection IP Address to Your IP Access List.](https://www.mongodb.com/docs/atlas/security/add-ip-address-to-list/#std-label-gswa-access-list)\n* A database user for your cluster in the project that contains the Federated Database Instance that you want to connect to. To learn more, see [Create a Database User for Your Cluster.](https://www.mongodb.com/docs/atlas/tutorial/create-mongodb-user-for-cluster/#std-label-gswa-user)\n* A Federated Database Instance on the Atlas cluster mapped to your Atlas Data Lake dataset. To learn more, see [Set Up a Federated Database Instance for Your Dataset.](https://mongodb.com/docs/datalake/tutorial/set-up-federated-database/#std-label-adl-create-federated-db) ## Procedure 1 ### Navigate to Atlas Data Federation in the Atlas UI. To navigate to the Atlas Data Federation page: 1. Log in to [MongoDB Atlas.](https://cloud.mongodb.com)\n2. Select Data Federation under Data Services on the left-hand navigation. 2 ### Click Connect for the Federated Database Instance that you wish to connect to. ## Example If you are connecting to the Federated Database Instance for querying the Atlas Data Lake dataset that you created using the examples, click Connect for DataLake0. 3 ### Get the connection string for your preferred method of connecting. You can connect using [mongosh](https://www.mongodb.com/docs/mongodb-shell/#mongodb-binary-bin.mongosh), a [MongoDB Driver](https://www.mongodb.com/docs/drivers/), and [MongoDB Compass](https://www.mongodb.com/docs/compass/current/). To learn how to get the connection string for your client, click the tab below for your connection method. If you are following the examples in the steps for deploying an Atlas Data Lake for the `sample_mflix.movies` collection and setting up a Federated Database Instance for this Data Lake dataset, click the MongoDB Shell tab below to connect using [mongosh](https://www.mongodb.com/docs/mongodb-shell/#mongodb-binary-bin.mongosh). 4 ### Connect your client to your Federated Database Instance using the connection string. ## Next Steps Now that you are connected to your federated database instance, proceed to [Run Queries Against Your Data Lake Dataset.](https://mongodb.com/docs/datalake/tutorial/adl-run-sample-queries/#std-label-adl-run-queries) ← [Set Up a Federated Database Instance for Your Dataset](https://mongodb.com/docs/datalake/tutorial/set-up-federated-database/ \"Previous Section\")[Run Queries Against Your Data Lake Dataset](https://mongodb.com/docs/datalake/tutorial/adl-run-sample-queries/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/tutorial/add-dataset-pipeline/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Create an Atlas Data Lake Pipeline On this page * [Prerequisites](#prerequisites)\n* [Create a Pipeline from the Atlas UI](#create-a-pipeline-from-the-service-ui)\n* [Create a Pipeline from the API](#create-a-pipeline-from-the-api)\n* [Next steps](#next-steps) **Looking for documentation for what used to be called \"Atlas Data Lake\"?** Atlas Data Lake is now called Atlas Data Federation. To learn more about the renamed federated query engine service, see [Atlas Data Federation](https://www.mongodb.com/docs/atlas/data-federation/). You can create Atlas Data Lake pipelines using the Atlas UI and Data Lake Pipelines API. This page guides you through the steps for creating an Atlas Data Lake pipeline. ## Prerequisites Before you begin, you must have the following: * [Backup-enabled](https://www.mongodb.com/docs/atlas/backup/cloud-backup/overview/#std-label-backup-cloud-provider) `M10` or higherAtlas cluster.\n* [Project Owner](https://www.mongodb.com/docs/atlas/reference/user-roles/#mongodb-authrole-Project-Owner) role for the project for which you want to deploy a Data Lake.\n* [Sample data](https://www.mongodb.com/docs/atlas/sample-data/#std-label-sample-data) loaded on your cluster (if you wish to try the example in the following[Create a Pipeline from the Atlas UI](#std-label-adl-add-pipeline-steps)). ## Create a Pipeline from the Atlas UI 1 ### Navigate to Atlas Data Lake in the Atlas UI. To navigate to the Atlas Data Lake page: 1. Log in to [MongoDB Atlas.](https://cloud.mongodb.com)\n2. Select Data Lake under Deployment on the left-hand navigation panel. 2 ### Click Add Data Lake Pipeline. 3 ### Define the data source for the pipeline. You can create a copy of data on your Atlas cluster in MongoDB-managed cloud object storage optimized for analytic queries with workload isolation. To set up a pipeline, specify the following in theSetup Pipeline page: 1. Select the Atlas cluster from the dropdown.  \n## Example  \nIf you loaded the sample data on your cluster, select theAtlas cluster where you loaded the sample data.\n2. Select the database on the specified cluster from the dropdown.  \n## Example  \nIf you selected the cluster where the sample data is loaded, select `sample_mflix`.\n3. Select the collection in the specified database from the dropdown.  \n## Example  \nIf you selected the `sample_mflix` database, select the`movies` collection in the `sample_mflix` database.\n4. Enter a name for the pipeline.  \n## Example  \nIf you are following the examples in this tutorial, enter`sample_mflix.movies` in the Pipeline Namefield.\n5. Click Continue. 4 ### Specify an ingestion schedule for your cluster data. You can specify how frequently your cluster data is extracted from your Atlas Backup Snapshots and ingested into Data Lake Datasets. Each snapshot represents your data at that point in time, which is stored in a workload isolated, analytic storage. You can query any snapshot data in the Data Lake datasets. You can choose Basic Schedule or On Demand. 5 ### Select the AWS region for storing your extracted data. Atlas Data Lake provides optimized storage in the following AWS regions:\n\n By default, Atlas Data Lake automatically selects the region closest to your Atlas cluster for storing extracted data. 6 ### Specify fields in your collection to create partitions. Enter the most commonly queried fields from the collection in thePartition Attributes section. To specify nested fields, use the [dot notation](https://www.mongodb.com/docs/manual/core/document/#dot-notation). Do not include quotes (`\"\"`) around nested fields that you specify using [dot notation](https://www.mongodb.com/docs/manual/core/document/#dot-notation). You can't specify fields inside an array. The specified fields are used to partition your data. ## Warning You can't specify field names that contain periods (`.`) for partitioning. The most frequently queried fields should be listed towards the top because they will have a larger impact on performance and cost than fields listed lower down the list. The order of fields is important in the same way as it is for [Compound Indexes](https://www.mongodb.com/docs/manual/core/index-compound/). Data is optimized for queries by the first field, followed by the second field, and so on. ## Example Enter `year` in the Most commonly queried fieldfield and `title` in the Second most commonly queried field field. Atlas Data Lake optimizes performance for the `year` field, followed by the `title` field. If you configure a Federated Database Instance for your Data Lake dataset, Atlas Data Federation optimizes performance for queries on the following fields: * the `year` field, and\n* the `year` field and the `title` field. Atlas Data Federation can also support a query on the `title` field only. However, in this case, Atlas Data Federation wouldn't be as efficient in supporting the query as it would be if the query were on the`title` field only. Performance is optimized in order; if a query omits a particular partition, Atlas Data Federation is less efficient in making use of any partitions that follow that. You can run Atlas Data Federation queries on fields not specified here, but Atlas Data Lake is less efficient in processing such queries. 7 ### (Optional) Specify fields inside your documents to exclude. By default, Atlas Data Lake extracts and stores all fields inside the documents in your collection. To specify fields to exclude: 1. Click Add Field.\n2. Enter field name in the Add Transformation Field Name window.  \n## Example  \n(Optional) Enter `fullplot` to exclude the field named`fullplot` in the `movies` collection.\n3. Click Done.\n4. Repeat steps for each field you wish to exclude. To remove a field from this list, click . 8 ### Click Finish to create the Data Lake. ## Create a Pipeline from the API To create an Atlas Data Lake pipeline through the API, send a `POST`request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `pipelines` endpoint. To learn more about the `pipelines` endpoint syntax and parameters for creating a pipeline, see[Create One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/createOneDataLakePipeline) ## Tip You can send a `GET` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) [availableSchedules](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAvailableSchedulesForPipeline) endpoint to retrieve the list of backup schedule policy items that you can use to create your Data Lake pipeline of type `PERIODIC_DPS`. ## Next steps Now that you've created your Data Lake pipeline, proceed to[Set Up a Federated Database Instance for Your Dataset.](https://mongodb.com/docs/datalake/tutorial/set-up-federated-database/#std-label-adl-create-federated-db) ← [Get Started](https://mongodb.com/docs/datalake/get-started/ \"Previous Section\")[Set Up a Federated Database Instance for Your Dataset](https://mongodb.com/docs/datalake/tutorial/set-up-federated-database/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/tutorial/set-up-federated-database/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Set Up a Federated Database Instance for Your Dataset On this page * [Prerequisites](#prerequisites)\n* [Procedure](#procedure)\n* [Next Steps](#next-steps) This page guides you through the steps for creating a federated database instance for you Data Lake dataset. ## Prerequisites Before you begin, you must have the following: * An Atlas Data Lake dataset in the same project where you intend to create the federated database instance.\n* [Project Owner](https://www.mongodb.com/docs/atlas/reference/user-roles/#mongodb-authrole-Project-Owner) role for the project where you want to create the federated database instance. ## Procedure 1 ### Navigate to Atlas Data Federation in the Atlas UI. To navigate to the Atlas Data Federation page: 1. Log in to [MongoDB Atlas.](https://cloud.mongodb.com)\n2. Select Data Federation under Data Services on the left-hand navigation. 2 ### Click Create Federated Database. 3 ### Select the configuration method. * For a guided experience, enable Visual Editor. (Default)\n* To edit the raw JSON, disable Visual Editor. 4 ### Create virtual databases, collections, and views and map them to your Data Lake dataset. Follow the steps in the tab below for your preferred Editor view in the UI. 5 ### Click Save to create the Federated Database Instance with virtual databases, collections, and views mapped to your Data Lake dataset. ## Next Steps Now that you've created a Federated Database Instance for your Data Lake dataset, proceed to [Connect to Your Federated Database Instance.](https://mongodb.com/docs/datalake/tutorial/adl-connect-federated-db-instance/#std-label-adl-connect-federated-db-instance) ← [Create an Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/tutorial/add-dataset-pipeline/ \"Previous Section\")[Connect to Your Federated Database Instance](https://mongodb.com/docs/datalake/tutorial/adl-connect-federated-db-instance/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/release-notes/data-lake/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Release Notes On this page * [2022 Releases](#2022-releases) **Looking for documentation for what used to be called \"Atlas Data Lake\"?** Atlas Data Lake is now called Atlas Data Federation. To learn more about the renamed federated query engine service, see [Atlas Data Federation](https://www.mongodb.com/docs/atlas/data-federation/). ## Note ### Release notes mention only releases with feature changes MongoDB releases Atlas Data Lake every two weeks, continuously improving Atlas Data Lake performance and stability. These release notes capture only those releases that contain feature changes. If a particular Atlas Data Lake release contains only performance and stability improvements, it is not included in these release notes. To identify which release version you are using, check the release version string for the release date. ## 2022 Releases ### 19 December 2022 Release * Adds [AWS region](https://mongodb.com/docs/datalake/#std-label-atlas-data-lake-regions): `ap-southeast-1` (Singapore). ### 13 September 2022 Release * Adds support for the Public API for configuring and utilizing Data Lake Pipelines. ### 23 August 2022 Release * Improves performance and stability.\n* Supports On-Demand Pipelines that can be triggered to ingest any existing backup snapshot. ### 21 June 2022 Release * Improves performance and stability. ### 07 June 2022 Release * Launches new Atlas Data Lake. To learn more, see [About Atlas Data Lake.](https://mongodb.com/docs/datalake/#std-label-atlas-data-lake-about)  \n## Important  \nThe federated query engine service previously called Atlas Data Lake is now called Atlas Data Federation. To learn more about Atlas Data Federation, see[Set Up and Query Data Federation.](https://www.mongodb.com/docs/atlas/data-federation/) ← [Data Lake Limitations](https://mongodb.com/docs/datalake/limitations/ \"Previous Section\")",
  "https://www.mongodb.com/docs/datalake/administration/ingest-data-on-demand/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Trigger Data Ingestion On Demand You can manually trigger an ingestion of snapshot data from theAtlas cluster to Atlas Data Lake datasets if you configured On Demand extraction in your Data Lake pipeline. You can trigger data ingestion from the Atlas UI and Data Lake Pipelines API. ## Trigger Data Ingestion from the Atlas UI 1 ### Log in to [MongoDB Atlas](https://cloud.mongodb.com). 2 ### Select Data Lake under Deployment on the left-hand navigation. 3 ### Click the vertical ellipsis () for the Data Lake for which you configured On Demand ingestion and select Trigger an On Demand Pipeline Run. 4 ### Select the snapshot, from which to ingest data, from the dropdown. The dropdown shows a list of all the snapshots on your Atlascluster. However, you can select only the snapshots from which Data Lake hasn't yet ingested data; the grayed-out snapshots are snapshots from which your Data Lake has already ingested data. You can also send a `GET` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `availableSnapshots` endpoint to retrieve the list of backup snapshots that you can use to trigger an on-demand pipeline run. To learn more about the API syntax and options for the `availableSnapshots` endpoint, see[Return Available Backup Snapshots for One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAvailableSnapshotsForPipeline) 5 ### Click Confirm. Atlas displays a blue banner at the top of the page that shows the data ingestion status. ## Trigger Data Ingestion from the API To trigger data ingestion through the API, send a `POST` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `trigger`endpoint with the name of the pipeline for which you want to trigger data ingestion. To learn more about the API syntax and parameters for the `trigger` endpoint, see [Trigger On-Demand Snapshot Ingestion.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/triggerOneOnDemandSnapshotIngestion) ## Tip You can send a `GET` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `availableSnapshots` endpoint to retrieve the list of backup snapshots that you can use to trigger an on-demand pipeline run. To learn more about the API syntax and options for the `availableSnapshots` endpoint, see[Return Available Backup Snapshots for One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAvailableSnapshotsForPipeline) ← [Edit an Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/administration/edit-data-pipeline/ \"Previous Section\")[Pause Data Ingestion for Your Data Lake Pipeline](https://mongodb.com/docs/datalake/administration/pause-resume-data-extraction/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/administration/delete-datalake-pipeline/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Delete Atlas Data Lake Pipeline You can delete your Data Lake pipeline at any time from the Atlas UIand Data Lake Pipelines API. When you delete your Data Lake pipeline,Atlas deletes all underlying datasets, including the data, and removes the Data Lake datasets from your federated database instances where it's referenced. You can't undo this operation. ## Delete a Pipeline from the Atlas UI To delete a Data Lake pipeline: 1 ### Navigate to Atlas Data Lake in the Atlas UI. To navigate to the Atlas Data Lake page: 1. Log in to [MongoDB Atlas.](https://cloud.mongodb.com)\n2. Select Data Lake under Deploymenton the left-hand navigation. 2 ### Click  in the Actions column for the pipeline that you wish to delete. 3 ### Click Delete in the confirmation window. ## Delete a Pipeline from the API To delete a pipeline through the API, send a `DELETE` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `pipelines`endpoint with the name of the pipeline that you want to delete. To learn more about the `pipelines` endpoint syntax and parameters for deleting a pipeline, see [Remove One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/removeOneDataLakePipeline) ← [Pause Data Ingestion for Your Data Lake Pipeline](https://mongodb.com/docs/datalake/administration/pause-resume-data-extraction/ \"Previous Section\")[Data Lake Limitations](https://mongodb.com/docs/datalake/limitations/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/administration/view-datalake-pipelines/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # View Atlas Data Lake Pipelines You can view all of your Data Lake pipelines through the Atlas UIand Data Lake Pipelines API. You can also retrieve all of your completed Data Lake pipeline data ingestion jobs from the API. ## View Atlas Data Lake Pipelines from the Atlas UI 1 ### Log in to [MongoDB Atlas](https://cloud.mongodb.com). 2 ### Select Data Lake under Deployment on the left-hand navigation. The page displays all the Data Lake pipelines in the project. For each Data Lake pipeline, the service also displays the following information:\n\n ## View Atlas Data Lake Pipelines from the API To retrieve all your Data Lake pipelines for a project through the API, send a `GET` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `pipelines` endpoint. To learn more about the `pipelines` endpoint syntax and parameters for retrieving all of your Data Lake pipelines, see [Return All Data Lake Pipelines from One Project.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAllDataLakePipelinesFromOneProject) To retrieve one of your Data Lake pipelines through the API, send a`GET` request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `pipelines` endpoint with the name of the Data Lake pipeline that you want to retrieve. To learn more about the `pipelines` endpoint syntax and parameters for retrieving one of your Data Lake pipelines, see[Return One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnOnePipelineInOneProject) ## View Atlas Data Lake Pipeline Runs from the API To retrieve all the completed Data Lake pipeline data ingestion jobs for a project through the API, send a `GET` request to the[Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `runs` endpoint. To learn more about the API syntax and options for the `runs`endpoint, see [Return All Data Lake Pipeline Runs from One Project.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAllDataLakePipelineRunsFromOneProject) To retrieve the details of one of your completed Data Lake pipeline data ingestion jobs through the API, send a `GET` request to the[Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `runs` endpoint with the unique identifier of the completed Data Lake pipeline data ingestion job that you want to retrieve. To learn more about the API syntax and options for the `runs` endpoint, see [Return One Data Lake Pipeline Run.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnOnePipelineRunInOneProject) ← [Manage Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/manage-adl-dataset-pipeline/ \"Previous Section\")[Edit an Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/administration/edit-data-pipeline/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/administration/pause-resume-data-extraction/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Pause Data Ingestion for Your Data Lake Pipeline On this page * [Pause Data Ingestion for Your Data Lake Pipeline](#pause-data-ingestion-for-your-data-lake-pipeline-1)\n* [Resume Data Ingestion for Your Data Lake Pipeline](#resume-data-ingestion-for-your-data-lake-pipeline) You can pause and resume ingestion of snapshot data from the Atlascluster to Atlas Data Lake datasets. You can't pause on-demand ingestion of snapshot data. ## Pause Data Ingestion for Your Data Lake Pipeline When you pause your Data Lake pipeline, Atlas doesn't ingest new datasets. You can continue to query previous snapshots from which data has been ingested. ### Pause Data Ingestion from the Atlas UI To pause a pipeline from the Atlas UI: 1 #### Navigate to Atlas Data Lake in the Atlas UI. To navigate to the Atlas Data Lake page: 1. Log in to [MongoDB Atlas.](https://cloud.mongodb.com)\n2. Select Data Lake under Deploymenton the left-hand navigation. 2 #### Click the pause button for the pipeline that you wish to pause. 3 #### Click Confirm in the Pause Ingestion confirmation window. When you pause your Data Lake pipeline, the Last Updatedcolumn for the pipeline in the Atlas UI shows the status for the pipeline as Paused. ### Pause Data Ingestion from the API To pause a pipeline through the API, send a `POST` request to the[Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `pause` endpoint with the name of the pipeline for which you want to pause the ingestion schedule. To learn more about the API syntax and parameters for the`pause` endpoint, see [Pause One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/pauseOnePipelineInOneProject) ## Resume Data Ingestion for Your Data Lake Pipeline When you resume data ingestion for a paused Atlas Data Lake pipeline,Atlas begins to take snapshots, which are then ingested in to your Data Lake datasets. ### Resume Data Ingestion from the Atlas UI To resume data ingestion from the Atlas UI: 1 #### Navigate to Atlas Data Lake in the Atlas UI. To navigate to the Atlas Data Lake page: 1. Log in to [MongoDB Atlas.](https://cloud.mongodb.com)\n2. Select Data Lake under Deploymenton the left-hand navigation. 2 #### Click  in the Actions column for the pipeline that you wish to resume and select Resume Ingestion. 3 #### Click Confirm in the Resume Ingestion confirmation window. When you resume data ingestion for a paused Atlas Data Lake pipeline, the Last Run Time column for the pipeline in theAtlas UI shows the date and time when data ingestion for the pipeline resumed. ### Resume Data Ingestion from the API To resume data ingestion for a pipeline from the API, send a `POST`request to the [Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `resume` endpoint with the name of the pipeline for which you want to resume the data ingestion. To learn more about the API syntax and parameters for the `resume` endpoint, see [Resume One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/resumeOnePipelineInOneProject) ← [Trigger Data Ingestion On Demand](https://mongodb.com/docs/datalake/administration/ingest-data-on-demand/ \"Previous Section\")[Delete Atlas Data Lake Pipeline](https://mongodb.com/docs/datalake/administration/delete-datalake-pipeline/ \"Next Section\") →",
  "https://www.mongodb.com/docs/datalake/administration/edit-data-pipeline/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Atlas Data Lake](https://mongodb.com/docs/datalake/) # Edit an Atlas Data Lake Pipeline On this page * [Edit a Pipeline from the Atlas UI](#edit-a-pipeline-from-the-service-ui)\n* [Edit a Pipeline from the API](#edit-a-pipeline-from-the-api) You can make changes to your Data Lake pipelines through the Atlas UI and Data Lake Pipelines API, including: * Edit the data extraction schedule\n* Edit the data storage region\n* Change the fields to exclude from your Data Lake datasets ## Edit a Pipeline from the Atlas UI 1 ### Log in to [MongoDB Atlas](https://cloud.mongodb.com). 2 ### Select Data Lake under Deployment on the left-hand navigation. 3 ### Click  in the Actions column for the pipeline that you wish to modify. 4 ### (Optional) Make changes to your data extraction schedule. Before making changes to your Basic Schedule, ensure that your desired data extraction frequency is similar to your current backup schedule. For example, if you wish to switch to`Daily`, you must have a `Daily` backup schedule configured in your policy. Or, if you want to switch to a schedule of once a week, you must have a `Weekly` backup schedule configured in your policy. To learn more, see [Backup Scheduling](https://www.mongodb.com/docs/atlas/backup/cloud-backup/overview/#backup-scheduling--retention--and-on-demand-backup-snapshots). You can also send a `GET` request to the Data Lake[availableSchedules](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAvailableSchedulesForPipeline) endpoint to retrieve the list of backup schedule policy items that you can use to change the schedule of your Data Lake pipeline. 5 ### (Optional) Make changes to your data storage region. Atlas Data Lake provides optimized storage in the following AWS regions: | Data Lake Regions  | AWS Regions    |\n| ------------------ | -------------- |\n| Virginia, USA      | us-east-1      |\n| Oregon, USA        | us-west-2      |\n| Sao Paulo, Brazil  | sa-east-1      |\n| Ireland            | eu-west-1      |\n| London, England    | eu-west-2      |\n| Frankfurt, Germany | eu-central-1   |\n| Mumbai, India      | ap-south-1     |\n| Singapore          | ap-southeast-1 |\n| Sydney, Australia  | ap-southeast-2 | 6 ### Click Continue. 7 ### (Optional) Make changes to the fields excluded from your Data Lake datasets. * Click Add Field and specify Field Nameto add fields to the excluded fields list.\n* Click Delete All to remove all the fields from the excluded fields list.\n* Click  next to a field to remove that field from the excluded fields list. 8 ### Click Review Changes to review the changes to your pipeline. 9 ### Click Apply Changes for the changes to take effect. ## Edit a Pipeline from the API To edit a pipeline through the API, send a `PATCH` request to the[Data Lake](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#tag/Data-Lake-Pipelines) `pipelines` endpoint with the name of the pipeline that you want to edit. To learn more about the `pipelines` endpoint syntax and parameters for updating a pipeline, see [Update One Data Lake Pipeline.](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/updateOneDataLakePipeline) ## Tip You can send a `GET` request to the[availableSchedules](https://www.mongodb.com/docs/atlas/reference/api-resources-spec/#operation/returnAvailableSchedulesForPipeline) endpoint to retrieve the list of backup schedule policy items that you can use to change the schedule of your Data Lake pipelines of type `PERIODIC_DPS`.\n\n"
}