{
  "https://www.mongodb.com/docs/kafka-connector/current/compatibility/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Compatibility ## MongoDB Compatibility ### MongoDB Kafka Sink Connector The MongoDB Kafka sink connector requires MongoDB v3.6 or later. ### MongoDB Kafka Source Connector The MongoDB Kafka source connector requires MongoDB v3.6 or later. If you are using MongoDB v3.6, the connector can only listen for changes on **collections**. If you need the connector to listen for changes on a **database** or **deployment**, you must use MongoDB v4.0 or later. ## Note We recommend that you only use the MongoDB Kafka Connector with an official version of the MongoDB server. We cannot guarantee it functions correctly with any other version. ## Kafka Compatibility The MongoDB Kafka Connector requires Confluent Kafka Connect v2.1.0 or later. It's tested against Apache Kafka v2.3 and later. You can use the Kafka Connect service with several Apache Kafka compatible platforms including the following: * Confluent Platform v5.3 or later\n* Microsoft Azure Event Hubs\n* Red Hat AMQ Streams The connector works directly with Kafka Connect. The connector does not connect directly to a Kafka cluster which means it's compatible with any Apache Kafka platform that supports Kafka Connect. If you have any questions about the connector, ask them on the[MongoDB Community Forums.](https://www.mongodb.com/community/forums/c/data/connectors-integrations/) ← [Issues & Help](https://mongodb.com/docs/kafka-connector/current/issues-and-help/ \"Previous Section\")",
  "https://www.mongodb.com/docs/kafka-connector/current/migrate-from-kafka-connect-mongodb/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Migrate from Kafka Connect MongoDB Use this guide to migrate your Kafka deployments from the community-created[Kafka Connect MongoDB](https://github.com/hpgrahsl/kafka-connect-mongodb) sink connector to the [official MongoDB Kafka connector.](https://github.com/mongodb/mongo-kafka) The following sections list the changes you must make to your Kafka Connect sink connector configuration settings and custom classes to transition to the MongoDB Kafka sink connector. ## Update Configuration Settings Make the following changes to the configuration settings of your Kafka Connect deployment before using them with your MongoDB Kafka connector deployment: * Replace values that include the package `at.grahsl.kafka.connect.mongodb`with the package `com.mongodb.kafka.connect`.\n* Replace your `connector.class` setting with the MongoDB Kafka sink connector class.  \n`| connector.class=com.mongodb.kafka.connect.MongoSinkConnector |  \n| ------------------------------------------------------------ |  \n`\n* Remove the `mongodb.` prefix from your Kafka Connect property names. For example, change `mongodb.connection.uri` to `connection.uri`.\n* Remove the `document.id.strategies` setting if it exists. If the value of this setting reference custom strategies, move them to the`document.id.strategy` setting. Read the [Update Custom Classes](#std-label-custom-class-changes)section to discover what changes you must make to your custom classes.\n* Replace any property names you use to specify per-topic or collection overrides that contain the `mongodb.collection` prefix with the equivalent key in[sink connector Kafka topic configuration topic properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/kafka-topic/#std-label-sink-configuration-topic-properties) ## Update Custom Classes If you use any custom classes in your Kafka Connect sink connector deployment, make the following changes to them before adding them to your MongoDB Kafka connector deployment: * Replace imports that include `at.grahsl.kafka.connect.mongodb` with`com.mongodb.kafka.connect`.\n* Replace references to the `MongoDbSinkConnector` class with the`MongoSinkConnector` class.\n* Update custom sink connector strategy classes to implement the`com.mongodb.kafka.connect.sink.processor.id.strategy.IdStrategy`interface.\n* Update references to the `MongoDbSinkConnectorConfig` class. In the MongoDB Kafka connector, the logic from that class is split into the following classes:  \n   * [MongoSinkConfig](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/MongoSinkConfig.java)  \n   * [MongoSinkTopicConfig](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/MongoSinkTopicConfig.java) ### Update Post Processor Subclasses If you have classes that subclass a post processor in your Kafka Connect connector deployment, update methods that override ones in the Kafka Connect`PostProcessor` class to match the method signatures of the MongoDB Kafka connector [PostProcessor class.](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/PostProcessor.java) ← [Monitoring](https://mongodb.com/docs/kafka-connector/current/monitoring/ \"Previous Section\")[Troubleshooting](https://mongodb.com/docs/kafka-connector/current/troubleshooting/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Sink Connector On this page * [Overview](#overview)\n* [Configuration Properties](#configuration-properties)\n* [Fundamentals](#fundamentals) ## Overview This section focuses on the **MongoDB Kafka sink connector**. The sink connector is a Kafka Connect connector that reads data from Apache Kafka and writes data to MongoDB. ## Configuration Properties To learn about configuration options for your sink connector, see the[Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/#std-label-kafka-sink-configuration-properties) section. ## Fundamentals To learn how features of the sink connector work and how to configure them, see the[Fundamentals](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/#std-label-kafka-sink-fundamentals) section. ← [Migrate an Existing Collection to a Time Series Collection](https://mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/ \"Previous Section\")[Sink Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/issues-and-help/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Issues & Help Often, the quickest way to get support for general questions is through the[MongoDB Community Forums.](https://www.mongodb.com/community/forums/tags/c/connectors-integrations/48/kafka-connector) Refer to our [support channels](https://www.mongodb.com/docs/manual/support/) documentation for more information. ## Bugs / Feature Requests If you think you've found a bug or want to see a new feature in the MongoDB Kafka Connector, please open a case in our issue management tool, JIRA: * [Create an account and login.](https://jira.mongodb.org)\n* Navigate to [the KAFKA project.](https://jira.mongodb.org/browse/KAFKA)\n* Click **Create**. Please provide as much information as possible about the issue and the steps to reproduce it. Bug reports in JIRA for the Kafka Connector project are **public**. If you've identified a security vulnerability in the connector or any other MongoDB project, please report it according to the instructions found in the[Create a Vulnerability Report.](https://www.mongodb.com/docs/manual/tutorial/create-a-vulnerability-report/) ## Pull Requests We're happy to accept contributions to help improve the connector. We will review user contributions to ensure they meet the standards of the codebase. To get started check out the source and work on a branch: `| git clone https://github.com/mongodb/mongo-kafka.git |\n| ---------------------------------------------------- |\n| cd mongo-kafka                                       |\n| git checkout -b myNewFeature                         | ` ← [How to Contribute](https://mongodb.com/docs/kafka-connector/current/contribute/ \"Previous Section\")[Compatibility](https://mongodb.com/docs/kafka-connector/current/compatibility/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/introduction/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Introduction Read the following sections to learn about the MongoDB Kafka Connector, Kafka Connect, and Apache Kafka: * [Kafka and Kafka Connect](https://mongodb.com/docs/kafka-connector/current/introduction/kafka-connect/)\n* [Install the Connector](https://mongodb.com/docs/kafka-connector/current/introduction/install/)\n* [Connect to MongoDB](https://mongodb.com/docs/kafka-connector/current/introduction/connect/)\n* [Data Formats](https://mongodb.com/docs/kafka-connector/current/introduction/data-formats/)\n* [Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/) ← [Quick Start](https://mongodb.com/docs/kafka-connector/current/quick-start/ \"Previous Section\")[Kafka and Kafka Connect](https://mongodb.com/docs/kafka-connector/current/introduction/kafka-connect/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/troubleshooting/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Troubleshooting Learn how to address issues you may encounter while running the MongoDB Kafka Connector. * [Invalid Resume Token](https://mongodb.com/docs/kafka-connector/current/troubleshooting/recover-from-invalid-resume-token/#std-label-kafka-troubleshoot-invalid-resume-token) ← [Migrate from Kafka Connect MongoDB](https://mongodb.com/docs/kafka-connector/current/migrate-from-kafka-connect-mongodb/ \"Previous Section\")[Invalid Resume Token](https://mongodb.com/docs/kafka-connector/current/troubleshooting/recover-from-invalid-resume-token/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/security-and-authentication/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Security and Authentication Read the following sections to learn how to secure communications between MongoDB and the MongoDB Kafka Connector: * [Encrypt the Messages Your Connector Sends with SSL/TLS](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/tls-and-x509/)\n* [Authenticate Your Connector with MongoDB using Amazon Web Services](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/mongodb-aws-auth/) ← [Apply Schemas](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/ \"Previous Section\")[SSL/TLS and X.509 Certificates](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/tls-and-x509/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # MongoDB Kafka Connector ## Overview The [MongoDB Kafka connector](https://www.mongodb.com/kafka-connector) is a Confluent-verified connector that persists data from Kafka topics as a data sink into MongoDB as well as publishes changes from MongoDB into Kafka topics as a data source. This guide provides information on available configuration options and examples to help you complete your implementation in the following sections: * [What's New](https://mongodb.com/docs/kafka-connector/current/whats-new/)\n* [Quick Start](https://mongodb.com/docs/kafka-connector/current/quick-start/)\n* [Introduction](https://mongodb.com/docs/kafka-connector/current/introduction/)\n* [Tutorials](https://mongodb.com/docs/kafka-connector/current/tutorials/)\n* [Sink Connector](https://mongodb.com/docs/kafka-connector/current/sink-connector/)\n* [Source Connector](https://mongodb.com/docs/kafka-connector/current/source-connector/)\n* [Security and Authentication](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/)\n* [Monitoring](https://mongodb.com/docs/kafka-connector/current/monitoring/)\n* [Migrate from the Community Connector](https://mongodb.com/docs/kafka-connector/current/migrate-from-kafka-connect-mongodb/)\n* [Troubleshooting](https://mongodb.com/docs/kafka-connector/current/troubleshooting/)\n* [How to Contribute](https://mongodb.com/docs/kafka-connector/current/contribute/)\n* [Issues & Help](https://mongodb.com/docs/kafka-connector/current/issues-and-help/)\n* [Compatibility](https://mongodb.com/docs/kafka-connector/current/compatibility/) [What's New](https://mongodb.com/docs/kafka-connector/current/whats-new/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/quick-start/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Quick Start On this page * [Overview](#overview)\n* [Install the Required Packages](#install-the-required-packages)\n* [Download the Sandbox](#download-the-sandbox)\n* [Start the Sandbox](#start-the-sandbox)\n* [Add Connectors](#add-connectors)\n* [Add a Source Connector](#add-a-source-connector)\n* [Add a Sink Connector](#add-a-sink-connector)\n* [Send the Contents of a Document through Your Connectors](#send-the-contents-of-a-document-through-your-connectors)\n* [Remove the Sandbox](#remove-the-sandbox)\n* [Next Steps](#next-steps) ## Overview This guide shows you how to configure the MongoDB Kafka Connector to send data between MongoDB and Apache Kafka. After completing this guide, you should understand how to use the Kafka Connect REST API to configure MongoDB Kafka Connectors to read data from MongoDB and write it to a Kafka topic, and to read data from a Kafka topic and write it to MongoDB. To complete the steps in this guide, you must download and work in a**sandbox**, a containerized development environment that includes services you need to build a sample _data pipeline_. Read the following sections to set up your sandbox and sample data pipeline. ## Note After you complete this guide, you can remove the environment by following the instructions in the [Remove the Sandbox](#std-label-kafka-quickstart-remove-the-sandbox)section. ## Install the Required Packages Download and install the following packages: * [Docker](https://docs.docker.com/get-docker/)\n* [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) ## Tip ### Read the Docker Documentation This guide uses the following Docker-specific terminology: * [Container](https://docs.docker.com/glossary/#container)\n* [Image](https://docs.docker.com/glossary/#image) Learn more about Docker from the Docker official[Get Started Guide.](https://docs.docker.com/get-started/) The sandbox uses Docker for convenience and consistency. To learn more about deployment options for Apache Kafka, see the following resources: * [Official Apache Kafka Quick Start](https://kafka.apache.org/quickstart)\n* [Official Install Confluent Platform Guide](https://docs.confluent.io/platform/current/installation/index.html) ## Download the Sandbox We created a sandbox that includes the services you need in this tutorial to build your sample data pipeline. To download the sandbox, clone the tutorial repository to your development environment. Then navigate to the directory that corresponds to the quickstart tutorial. If you use bash or a similar shell, use the following commands: `| git clone https://github.com/mongodb-university/kafka-edu.git |\n| ------------------------------------------------------------- |\n| cd kafka-edu/docs-examples/mongo-kafka-base                   | ` ## Start the Sandbox The sandbox starts the following services in Docker containers: * MongoDB, configured as a replica set\n* Apache Kafka\n* Kafka Connect with the MongoDB Kafka Connector installed\n* Apache Zookeeper which manages the configuration for Apache Kafka To start the sandbox run the following command from the tutorial directory: ```shell docker-compose -p mongo-kafka up -d \n``` When you start the sandbox, Docker downloads any images it needs to run. ## Note ### How long does the download take? In total, the Docker images for this tutorial require about 2.4 GB of space. The following list shows how long it takes to download the images with different internet speeds: * 40 megabits per second: 8 minutes\n* 20 megabits per second: 16 minutes\n* 10 megabits per second: 32 minutes After Docker downloads and builds the images, you should see the following output in your development environment: ``` ... Creating zookeeper ... done Creating broker    ... done Creating schema-registry ... done Creating connect         ... done Creating rest-proxy      ... done\n\n Creating mongo1-setup    ... done \n``` ## Note ### Port Mappings The sandbox maps the following services to ports on your host machine: * The sandbox MongoDB server maps to port `35001` on your host machine\n* The sandbox Kafka Connect JMX server maps to port `35000` on your host machine These ports must be free to start the sandbox. ## Add Connectors To complete the sample data pipeline, you must add connectors to Kafka Connect to transfer data between Kafka Connect and MongoDB. Add a **source connector** to transfer data from MongoDB to Apache Kafka. Add a **sink connector** to transfer data from Apache Kafka to MongoDB. To add connectors in the sandbox, first start an interactive bash shell in your Docker container using the following command: ```shell docker exec -it mongo1 /bin/bash \n``` After your shell session starts, you should see the following prompt: ``` MongoDB Kafka Connector Sandbox $ \n``` ### Add a Source Connector Use the shell in your Docker container to add a source connector using the Kafka Connect REST API. The following API request adds a source connector configured with the following properties: * The class Kafka Connect uses to instantiate the connector\n* The connection URI, database, and collection of the MongoDB replica set from which the connector reads data\n* An aggregation pipeline that adds a field `travel` with the value`\"MongoDB Kafka Connector\"` to inserted documents the connector reads from MongoDB ```bash curl -X POST \\      -H \"Content-Type: application/json\" \\      --data '      {\"name\": \"mongo-source\",       \"config\": {          \"connector.class\":\"com.mongodb.kafka.connect.MongoSourceConnector\",          \"connection.uri\":\"mongodb://mongo1:27017/?replicaSet=rs0\",          \"database\":\"quickstart\",          \"collection\":\"sampleData\",          \"pipeline\":\"[{\\\"$match\\\": {\\\"operationType\\\": \\\"insert\\\"}}, {$addFields : {\\\"fullDocument.travel\\\":\\\"MongoDB Kafka Connector\\\"}}]\"          }      }      ' \\      http://connect:8083/connectors -w \"\\n\" \n``` ## Note ### Why do I see the message 'Failed to connect'? It takes up to three minutes for the Kafka Connect REST API to start. If you receive the following error, wait three minutes and run the preceding command again: ``` ... curl: (7) Failed to connect to connect port 8083: Connection refused \n``` To confirm that you added the source connector, run the following command: ```shell curl -X GET http://connect:8083/connectors \n``` The preceding command should output the names of the running connectors: ``` [\"mongo-source\"] \n``` To learn more about source connector properties, see the page on[Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/#std-label-source-configuration-index) To learn more about aggregation pipelines, see the MongoDB manual page on[Aggregation Pipelines.](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/) ### Add a Sink Connector Use the shell in your Docker container to add a sink connector using the Kafka Connect REST API. The following API request adds a sink connector configured with the following properties: * The class Kafka Connect uses to instantiate the connector\n* The connection URI, database, and collection of the MongoDB replica set to which the connector writes data\n* The Apache Kafka topic from which the connector reads data\n* A change data capture handler for MongoDB change event documents ```bash curl -X POST \\      -H \"Content-Type: application/json\" \\      --data '      {\"name\": \"mongo-sink\",       \"config\": {          \"connector.class\":\"com.mongodb.kafka.connect.MongoSinkConnector\",          \"connection.uri\":\"mongodb://mongo1:27017/?replicaSet=rs0\",\n\n          \"collection\":\"topicData\",          \"topics\":\"quickstart.sampleData\",          \"change.data.capture.handler\": \"com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler\"          }      }      ' \\      http://connect:8083/connectors -w \"\\n\" \n``` To confirm that you added both source and sink connector, run the following command: ```shell curl -X GET http://connect:8083/connectors \n``` The preceding command should output the names of the running connectors: ``` [\"mongo-source\", \"mongo-sink\"] \n``` To learn more about sink connector properties, see the page on[Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/#std-label-kafka-sink-configuration-properties) To learn more about change data capture events, see the[Change Data Capture Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/#std-label-sink-fundamentals-cdc-handler) guide. ### Send the Contents of a Document through Your Connectors To send the contents of a document through your connectors, insert a document into the MongoDB collection from which your source connector reads data. To insert a new document into your collection, enter the MongoDB shell from the shell in your Docker container using the following command: ```shell mongosh mongodb://mongo1:27017/?replicaSet=rs0 \n``` After you run the preceding command, you should see the following prompt: ```shell rs0 [primary] test> \n``` From the MongoDB shell, insert a document into the `sampleData`collection of the `quickstart` database using the following commands: ```javascript use quickstart db.sampleData.insertOne({\"hello\":\"world\"}) \n``` After you insert a document into the `sampleData` collection, confirm that your connectors processed the change. Check the contents of the `topicData`collection using the following command: ```javascript db.topicData.find() \n``` You should see output that resembles the following: ``` [     {       _id: ObjectId(...),       hello: 'world',       travel: 'MongoDB Kafka Connector'     } ] \n``` Exit the MongoDB shell with the following command: ```shell exit \n``` ## Remove the Sandbox To conserve resources in your development environment, remove the sandbox. Before you remove the sandbox, exit from the shell session in your Docker container by running the following command: ```shell exit \n``` You can choose to remove both the Docker containers and images, or exclusively the containers. If you remove the containers and images, you must download them again to restart your sandbox which is approximately 2.4 GB in size. If you exclusively remove the containers, you can reuse the images and avoid downloading most of the large files in the sample data pipeline. Select the tab that corresponds to the removal task you want to run. ## Next Steps To learn how to install the MongoDB Kafka Connector, see the [Install the MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/introduction/install/#std-label-kafka-installation) guide. To learn more about how to process and move data from Apache Kafka to MongoDB, see the [Sink Connector](https://mongodb.com/docs/kafka-connector/current/sink-connector/#std-label-kafka-sink-overview) guide. To learn more about how to process and move data from MongoDB to Apache Kafka, see the [Source Connector](https://mongodb.com/docs/kafka-connector/current/source-connector/#std-label-kafka-source-overview) guide. ← [What's New](https://mongodb.com/docs/kafka-connector/current/whats-new/ \"Previous Section\")[Introduction](https://mongodb.com/docs/kafka-connector/current/introduction/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Tutorials The tutorials in this section show you how to perform specific tasks with the MongoDB Kafka Connector. For instructions on setting up your environment for the following tutorials, read the [Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-kafka-tutorials-docker-setup) first: * [Explore Change Streams](https://mongodb.com/docs/kafka-connector/current/tutorials/explore-change-streams/#std-label-kafka-tutorial-explore-change-streams)\n* [Getting Started with the MongoDB Kafka Source Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/source-connector/#std-label-kafka-tutorial-source-connector)\n* [Getting Started with the MongoDB Kafka Sink Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/sink-connector/#std-label-kafka-tutorial-sink-connector)\n* [Replicate Data with a Change Data Capture Handler](https://mongodb.com/docs/kafka-connector/current/tutorials/replicate-with-cdc/#std-label-kafka-tutorial-replicate-with-cdc)\n* [Migrate an Existing Collection to a Time Series Collection](https://mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/#std-label-kafka-tutorial-migrate-time-series) ← [Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/ \"Previous Section\")[Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/contribute/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # How to Contribute We are happy to accept contributions to help improve the MongoDB Kafka Connector. We guide community contributions to ensure they meet the standards of the codebase. Please ensure that your pull request meets the following criteria: * Includes documentation comments describing your feature\n* Includes unit tests that cover the functionality of your feature\n* Passes the Gradle `check` task, which includes the following tasks:  \n   1. `test` task  \n   2. `integrationTest` task  \n   3. `spotlessCheck` task To get started, check out the source code and create a branch by running the following commands in your shell: `| git clone https://github.com/mongodb/mongo-kafka.git |\n| ---------------------------------------------------- |\n| cd mongo-kafka                                       |\n| git checkout -b <your branch name>                   | ` Once you push your changes to your feature branch, make sure it passes the Gradle checks. You can run the checks with the following command: ```shell ./gradlew clean check --continue -Dorg.mongodb.test.uri=<your local mongodb replica set connection uri> \n``` ## Important ### Integration Tests You must have a local MongoDB replica set running to perform Gradle integration tests. To learn how to set up a MongoDB replica set, see[Deploy a Replica Set](https://www.mongodb.com/docs/manual/tutorial/deploy-replica-set/) in the MongoDB manual. ## Note ### Skipped Tests You can skip tests in the `integrationTest` task related to the following areas unless your code specifically modifies connector behavior related to these areas: * Specific versions of MongoDB\n* Authentication You can run the authentication tests by enabling authentication in your local MongoDB replica set and specifying your credentials in your connection URI. To learn how to enable authentication in a replica set, see[Deploy Replica Set With Keyfile Authentication](https://www.mongodb.com/docs/manual/tutorial/deploy-replica-set-with-keyfile-access-control/)in the MongoDB manual. You can run tests related to a specific MongoDB version by deploying a local replica set with that version of MongoDB. To learn more about the connector source code, see the [GitHub repository.](https://github.com/mongodb/mongo-kafka) To learn more about Gradle, see the official[Gradle website.](https://docs.gradle.org/) ← [Invalid Resume Token](https://mongodb.com/docs/kafka-connector/current/troubleshooting/recover-from-invalid-resume-token/ \"Previous Section\")[Issues & Help](https://mongodb.com/docs/kafka-connector/current/issues-and-help/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Source Connector On this page * [Overview](#overview)\n* [Configuration Properties](#configuration-properties)\n* [Usage Examples](#usage-examples)\n* [Fundamentals](#fundamentals) ## Overview This section focuses on the **MongoDB Kafka source connector**. The source connector is a Kafka Connect connector that reads data from MongoDB and writes data to Apache Kafka. The source connector works by opening a single change stream with MongoDB and sending data from that change stream to Kafka Connect. Your source connector maintains its change stream for the duration of its runtime, and your connector closes its change stream when you stop it. ## Configuration Properties To learn about configuration options for your source connector, see the[Configuration Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/#std-label-source-configuration-index) section. ## Usage Examples To view examples of source connector configurations, see the[Usage Examples](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/#std-label-kafka-source-usage-examples) section. ## Fundamentals To learn how features of the source connector work and how to configure them, see the[Fundamentals](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/#std-label-kafka-source-fundamentals) section. ← [Change Data Capture Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/ \"Previous Section\")[Source Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/whats-new/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # What's New On this page * [What's New in 1.9.1](#what-s-new-in-1.9.1)\n* [What's New in 1.9](#what-s-new-in-1.9)\n* [What's New in 1.8.1](#what-s-new-in-1.8.1)\n* [What's New in 1.8](#what-s-new-in-1.8)\n* [What's New in 1.7](#what-s-new-in-1.7)\n* [What's New in 1.6.1](#what-s-new-in-1.6.1)\n* [What's New in 1.6](#what-s-new-in-1.6)\n* [What's New in 1.5](#what-s-new-in-1.5)\n* [What's New in 1.4](#what-s-new-in-1.4)\n* [What's New in 1.3](#what-s-new-in-1.3)\n* [What's New in 1.2](#what-s-new-in-1.2)\n* [What's New in 1.1](#what-s-new-in-1.1)\n* [What's New in 1.0](#what-s-new-in-1.0) Learn what's new by version: * [Version 1.9.1](#std-label-kafka-connector-whats-new-1.9.1)\n* [Version 1.9](#std-label-kafka-connector-whats-new-1.9)\n* [Version 1.8.1](#std-label-kafka-connector-whats-new-1.8.1)\n* [Version 1.8](#std-label-kafka-connector-whats-new-1.8)\n* [Version 1.7](#std-label-kafka-connector-whats-new-1.7)\n* [Version 1.6.1](#std-label-kafka-connector-whats-new-1.6.1)\n* [Version 1.6](#std-label-kafka-connector-whats-new-1.6)\n* [Version 1.5](#std-label-kafka-connector-whats-new-1.5)\n* [Version 1.4](#std-label-kafka-connector-whats-new-1.4)\n* [Version 1.3](#std-label-kafka-connector-whats-new-1.3)\n* [Version 1.2](#std-label-kafka-connector-whats-new-1.2)\n* [Version 1.1](#std-label-kafka-connector-whats-new-1.1)\n* [Version 1.0](#std-label-kafka-connector-whats-new-1.0) ## What's New in 1.9.1 * Corrected the behavior of schema inference for documents in nested arrays. ## What's New in 1.9\n\n ## What's New in 1.8.1 * Corrected the type returned by `getAttribute()` and`getAttributes()` method calls in JMX MBeans to `Attribute`. ## What's New in 1.8 * Updated the MongoDB Java driver dependency to version 4.7.\n* Added several logger events and details in source and sink connectors to help with debugging. For a complete list of updates, see the[KAFKA-302](https://jira.mongodb.org/browse/KAFKA-302) issue in JIRA.\n* Added JMX monitoring support for the source and sink connectors. To learn more about monitoring connectors, see the[Monitoring](https://mongodb.com/docs/kafka-connector/current/monitoring/#std-label-kafka-monitoring) page. ### Sink Connector * Added support for the Debezium MongoDB change stream CDC handler. You can now configure the connector to listen for events produced by this handler. ## What's New in 1.7 * Updated the MongoDB Java driver dependency to version 4.5 ### Sink Connector * Added dead letter queue error reports in the event[the connector experiences bulk write errors](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/#std-label-sink-configuration-error-handling-dlq-errors)\n* Added support for unordered bulk writes with the `bulk.write.ordered` [configuration property](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/connector-message/#std-label-sink-configuration-message-processing)\n* Added warning when attempting to use a Change Data Capture (CDC) handler with a post processor\n* Removed support for the `max.num.retries` configuration property\n* Removed support for the `retries.defer.timeout` configuration property ## Important ### Disable Retries Through Connection URI To disable retries, specify the `retryWrites=false` option in your MongoDB connection URI. The following configuration, which contains a placeholder MongoDB connection URI, disables retries: `| connection.uri=mongodb://mongodb0.example.com:27017,mongodb1.example.com:27017,mongodb2.example.com:27017/?replicaSet=myRepl&retryWrites=false |\n| ---------------------------------------------------------------------------------------------------------------------------------------------- | ` To learn more about connecting the MongoDB Kafka Connector to MongoDB, see the[Connect to MongoDB](https://mongodb.com/docs/kafka-connector/current/introduction/connect/#std-label-kafka-intro-connect) guide. To learn more about connection URI options, see the[Connection Options](https://www.mongodb.com/docs/drivers/java/sync/v4.7/fundamentals/connection/connection-options/)guide in the MongoDB Java driver documentation. ### Source Connector * Added support for user-defined topic separators with the`topic.separator` configuration property\n* Added support for the[allow disk use](https://www.mongodb.com/docs/manual/reference/command/aggregate/#std-label-aggregate-cmd-allowDiskUse)field of the MongoDB Query API in the copy existing aggregation with the`copy.existing.allow.disk.use` [configuration property](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/copy-existing/#std-label-source-configuration-copy-existing)\n* Added support for [Avro schema namespaces](https://avro.apache.org/docs/current/spec.html#names)in the `output.schema.value` and `output.schema.key` [configuration properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/output-format/#std-label-source-configuration-output-format) ### Bug Fixes * Fixed Avro schema union validation ## What's New in 1.6.1 * Updated MongoDB Java driver dependency to 4.3.1 in the combined JARs ### Bug Fixes * Fixed connection validator user privilege check\n* Fixed a bug in `UuidProvidedIn[Key|Value]Strategy` classes that prevented them from loading ## What's New in 1.6\n\n ### Sink Connector * Added support for [automatic time-series collection creation](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/time-series/#std-label-sink-configuration-time-series)in MongoDB 5.0 to efficiently store sequences of measurements over a period of time. Learn how to configure connectors to [Migrate an Existing Collection to a Time Series Collection.](https://mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/#std-label-tutorial-migrate-time-series)\n* Improved the error logging for bulk write exceptions ### Source Connector * No new changes, additions or improvements ### Bug Fixes * Corrected the behavior of `LazyBsonDocument#clone` to respond to any changes made once unwrapped\n* Fixed the timestamp integer overflow in the Source Connector\n* Updated to enable recovery when calling the `getMore()` method in the Source Connector\n* Updated to enable recovery from broken change stream due to event sizes that are greater than 16 MB in the Source Connector ## What's New in 1.5 * Updated the MongoDB Java driver dependency to version 4.2 ### Sink Connector * Added the `DeleteOneBusinessKeyStrategy` write strategy to remove records from a topic\n* Added support for handling errant records that cause problems when processing them\n* Added support for Qlik Replicate Change Data Capture (CDC) to process event streams ### Source Connector * Replaced `BsonDocument` with `RawBsonDocument`\n* Improved the `copy.existing` namespace handling\n* Improved the error messages for invalid pipeline operators\n* Improved the efficiency of heartbeats by making them tombstone messages ### Bug Fixes * Corrected the inferred schema naming conventions\n* Updated to ensure that schemas can be backwards compatible\n* Fixed the Sink validation issue with `topics.regex`\n* Fixed the Sink NPE issue when using with Confluent Connect 6.1.0\n* Updated to ensure that the change stream cursor closes so it only reports errors that exist\n* Changed to include or exclude the `_id` field for a projection only if it's explicitly added ## What's New in 1.4 * Updated the MongoDB Java Driver to version 4.1 ### Sink Connector * Added support for Change Data Capture (CDC) based on MongoDB change stream events\n* Added the `NamespaceMapper` interface to allow for dynamic namespace mapping ### Source Connector * Added the `TopicMapper` interface to allow topic mapping ### Bug Fixes * Changed the top-level inferred schema to be mandatory\n* Fixed a validation issue and synthetic configuration property in the Sink Connector\n* Corrected general exception logging\n* Updated to clone the `LazyBsonDocument` instead of the unwrapped`BsonDocument` ## What's New in 1.3 * Added automated integration testing for the latest Kafka Connector and Confluent Platform versions to ensure compatibility ### Sink Connector * Added support for records that contain `Bson` byte types\n* Added support for the `errors.tolerance` property\n* Changed `max.num.retries` default to `1`\n* Improved the error messages for business key errors\n* Improved the error handling for `List` and JSON array configuration options\n* Updated to use the dot notation for filters in key update strategies ### Source Connector * Added support to output a key or value as a `Bson` byte type\n* Added support for schema and custom Avro schema definitions\n* Added support for dead letter queue and the `errors.tolerance` property\n* Added configurations for the following formatters:  \n   * `DefaultJson`  \n   * `ExtendedJson`  \n   * `SimplifiedJson`\n* Added configuration for `copy.existing.pipeline` to allow you to use indexes during the copying process\n* Added configuration for `copy.existing.namespace.regex` to allow you to copy the filtering of namespaces\n* Added configuration for `offset.partition.name` to allow for custom partitioning naming strategies\n* Updated to validate that the `fullDocument` field is a document\n* Updated to sanitize the connection string in the offset partition map to improve maintenance of the `connection.uri`, `database`, and`collection` parameters\n* Updated to disable publishing a source record without a topic name ### Bug Fixes * Stopped MongoDB 3.6 from copying existing issues when the collection didn't exist in the Source Connector ## What's New in 1.2 ### Sink Connector ## Important We deprecated the following post processors:\n\n If you are using one of these post processors, use the respective one instead for future compatibility: * `BlockListKeyProjector`\n* `BlockListValueProjector`,\n* `AllowListKeyProjector`\n* `AllowListValueProjector` * Added configurations for the following properties:  \n   * `document.id.strategy.overwrite.existing`  \n   * `UuidStrategy` output types  \n   * `document.id.strategy.partial.value.projection.type`  \n   * `document.id.strategy.partial.value.projection.list`  \n   * `document.id.strategy.partial.key.projection.type`  \n   * `document.id.strategy.partial.key.projection.list`  \n   * `UuidProvidedInKeyStrategy`  \n   * `UuidProvidedInValueStrategy`\n* Added the `UpdateOneBusinessKeyTimestampStrategy` post processor\n* Added built-in support for parallelism and scalable data copying by assigning topic partitions to tasks ### Source Connector * Improved the error messaging for missing resume tokens ### Bug Fixes * Removed failures with the `MongoCopyDataManager` when the source database does not exist\n* Fixed the copying the existing resumability error in the Source Connector ## What's New in 1.1 ### Sink Connector * Added support for the `topics.regex` property\n* Updated to ignore unused source record key or value fields\n* Added validation for the connection using `MongoSìnkConnector.validate` ### Source Connector * Added validation for the connection using `MongoSourceConnector.validate` ### Bug Fixes * Removed the `\"Unrecognized field: startAfter\"` error for resuming a change stream in the Source Connector ## What's New in 1.0 The initial GA release. ← [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Previous Section\")[Quick Start](https://mongodb.com/docs/kafka-connector/current/quick-start/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/docker-success/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) `| ...                               |\n| --------------------------------- |\n| Creating zookeeper ... done       |\n| Creating broker    ... done       |\n| Creating schema-registry ... done |\n| Creating connect         ... done |\n| Creating rest-proxy      ... done |\n| Creating mongo1          ... done |\n| Creating mongo1-setup    ... done | ` [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/cdc-handler/output/status-2/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) Kafka topics: ... The status of the connectors: sink | mongo-cdc-sink | RUNNING | RUNNING | com.mongodb.kafka.connect.MongoSinkConnector source | mongo-cdc-source | RUNNING | RUNNING | com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors \\[ \"mongo-cdc-sink\" \"mongo-cdc-source\" \\] ... [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/cdc-handler/output/status/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) Kafka topics: ... The status of the connectors: source | mongo-cdc-source | RUNNING | RUNNING | com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors \\[ \"mongo-cdc-source\" \\] ... [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/source-connector/output/kc/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) Partition: 0 Offset: 0 Key (198 bytes): {\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"{\"\\_id\": {\"\\_data\": \"8263496B53000000022B022C0100296E5A1004516A6011E8F74ADEA2D28F5138C12D4146645F6964006463496B5280D076018B3305360004\"}}\"} Value (572 bytes): {\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"{\"\\_id\": {\"\\_data\": \"8263496B53000000022B022C0100296E5A1004516A6011E8F74ADEA2D28F5138C12D4146645F6964006463496B5280D076018B3305360004\"}, \"operationType\": \"insert\", \"clusterTime\": {\"$timestamp\": {\"t\": 1665755987, \"i\": 2}}, \"wallTime\": {\"$date\": 1665755987015}, \"fullDocument\": {\"\\_id\": {\"$oid\": \"63496b5280d076018b330536\"}, \"order\\_id\": 1, \"item\": \"coffee\"}, \"ns\": {\"db\": \"Tutorial1\", \"coll\": \"orders\"}, \"documentKey\": {\"\\_id\": {\"$oid\": \"63496b5280d076018b330536\"}}}\"} [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/setup/status-setup/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) `| Kafka topics:                                                                                |\n| -------------------------------------------------------------------------------------------- |\n| \"topic\": \"docker-connect-status\",                                                            |\n| \"topic\": \"docker-connect-offsets\",                                                           |\n| \"topic\": \"docker-connect-configs\",                                                           |\n| \"topic\": \"__consumer_offsets\",                                                               |\n| The status of the connectors:                                                                |\n| Currently configured connectors                                                              |\n| []                                                                                           |\n| Version of MongoDB Connector for Apache Kafka installed:                                     |\n| {\"class\":\"com.mongodb.kafka.connect.MongoSinkConnector\",\"type\":\"sink\",\"version\":\"1.8.0\"}     |\n| {\"class\":\"com.mongodb.kafka.connect.MongoSourceConnector\",\"type\":\"source\",\"version\":\"1.8.0\"} | ` [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/time-series/output/kc/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) {\"schema\":{ ... }, \"payload\": \"{ \"\\_id\": { \"$oid\": \"628e9...\"}, \"company\\_symbol\": \"MSP\", \"Company\\_name\": \"MASSIVE SUBMARINE PARTNERS\", \"price\": 309.98, \"tx\\_time\": { \"$date\": 16535...\" }\"} [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/time-series/output/status/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) Kafka topics: ... The status of the connectors: source | mongo-source-marketdata | RUNNING | RUNNING | com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors \\[ \"mongo-source-marketdata\" \\] ... [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/includes/tutorials/sink-connector/output/kc/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) Kafka topics: ... The status of the connectors: sink | mongo-tutorial-sink | RUNNING | RUNNING | com.mongodb.kafka.connect.MongoSinkConnector Currently configured connectors \\[ \"mongo-tutorial-sink\" \\] ... [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Fundamentals Read the following sections to learn how MongoDB Kafka source connector features work and how to configure them: * [Receive Real-time Updates on Data Changes in MongoDB](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/)\n* [Apply Schemas to Documents](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/) ← [Specify a Schema](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/schema/ \"Previous Section\")[Change Streams](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Usage Examples Read the following sections to learn how to configure your MongoDB Kafka source connector to perform specific tasks: * [Filter and Transform Your MongoDB Change Stream with an Aggregation Pipeline](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/custom-pipeline/)\n* [Listen for Changes in Multiple MongoDB Collections](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/multiple-sources/)\n* [Customize the Name of the Topic to which your Source Connector Publishes Records](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/topic-naming/)\n* [Copy Data From a MongoDB Collection onto an Apache Kafka Topic](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/)\n* [Ensure Documents Processed by Your Source Connector Conform to a Schema](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/schema/) ← [All Source Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/all-properties/ \"Previous Section\")[Customize a Pipeline to Filter Change Events](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/custom-pipeline/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Copy Existing Data This usage example demonstrates how to copy data from a MongoDB collection to an Apache Kafka topic using the MongoDB Kafka source connector. ## Example Suppose you need to copy a MongoDB collection to Apache Kafka and filter some of the data. Your requirements and your solutions are as follows: | Requirement                                                                                                   | Solution                                                                                                   |\n| ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| Copy the customers collection of the shopping database in your MongoDB deployment onto an Apache Kafka topic. | See the [Copy Data](#std-label-source-usage-example-copy-existing-data-copy-data) section of this guide.   |\n| Only copy documents that have the value \"Mexico\" in the country field.                                        | See the [Filter Data](#std-label-source-usage-example-copy-existing-data-mask-data) section of this guide. | The `customers` collection contains the following documents:\n\n ` ### Copy Data Copy the contents of the `customers` collection of the `shopping` database by specifying the following configuration options in your source connector: ``` database=shopping collection=customers startup.mode=copy_existing \n``` Your source connector copies your collection by creating change event documents that describe inserting each document into your collection. ## Note ### Data Copy Can Produce Duplicate Events If any system changes the data in the database while the source connector converts existing data from it, MongoDB may produce duplicate change stream events to reflect the latest changes. Since the change stream events on which the data copy relies are idempotent, the copied data is eventually consistent. To learn more about change event documents, see the[Change Streams](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/#std-label-source-connector-fundamentals-change-event) guide. To learn more about the `startup.mode` option, see[Startup Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/#std-label-source-configuration-startup) ### Filter Data You can filter data by specifying an aggregation pipeline in the`startup.mode.copy.existing.pipeline` option of your source connector configuration. The following configuration specifies an aggregation pipeline that matches all documents with \"Mexico\" in the `country` field: ``` startup.mode.copy.existing.pipeline=[{ \"$match\": { \"country\": \"Mexico\" } }] \n``` To learn more about the `startup.mode.copy.existing.pipeline` option, see[Startup Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/#std-label-source-configuration-startup) To learn more about aggregation pipelines, see the following resources: * [Customize a Pipeline to Filter Change Events](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/custom-pipeline/#std-label-source-usage-example-custom-pipeline) Usage Example\n* [Aggregation](https://www.mongodb.com/docs/manual/aggregation/) in the MongoDB manual. ### Specify the Configuration Your final source connector configuration to copy the `customers` collection should look like this: ``` connector.class=com.mongodb.kafka.connect.MongoSourceConnector connection.uri=<your production MongoDB connection uri> database=shopping collection=customers startup.mode=copy_existing startup.mode.copy.existing.pipeline=[{ \"$match\": { \"country\": \"Mexico\" } }] \n``` Once your connector copies your data, you see the following change event document corresponding to the[preceding sample collection](#std-label-usage-example-copy-sample-document)in the `shopping.customers` Apache Kafka topic: ``` {   \"_id\": { \"_id\": 1, \"copyingData\": true },   \"operationType\": \"insert\",   \"documentKey\": { \"_id\": 1 },   \"fullDocument\": {     \"_id\": 1,     \"country\": \"Mexico\",     \"purchases\": 2,     \"last_viewed\": { \"$date\": \"2021-10-31T20:30:00.245Z\" }   },   \"ns\": { \"db\": \"shopping\", \"coll\": \"customers\" } } \n``` ## Note ### Write the Data in your Topic into a Collection Use a change data capture handler to convert change event documents in an Apache Kafka topic into MongoDB write operations. To learn more, see the[Change Data Capture Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/#std-label-sink-fundamentals-cdc-handler) guide. ← [Topic Naming](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/topic-naming/ \"Previous Section\")[Specify a Schema](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/schema/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/schema/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Specify a Schema This usage example demonstrates how you can configure your MongoDB Kafka source connector to apply a custom **schema** to your data. A schema is a definition that specifies the structure and type information about data in an Apache Kafka topic. Use a schema when you need to ensure the data on the topic populated by your source connector has a consistent structure. To learn more about using schemas with the connector, see the[Apply Schemas](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/#std-label-kafka-source-apply-schemas) guide. ## Example Suppose your application keeps track of customer data in a MongoDB collection, and you need to publish this data to a Kafka topic. You want the subscribers of the customer data to receive consistently formatted data. You choose to apply a schema to your data. Your requirements and your solutions are as follows: | Requirement                                     | Solution                                                                                                                                                                                       |\n| ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Receive customer data from a MongoDB collection | Configure a MongoDB source connector to receive updates to data from a specific database and collection.See [Receive Data from a Collection](#std-label-usage-example-schema-read-collection). |\n| Provide the customer data schema                | Specify a schema that corresponds to the structure and data types of the customer data.See [Create a Custom Schema](#std-label-usage-example-schema-custom-schema).                            |\n| Omit Kafka metadata from the customer data      | Include only the data from the fullDocument field.See [Omit Metadata from Published Records](#std-label-usage-example-schema-omit-metadata).                                                   | For the full configuration file that meets the requirements above, see[Specify the Configuration.](#std-label-usage-example-schema-config) ### Receive Data from a Collection To configure your source connector to receive data from a MongoDB collection, specify the database and collection name. For this example, you can configure the connector to read from the `purchases` collection in the`customers` database as follows: `| database=customers   |\n| -------------------- |\n| collection=purchases | ` ### Create a Custom Schema A sample customer data document from your collection contains the following information: ``` {   \"name\": \"Zola\",   \"visits\": [     {       \"$date\": \"2021-07-25T17:30:00.000Z\"     },     {       \"$date\": \"2021-10-03T14:06:00.000Z\"     }   ],   \"goods_purchased\": {     \"apples\": 1,     \"bananas\": 10   } } \n``` From the sample document, you decide your schema should present the fields using the following data types:\n\n You can describe your data using the Apache Avro schema format as shown in the example schema below: ``` {   \"type\": \"record\",   \"name\": \"Customer\",   \"fields\": [{       \"name\": \"name\",       \"type\": \"string\"     },{       \"name\": \"visits\",       \"type\": {         \"type\": \"array\",         \"items\": {           \"type\": \"long\",           \"logicalType\": \"timestamp-millis\"         }       }     },{       \"name\": \"goods_purchased\",       \"type\": {         \"type\": \"map\",         \"values\": \"int\"       }     }   ] } \n``` ## Important ### Converters If you want to send your data through Apache Kafka with Avro binary encoding, you must use an Avro converter. For more information, see the guide on[Converters.](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-intro-converters) ### Omit Metadata from Published Records The connector publishes the customer data documents and metadata that describes the document to a Kafka topic. You can set the connector to include only the document data contained in the `fullDocument` field of the record using the following setting: ``` publish.full.document.only=true \n``` For more information on the `fullDocument` field, see the[Change Streams](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/#std-label-source-connector-fundamentals-change-event) guide. ### Specify the Configuration Your custom schema connector configuration should resemble the following: ``` connector.class=com.mongodb.kafka.connect.MongoSourceConnector connection.uri=<your MongoDB connection URI> database=customers collection=purchases publish.full.document.only=true output.format.value=schema output.schema.value={\\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"Customer\\\", \\\"fields\\\": [{\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"}, {\\\"name\\\": \\\"visits\\\", \\\"type\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"long\\\", \\\"logicalType\\\": \\\"timestamp-millis\\\"}}}, {\\\"name\\\": \\\"goods_purchased\\\", \\\"type\\\": {\\\"type\\\": \\\"map\\\", \\\"values\\\": \\\"int\\\"}}]} value.converter.schemas.enable=true value.converter=org.apache.kafka.connect.json.JsonConverter key.converter=org.apache.kafka.connect.storage.StringConverter \n``` ## Note ### Embedded Schema In the preceding configuration, the Kafka Connect JSON Schema Converter embeds the custom schema in your messages. To learn more about the JSON Schema converter, see the[Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-json-schema-converter-sample-properties) guide. For more information on specifying schemas, see the [Apply Schemas](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/#std-label-kafka-source-apply-schemas) guide. ← [Copy Existing Data](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/ \"Previous Section\")[Fundamentals](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/multiple-sources/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Listen for Changes on Multiple Sources This usage example demonstrates how to configure a MongoDB Kafka source connector to listen for change events on multiple MongoDB collections, and publish them to a Kafka topic. If you need your connector to listen for change events on a more particular set of databases and collections, you can use a **pipeline**. A pipeline is a MongoDB aggregation pipeline composed of instructions to the database to filter or transform data. See the next section for examples of how to configure your connector `pipeline` setting to match multiple database and collection names using a regular expression. ## Note The `database` and `collection` configuration settings also affect which databases and collections on which the connector listens for change events. To learn more about these settings, see the[MongoDB Source Connection Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/mongodb-connection/#std-label-source-configuration-mongodb-connection) guide. ## Examples The following examples show you how to use an aggregation pipeline to select specific database or collection names on which to listen for change events. ### Include Change Events from Multiple Databases You can define an aggregation pipeline to select only change events on multiple databases by specifying the following in the `pipeline`setting: * A `$match` aggregation operator\n* The `ns.db`, field which identifies the database part of the namespace\n* The `$regex` operator and a regular expression that matches the database names The following sample configuration shows how you can set your source connector to listen for change events on the `sandbox` and `firewall` databases: `| pipeline=[{\"$match\": {\"ns.db\": {\"$regex\": \"/^(sandbox\\|firewall)$/\"}}}] |\n| ----------------------------------------------------------------------- | ` ### Exclude Change Events from Multiple Collections You can define an aggregation pipeline to ignore change events on multiple collections by specifying the following in the `pipeline`setting: * A `$match` aggregation operator\n* The `ns.coll` field, which identifies the collection part of the namespace\n* The `$regex` operator and a regular expression that matches the collection names\n* The `$not` operator which instructs the enclosing `$regex` operator to match everything the regular expression does not match The following sample configuration shows how you can set your source connector to filter out change events that originate from all collections named \"hyperspace\" in any database: ``` pipeline=[{\"$match\": {\"ns.coll\": {\"$regex\": {\"$not\": \"/^hyperspace$/\"}}}}] \n``` ## Additional Information * The [$match aggregation operator](https://www.mongodb.com/docs/manual/reference/operator/aggregation/match/)\n* [MongoDB change events](https://www.mongodb.com/docs/manual/reference/change-events/)\n* [MongoDB namespace](https://www.mongodb.com/docs/manual/reference/limits/#faq-dev-namespace)\n* Regular expression syntax using the [Patterns class](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html)\n* [$not logical query operator and regular expressions](https://www.mongodb.com/docs/manual/reference/operator/query/not/#-not-and-regular-expressions) ← [Customize a Pipeline to Filter Change Events](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/custom-pipeline/ \"Previous Section\")[Topic Naming](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/topic-naming/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Source Connector Configuration Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview In this section, you can read descriptions of MongoDB Kafka source connector properties, including essential Confluent Kafka Connect settings and connector-specific settings. For an example source connector configuration file, see[MongoSourceConnector.properties.](https://github.com/mongodb/mongo-kafka/blob/master/config/MongoSourceConnector.properties) For source connector performance recommendations, see [Tuning the Source Connector.](https://www.mongodb.com/developer/products/connectors/tuning-mongodb-kafka-connector/#tuning-the-source-connector) ## Settings See the following categories for a list of related configuration properties:\n\n See the [Confluent Source Connector configuration documentation](https://docs.confluent.io/platform/current/installation/configuration/connect/source-connect-configs.html)for more information on these settings. ← [Source Connector](https://mongodb.com/docs/kafka-connector/current/source-connector/ \"Previous Section\")[MongoDB Source Connection Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/mongodb-connection/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/all-properties/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # All Source Connector Configuration Properties On this page * [Overview](#overview)\n* [MongoDB Connection](#mongodb-connection)\n* [Kafka Topic](#kafka-topic)\n* [Change Streams](#change-streams)\n* [Output Format](#output-format)\n* [Startup](#startup)\n* [Error Handling and Resuming from Interruption](#error-handling-and-resuming-from-interruption) ## Overview On this page, you can view all available configuration properties for your MongoDB Kafka source connector. This page duplicates the content of the other source connector configuration properties pages. To view a list of all source connector configuration properties pages, see the [Source Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/#std-label-source-configuration-index) page. ## MongoDB Connection Use the following configuration settings to specify how your MongoDB Kafka source connector establishes a connection and communicates with your MongoDB cluster. To view only the options related to your MongoDB connection, see the[MongoDB Source Connection Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/mongodb-connection/#std-label-source-configuration-mongodb-connection) page.\n\n ## Kafka Topic Use the following configuration settings to specify which Kafka topics the MongoDB Kafka source connector should publish data to. To view only the options related to your Kafka topic, see the[Kafka Topic Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/kafka-topic/#std-label-source-configuration-kafka-topic) page.\n\n ## Change Streams Use the following configuration settings to specify aggregation pipelines for change streams and read preferences for change stream cursors when working with the MongoDB Kafka source connector. To view only the options related to change streams, see the[Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/#std-label-source-configuration-change-stream) page.\n\n ## Output Format Use the following configuration settings to specify the format of data the MongoDB Kafka source connector publishes to Kafka topics. To view only the options related to the format of your output, see the[Output Format Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/output-format/#std-label-source-configuration-output-format) page.\n\n ## Startup Use the following configuration settings to configure startup of the MongoDB Kafka source connector to convert MongoDB collections into Change Stream events. To view only the options related to startup, see the[Startup Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/#std-label-source-configuration-startup) page.\n\n ## Error Handling and Resuming from Interruption Use the following configuration settings to specify how the MongoDB Kafka source connector behaves when it encounters errors and to specify settings related to resuming interrupted reads. To view only the options related to handling errors, see the[Error Handling and Resuming from Interruption Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/#std-label-source-configuration-error-handling) page.\n\n ← [Error Handling and Resuming from Interruption Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/ \"Previous Section\")[Usage Examples](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/topic-naming/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Topic Naming The examples on this page show how to configure your MongoDB Kafka source connector to customize the name of the topic to which it publishes records. By default, the MongoDB Kafka source connector publishes change event data to a Kafka topic with the same name as the MongoDB **namespace** from which the change events originated. A namespace is a string that's composed of the database and collection name concatenated with a dot \".\" character. The following examples show different ways that you can customize the Kafka topics to which the connector publishes change event data: * [Topic Prefix](#std-label-topic-naming-prefix-example)\n* [Topic Suffix](#std-label-topic-naming-suffix-example)\n* [Topic Namespace Map](#std-label-topic-naming-namespace-map-example)\n* [Topic Namespace Map with Wildcard](#std-label-topic-naming-namespace-map-wildcard-example) ## Topic Prefix Example You can configure your source connector to prepend a string to the namespace of the change event data, and publish records to that Kafka topic. This setting automatically concatenates your prefix with your namespace with the \".\" character. To specify the topic prefix, use the `topic.prefix` configuration setting as shown in the following example: `| topic.prefix=myPrefix |\n| --------------------- |\n| database=test         |\n| collection=data       | ` Once set, your connector publishes any changes to the `data` collection in the `test` database to the Kafka topic named `myPrefix.test.data`. ## Topic Suffix Example You can configure your source connector to append a string to the namespace of the change event data, and publish records to that Kafka topic. This setting automatically concatenates your namespace with your suffix with the \".\" character. To specify the topic suffix, use the `topic.suffix` configuration setting as shown in the following example: ``` topic.suffix=mySuffix database=test collection=data \n``` Once set, your connector publishes any changes to the `data` collection in the `test` database to the Kafka topic named `test.data.mySuffix`. ## Topic Namespace Map Example You can configure your source connector to map namespace values to Kafka topic names for incoming change event data. If the database name or namespace of the change event matches one of the fields in the map, the connector publishes the record to the value that corresponds to that mapping. If the database name or namespace of the change event do not match any mapping, the connector publishes the record using the default topic naming scheme unless otherwise specified by a different topic naming setting. Any mapping that includes both database and collection takes precedence over mappings that only specify the source database name. ## Important The namespace map matching occurs before the connector applies any other topic naming setting. If defined, the connector applies the`topic.prefix` and the `topic.suffix` settings to the topic name after the mapping. The following example shows how to specify the `topic.namespace.map`setting to define a topic namespace mappings from the `carDb` database to the `automobiles` topic and the `carDb.ev` namespace to the`electricVehicles` topic: ``` topic.namespace.map={\"carDb\": \"automobiles\", \"carDb.ev\": \"electricVehicles\"} \n``` Since the `carDb.ev` namespace mapping takes precedence over the `carDb`mapping, the connector performs the following actions: * If the change event came from the database `carDb` and collection `ev`, the connector sets the destination to the `electricVehicles` topic.\n* If the change event came from the database `carDb` and a collection other than `ev`, the connector sets the destination to the`automobiles.<collectionName>`topic.\n* If the change document came from any database other than `carDb`, the connector sets the destination topic to the default namespace naming scheme.\n* If defined, the connector applies the `topic.prefix` and`topic.suffix` settings to the destination topic name after it performs namespace mapping. ## Topic Namespace Map with Wildcard Example\n\n ``` topic.namespace.map={\"carDb\": \"automobiles\", \"carDb.ev\": \"electricVehicles\", \"*\": \"otherVehicles\"} \n``` In the preceding wildcard example, the connector publishes change documents that originated from all databases other than `carDb` to the`otherVehicles` topic. ← [Listen for Changes on Multiple Sources](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/multiple-sources/ \"Previous Section\")[Copy Existing Data](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/custom-pipeline/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Customize a Pipeline to Filter Change Events This usage example demonstrates how to configure a **pipeline** to customize the data that your MongoDB Kafka source connector consumes. A pipeline is a MongoDB aggregation pipeline composed of instructions to the database to filter or transform data. MongoDB notifies the connector of data changes that match your aggregation pipeline on a **change stream**. A change stream is a sequence of events that describe data changes a client made to a MongoDB deployment in real-time. For more information, see the MongoDB Server manual entry on[Change Streams.](https://www.mongodb.com/docs/manual/changeStreams/) ## Example Suppose you're an event coordinator who needs to collect names and arrival times of each guest at a specific event. Whenever a guest checks into the event, an application inserts a new document that contains the following details: `| {                                       |\n| --------------------------------------- |\n| \"_id\": ObjectId(...),                   |\n| \"eventId\": 321,                         |\n| \"name\": \"Dorothy Gale\",                 |\n| \"arrivalTime\": 2021-10-31T20:30:00.245Z |\n| }                                       | ` You can define your connector `pipeline` setting to instruct the change stream to filter the change event information as follows: * Create change events for insert operations and omit events for all other types of operations.\n* Create change events only for documents that match the `fullDocument.eventId`value \"321\" and omit all other documents.\n* Omit the `_id` and `eventId` fields from the `fullDocument` object using a projection. To apply these transformations, assign the following aggregation pipeline to your `pipeline` setting: ``` pipeline=[{\"$match\": { \"$and\": [{\"operationType\": \"insert\"}, { \"fullDocument.eventId\": 321 }] } }, {\"$project\": { \"fullDocument._id\": 0, \"fullDocument.eventId\": 0 } } ] \n``` ## Important Make sure that the results of the pipeline contain the top-level `_id`field of the `payload` object, which MongoDB uses as the value of the[resume token.](https://www.mongodb.com/docs/manual/changeStreams/#resume-a-change-stream) When the application inserts the sample document, your configured connector publishes the following record to your Kafka topic: ``` {   ...   \"payload\": {     _id: { _data: ... },     \"operationType\": \"insert\",     \"fullDocument\": {       \"name\": \"Dorothy Gale\",       \"arrivalTime\": \"2021-10-31T20:30:00.245Z\",     },     \"ns\": { ... },     \"documentKey\": {       _id: {\"$oid\": ... }     }   } } \n``` For more information on managing change streams with the source connector, see the connector documentation on [Change Streams.](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/#std-label-kafka-source-change-streams) ← [Usage Examples](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/ \"Previous Section\")[Listen for Changes on Multiple Sources](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/multiple-sources/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Startup Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to configure startup of the MongoDB Kafka source connector to convert MongoDB collections into Change Stream events. ## Tip For an example using the copy existing feature, see the[Copy Existing Data](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/#std-label-source-usage-example-copy-existing-data) Usage Example. For a list of source connector configuration settings organized by category, see the guide on [Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/) ## Settings\n\n ← [Output Format Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/output-format/ \"Previous Section\")[Error Handling and Resuming from Interruption Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Change Stream Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify aggregation pipelines for change streams and read preferences for change stream cursors when working with the MongoDB Kafka source connector. For a list of source connector configuration settings organized by category, see the guide on [Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/) ## Settings\n\n ← [Kafka Topic Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/kafka-topic/ \"Previous Section\")[Output Format Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/output-format/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/monitoring/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Monitoring On this page * [Overview](#overview)\n* [Use Cases](#use-cases)\n* [Sink Connector](#sink-connector)\n* [Source Connector](#source-connector)\n* [Monitor the Connector](#monitor-the-connector)\n* [Enable Monitoring](#enable-monitoring)\n* [Types of Metrics](#types-of-metrics)\n* [JMX Paths](#jmx-paths)\n* [Example - Monitor the Quick Start](#example---monitor-the-quick-start)\n* [Download JConsole.](#download-jconsole.)\n* [Start the Quick Start pipeline and add connectors.](#start-the-quick-start-pipeline-and-add-connectors.)\n* [Start JConsole.](#start-jconsole.)\n* [Connect to the Kafka Connect JMX server.](#connect-to-the-kafka-connect-jmx-server.)\n* [Explore your connectors' metrics.](#explore-your-connectors--metrics.)\n* [Stop and remove the Quick Start environment.](#stop-and-remove-the-quick-start-environment.)\n* [Available Metrics](#available-metrics)\n* [Sink Connector JMX Metrics](#sink-connector-jmx-metrics)\n* [Source Connector JMX Metrics](#source-connector-jmx-metrics) ## Overview Learn how to observe the behavior of your MongoDB Kafka source connector or MongoDB Kafka sink connector through **monitoring**. Monitoring is the process of getting information about the activities a running program performs for use in an application or an application performance management library. To learn how monitoring works in the connector and how to use it, see the [Use Cases](#std-label-kafka-monitoring-information) section. To view an example that shows how to monitor a running connector, see the [Example - Monitor the Quick Start](#std-label-kafka-monitoring-example) section. To view a list of all metrics produced by MongoDB source and sink connectors, see the [Available Metrics](#std-label-kafka-monitoring-all-attributes) section. ## Use Cases This section describes use cases for monitoring MongoDB source and sink connectors, and how you can use the metrics your connector provides to satisfy those use cases. ## Tip ### Computed Values To learn what types of metrics the connector provides and when you must implement logic to compute a value, see[Types of Metrics.](#std-label-kafka-monitoring-types-of-metrics) ### Sink Connector The following table describes some use cases for monitoring the MongoDB sink connector and the metrics the sink connector provides to satisfy those use cases:\n\n You can find descriptions of all MongoDB sink connector metrics in the[Sink Connector JMX Metrics](#std-label-kafka-monitoring-sink-attributes) section. ### Source Connector The following table describes some use cases for monitoring the MongoDB source connector and the metrics the source connector provides to satisfy those use cases:\n\n You can find descriptions of all MongoDB source connector metrics in the[Source Connector JMX Metrics](#std-label-kafka-monitoring-source-attributes) section. ## Monitor the Connector The MongoDB Kafka Connector uses **Java Management Extensions (JMX)** to enable monitoring. JMX is a technology included in the Java Platform, Standard Edition that provides tools to monitor applications and devices. You can view the metrics produced by the connector with any JMX console, such as JConsole. The MongoDB Kafka Connector provides metrics for individual **tasks**. Tasks are classes instantiated by Kafka Connect that copy data to and from datastores and Apache Kafka. The names and responsibilities of the two types of tasks in Kafka Connect are as follows: * A source task copies data from a data store to Apache Kafka.\n* A sink task copies data from Apache Kafka to a data store. A sink connector configures one or more sink tasks. A source connector configures one or more source tasks. To learn more about JMX, see the following resources from Oracle: * [Java Management Extensions Guide](https://docs.oracle.com/en/java/javase/17/jmx/introduction-jmx-technology.html)\n* [Using JConsole](https://docs.oracle.com/en/java/javase/17/management/using-jconsole.html) To learn more about tasks and connectors in Kafka Connect, see the following resources: * [Kafka Connect Concepts from Confluent](https://docs.confluent.io/platform/current/connect/concepts.html#tasks)\n* [Kafka Connect API documentation for the Task interface](https://kafka.apache.org/32/javadoc/index.html?org/apache/kafka/connect/connector/Task.html)\n* [Kafka Connect API documentation for the Connector abstract class](https://kafka.apache.org/32/javadoc/org/apache/kafka/connect/connector/Connector.html) ### Enable Monitoring The MongoDB Kafka Connector uses Kafka Connect's metrics infrastructure to serve metrics. To read the metrics produced by your connector, enable JMX in your Kafka Connect deployment. To learn how to enable JMX for a Kafka Connect instance running on your host machine, see the[Official Kafka Documentation.](https://kafka.apache.org/documentation/#monitoring) To learn how to enable JMX for a containerized Kafka Connect deployment, see[Kafka Monitoring and Metrics Using JMX with Docker.](https://docs.confluent.io/platform/current/installation/docker/operations/monitoring.html) ### Types of Metrics The connector provides metrics related to the following types of quantities: * The number of times an event has occurred in total for a connector task\n* The value related to the most recent occurrence of an event For some use cases, you must perform extra computations with the metrics the connector provides. For example, you must compute the following values from provided metrics: * The rate of change of a metric\n* The value of a metric over a span of time\n* The difference between one metric and another metric To view some examples of computed metrics, see the[Use Cases](#std-label-kafka-monitoring-use-cases) section. ### JMX Paths The MongoDB Kafka Connector and Kafka Connect both produce metrics for MongoDB connector tasks. Both sets of metrics provide information about how your tasks interact with Kafka Connect, but only the MongoDB Kafka Connector metrics provide information about how your tasks interact with MongoDB. The MongoDB Kafka Connector produces metrics under the following JMX paths: * `com.mongodb.kafka.connect.sink-task-metrics.sink-task-<monitonically increasing number>`\n* `com.mongodb.kafka.connect.source-task-metrics.source-task-<monitonically increasing number>`\n* `com.mongodb.kafka.connect.source-task-metrics.source-task-change-stream-<monitonically increasing number>`\n* `com.mongodb.kafka.connect.source-task-metrics.source-task-copy-existing-<monitonically increasing number>` Kafka Connect produces metrics under the following JMX paths:\n\n To relate Kafka Connect metrics to MongoDB Kafka Connector metrics, you must remember the order in which you added your connectors to Kafka Connect. ## Note ### Naming Conflicts If the MongoDB Kafka Connector ever encounters a naming conflict when it attempts to register an `MBean` on a JMX path, the MongoDB Kafka Connector adds a version suffix to the `MBean`. For example, if the connector tries to register an `MBean` under the path`com.mongodb.kafka.connect.sink-task-metrics.sink-task-0` and is unable to do so, it attempts to register the `MBean` under`com.mongodb.kafka.connect.sink-task-metrics.sink-task-0-v1`. #### Example Assume you add a single MongoDB source connector named `my-source-connector` to your deployment. The MongoDB source connector writes metrics to the following JMX path: * `com.mongodb.kafka.connect.sink-task-metrics.sink-task-0` Kafka Connect writes metrics for this task under the following path: * `kafka.connect.sink-task-metrics.my-source-connector` ## Example - Monitor the Quick Start The sample environment provided in the Quick Start exposes metrics on your host machine at the URI `localhost:35000`. To view these metrics with JConsole, perform the following actions: 1 ### Download JConsole. JConsole is part of the Java Platform, Standard Edition. To download JConsole, download the[Java SE Development Kit](https://www.oracle.com/java/technologies/downloads/)from Oracle. 2 ### Start the Quick Start pipeline and add connectors. Follow the Quick Start until the [Send the Contents of a Document through Your Connectors](https://mongodb.com/docs/kafka-connector/current/quick-start/#std-label-kafka-quick-start-send-a-document)step. 3 ### Start JConsole. Run the following command from your command line to start JConsole: ```shell jconsole \n``` 4 ### Connect to the Kafka Connect JMX server. 1 #### Enter your JMX Server URI. Enter the URI `localhost:35000` into the Remote Processtext input box in the JConsole interface. 2 #### Click Connect. 3 #### In the dialog box, click Insecure Connection. 5 ### Explore your connectors' metrics. 1 #### Navigate to the MBeans tab in JConsole. 2 #### Inspect connector metrics. Notice that the `com.mongodb.kafka.connect.sink-task-metrics.sink-task-0.records`attribute has a value of `0`. This value indicates that your sink task has not recieved any records from Apache Kafka. 3 #### Continue the Quick Start. Continue the Quick Start until, but not through, the[Remove the Sandbox](https://mongodb.com/docs/kafka-connector/current/quick-start/#std-label-kafka-quickstart-remove-the-sandbox) step. 4 #### Return to JConsole. Navigate back to the MBeans tab in JConsole. The `com.mongodb.kafka.connect.sink-task-metrics.sink-task-0.records`attribute should now have a value of `1`. 6 ### Stop and remove the Quick Start environment. To stop and remove the Quick Start environment, follow the[Remove the Sandbox](https://mongodb.com/docs/kafka-connector/current/quick-start/#std-label-kafka-quickstart-remove-the-sandbox) step of the Quick Start. ## Available Metrics Use the attributes in the tables in this section to monitor the behavior of your source and sink connectors through Java Management Extensions (JMX). ## Tip ### JMX Attributes JMX represents an individual metric as an attribute of an `MBean`. To learn more about attributes and `MBeans`, see the[Standard MBeans Tutorial](https://docs.oracle.com/javase/tutorial/jmx/mbeans/standard.html)from Oracle. ## Note ### Poll and Put Methods A MongoDB source connector task has a `poll()` method to retrieve documents from MongoDB and send them to Apache Kafka. A MongoDB sink connector task has a `put()` method to retrieve documents from Apache Kafka and send them to MongoDB. To learn more about `poll()` and `put()` methods, see the following resources:\n\n ### Sink Connector JMX Metrics\n\n ### Source Connector JMX Metrics ## Note ### Initial Commands and getMore Commands Some metrics for source connector tasks distinguish between**initial commands** and `getMore` commands. An initial command is a `find` or `aggregate` command sent to a MongoDB instance that retrieves the first set of documents in a client-side MongoDB cursor. The `getMore` command is the MongoDB command that fetches the subsequent sets of documents in a cursor. To learn more about `getMore` commands, see the[getMore](https://www.mongodb.com/docs/manual/reference/command/getMore/) page.\n\n ← [MongoDB AWS-based Authentication](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/mongodb-aws-auth/ \"Previous Section\")[Migrate from Kafka Connect MongoDB](https://mongodb.com/docs/kafka-connector/current/migrate-from-kafka-connect-mongodb/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/kafka-topic/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Kafka Topic Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify which Kafka topics the MongoDB Kafka source connector should publish data to. For a list of source connector configuration settings organized by category, see the guide on [Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/) ## Settings\n\n ← [MongoDB Source Connection Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/mongodb-connection/ \"Previous Section\")[Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/mongodb-connection/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # MongoDB Source Connection Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify how your MongoDB Kafka source connector establishes a connection and communicates with your MongoDB cluster. For a list of source connector configuration settings organized by category, see the guide on [Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/) ## Settings\n\n ← [Source Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/ \"Previous Section\")[Kafka Topic Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/kafka-topic/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/copy-existing/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Copy Existing Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview ## Important ### copy.existing\\* Properties are Deprecated Starting in Version 1.9 of the MongoDB Kafka Connector, `copy.existing*` properties are deprecated and may be removed in a future release. You should use`startup.mode*` properties to configure the copy existing feature. To learn about `startup.mode*` settings, see[Startup Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/#std-label-source-configuration-startup) Use the following configuration settings to enable the copy existing feature which converts MongoDB collections into Change Stream events. ## Tip ### See also:  For an example of the copy existing feature, see the[Copy Existing Data](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/#std-label-source-usage-example-copy-existing-data) Usage Example. For a list of source connector configuration settings organized by category, see the guide on [Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/) ## Settings\n\n [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Error Handling and Resuming from Interruption Properties On this page * [Overview](#overview)\n* [Settings](#settings)\n* [Heartbeats with Single Message Transforms](#heartbeats-with-single-message-transforms) ## Overview Use the following configuration settings to specify how the MongoDB Kafka source connector behaves when it encounters errors and to specify settings related to resuming interrupted reads. ## Settings\n\n ## Heartbeats with Single Message Transforms If you enable heartbeats and specify **Single Message Transforms (SMTs)** in your Kafka Connect deployment, you must exclude your heartbeat messages from your SMTs. SMTs are a feature of Kafka Connect that enables you to specify transformations on the messages that pass through your source connector without having to deploy a stream processing application. To exclude heartbeat messages from your SMTs, you must create and apply a**predicate** to your SMTs. Predicates are a feature of SMTs that enables you to check if a message matches a conditional statement before applying a transformation. The following configuration defines the `IsHeartbeat` predicate which matches heartbeat messages sent to the default heartbeat topic: `| predicates=IsHeartbeat                                                                      |\n| ------------------------------------------------------------------------------------------- |\n| predicates.IsHeartbeat.type=org.apache.kafka.connect.transforms.predicates.TopicNameMatches |\n| predicates.IsHeartbeat.pattern=__mongodb_heartbeats                                         | ` The following configuration uses the preceding predicate to exclude heartbeat messages from an `ExtractField` transformation: ``` transforms=Extract transforms.Extract.type=org.apache.kafka.connect.transforms.ExtractField$Key transforms.Extract.field=<the field to extract from your Apache Kafka key> transforms.Extract.predicate=IsHeartbeat transforms.Extract.negate=true # apply the default key schema as the extract transformation requires a struct object output.format.key=schema \n``` If you do not exclude your heartbeat messages from the preceding transformation, your connector raises the following error once it processes a heartbeat message: ``` ERROR WorkerSourceTask{id=mongo-source-0} Task threw an uncaught and unrecoverable exception. Task is being killed ... ... Only Struct objects supported for [field extraction], found: java.lang.String \n``` To learn more about SMTs, see[How to Use Single Message Transforms in Kafka Connect](https://www.confluent.io/blog/kafka-connect-single-message-transformation-tutorial-with-examples/)from Confluent. To learn more about predicates, see[Filter (Apache Kafka)](https://docs.confluent.io/platform/current/connect/transforms/filter-ak.html#predicates)from Confluent. To learn more about the `ExtractField` transformation, see[ExtractField](https://docs.confluent.io/platform/current/connect/transforms/extractfield.html)from Confluent. To learn more about the default key schema, see the[Default Schemas](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/#std-label-kafka-source-apply-schemas-default-schema) page. ← [Startup Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/ \"Previous Section\")[All Source Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/all-properties/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Change Streams On this page * [Overview](#overview)\n* [Change Streams](#change-streams-1)\n* [Aggregation](#aggregation)\n* [Change Event Structure](#change-event-structure)\n* [Performance](#performance)\n* [Source Connectors](#source-connectors)\n* [Resume Tokens](#resume-tokens) ## Overview In this guide, you can learn about **change streams** and how they are used in a MongoDB Kafka source connector. ## Change Streams Change streams are a MongoDB feature that allow you to receive real-time updates on data changes. Change streams return **change event documents**. A change event document contains idempotent instructions to describe a change that occurred in your MongoDB deployment and metadata related to that change. Change event documents are generated from data in the [oplog.](https://www.mongodb.com/docs/manual/reference/glossary/#std-term-oplog) ## Important ### Change streams only run on MongoDB replica sets and sharded clusters A standalone MongoDB instance cannot produce a change stream. To view a list of all configuration options for change streams, see the[Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/#std-label-source-configuration-change-stream) page. To learn more about change streams, see the following resources: * [Change Streams](https://www.mongodb.com/docs/manual/changeStreams/) in the MongoDB manual\n* [An Introduction to Change Streams](https://www.mongodb.com/blog/post/an-introduction-to-change-streams)blog post To learn more about the oplog, see the MongoDB manual entry on the[Replica Set Oplog.](https://www.mongodb.com/docs/manual/core/replica-set-oplog/) ### Aggregation Use an aggregation pipeline to configure your source connector's change stream. Some of the ways you can configure your connector's change stream are as follows: * Filter change events by operation type\n* Project specific fields\n* Update the value of fields\n* Add fields\n* Trim the amount of data generated by the change stream To learn which aggregation operators you can use with a change stream, see the [Modify Change Stream Output](https://www.mongodb.com/docs/manual/changeStreams/#modify-change-stream-output)guide in the MongoDB manual. To view examples that use an aggregation pipeline to modify a change stream, see the following pages: * [Customize a Pipeline to Filter Change Events](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/custom-pipeline/#std-label-source-usage-example-custom-pipeline) Usage Example\n* [Copy Existing Data](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/copy-existing-data/#std-label-source-usage-example-copy-existing-data) Usage Example ### Change Event Structure Find the complete structure of change event documents, including descriptions of all fields,[in the MongoDB manual.](https://www.mongodb.com/docs/manual/reference/change-events/#std-label-change-stream-output) ## Note ### The Full Document Option If you want Kafka Connect to receive just the document created or modified from your change operation, use the `publish.full.document.only=true`option. For more information, see the [Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/#std-label-source-configuration-change-stream)page. ### Performance The oplog is a special capped collection which cannot use indexes. For more information on this limitation, see[Change Streams Production Recommendations.](https://www.mongodb.com/docs/manual/administration/change-streams-production-recommendations/#indexes)\n\n ## Source Connectors The source connector works by opening a single change stream with MongoDB and sending data from that change stream to Kafka Connect. Your source connector maintains its change stream for the duration of its runtime, and your connector closes its change stream when you stop it. To view the available options to configure your source connector's change stream, see the [Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/#std-label-source-configuration-change-stream) page. ### Resume Tokens Your connector uses a **resume token** as its **offset**. An offset is a value your connector stores in an Apache Kafka topic to keep track of what source data it has processed. Your connector uses its offset value when it must recover from a restart or crash. A resume token is a piece of data that references the`_id` field of a change event document in your MongoDB oplog. If your source connector does not have an offset, such as when you start the connector for the first time, your connector starts a new change stream. Once your connector receives its first change event document and publishes that document to Apache Kafka, your connector stores the resume token of that document as its offset. If the resume token value of your source connector's offset does not correspond to any entry in your MongoDB deployment's oplog, your connector has an invalid resume token. To learn how to recover from an invalid resume token, see the[invalid token troubleshooting guide.](https://mongodb.com/docs/kafka-connector/current/troubleshooting/recover-from-invalid-resume-token/#std-label-kafka-troubleshoot-invalid-resume-token) To learn more about resume tokens, see the following resources: * [Resume a Change Stream](https://www.mongodb.com/docs/manual/changeStreams/#resume-a-change-stream)in the MongoDB manual\n* [Change Events](https://www.mongodb.com/docs/manual/reference/change-events/#std-label-change-stream-event-id)in the MongoDB manual To learn more about offsets, see the following resources: * Kafka Connect `offset.storage.topic` [configuration option documentation](https://docs.confluent.io/platform/current/installation/configuration/connect/index.html#offset-storage-topic)\n* Kafka Connect `OffsetStorageReader` [API documentation](https://kafka.apache.org/0110/javadoc/org/apache/kafka/connect/storage/OffsetStorageReader.html) ← [Fundamentals](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/ \"Previous Section\")[Apply Schemas](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/output-format/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Output Format Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify the format of data the MongoDB Kafka source connector publishes to Kafka topics. For a list of source connector configuration settings organized by category, see the guide on [Source Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/) ## Settings\n\n ← [Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/ \"Previous Section\")[Startup Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Apply Schemas On this page * [Overview](#overview)\n* [Default Schemas](#default-schemas)\n* [Key Schema](#key-schema)\n* [Value Schema](#value-schema)\n* [Schemas For Transformed Documents](#schemas-for-transformed-documents)\n* [Specify Schemas](#specify-schemas)\n* [Infer a Schema](#infer-a-schema) ## Overview In this guide, you can learn how to apply schemas to incoming documents in a MongoDB Kafka source connector. There are two types of schema in Kafka Connect, **key schema** and**value schema**. Kafka Connect sends messages to Apache Kafka containing both your value and a key. A key schema enforces a structure for keys in messages sent to Apache Kafka. A value schema enforces a structure for values in messages sent to Apache Kafka. ## Important ### Note on Terminology The word \"key\" has a slightly different meaning in the context of BSON and Apache Kafka. In BSON, a \"key\" is a unique string identifier for a field in a document. In Apache Kafka, a \"key\" is a byte array sent in a message used to determine what partition of a topic to write the message to. Kafka keys can be duplicates of other keys or `null`. Specifying schemas in the connector is optional, and you can specify any of the following combinations of schemas: * Only a value schema\n* Only a key schema\n* Both a value and key schema\n* No schemas ## Tip ### Benefits of Schema To see a discussion on the benefits of using schemas with Kafka Connect, see [this article from Confluent.](https://docs.confluent.io/platform/current/schema-registry/index.html#ak-serializers-and-deserializers-background) If you want to send data through Apache Kafka with a specific data format, such as Apache Avro or JSON Schema, see the [Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-intro-converters) guide. To learn more about keys and values in Apache Kafka, see the[official Apache Kafka introduction.](http://kafka.apache.org/intro#intro%5Fconcepts%5Fand%5Fterms) ## Default Schemas The connector provides two default schemas: * [A key schema for the \\_id field of MongoDB change event documents.](#std-label-source-default-key-schema)\n* [A value schema for MongoDB change event documents.](#std-label-source-default-value-schema) To learn more about change events, see our[guide on change streams.](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/) To learn more about default schemas, see the default schemas[here in the MongoDB Kafka Connector source code.](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/source/schema/AvroSchemaDefaults.java) ### Key Schema The connector provides a default key schema for the `_id` field of change event documents. You should use the default key schema unless you remove the`_id` field from your change event document using either of the transformations[described in this guide here.](#std-label-source-schema-for-modified-document) If you specify either of these transformations and want to use a key schema for your incoming documents, you must specify a key schema[as described in the specify a schema section of this guide.](#std-label-source-specify-avro-schema) You can enable the default key schema with the following option: `| output.format.key=schema |\n| ------------------------ | ` ### Value Schema The connector provides a default value schema for change event documents. You should use the default value schema unless you transform your change event documents[as described in this guide here.](#std-label-source-schema-for-modified-document)\n\n You can enable the default value schema with the following option: ``` output.format.value=schema \n``` ## Schemas For Transformed Documents There are two ways you can transform your change event documents in a source connector: * The `publish.full.document.only=true` option\n* An aggregation pipeline that modifies the structure of change event documents If you transform your MongoDB change event documents, you must do the following to apply schemas: * [Specify schemas](#std-label-source-specify-avro-schema)\n* [Have the connector infer a value schema](#std-label-source-infer-a-schema) To learn more about the preceding configuration options, see the[Change Stream Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/change-stream/#std-label-source-configuration-change-stream) page. ### Specify Schemas You can specify schemas for incoming documents using Avro schema syntax. Click on the following tabs to see how to specify a schema for document values and keys: To view an example that demonstrates how to specify a schema, see the[Specify a Schema](https://mongodb.com/docs/kafka-connector/current/source-connector/usage-examples/schema/#std-label-source-usage-example-schema) usage example. To learn more about Avro Schema, see the[Data Formats](https://mongodb.com/docs/kafka-connector/current/introduction/data-formats/#std-label-kafka-df-avro-schema) guide. ## Important ### Converters If you want to send your data through Apache Kafka with Avro binary encoding, you must use an Avro converter. For more information, see the guide on[Converters.](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-intro-converters) ### Infer a Schema You can have your source connector infer a schema for incoming documents. This option works well for development and for data sources that do not frequently change structure, but for most production deployments we recommend that you[specify a schema.](#std-label-source-specify-avro-schema) You can have the connector infer a schema by specifying the following options: ``` output.format.value=schema output.schema.infer.value=true \n``` The source connector can infer schemas for incoming documents that contain nested documents stored in arrays. Starting in Version 1.9 of the connector, schema inference will gather the appropriate data type for fields instead of defaulting to a `string` type assignment if there are differences between nested documents described by the following cases: * A field is present in one document but missing in another.\n* A field is present in one document but `null` in another.\n* A field is an array with elements of any type in one document but has additional elements or elements of other data types in another.\n* A field is an array with elements of any type in one document but an empty array in another. If field types conflict between nested documents, the connector pushes the conflict down to the schema for the field and defaults to a`string` type assignment. ## Note ### Cannot Infer Key Schema The connector does not support key schema inference. If you want to use a key schema and transform your MongoDB change event documents, you must specify a key schema as described in[the specify schemas section of this guide.](#std-label-source-specify-avro-schema) ← [Change Streams](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/ \"Previous Section\")[Security and Authentication](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/security-and-authentication/mongodb-aws-auth/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # MongoDB AWS-based Authentication On this page * [Overview](#overview)\n* [Connector Connection Settings](#connector-connection-settings) ## Overview In this guide, you can learn how to authenticate your MongoDB Kafka Connector with your MongoDB replica set using the `MONGODB-AWS` authentication mechanism. The `MONGODB-AWS` authentication mechanism uses your Amazon Web Services Identity and Access Management (AWS IAM) credentials to authenticate your user. To learn how to set up your MongoDB replica set in MongoDB Atlas to use AWS IAM credentials, see the guide on [How to Set Up Unified AWS Access.](https://www.mongodb.com/docs/atlas/security/set-up-unified-aws-access/) ## Important You need to use MongoDB Kafka Connector version 1.5 of later to connect to a MongoDB server set up to authenticate using your AWS IAM credentials. AWS IAM credential authentication is available in MongoDB server version 4.4 and later. ## Connector Connection Settings You can specify your `MONGODB-AWS` authentication credentials in your connection URI connector property as shown in the following example: `| connection.uri=mongodb://<AWS access key id>:<AWS secret access key>@<hostname>:<port>/?authSource=<authentication database>&authMechanism=MONGODB-AWS&authMechanismProperties=AWS_SESSION_TOKEN:<AWS session token> |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ` The preceding example uses the following placeholders which you need to replace:\n\n ← [SSL/TLS and X.509 Certificates](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/tls-and-x509/ \"Previous Section\")[Monitoring](https://mongodb.com/docs/kafka-connector/current/monitoring/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/security-and-authentication/tls-and-x509/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # SSL/TLS and X.509 Certificates On this page * [Overview](#overview)\n* [Prerequisites](#prerequisites)\n* [Store Certificates on the Worker](#store-certificates-on-the-worker)\n* [Keystore](#keystore)\n* [Truststore](#truststore)\n* [Add Credentials to the Connector](#add-credentials-to-the-connector) ## Overview In this guide, you can learn how to secure communications between your MongoDB Kafka Connector worker and your MongoDB cluster. To secure your connection, you must perform the following tasks: * [Create the certificates](#std-label-prerequisites-mkc)\n* [Store the certificates on the worker host machine](#std-label-key-store-trust-store-setup-mkc)\n* [Supply the certificates' credentials to the connector](#std-label-supply-credentials-to-mkc) ## Note If you host your MongoDB cluster on [MongoDB Atlas](https://www.mongodb.com/docs/atlas/) or your cluster does not explicitly require certificates, you can already communicate securely and do not need to follow the steps in this guide. ## Prerequisites This guide requires prior knowledge of the following concepts: * [Transport Layer Security](https://en.wikipedia.org/wiki/Transport%5FLayer%5FSecurity)\n* [X.509](https://en.wikipedia.org/wiki/x.509)\n* [Certificate Authorities (CA)](https://en.wikipedia.org/wiki/Certificate%5Fauthority)\n* [PKCS 12](https://en.wikipedia.org/wiki/PKCS%5F12)\n* [OpenSSL](https://www.openssl.org/)\n* [keytool](https://docs.oracle.com/en/java/javase/12/tools/keytool.html) ## Store Certificates on the Worker Store your certificates in a **keystore** and **truststore** to secure your certificate credentials for each server you run your connector worker instance on. ### Keystore You can use a keystore to store private keys and identity certificates. The keystore uses the key and certificate to verify the client's identity to external hosts. If your SSL/TLS configuration requires a client certificate to connect to your worker instance, generate a secure private key and include the client certificate bundled with the intermediate CA. Then, store this information in your keystore by using the following `openssl` command to generate a PKCS 12 file: `| openssl pkcs12 -export -inkey <your private key> \\ |\n| -------------------------------------------------- |\n| -in <your bundled certificate> \\                   |\n| -out <your output pkcs12 file>                     | ` ### Truststore You can use a truststore to store certificates from a CA. The truststore uses the certificates to identify parties the client trusts. Some examples of these certificates are a root CA, intermediate CA and your MongoDB cluster's end entity certificate. Import the certificates of parties that you trust into your truststore by using the following `keytool` command: ```bash keytool -import -trustcacerts -import -file <your root or intermediate CA> \n``` If your SSL/TLS configuration requires the end entity certificate for your MongoDB cluster, import it into your truststore with the following command: ```bash keytool -import -file <your server bundled certificate> -keystore <your keystore name> \n``` For more information on how to set up a client keystore and truststore for testing purposes, see[OpenSSL Client Certificates for Testing.](https://www.mongodb.com/docs/manual/appendix/security/appendixC-openssl-client/#appendix-c-openssl-client-certificates-for-testing) ## Add Credentials to the Connector The connector worker processes JVM options from your `KAFKA_OPTS`environment variable. The environment variable contains the path and password to your keystore and truststore. Export the following JVM options in your `KAFKA_OPTS` variable: ```bash\n\n -Djavax.net.ssl.trustStore=<your path to truststore> \\ -Djavax.net.ssl.trustStorePassword=<your truststore password> \\ -Djavax.net.ssl.keyStore=<your path to keystore> \\ -Djavax.net.ssl.keyStorePassword=<your keystore password>\" \n``` When the worker processes the JVM options, the connector attempts to connect by using the SSL/TLS protocol and certificates in your keystore and truststore. ← [Security and Authentication](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/ \"Previous Section\")[MongoDB AWS-based Authentication](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/mongodb-aws-auth/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/introduction/connect/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Connect to MongoDB On this page * [Overview](#overview)\n* [How to Connect](#how-to-connect)\n* [How to Configure Your Connection](#how-to-configure-your-connection)\n* [Authentication](#authentication) ## Overview Learn how to connect the MongoDB Kafka Connector to MongoDB using a**connection Uniform Resource Identifier (URI)**. A connection URI is a string that contains the following information: * The address of your MongoDB deployment _required_\n* Connection settings _optional_\n* Authentication settings _optional_\n* Authentication credentials _optional_ The following is an example of a connection URI for a MongoDB replica set: `| mongodb://mongodb0.example.com:27017,mongodb1.example.com:27017,mongodb2.example.com:27017/?replicaSet=myRepl |\n| ------------------------------------------------------------------------------------------------------------- | ` To learn more about the connection URI format, see[Connection String URI Format](https://www.mongodb.com/docs/manual/reference/connection-string/) in the MongoDB Manual. ## How to Connect Specify a connection URI with the following configuration option in both a source and sink connector: ``` connection.uri=<your connection uri> \n``` To learn more about this configuration option, see the following resources: * [Source connector configuration options](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/mongodb-connection/)\n* [Sink connector configuration options](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-connection/) ## How to Configure Your Connection The MongoDB Kafka Connector uses the **MongoDB Java driver** to parse your connection URI. The MongoDB Java driver is an artifact that enables Java applications like Kafka Connect to interact with MongoDB. ## Important ### Driver Version Version 1.9 of the MongoDB Kafka Connector uses version 4.7 of the MongoDB Java driver. To learn what connection URI options are available in the connector, see[the MongoDB Java driver Connection guide.](https://www.mongodb.com/docs/drivers/java/sync/v4.7/fundamentals/connection/#connection-options) ### Authentication All authentication mechanisms available in the MongoDB Java driver are available in the MongoDB Kafka Connector. The following is an example of a connection URI that authenticates with MongoDB using`SCRAM-SHA-256` authentication: ``` mongodb://<username>:<password>@<hostname>:<port>/?authSource=<authenticationDb>&authMechanism=SCRAM-SHA-256 \n``` ## Important ### Load Secrets To avoid storing your authentication secrets as plain text in your `connection.uri`setting, load your secrets from a secure location as your connector starts. To learn how to load your secrets as your connector starts, see[the Externalize Secrets guide from Confluent.](https://docs.confluent.io/platform/current/connect/security.html#externalizing-secrets) To learn what authentication mechanisms are available, see[the MongoDB Java driver Authentication Mechanisms guide.](https://www.mongodb.com/docs/drivers/java/sync/v4.7/fundamentals/auth/#mechanisms) To learn more about authentication in the connector, see the[Security and Authentication guide.](https://mongodb.com/docs/kafka-connector/current/security-and-authentication/) ← [Install the MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/introduction/install/ \"Previous Section\")[Data Formats](https://mongodb.com/docs/kafka-connector/current/introduction/data-formats/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/troubleshooting/recover-from-invalid-resume-token/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Invalid Resume Token On this page * [Overview](#overview)\n* [Stack Trace](#stack-trace)\n* [Cause](#cause)\n* [Solutions](#solutions)\n* [Temporarily Tolerate Errors](#temporarily-tolerate-errors)\n* [Reset Stored Offsets](#reset-stored-offsets)\n* [Prevention](#prevention) ## Overview Learn how to recover from an invalid resume token in a MongoDB Kafka source connector. ### Stack Trace The following stack trace indicates that the source connector has an invalid resume token: `| ...                                                                      |\n| ------------------------------------------------------------------------ |\n| org.apache.kafka.connect.errors.ConnectException: ResumeToken not found. |\n| Cannot create a change stream cursor                                     |\n| ...                                                                      |\n| Command failed with error 286 (ChangeStreamHistoryLost): 'PlanExecutor   |\n| error during aggregation :: caused by :: Resume of change stream was not |\n| possible, as the resume point may no longer be in the oplog              |\n| ...                                                                      | ` ## Cause When the ID of your source connector's resume token does not correspond to any entry in your MongoDB deployment's [oplog](https://www.mongodb.com/docs/manual/core/replica-set-oplog/#std-label-replica-set-oplog), your connector has no way to determine where to begin to process your MongoDB change stream. Click the following tabs to see scenarios in which you can experience this issue: For more information on the oplog, see the[MongoDB Manual.](https://www.mongodb.com/docs/manual/core/replica-set-oplog/#std-label-replica-set-oplog) For more information on change streams, see the[Change Streams](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/#std-label-kafka-source-change-streams) guide. ## Solutions You can recover from an invalid resume token using one of the following strategies: * [Temporarily Tolerate Errors](#std-label-temporarily-tolerate-errors)\n* [Reset Stored Offsets](#std-label-troubleshoot-reset-stored-offsets) ### Temporarily Tolerate Errors You can configure your source connector to tolerate errors while you produce a change stream event that updates the connector's resume token. This recovery strategy is the simplest, but there is a risk that your connector briefly ignores errors unrelated to the invalid resume token. If you aren't comfortable briefly tolerating errors in your deployment, you can[delete stored offsets](#std-label-troubleshoot-delete-stored-offsets) instead. To configure your source connector to temporarily tolerate errors:\n\n For more information on the `errors.tolerance` option, see the[Error Handling and Resuming from Interruption Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/#std-label-source-configuration-error-handling) page. ### Reset Stored Offsets You can reset your Kafka Connect offset data, which contains your resume token, to allow your connector to resume processing your change stream. To reset your offset data, change the value of the`offset.partition.name` configuration property to a partition name that does not exist on your Kafka deployment. You can set your `offset.partition.name`property like this: ``` offset.partition.name=<a string> \n``` ## Tip ### Naming your Offset Partitions Consider using the following pattern to name your offset partitions: ``` offset.partition.name=<source connector name>.<monotonically increasing number> \n``` This pattern provides the following benefits: * Records the number of times you reset your connector\n* Documents to which connector an offset partition belongs ## Example Assume you named your source connector `\"source-values\"` and you are setting the `offset.partition.name` property for the first time. You would configure your connector as follows: ``` offset.partition.name=source-values.1 \n``` The next time you reset your connector's offset data, configure your connector as follows: ``` offset.partition.name=source-values.2 \n``` To learn more about the `offset.partition.name` configuration property, see the [Error Handling and Resuming from Interruption Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/#std-label-source-configuration-error-handling) page. To learn about naming your connector, see the official[Apache Kafka](https://kafka.apache.org/documentation/#connect%5Fconfiguring)documentation. ## Prevention To prevent invalid resume token errors caused by an[infrequently updated namespace](#std-label-invalid-resume-token-cause), enable**heartbeats**. Heartbeats is a feature of your source connector that causes your connector to update its resume token at regular intervals as well as when the contents of your source MongoDB namespace changes. Specify the following option in your source connector configuration to enable heartbeats: ``` heartbeat.interval.ms=<a positive integer> \n``` To learn more about heartbeats, see the[Error Handling and Resuming from Interruption Properties](https://mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/error-handling/#std-label-source-configuration-error-handling) guide. ← [Troubleshooting](https://mongodb.com/docs/kafka-connector/current/troubleshooting/ \"Previous Section\")[How to Contribute](https://mongodb.com/docs/kafka-connector/current/contribute/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/introduction/data-formats/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Data Formats On this page * [Overview](#overview)\n* [JSON](#json)\n* [Raw JSON](#raw-json)\n* [BSON](#bson)\n* [JSON Schema](#json-schema)\n* [Avro](#avro)\n* [Avro Schema](#avro-schema)\n* [Avro Binary Encoding](#avro-binary-encoding)\n* [Byte Arrays](#byte-arrays) ## Overview In this guide, you can learn about the data formats you use when working with the MongoDB Kafka Connector and your pipeline. This guide uses the following sample document to show the behavior of the different formats: `| {company:\"MongoDB\"} |\n| ------------------- | ` ## JSON JSON is a data-interchange format based on JavaScript object notation. You represent the [sample document](#std-label-kafka-df-sample-doc) in JSON like this: ``` {\"company\":\"MongoDB\"} \n``` You may encounter the following data formats related to JSON when working with the connector: * [Raw JSON](#std-label-kafka-df-raw-json)\n* [BSON](#std-label-kafka-df-bson)\n* [JSON Schema](#std-label-kafka-df-json-schema) For more information on JSON, see the [official JSON website.](https://www.json.org/json-en.html) ### Raw JSON Raw JSON is a data format that consists of JSON objects written as strings. You represent the[sample document](#std-label-kafka-df-sample-doc) in Raw JSON like this: ``` \"{\\\"company\\\":\\\"MongoDB\\\"}\" \n``` You use Raw JSON when you specify a String converter on a source or sink connector. To view connector configurations that specify a String converter, see the [Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-string-converter-sample-properties) guide. ### BSON BSON is a binary serialization encoding for JSON-like objects. BSON encodes the [sample document](#std-label-kafka-df-sample-doc) like this: ``` \\x1a\\x00\\x00\\x00\\x02company\\x00\\x08\\x00\\x00\\x00MongoDB\\x00\\x00 \n``` Your connectors use the BSON format to send and receive documents from your MongoDB deployment. For more information on BSON, see [the BSON specification.](https://bsonspec.org/) ### JSON Schema JSON Schema is a syntax for specifying **schemas** for JSON objects. A schema is a definition attached to an Apache Kafka Topic that defines valid values for that topic. You can specify a schema for the [sample document](#std-label-kafka-df-sample-doc)with JSON Schema like this: ``` {    \"$schema\":\"http://json-schema.org/draft-07/schema\",    \"$id\":\"unique id\",    \"type\":\"object\",    \"title\":\"Example Schema\",    \"description\":\"JSON Schema for the sample document.\",    \"required\":[       \"company\"    ],    \"properties\":{       \"company\":{          \"$id\":\"another unique id\",          \"type\":\"string\",          \"title\":\"Company\",          \"description\":\"A field to hold the name of a company\"       }    },    \"additionalProperties\":false } \n``` You use JSON Schema when you apply JSON Schema converters to your connectors. To view connector configurations that specify a JSON Schema converter, see the [Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-json-schema-converter-sample-properties)guide. For more information, see the official[JSON Schema website.](https://json-schema.org/) ## Avro\n\n * [Avro schema](#std-label-kafka-df-avro-schema)\n* [Avro binary encoding](#std-label-kafka-df-avro-encoding) For more information on Apache Avro, see the[Apache Avro Documentation.](https://avro.apache.org/docs/current/index.html) ### Avro Schema Avro schema is a JSON-based schema definition syntax. Avro schema supports the specification of the following groups of data types: * [Primitive Types](https://avro.apache.org/docs/current/spec.html#schema%5Fprimitive)\n* [Complex Types](https://avro.apache.org/docs/current/spec.html#schema%5Fcomplex)\n* [Logical Types](https://avro.apache.org/docs/current/spec.html#Logical+Types) ## Warning ### Unsupported Avro Types The connector does not support the following Avro types: * `enum` types. Use `string` instead.\n* `fixed` types. Use `bytes` instead.\n* `null` as a primitive type. However, `null` as an element in a `union` is supported.\n* `union` types with more than 2 elements.\n* `union` types with more than one `null` element. ## Important ### Sink Connectors and Logical Types The MongoDB Kafka sink connector supports all Avro schema primitive and complex types, however sink connectors support only the following logical types: * `decimal`\n* `date`\n* `time-millis`\n* `time-micros`\n* `timestamp-millis`\n* `timestamp-micros` You can construct an Avro schema for the [sample document](#std-label-kafka-df-sample-doc)like this: ``` {   \"type\": \"record\",   \"name\": \"example\",   \"doc\": \"example documents have a company field\",   \"fields\": [     {       \"name\": \"company\",       \"type\": \"string\"     }   ] } \n``` You use Avro schema when you[define a schema for a MongoDB Kafka source connector.](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/#std-label-source-specify-avro-schema) For a list of all Avro schema types, see the[Apache Avro specification.](https://avro.apache.org/docs/current/spec.html) ### Avro Binary Encoding Avro specifies a binary serialization encoding for JSON objects defined by an Avro schema. If you use the[preceding Avro schema](#std-label-kafka-df-avro-schema), you can represent the[sample document](#std-label-kafka-df-sample-doc) with Avro binary encoding like this: ``` \\x0eMongoDB \n``` You use Avro binary encoding when you specify an Avro converter on a source or sink connector. To view connector configurations that specify an Avro converter, see the [Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-avro-converter-sample-properties)guide. To learn more about Avro binary encoding, see[this section of the Avro specification.](https://avro.apache.org/docs/current/spec.html#Data+Serialization+and+Deserialization) ## Byte Arrays A byte array is a consecutive sequence of unstructured bytes. You can represent the sample document as a byte array using any of the encodings mentioned above. You use byte arrays when your converters send data to or receive data from Apache Kafka. For more information on converters, see the[Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-intro-converters) guide. ← [Connect to MongoDB](https://mongodb.com/docs/kafka-connector/current/introduction/connect/ \"Previous Section\")[Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/introduction/kafka-connect/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Kafka and Kafka Connect On this page * [Overview](#overview)\n* [Apache Kafka](#apache-kafka)\n* [Kafka Connect](#kafka-connect)\n* [Diagram](#diagram) ## Overview In this guide, you can learn the following foundational information about Apache Kafka and Kafka Connect: * What Apache Kafka and Kafka Connect are\n* What problems Apache Kafka and Kafka Connect solve\n* Why Apache Kafka and Kafka Connect are useful\n* How data moves through an Apache Kafka and Kafka Connect pipeline ## Apache Kafka Apache Kafka is an open source publish/subscribe messaging system. Apache Kafka provides a flexible, **fault tolerant**, and **horizontally scalable** system to move data throughout datastores and applications. A system is fault tolerant if the system can continue operating even if certain components of the system stop working. A system is horizontally scalable if the system can be expanded to handle larger workloads by adding more machines rather than by improving a machine's hardware. For more information on Apache Kafka, see the following resources: * [Confluent \"What is Apache Kafka?\" Page](https://www.confluent.io/what-is-apache-kafka/)\n* [Apache Kafka Official Documentation](https://kafka.apache.org/documentation/) ## Kafka Connect Kafka Connect is a component of Apache Kafka that solves the problem of connecting Apache Kafka to datastores such as MongoDB. Kafka Connect solves this problem by providing the following resources: * A fault tolerant runtime for transferring data to and from datastores.\n* A framework for the Apache Kafka community to share solutions for connecting Apache Kafka to different datastores. The Kafka Connect framework defines an API for developers to write reusable**connectors**. Connectors enable Kafka Connect deployments to interact with a specific datastore as a data source or a data sink. The MongoDB Kafka Connector is one of these connectors. For more information on Kafka Connect, see the following resources: * [Confluent Kafka Connect Page](https://docs.confluent.io/platform/current/connect/index.html)\n* [Apache Kafka Official Documentation, Kafka Connect Guide](https://kafka.apache.org/documentation/#connect)\n* [Apache Foundation Video Walk-Through of the Kafka Connect Framework](https://www.youtube.com/watch?v=EXviLqXFoQI) ## Tip ### Use Kafka Connect instead of Producer/Consumer Clients when Connecting to Datastores While you could write your own application to connect Apache Kafka to a specific datastore using producer and consumer clients, Kafka Connect may be a better fit for you. Here are some reasons to use Kafka Connect: * Kafka Connect has a fault tolerant distributed architecture to ensure a reliable pipeline.\n* There are a large number of community maintained connectors for connecting Apache Kafka to popular datastores like MongoDB, PostgreSQL, and MySQL using the Kafka Connect framework. This reduces the amount of boilerplate code you need to write and maintain to manage database connections, error handling, dead-letter queue integration, and other problems involved in connecting Apache Kafka with a datastore.\n* You have the option to use a managed Kafka Connect cluster from Confluent. ## Diagram The following diagram shows how information flows through an example data pipeline built with Apache Kafka and Kafka Connect. The example pipeline uses a MongoDB cluster as a data source, and a MongoDB cluster as a data sink. ![Dataflow diagram of Kafka Connect deployment.](https://mongodb.com/docs/kafka-connector/current/includes/figures/connect-data-flow.png) All connectors and datastores in the example pipeline are optional, and you can swap them out for whatever connectors and datastores you need for your deployment. ← [Introduction](https://mongodb.com/docs/kafka-connector/current/introduction/ \"Previous Section\")[Install the MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/introduction/install/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/introduction/install/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Install the MongoDB Kafka Connector On this page * [Overview](#overview)\n* [Install the Connector on Confluent Platform](#install-the-connector-on-confluent-platform)\n* [Install the Connector on Apache Kafka](#install-the-connector-on-apache-kafka)\n* [Download a Connector JAR File](#download-a-connector-jar-file) ## Overview Learn how to install the MongoDB Kafka Connector. The connector is available for Confluent Platform and Apache Kafka deployments. To see installation instructions for your deployment type, navigate to one of the following sections: * [Install the Connector on Confluent Platform](#std-label-kafka-connector-install-confluent)\n* [Install the Connector on Apache Kafka](#std-label-kafka-connector-install-apache) ## Install the Connector on Confluent Platform Click the following tabs to see instructions for how to install the connector on Confluent Platform: ## Install the Connector on Apache Kafka 1. Locate and download the uber JAR to get all the dependencies required for the connector. Check the[reference table](#std-label-kafka-connector-installation-reference)to find the uber JAR.  \n## Note  \nIf you are unable to use the uber JAR or prefer to manage your own dependencies, download the JAR that contains the minimum required dependencies and resolve any runtime dependencies. You can use a plugin such as [Maven dependency:tree](https://maven.apache.org/plugins/maven-dependency-plugin/tree-mojo.html)to generate the dependency tree.\n2. Copy the JAR and any dependencies into the Kafka plugins directory which you can specify in your[plugin.path](https://kafka.apache.org/documentation/#connectconfigs%5Fplugin.path)configuration setting (e.g. `plugin.path=/usr/local/share/kafka/plugins`). ## Note If you intend to run the connector as distributed worker processes, you must repeat this process for each server or virtual machine. ## Download a Connector JAR File You can download the connector source and JAR files from the following locations: | Kafka Connector GitHub repository (source code) | [mongodb/mongo-kafka](https://github.com/mongodb/mongo-kafka)                                  |\n| ----------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| Maven Central repository (JAR files)            | [mongo-kafka-connect](https://search.maven.org/artifact/org.mongodb.kafka/mongo-kafka-connect) | You can identify the contents of the JAR files by the suffix in the filename. Consult the following table for a description of each suffix: | Suffix        | Description                                                                     |\n| ------------- | ------------------------------------------------------------------------------- |\n| **all**       | The uber JAR that contains the connector, MongoDB dependencies, and Apache Avro |\n| **confluent** | Contains the minimum requirements for the connector and Confluent Platform      |\n| **javadoc**   | Contains the Javadoc documentation for the connector classes                    |\n| **sources**   | Contains the source code that corresponds to the compiled connector classes     | For example,`mongo-kafka-connect-1.9.1-all.jar`is the uber JAR for the version 1.9.1 connector.\n\n",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Fundamentals Read the following sections to learn how MongoDB Kafka sink connector features work and how to configure them: * [Specify How the Connector Writes Data to MongoDB](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/write-strategies/)\n* [Modify Sink Records](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/)\n* [Handle Errors](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/error-handling-strategies/)\n* [Convert Change Data Capture Events to Write Operations](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/) ← [All Sink Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/all-properties/ \"Previous Section\")[Write Model Strategies](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/write-strategies/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Sink Connector Configuration Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview In this section, you can read descriptions of the MongoDB Kafka sink connector properties, including essential Confluent Kafka Connect settings and MongoDB Kafka Connector-specific settings. For an example sink connector configuration file, see[MongoSinkConnector.properties.](https://github.com/mongodb/mongo-kafka/blob/master/config/MongoSinkConnector.properties) For sink connector performance recommendations, see [Tuning the Sink Connector.](https://www.mongodb.com/developer/products/connectors/tuning-mongodb-kafka-connector/#tuning-the-sink-connector) ## Settings See the following categories for a list of related configuration properties:\n\n See the [Confluent Sink Connector documentation](https://docs.confluent.io/current/installation/configuration/connect/sink-connect-configs.html)for more information on these settings. ← [Sink Connector](https://mongodb.com/docs/kafka-connector/current/sink-connector/ \"Previous Section\")[MongoDB Connection Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-connection/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/introduction/converters/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Converters On this page * [Overview](#overview)\n* [Available Converters](#available-converters)\n* [Converters with Schemas](#converters-with-schemas)\n* [Connector Configuration](#connector-configuration)\n* [Avro Converter](#avro-converter)\n* [Protobuf Converter](#protobuf-converter)\n* [JSON Schema Converter](#json-schema-converter)\n* [JSON Converter](#json-converter)\n* [String Converter (Raw JSON)](#string-converter--raw-json-) ## Overview This guide describes how to use **converters** with the MongoDB Kafka Connector. Converters are programs that translate between bytes and Kafka Connect's runtime data format. Converters pass data between Kafka Connect and Apache Kafka. The connector passes data between MongoDB and Kafka Connect. The following diagram shows these relationships: ![Diagram illustrating converters' role in Kafka Connect](https://mongodb.com/docs/kafka-connector/current/includes/figures/converters.png) To learn more about converters, see the following resources: * [Article from Confluent.](https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#configuring-converters)\n* [Confluent Article on Kafka Connect Concepts](https://docs.confluent.io/platform/current/connect/concepts.html#converters)\n* [Converter Interface API Documentation](https://kafka.apache.org/32/javadoc/org/apache/kafka/connect/storage/Converter.html) ## Available Converters As the connector converts your MongoDB data into Kafka Connect's runtime data format, the connector works with all available converters. ## Important ### Use the Same Converter for your Source and Sink Connectors You must use the same converter in your MongoDB Kafka source connector and MongoDB Kafka sink connector. For example, if your source connector writes to a topic using Protobuf, your sink connector must use Protobuf to read from the topic. To learn what converter to use, [see this page from Confluent.](https://docs.confluent.io/platform/current/schema-registry/connect.html) ### Converters with Schemas If you use a schema-based converter such as the Kafka Connect Avro Converter (Avro Converter), Kafka Connect Protobuf Converter, or Kafka Connect JSON Schema Converter, you should define a schema in your source connector. To learn how to specify a schema, see the[Apply Schemas](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/specify-schema/#std-label-kafka-source-apply-schemas) guide. ## Connector Configuration This section provides templates for properties files to configure the following converters in a connector pipeline: * [Avro Converter](#std-label-avro-converter-sample-properties)\n* [Protobuf Converter](#std-label-protobuf-converter-sample-properties)\n* [JSON Schema Converter](#std-label-json-schema-converter-sample-properties)\n* [JSON Converter](#std-label-json-converter-sample-properties)\n* [String Converter](#std-label-string-converter-sample-properties) ### Avro Converter Click the following tabs to view properties files that work with the Avro converter: To use the preceding properties file, replace the placeholder text in angle brackets with your information. ### Protobuf Converter Click the following tabs to view properties files that work with the Protobuf converter: To use the preceding properties file, replace the placeholder text in angle brackets with your information. ### JSON Schema Converter Click the following tabs to view properties files that work with the JSON Schema converter: To use the preceding properties file, replace the placeholder text in angle brackets with your information. ### JSON Converter Click the following tabs to view properties files that work with the JSON converter: To use the preceding properties file, replace the placeholder text in angle brackets with your information. ### String Converter (Raw JSON)\n\n To use the preceding properties file, replace the placeholder text in angle brackets with your information. ← [Data Formats](https://mongodb.com/docs/kafka-connector/current/introduction/data-formats/ \"Previous Section\")[Tutorials](https://mongodb.com/docs/kafka-connector/current/tutorials/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/cdc/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Change Data Capture Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify a class the MongoDB Kafka sink connector uses to process change data capture (CDC) events. See the guide on [Sink Connector Change Data Capture](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/)for examples using the built-in `ChangeStreamHandler` and handlers for the Debezium and Qlik Replicate event producers. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings | Name                            | Description                                                                                                                                                                                                                                                                                                                                                                         |\n| ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **change.data.capture.handler** | **Type:** string**Description:**The class name of the CDC handler to use for converting changes into event streams. See[Available CDC Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/#std-label-available-cdc-handlers)for a list of CDC handlers.**Default**: \"\"**Accepted Values**: An empty string or a fully qualified Java class name | ← [Topic Override Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/topic-override/ \"Previous Section\")[Kafka Time Series Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/time-series/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/all-properties/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # All Sink Connector Configuration Properties On this page * [Overview](#overview)\n* [MongoDB Connection](#mongodb-connection)\n* [MongoDB Namespace](#mongodb-namespace)\n* [Connector Topic](#connector-topic)\n* [Connector Message Processing](#connector-message-processing)\n* [Connector Error Handling](#connector-error-handling)\n* [Post Processors](#post-processors)\n* [ID Strategy](#id-strategy)\n* [Write Model Strategy](#write-model-strategy)\n* [Topic Override](#topic-override)\n* [Change Data Capture](#change-data-capture)\n* [Time Series](#time-series) ## Overview On this page, you can view all available configuration properties for your MongoDB Kafka sink connector. This page duplicates the content of the other sink connector configuration properties pages. To view a list of all sink connector configuration properties pages, see the [Sink Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/#std-label-kafka-sink-configuration-properties) page. ## MongoDB Connection Use the following configuration settings to specify how your MongoDB Kafka sink connector connects and communicates with your MongoDB cluster. To view only the options related to configuring your MongoDB connection, see the [MongoDB Connection Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-connection/#std-label-sink-configuration-mongodb-connection) page.\n\n ## MongoDB Namespace Use the following configuration settings to specify which MongoDB database and collection that your MongoDB Kafka sink connector writes data to. You can use the default `DefaultNamespaceMapper` or specify a custom class. To view only the options related to specifying where the connector writes data, see the [MongoDB Namespace Mapping Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-namespace/#std-label-sink-configuration-namespace-mapping) page.\n\n #### FieldPathNamespaceMapper Settings If you configure your sink connector to use the `FieldPathNamespaceMapper`, you can specify which database and collection to sink a document based on the data's field values. To enable this mapping behavior, set your sink connector `namespace.mapper`configuration property to the fully-qualified class name as shown below: ``` namespace.mapper=com.mongodb.kafka.connect.sink.namespace.mapping.FieldPathNamespaceMapper \n``` The `FieldPathNamespaceMapper` requires you to specify the following settings: * One or both mapping properties to a database and collection\n* One of the `key` or `value` mappings to a database\n* One of the `key` or `value` mappings to a collection You can use the following settings to customize the behavior of the`FieldPathNamespaceMapper`:\n\n ## Connector Topic Use the following configuration settings to specify which Kafka topics the MongoDB Kafka sink connector should watch for data. To view only the options related to specifying Kafka topics, see the[Kafka Topic Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/kafka-topic/#std-label-sink-configuration-topic-properties) page.\n\n ## Connector Message Processing Use the settings on this page to configure the message processing behavior of the MongoDB Kafka sink connector including the following: * Message batch size\n* Rate limits\n* Number of parallel tasks To view only the options related to change data capture handlers, see the[Connector Message Processing Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/connector-message/#std-label-sink-configuration-message-processing) page.\n\n ## Connector Error Handling Use the following configuration settings to specify how the MongoDB Kafka sink connector handles errors and to configure the dead letter queue. To view only the options related to handling errors, see the[Connector Error Handling Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/#std-label-sink-configuration-error-handling) page.\n\n ## Post Processors Use the following configuration settings to specify how the MongoDB Kafka sink connector should transform Kafka data before inserting it into MongoDB. To view only the options related to post-processors, see the[Sink Connector Post-processor Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/post-processors/#std-label-sink-configuration-post-processors) page.\n\n ## ID Strategy Use the following configuration settings to specify how the MongoDB Kafka sink connector should determine the `_id` value for each document it writes to MongoDB. To view only the options related to determining the `_id` field of your documents, see the [Sink Connector Id Strategy Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/id-strategy/#std-label-sink-configuration-id-strategy) page.\n\n ## Write Model Strategy Use the strategies in the following table to specify how the MongoDB Kafka sink connector writes data into MongoDB. You can specify a write strategy with the following configuration: ``` writemodel.strategy=<a writemodel strategy> \n``` To view only the options related to write model strategies, see the[Sink Connector Write Model Strategies](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/write-strategies/#std-label-sink-configuration-write-model-strategy) page.\n\n ## Topic Override Use the following MongoDB Kafka sink connector configuration settings to override global or default property settings for specific topics. To view only the options related to overriding topic settings, see the[Topic Override Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/topic-override/#std-label-sink-configuration-topic-override) page. | Name                                          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **topic.override.<topicName>.<propertyName>** | **Type:** string**Description:**Specify a topic and property name to override the corresponding global or default property setting.ExampleThe topic.override.foo.collection=bar setting instructs the sink connector to store data from the foo topic in the barcollection.NoteYou can specify any valid configuration setting in the<propertyName> segment on a per-topic basis exceptconnection.uri and topics.**Default**: \"\"**Accepted Values**: Accepted values specific to the overridden property | ## Change Data Capture Use the following configuration settings to specify a class the MongoDB Kafka sink connector uses to process change data capture (CDC) events. See the guide on [Sink Connector Change Data Capture](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/)for examples using the built-in `ChangeStreamHandler` and handlers for the Debezium and Qlik Replicate event producers. To view only the options related to change data capture handlers, see the[Change Data Capture Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/cdc/#std-label-sink-configuration-change-data-capture) page.\n\n ## Time Series Use the following configuration settings to specify how the MongoDB Kafka sink connector should sink data to a MongoDB time series collection. To view only the options related to time series collections, see the[Kafka Time Series Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/time-series/#std-label-sink-configuration-time-series) page.\n\n For an example on how to convert an existing collection to a time series collection, see the tutorial on how to [Migrate an Existing Collection to a Time Series Collection.](https://mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/#std-label-tutorial-migrate-time-series) ← [Kafka Time Series Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/time-series/ \"Previous Section\")[Fundamentals](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-namespace/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # MongoDB Namespace Mapping Configuration Properties On this page * [Overview](#overview)\n* [Settings](#settings)\n* [FieldPathNamespaceMapper Settings](#fieldpathnamespacemapper-settings) ## Overview Use the following configuration settings to specify which MongoDB database and collection that your MongoDB Kafka sink connector writes data to. You can use the default `DefaultNamespaceMapper` or specify a custom class. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ## FieldPathNamespaceMapper Settings If you configure your sink connector to use the `FieldPathNamespaceMapper`, you can specify which database and collection to sink a document based on the data's field values. To enable this mapping behavior, set your sink connector `namespace.mapper`configuration property to the fully-qualified class name as shown below: ``` namespace.mapper=com.mongodb.kafka.connect.sink.namespace.mapping.FieldPathNamespaceMapper \n``` The `FieldPathNamespaceMapper` requires you to specify the following settings: * One or both mapping properties to a database and collection\n* One of the `key` or `value` mappings to a database\n* One of the `key` or `value` mappings to a collection You can use the following settings to customize the behavior of the`FieldPathNamespaceMapper`:\n\n ← [MongoDB Connection Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-connection/ \"Previous Section\")[Kafka Topic Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/kafka-topic/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/kafka-topic/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Kafka Topic Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify which Kafka topics the MongoDB Kafka sink connector should watch for data. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ← [MongoDB Namespace Mapping Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-namespace/ \"Previous Section\")[Connector Message Processing Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/connector-message/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/write-strategies/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Sink Connector Write Model Strategies On this page * [Overview](#overview)\n* [Strategies](#strategies) ## Overview Use the strategies in the following table to specify how the MongoDB Kafka sink connector writes data into MongoDB. You can specify a write strategy with the following configuration: `| writemodel.strategy=<a writemodel strategy> |\n| ------------------------------------------- | ` For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Strategies\n\n ← [Sink Connector Id Strategy Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/id-strategy/ \"Previous Section\")[Topic Override Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/topic-override/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/topic-override/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Topic Override Properties On this page * [Overview](#overview)\n* [Settings](#settings)\n* [Example](#example) ## Overview Use the following MongoDB Kafka sink connector configuration settings to override global or default property settings for specific topics. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings | Name                                          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **topic.override.<topicName>.<propertyName>** | **Type:** string**Description:**Specify a topic and property name to override the corresponding global or default property setting.ExampleThe topic.override.foo.collection=bar setting instructs the sink connector to store data from the foo topic in the barcollection.NoteYou can specify any valid configuration setting in the<propertyName> segment on a per-topic basis exceptconnection.uri and topics.**Default**: \"\"**Accepted Values**: Accepted values specific to the overridden property | ## Example You can override the sink connector to sink data from specific topics. The following example configuration shows how you can define configuration settings for a topic named `topicA`:\n\n ` After applying these configuration settings, the sink connector performs the following for data consumed from `topicA`: * Write documents to the MongoDB collection `collectionA` in batches of up to 100.\n* Generate a UUID value for each new document and write it to the `_id`field.\n* Omit fields `k2` and `k4` from the value projection using the`BlockList` projection type. For an example of how to configure the Block List Projector, see the[Post Processors](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/#std-label-sink-blocklist-projector-example) guide. ← [Sink Connector Write Model Strategies](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/write-strategies/ \"Previous Section\")[Change Data Capture Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/cdc/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/connector-message/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Connector Message Processing Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the settings on this page to configure the message processing behavior of the MongoDB Kafka sink connector including the following: * Message batch size\n* Rate limits\n* Number of parallel tasks For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ← [Kafka Topic Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/kafka-topic/ \"Previous Section\")[Connector Error Handling Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-connection/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # MongoDB Connection Configuration Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify how your MongoDB Kafka sink connector connects and communicates with your MongoDB cluster. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ← [Sink Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/ \"Previous Section\")[MongoDB Namespace Mapping Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/mongodb-namespace/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/id-strategy/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Sink Connector Id Strategy Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify how the MongoDB Kafka sink connector should determine the `_id` value for each document it writes to MongoDB. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ← [Sink Connector Post-processor Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/post-processors/ \"Previous Section\")[Sink Connector Write Model Strategies](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/write-strategies/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/post-processors/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Sink Connector Post-processor Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify how the MongoDB Kafka sink connector should transform Kafka data before inserting it into MongoDB. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ← [Connector Error Handling Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/ \"Previous Section\")[Sink Connector Id Strategy Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/id-strategy/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Connector Error Handling Properties On this page * [Overview](#overview)\n* [Settings](#settings)\n* [Bulk Write Exceptions](#bulk-write-exceptions)\n* [Dead Letter Queue Configuration Example](#dead-letter-queue-configuration-example) ## Overview Use the following configuration settings to specify how the MongoDB Kafka sink connector handles errors and to configure the dead letter queue. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n ## Bulk Write Exceptions The connector can report the following exceptions to your dead letter queue as context headers when performing bulk writes:\n\n To enable bulk write exception reporting to the dead letter queue, use the following connector configuration: ``` errors.tolerance=all errors.deadletterqueue.topic.name=<name of topic to use as dead letter queue> errors.deadletterqueue.context.headers.enable=true \n``` ## Dead Letter Queue Configuration Example Apache Kafka version 2.6 added support for handling errant records. The Kafka connector automatically sends messages that it cannot process to the**dead letter queue**. Once on the dead letter queue, you can inspect the errant records, update them, and resubmit them for processing. The following is an example configuration for enabling the dead letter queue topic `example.deadletterqueue`. This configuration specifies that the dead letter queue and log file should record invalid messages, and that the dead letter queue messages should include context headers. ``` mongo.errors.tolerance=all mongo.errors.log.enable=true errors.log.include.messages=true errors.deadletterqueue.topic.name=example.deadletterqueue errors.deadletterqueue.context.headers.enable=true \n``` To learn more about dead letter queues, see [Write Errors and Errant Messages to a Topic.](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/error-handling-strategies/#std-label-kafka-sink-errors-dlq) ← [Connector Message Processing Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/connector-message/ \"Previous Section\")[Sink Connector Post-processor Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/post-processors/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/time-series/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Kafka Time Series Properties On this page * [Overview](#overview)\n* [Settings](#settings) ## Overview Use the following configuration settings to specify how the MongoDB Kafka sink connector should sink data to a MongoDB time series collection. For a list of sink connector configuration settings organized by category, see the guide on [Sink Connector Configuration Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/) ## Settings\n\n For an example on how to convert an existing collection to a time series collection, see the tutorial on how to [Migrate an Existing Collection to a Time Series Collection.](https://mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/#std-label-tutorial-migrate-time-series) ← [Change Data Capture Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/cdc/ \"Previous Section\")[All Sink Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/all-properties/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Change Data Capture Handlers On this page * [Overview](#overview)\n* [Specify a CDC Handler](#specify-a-cdc-handler)\n* [Available CDC Handlers](#available-cdc-handlers)\n* [Create Your Own CDC Handler](#create-your-own-cdc-handler)\n* [How to Use Your CDC Handler](#how-to-use-your-cdc-handler) ## Overview Learn how to **replicate** your **change data capture (CDC)** events with a MongoDB Kafka sink connector. CDC is a software architecture that converts changes in a datastore into a stream of **CDC events**. A CDC event is a message containing a reproducible representation of a change performed on a datastore. Replicating data is the process of applying the changes contained in CDC events from one data store onto a different datastore so that the changes occur in both datastores. Use a **CDC handler** to replicate CDC events stored on an Apache Kafka topic into MongoDB. A CDC handler is a program that translates CDC events from a specific**CDC event producer** into MongoDB write operations. A CDC event producer is an application that generates CDC events. CDC event producers can be datastores, or applications that watch datastores and generate CDC events corresponding to changes in the datastores. ## Note MongoDB change streams is an example of a CDC architecture. To learn more about change streams, see[the MongoDB Kafka Connector guide on Change Streams.](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/) If you would like to view a tutorial demonstrating how to replicate data, see the[Replicate Data With a Change Data Capture Handler tutorial.](https://mongodb.com/docs/kafka-connector/current/tutorials/replicate-with-cdc/) ## Important ### CDC and Post Processors You cannot apply a [post processor](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/#std-label-sink-fundamentals-post-processors)to CDC event data. If you attempt to specify both, the connector logs a warning. ## Specify a CDC Handler You can specify a CDC handler on your sink connector with the following configuration option: `| change.data.capture.handler=<cdc handler class> |\n| ----------------------------------------------- | ` To learn more, see[change data capture configuration options.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/cdc/) ### Available CDC Handlers The sink connector provides CDC handlers for the following CDC event producers: * MongoDB\n* [Debezium](https://debezium.io/)\n* [Qlik Replicate](https://www.qlik.com/us/products/qlik-replicate) Click the following tabs to learn how to configure CDC handlers for the preceding event producers: ## Create Your Own CDC Handler If none of the prebuilt CDC handlers fit your use case, you can create your own. Your custom CDC handler is a Java class that implements the `CdcHandler` interface. To learn more, see the[source code for the CdcHandler interface.](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/cdc/CdcHandler.java) To view examples of CDC handler implementations, see[the source code for the prebuilt CDC handlers.](https://github.com/mongodb/mongo-kafka/tree/master/src/main/java/com/mongodb/kafka/connect/sink/cdc) ### How to Use Your CDC Handler To configure your sink connector to use your custom CDC Handler, you must perform the following actions:\n\n To learn how to compile a class to a JAR file,[see this guide from Oracle.](https://docs.oracle.com/javase/tutorial/deployment/jar/build.html) ← [Error Handling](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/error-handling-strategies/ \"Previous Section\")[Source Connector](https://mongodb.com/docs/kafka-connector/current/source-connector/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/write-strategies/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Write Model Strategies On this page * [Overview](#overview)\n* [Bulk Write Operations](#bulk-write-operations)\n* [How to Specify Write Model Strategies](#how-to-specify-write-model-strategies)\n* [Specify a Business Key](#specify-a-business-key)\n* [Examples](#examples)\n* [Update One Timestamps Strategy](#update-one-timestamps-strategy)\n* [Replace One Business Key Strategy](#replace-one-business-key-strategy)\n* [Delete One Business Key Strategy](#delete-one-business-key-strategy)\n* [Custom Write Model Strategies](#custom-write-model-strategies)\n* [Sample Write Model Strategy](#sample-write-model-strategy)\n* [How to Install Your Strategy](#how-to-install-your-strategy) ## Overview This guide shows you how to change the way your MongoDB Kafka sink connector writes data to MongoDB. You can change how your connector writes data to MongoDB for use cases including the following: * Insert documents instead of upserting them\n* Replace documents that match a filter other than the `_id` field\n* Delete documents that match a filter You can configure how your connector writes data to MongoDB by specifying a**write model strategy**. A write model strategy is a class that defines how your sink connector should write data using **write models**. A write model is a MongoDB Java driver interface that defines the structure of a write operation. To learn how to modify the sink records your connector receives before your connector writes them to MongoDB, read the guide on[Sink Connector Post Processors.](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/#std-label-sink-fundamentals-post-processors) To see a write model strategy implementation, see the source code of the[InsertOneDefaultStrategy class.](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/writemodel/strategy/InsertOneDefaultStrategy.java) ## Bulk Write Operations The sink connector writes data to MongoDB using bulk write operations. Bulk writes group multiple write operations, such as inserts, updates, or deletes, together. By default, the sink connector performs ordered bulk writes, which guarantee the order of data changes. In an ordered bulk write, if any write operation results in an error, the connector skips the remaining writes in that batch. If you don't need to guarantee the order of data changes, you can set the `bulk.write.ordered` setting to `false` so that the connector performs unordered bulk writes. The sink connector performs unordered bulk writes in parallel, which can improve performance. In addition, when you enable unordered bulk writes and set the`errors.tolerance` setting to `all`, even if any write operation in your bulk write fails, the connector continues to perform the remaining write operations in the batch that do not return errors. ## Tip To learn more about the `bulk.write.ordered` setting, see the[Connector Message Processing Properties.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/connector-message/#std-label-sink-configuration-message-processing) To learn more about bulk write operations, see the following documentation: * [Server manual entry on ordered and unordered bulk operations.](https://www.mongodb.com/docs/manual/reference/method/Bulk/#ordered-and-unordered-bulk-operations)\n* [Bulk write operations in Java](https://www.mongodb.com/docs/drivers/java/sync/v4.7/fundamentals/crud/write-operations/bulk/#order-of-execution) ## How to Specify Write Model Strategies To specify a write model strategy, use the following setting: `| writemodel.strategy=<write model strategy classname> |\n| ---------------------------------------------------- | `\n\n ### Specify a Business Key A business key is a value composed of one or more fields in your sink record that identifies it as unique. By default, the sink connector uses the `_id`field of the sink record to retrieve the business key. To specify a different business key, configure the Document Id Adder post processor to use a custom value. You can configure the Document Id Adder to set the `_id` field from the sink record key as shown in the following example properties: ``` document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialKeyStrategy document.id.strategy.partial.key.projection.list=<comma-separated field names> document.id.strategy.partial.key.projection.type=AllowList \n``` Alternatively, you can configure it to set the `_id` field from the sink record value as shown in the following example properties: ``` document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy document.id.strategy.partial.value.projection.list=<comma-separated field names> document.id.strategy.partial.value.projection.type=AllowList \n``` ## Important ### Improve Write Performance Create a unique index in your target collection that corresponds to the fields of your business key. This improves the performance of write operations from your sink connector. See the guide on[unique indexes](https://www.mongodb.com/docs/manual/core/index-unique/) for more information. The following write model strategies require a business key: * `ReplaceOneBusinessKeyStrategy`\n* `DeleteOneBusinessKeyStrategy`\n* `UpdateOneBusinessKeyTimestampStrategy` For more information on the Document Id Adder post processor, see[Configure the Document Id Adder Post Processor.](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/#std-label-sink-post-processors-document-id-adder) ## Examples This section shows examples of configuration and output of the following write model strategies: * [Update One Timestamps Strategy](#std-label-kafka-sink-write-model-time-example)\n* [Replace One Business Key Strategy](#std-label-kafka-sink-write-model-replace-example)\n* [Delete One Business Key Strategy](#std-label-kafka-sink-write-model-delete-example) ### Update One Timestamps Strategy You can configure the Update One Timestamps strategy to add and update timestamps when writing documents to MongoDB. This strategy performs the following actions: * When the connector inserts a new MongoDB document, it sets the`_insertedTS` and `_modifiedTS` fields to the current time on the connector's server.\n* When the connector updates an existing MongoDB document, it updates the`_modifiedTS` field to the current time on the connector's server. Suppose you want to track the position of a train along a route, and your sink connector receives messages with the following structure: ``` {   \"_id\": \"MN-1234\",   \"start\": \"Beacon\",   \"destination\": \"Grand Central\"   \"position\": [ 40, -73 ] } \n``` Use the `ProvidedInValueStrategy` to specify that your connector should use the `_id` value of the message to assign the `_id` field in your MongoDB document. Specify your id and write model strategy properties as follows: ``` document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy \n``` After your sink connector processes the preceding example record, it inserts a document that contains the `_insertedTS` and `_modifiedTS` fields as shown in the following document: ``` {   \"_id\": \"MN-1234\",   \"_insertedTS\": ISODate(\"2021-09-20T15:08:000Z\"),   \"_modifiedTS\": ISODate(\"2021-09-20T15:08:000Z\"),   \"start\": \"Beacon\",   \"destination\": \"Grand Central\"\n\n } \n``` After one hour, the train reports its new location along its route with a new position as shown in the following record: ``` {   \"_id\": \"MN-1234\",   \"start\": \"Beacon\",   \"destination\": \"Grand Central\"   \"position\": [ 42, -75 ] } \n``` Once your sink connector processes the preceding record, it inserts a document that contains the following data: ``` {   \"_id\": \"MN-1234\",   \"_insertedTS\": ISODate(\"2021-09-20T15:08:000Z\"),   \"_modifiedTS\": ISODate(\"2021-09-20T16:08:000Z\"),   \"start\": \"Beacon\",   \"destination\": \"Grand Central\"   \"position\": [ 42, -75 ] } \n``` For more information on the `ProvidedInValueStrategy`, see the section on how to [Configure the Document Id Adder Post Processor.](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/#std-label-sink-post-processors-document-id-adder) ### Replace One Business Key Strategy You can configure the Replace One Business Key strategy to replace documents that match the value of the business key. To define a business key on multiple fields of a record and configure the connector to replace documents that contain matching business keys, perform the following tasks: 1. Create a [unique index](https://www.mongodb.com/docs/manual/core/index-unique/) in your collection that corresponds to your business key fields.\n2. Specify the `PartialValueStrategy` id strategy to identify the fields that belong to the business key in the connector configuration.\n3. Specify the `ReplaceOneBusinessKeyStrategy` write model strategy in the connector configuration. Suppose you want to track airplane capacity by the flight number and airport location represented by `flight_no` and `airport_code`, respectively. An example message contains the following information: ``` {   \"flight_no\": \"Z342\",   \"airport_code\": \"LAX\",   \"seats\": {     \"capacity\": 180,     \"occupied\": 152   } } \n``` To implement the strategy, using `flight_no` and `airport_code` as the business key, first create a unique index on these fields in the MongoDB shell: ```javascript db.collection.createIndex({ \"flight_no\": 1, \"airport_code\": 1}, { unique: true }) \n``` Next, specify the `PartialValueStrategy` strategy and business key fields in the a projection list. Specify the id and write model strategy configuration as follows: ``` document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy document.id.strategy.partial.value.projection.list=flight_no,airport_code document.id.strategy.partial.value.projection.type=AllowList writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy \n``` The sample data inserted into the collection contains the following: ``` {   \"flight_no\": \"Z342\"   \"airport_code\": \"LAX\",   \"seats\": {     \"capacity\": 180,     \"occupied\": 152   } } \n``` When the connector processes sink data that matches the business key of the existing document, it replaces the document with the new values without changing the business key fields: ``` {   \"flight_no\": \"Z342\"   \"airport_code\": \"LAX\",   \"status\": \"canceled\" } \n``` After the connector processes the sink data, it replaces the original sample document in MongoDB with the preceding one. ### Delete One Business Key Strategy You can configure the connector to remove a document when it receives messages that match a business key using the Delete One Business Key strategy. To set a business key from multiple fields of a record and configure the connector to delete a document that contains a matching business key, perform the following tasks:\n\n Suppose you need to delete a calendar event from a specific year from a collection that contains a document that resembles the following: ``` {   \"year\": 2005,   \"month\": 3,   \"day\": 15,   \"event\": \"Dentist Appointment\" } \n``` To implement the strategy, using `year` as the business key, first create a unique index on these fields in the MongoDB shell: ```javascript db.collection.createIndex({ \"year\": 1 }, { unique: true }) \n``` Next, specify your business key and write model strategy in your configuration as follows: ``` document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy document.id.strategy.partial.value.projection.list=year document.id.strategy.partial.value.projection.type=AllowList writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneBusinessKeyStrategy \n``` If your connector processes a sink record that contains the business key`year`, it deletes the first document with a matching field value returned by MongoDB. Suppose your connector processes a sink record that contains the following value data: ``` {   \"year\": 2005,   ... } \n``` When the connector processes the preceding record, it deletes the first document from the collection that contains a `year` field with a value of \"2005\" such as the original[\"Dentist Appointment\" sample document.](#std-label-delete-one-business-key-sample-document) ## Custom Write Model Strategies If none of the write model strategies included with the connector fit your use case, you can create your own. A write model strategy is a Java class that implements the`WriteModelStrategy` interface and must override the `createWriteModel()`method. See the[source code for the WriteModelStrategy interface](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/writemodel/strategy/WriteModelStrategy.java)for the required method signature. ### Sample Write Model Strategy The following custom write model strategy returns a write operation that replaces a MongoDB document that matches the `_id` field of your sink record with the value of the `fullDocument` field of your sink record: ```java /**  * Custom write model strategy  *  * This class reads the 'fullDocument' field from a change stream and  * returns a ReplaceOne operation.  */ public class CustomWriteModelStrategy implements WriteModelStrategy {   private static String ID = \"_id\";   @Override   public WriteModel<BsonDocument> createWriteModel(final SinkDocument document) {     BsonDocument changeStreamDocument = document.getValueDoc()         .orElseThrow(() -> new DataException(\"Missing value document\"));     BsonDocument fullDocument = changeStreamDocument.getDocument(\"fullDocument\", new BsonDocument());     if (fullDocument.isEmpty()) {       return null; // Return null to indicate no op.     }     return new ReplaceOneModel<>(Filters.eq(ID, fullDocument.get(ID)), fullDocument);   } } \n``` For another example of a custom write model strategy, see the[UpsertAsPartOfDocumentStrategy](https://github.com/mongodb-university/kafka-edu/blob/main/custom-write-strategy-example/src/main/java/custom/writestrategy/UpsertAsPartOfDocumentStrategy.java)example strategy on GitHub. ### How to Install Your Strategy To configure your sink connector to use a custom write strategy, you must complete the following actions:\n\n To learn how to compile a class to a JAR file, see the[JAR deployment guide](https://docs.oracle.com/javase/tutorial/deployment/jar/build.html)from the Java SE documentation. ← [Fundamentals](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/ \"Previous Section\")[Sink Connector Post Processors](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/error-handling-strategies/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Error Handling On this page * [Overview](#overview)\n* [Handle Errors](#handle-errors)\n* [Stop For All Errors](#stop-for-all-errors)\n* [Tolerate All Errors](#tolerate-all-errors)\n* [Write Errors and Errant Messages to a Topic](#write-errors-and-errant-messages-to-a-topic)\n* [Log Errors](#log-errors)\n* [Handle Errors at the Connector Level](#handle-errors-at-the-connector-level) ## Overview In this guide, you can learn how to handle errors in your MongoDB Kafka sink connector. The following list shows some common scenarios that cause your sink connector to experience an error: * You write to a topic using Avro serialization and try to decode your messages from that topic using Protobuf deserialization\n* You use a change data capture handler on a message that does not contain change event documents\n* You apply an invalid single message transform to incoming documents When your sink connector encounters an error it does two actions: * [Handles the Error](#std-label-kafka-sink-handle-errors)\n* [Logs the Error](#std-label-kafka-sink-log-errors) ## Handle Errors When your connector encounters an error, it needs to handle it in some way. Your sink connector can do the following in response to an error: * [Stop For All Errors](#std-label-kafka-sink-errors-stop) _default_\n* [Tolerate All Errors](#std-label-kafka-sink-tolerate-errors)\n* [Write Errors and Errant Messages to a Topic](#std-label-kafka-sink-errors-dlq) ### Stop For All Errors By default, your sink connector terminates and stops processing messages when it encounters an error. This is a good option for you if any error in your sink connector indicates a serious problem. When your sink connector crashes, you must do one of the following actions and then restart your connector to resume processing messages: * Allow your sink connector to temporarily [tolerate errors](#std-label-kafka-sink-tolerate-errors)\n* Update your sink connector's configuration to allow it to process the message\n* Remove the errant message from your topic You can have your sink connector stop when it encounters an error by either not specifying any value for the `errors.tolerance` option, or by adding the following to your connector configuration: `| errors.tolerance=none |\n| --------------------- | ` ### Tolerate All Errors You can configure your sink connector to tolerate all errors and never stop processing messages. This is a good option for getting your sink connector up and running quickly, but you run the risk of missing problems in your connector as you do not receive any feedback if something goes wrong. You can have your sink connector tolerate all errors by specifying the following option: ``` errors.tolerance=all \n``` ## Warning ### Ordered Bulk Writes Can Result in Skipped Messages If you set your connector to tolerate errors and use ordered bulk writes, you may lose data. If you set your connector to tolerate errors and use unordered bulk writes, you lose less data. To learn more about bulk write operations, see the [Write Model Strategies page.](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/write-strategies/#std-label-sink-connector-bulk-write-ops) ### Write Errors and Errant Messages to a Topic You can configure your sink connector to write errors and errant messages to a topic, called a **dead letter queue**, for you to inspect or process further. A dead letter queue is a location in message queueing systems such as Apache Kafka where the system routes errant messages instead of crashing or ignoring the error. Dead letter queues combine the feedback of stopping the program with the durability of tolerating all errors, and are a good error handling starting point for most deployments. You can have your sink connector route all errant messages to a dead letter queue by specifying the following options: ``` errors.tolerance=all errors.deadletterqueue.topic.name=<name of topic to use as dead letter queue> \n``` If you want to include the specific reason for the error as well as the errant message, use the following option: ```\n\n \n``` To learn more about dead letter queues, see Confluent's guide on[Dead Letter Queues.](https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html#dead-letter-queue) To view another dead letter queue configuration example, see [Dead Letter Queue Configuration Example.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/#std-label-sink-dead-letter-queue-configuration-example) To learn about the exceptions your connector defines and writes as context headers to the dead letter queue, see [Bulk Write Exceptions.](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/#std-label-sink-configuration-error-handling-dlq-errors) ## Log Errors You can record tolerated and untolerated errors to a log file. Click on the tabs to see how to log errors: If you would like to log metadata about your message, such as your message's topic and offset, use the following option: ``` errors.log.include.messages=true \n``` For more information, see Confluent's guide on[logging with Kafka Connect.](https://docs.confluent.io/platform/current/connect/logging.html) ## Handle Errors at the Connector Level The sink connector provides options that allow you to configure error handling at the connector level. The options are as follows: | Kafka Connect Option | MongoDB Kafka Connector Option |\n| -------------------- | ------------------------------ |\n| errors.tolerance     | mongo.errors.tolerance         |\n| errors.log.enable    | mongo.errors.log.enable        | You want to use these options if you want your connector to respond differently to errors related to MongoDB than to errors related to the Kafka Connect framework. For more information, see the following resources: * [Connector Error Handling Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/error-handling/#std-label-sink-configuration-error-handling)\n* [New Names for Error Tolerance Options JIRA Ticket](https://jira.mongodb.org/browse/KAFKA-215) ← [Sink Connector Post Processors](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/ \"Previous Section\")[Change Data Capture Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Migrate an Existing Collection to a Time Series Collection On this page * [Migrate a Collection to a Time Series Collection](#migrate-a-collection-to-a-time-series-collection)\n* [Summary](#summary)\n* [Learn More](#learn-more) Follow this tutorial to learn how to convert an existing MongoDB collection to a **time series collection** using the MongoDB Kafka Connector. Time series collections efficiently store time series data. Time series data consists of measurements taken at time intervals, metadata that describes the measurement, and the time of the measurement. To convert data from a MongoDB collection to a time series collection using the connector, you need to perform the following tasks: 1. Identify the time field common to all documents in the collection.\n2. Configure a source connector to copy the existing collection data to a Kafka topic.\n3. Configure a sink connector to copy the Kafka topic data to the time series collection. In this tutorial, you perform these preceding tasks to migrate stock data from a collection to a time series collection. The time series collection stores and indexes the data more efficiently and retains the ability to analyze stock performance over time using aggregation operators. ## Migrate a Collection to a Time Series Collection 1 ### Complete the Tutorial Setup Complete the steps in the [Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-kafka-tutorials-docker-setup) to start the the Confluent Kafka Connect and MongoDB environment. 2 ### Generate Sample Data Run the following command to start a script in your Docker environment that generates a sample collection containing fabricated stock symbols and their prices in your tutorial MongoDB replica set: `| docker exec -ti mongo1 /bin/bash -c \"cd /stockgenmongo/ && python3 stockgen.py -db Stocks -col PriceData\" |\n| --------------------------------------------------------------------------------------------------------- | ` Once the data generator starts running, you should see the generated data that resembles the following: ```bash ... 1 _id=528e9... MSP MASSIVE SUBMARINE PARTNERS traded at 31.08 2022-05-25 21:15:15 2 _id=528e9... RWH RESPONSIVE_WHOLESALER HOLDINGS traded at 18.42 2022-05-25 21:15:15 3 _id=528e9... FAV FUZZY ATTACK VENTURES traded at 31.08 2022-05-25 21:15:15 ... \n``` 3 ### Configure the Source Connector In a seperate terminal window, create an interactive shell session on the tutorial Docker container downloaded for the Tutorial Setup using the following command: ```bash docker exec -it mongo1 /bin/bash \n``` Create a source configuration file called `stock-source.json` with the following command: ```bash nano stock-source.json \n``` Paste the following configuration information into the file and save your changes: ``` {   \"name\": \"mongo-source-marketdata\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",     \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",     \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",     \"publish.full.document.only\": \"true\",     \"connection.uri\": \"mongodb://mongo1\",     \"topic.prefix\": \"marketdata\",     \"database\": \"Stocks\",     \"collection\": \"PriceData\",     \"copy.existing\": \"true\"   } } \n``` This configuration instructs the connector to copy existing data from the `PriceData` MongoDB collection to the`marketdata.Stocks.PriceData` Kafka topic, and once complete, any future data inserted in that collection. Run the following command in the shell to start the source connector using the configuration file you created: ```bash cx stock-source.json \n``` ## Note The `cx` command is a custom script included in the tutorial development environment. This script runs the following equivalent request to the Kafka Connect REST API to create a new connector: ```bash\n\n \n``` Run the following command in the shell to check the status of the connectors: ```bash status \n``` If your source connector started successfully, you should see the following output: ``` Kafka topics: ... The status of the connectors: source  |  mongo-source-marketdata  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors [ \"mongo-source-marketdata\" ] ... \n``` Once the source connector starts up, confirm the Kafka topic received the collection data by running the following command: ``` kafkacat -b broker:29092 -C -t marketdata.Stocks.PriceData \n``` The output should show topic data as it is published by the source connector that resembles the following: ``` {\"schema\":{ ... }, \"payload\": \"{ \"_id\": { \"$oid\": \"628e9...\"}, \"company_symbol\": \"MSP\", \"Company_name\": \"MASSIVE SUBMARINE PARTNERS\", \"price\": 309.98, \"tx_time\": { \"$date\": 16535...\" }\"} \n``` You can exit `kafkacat` by typing `CTRL+C`. 4 ### Configure the Sink Connector Configure a sink connector to read data from the Kafka topic and write it to a time series collection named `StockDataMigrate` in a database named `Stocks`. Create a sink configuration file called `stock-sink.json` with the following command: ```bash nano stock-sink.json \n``` Paste the following configuration information into the file and save your changes: ``` {   \"name\": \"mongo-sink-marketdata\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSinkConnector\",     \"topics\": \"marketdata.Stocks.PriceData\",     \"connection.uri\": \"mongodb://mongo1\",     \"database\": \"Stocks\",     \"collection\": \"StockDataMigrate\",     \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",     \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",     \"timeseries.timefield\": \"tx_time\",     \"timeseries.timefield.auto.convert\": \"true\",     \"timeseries.timefield.auto.convert.date.format\": \"yyyy-MM-dd'T'HH:mm:ss'Z'\"   } } \n``` ## Tip The sink connector configuration above uses the time field date format converter. Alternatively, you can use the `TimestampConverter`Single Message Transform (SMT) to convert the `tx_time` field from a`String` to an `ISODate`. When using the `TimestampConverter` SMT, you must define a schema for the data in the Kafka topic. For information on how to use the `TimestampConverter` SMT, see the[TimestampConverter](https://docs.confluent.io/platform/current/connect/transforms/timestampconverter.html#timestampconverter)Confluent documentation. Run the following command in the shell to start the sink connector using the configuration file you updated: ```bash cx stock-sink.json \n``` After your sink connector finishes processing the topic data, the documents in the `StockDataMigrate` time series collection contain the `tx_time` field with an `ISODate` type value. 5 ### Verify the Time Series Collection Data Once the sink connector completes processing the topic data, the`StockDataMigrate` time series collection should contain all the market data from your `PriceData` collection. To view the data in MongoDB, run the following command to connect to your replica set using `mongosh`: ```shell mongosh \"mongodb://mongo1\" \n``` At the prompt, type the following commands to retrieve all the documents in the `Stocks.StockDataMigrate` MongoDB namespace: ``` use Stocks db.StockDataMigrate.find() \n``` You should see a list of documents returned from the command that resemble the following document: ```javascript {\n\n     _id: ObjectId(\"628e9...\"),     symbol: 'FAV',     price: 18.43,     company_name: 'FUZZY ATTACK VENTURES' } \n``` ## Summary In this tutorial, you created a stock ticker data generator that periodically wrote data into a MongoDB collection. You configured a source connector to copy the data into a Kafka topic and configured a sink connector to write that data into a new MongoDB time series collection. ## Learn More Read the following resources to learn more about concepts mentioned in this tutorial: * [Kafka Time Series Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/time-series/#std-label-sink-configuration-time-series)\n* [Time Series Collections](https://www.mongodb.com/docs/manual/core/timeseries-collections/) ## Note Send your tutorial feedback or ideas for future MongoDB Kafka Connector tutorials through this [feedback form.](https://tinyurl.com/mongokafkatutorialfeedback) ← [Replicate Data with a Change Data Capture Handler](https://mongodb.com/docs/kafka-connector/current/tutorials/replicate-with-cdc/ \"Previous Section\")[Sink Connector](https://mongodb.com/docs/kafka-connector/current/sink-connector/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/replicate-with-cdc/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Replicate Data with a Change Data Capture Handler On this page * [Overview](#overview)\n* [Replicate Data with a CDC Handler](#replicate-data-with-a-cdc-handler)\n* [Complete the Tutorial Setup](#complete-the-tutorial-setup)\n* [Start Interactive Shells](#start-interactive-shells)\n* [Configure the Source Connector](#configure-the-source-connector)\n* [Configure the Sink Connector](#configure-the-sink-connector)\n* [Monitor the Kafka Topic](#monitor-the-kafka-topic)\n* [Write Data into the Source and Watch the Data Flow](#write-data-into-the-source-and-watch-the-data-flow)\n* [(Optional) Generate Additional Changes](#-optional--generate-additional-changes)\n* [Summary](#summary)\n* [Learn More](#learn-more) ## Overview Follow this tutorial to learn how to use a**change data capture (CDC) handler** to replicate data with the MongoDB Kafka Connector. A CDC handler is an application that translates CDC events into MongoDB write operations. Use a CDC handler when you need to reproduce the changes in one datastore into another datastore. In this tutorial, you configure and run MongoDB Kafka source and sink connectors to make two MongoDB collections contain the same documents using CDC. The source connector writes change stream data from the original collection to a Kafka topic and the sink connector writes the Kafka topic data to the target MongoDB collection. If you want to learn more about how CDC handlers work, see the[Change Data Capture Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/#std-label-sink-fundamentals-cdc-handler) guide. ## Replicate Data with a CDC Handler 1 ### Complete the Tutorial Setup Complete the steps in the [Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-kafka-tutorials-docker-setup) to start the the Confluent Kafka Connect and MongoDB environment. 2 ### Start Interactive Shells Start two interactive shells on the Docker container in separate windows. In the tutorial, you can use the shells to run and observe different tasks. Run the following command from a terminal to start an interactive shell. `| docker exec -it mongo1 /bin/bash |\n| -------------------------------- | ` We will refer to this interactive shell as **CDCShell1** throughout this tutorial. Run the following command in a second terminal to start an interactive shell: ```bash docker exec -it mongo1 /bin/bash \n``` We will refer to this interactive shell as **CDCShell2** throughout this tutorial. Arrange the two windows on your screen to keep both of them visible to see real-time updates. Use **CDCShell1** to configure your connectors and monitor your Kafka topic. Use **CDCShell2** to perform write operations in MongoDB. 3 ### Configure the Source Connector In **CDCShell1**, configure a source connector to read from the`CDCTutorial.Source` MongoDB namespace and write to the`CDCTutorial.Source` Kafka topic. Create a configuration file called `cdc-source.json` using the following command: ```bash nano cdc-source.json \n``` Paste the following configuration information into the file and save your changes: ``` {   \"name\": \"mongo-cdc-source\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",     \"connection.uri\": \"mongodb://mongo1\",     \"database\": \"CDCTutorial\",     \"collection\": \"Source\"   } } \n``` Run the following command in **CDCShell1** to start the source connector using the configuration file you created: ```bash cx cdc-source.json \n``` ## Note The `cx` command is a custom script included in the tutorial development environment. This script runs the following equivalent request to the Kafka Connect REST API to create a new connector: ```bash\n\n \n``` Run the following command in the shell to check the status of the connectors: ```bash status \n``` If your source connector started successfully, you should see the following output: ``` Kafka topics: ... The status of the connectors: source  |  mongo-cdc-source  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors [ \"mongo-cdc-source\" ] ... \n``` 4 ### Configure the Sink Connector In **CDCShell1**, configure a sink connector to copy data from the`CDCTutorial.Source` Kafka topic to `CDCTutorial.Destination`MongoDB namespace. Create a configuration file called `cdc-sink.json` using the following command: ```bash nano cdc-sink.json \n``` Paste the following configuration information into the file and save your changes: ``` {   \"name\": \"mongo-cdc-sink\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSinkConnector\",     \"topics\": \"CDCTutorial.Source\",     \"change.data.capture.handler\": \"com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler\",     \"connection.uri\": \"mongodb://mongo1\",     \"database\": \"CDCTutorial\",     \"collection\": \"Destination\"   } } \n``` Run the following command in the shell to start the sink connector using the configuration file you created: ```bash cx cdc-sink.json \n``` Run the following command in the shell to check the status of the connectors: ```bash status \n``` If your sink connector started successfully, you should see the following output: ``` Kafka topics: ... The status of the connectors: sink    |  mongo-cdc-sink    |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSinkConnector source  |  mongo-cdc-source  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors [ \"mongo-cdc-sink\" \"mongo-cdc-source\" ] ... \n``` 5 ### Monitor the Kafka Topic In **CDCShell1**, monitor the Kafka topic for incoming events. Run the following command to start the `kafkacat` application which outputs data published to the topic: ```bash kc CDCTutorial.Source \n``` ## Note The `kc` command is a custom script included in the tutorial development environment that calls the `kafkacat` application with options to connect to Kafka and format the output of the specified topic. Once started, you should see the following output that indicates there is currently no data to read: ``` % Reached end of topic CDCTutorial.Source [0] at offset 0 \n``` 6 ### Write Data into the Source and Watch the Data Flow In **CDCShell2**, connect to MongoDB using `mongosh`, the MongoDB shell by running the following command: ```bash mongosh \"mongodb://mongo1\" \n``` After you connect successfully, you should see the following MongoDB shell prompt: ``` rs0 [direct: primary] test> \n``` At the prompt, type the following commands to insert a new document into the `CDCTutorial.Source` MongoDB namespace: ``` use CDCTutorial db.Source.insertOne({ proclaim: \"Hello World!\" }); \n``` Once MongoDB completes the insert command, you should receive an acknowledgment that resembles the following text: ``` {   acknowledged: true,   insertedId: ObjectId(\"600b38ad...\") } \n``` The source connector picks up the change and publishes it to the Kafka topic. You should see the following topic message in your**CDCShell1** window: ``` {   \"schema\": { \"type\": \"string\", \"optional\": false },   \"payload\": {     \"_id\": { \"_data\": \"8260...\" },     \"operationType\": \"insert\",     \"clusterTime\": { \"$timestamp\": { \"t\": 1611..., \"i\": 2 } },     \"wallTime\": { \"$date\": \"...\" },\n\n       \"_id\": { \"$oid\": \"600b38ad...\" },       \"proclaim\": \"Hello World!\"     },     \"ns\": { \"db\": \"CDCTutorial\", \"coll\": \"Source\" },     \"documentKey\": { \"_id\": { \"$oid\": \"600b38a...\" } }   } } \n``` The sink connector picks up the Kafka message and sinks the data into MongoDB. You can retrieve the document from the`CDCTutorial.Destination` namespace in MongoDB by running the following command in the MongoDB shell you started in **CDCShell2**: ``` db.Destination.find() \n``` You should see the following document returned as the result: ``` [   {     _id: ObjectId(\"600b38a...\"),     proclaim: 'Hello World'   } ] \n``` 7 ### (Optional) Generate Additional Changes Try removing documents from the `CDCTutorial.Source` namespace by running the following command from the MongoDB shell: ``` db.Source.deleteMany({}) \n``` You should see the following topic message in your **CDCShell1**window: ``` {   \"schema\": { \"type\": \"string\", \"optional\": false },   \"payload\": {     \"_id\": { \"_data\": \"8261....\" },     ...     \"operationType\": \"delete\",     \"clusterTime\": { \"$timestamp\": { \"t\": 1631108282, \"i\": 1 } },     \"ns\": { \"db\": \"CDCTutorial\", \"coll\": \"Source\" },     \"documentKey\": { \"_id\": { \"$oid\": \"6138...\" } }   } } \n``` Run the following command to retrieve the current number of documents in the collection: ``` db.Destination.count() \n``` This returns the following output, indicating the collection is empty: ``` 0 \n``` Run the following command to exit the MongoDB shell: ``` exit \n``` ## Summary In this tutorial, you set up a source connector to capture changes to a MongoDB collection and send them to Apache Kafka. You also configured a sink connector with a MongoDB CDC Handler to move the data from Apache Kafka to a MongoDB collection. ## Learn More Read the following resources to learn more about concepts mentioned in this tutorial: * [Change Data Capture Handlers](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/#std-label-sink-fundamentals-cdc-handler)\n* [Change Streams](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/#std-label-kafka-source-change-streams)\n* [Sink Connector](https://mongodb.com/docs/kafka-connector/current/sink-connector/#std-label-kafka-sink-overview)\n* [Source Connector](https://mongodb.com/docs/kafka-connector/current/source-connector/#std-label-kafka-source-overview) ## Note Send your tutorial feedback or ideas for future MongoDB Kafka Connector tutorials through this [feedback form.](https://tinyurl.com/mongokafkatutorialfeedback) ← [Getting Started with the MongoDB Kafka Sink Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/sink-connector/ \"Previous Section\")[Migrate an Existing Collection to a Time Series Collection](https://mongodb.com/docs/kafka-connector/current/tutorials/migrate-time-series/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/sink-connector/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Getting Started with the MongoDB Kafka Sink Connector On this page * [Get Started with the MongoDB Kafka Sink Connector](#get-started-with-the-mongodb-kafka-sink-connector)\n* [Summary](#summary)\n* [Learn More](#learn-more) Follow this tutorial to learn how to configure a MongoDB Kafka sink connector to read data from an Apache Kafka topic and write it to a MongoDB collection. ## Get Started with the MongoDB Kafka Sink Connector 1 ### Complete the Tutorial Setup Complete the steps in the [Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-kafka-tutorials-docker-setup) to start the the Confluent Kafka Connect and MongoDB environment. 2 ### Configure the Sink Connector Create an interactive shell session on the tutorial Docker Container using the following command: `| docker exec -it mongo1 /bin/bash |\n| -------------------------------- | ` Create a source configuration file called `simplesink.json` with the following command: ```bash nano simplesink.json \n``` Paste the following configuration information into the file and save your changes: ``` {   \"name\": \"mongo-tutorial-sink\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSinkConnector\",     \"topics\": \"Tutorial2.pets\",     \"connection.uri\": \"mongodb://mongo1\",     \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",     \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",     \"value.converter.schemas.enable\": false,     \"database\": \"Tutorial2\",     \"collection\": \"pets\"   } } \n``` ## Note The highlighted lines in the configuration properties specify**converters** which instruct the connector how to translate the data from Kafka. Run the following command in the shell to start the sink connector using the configuration file you created: ```bash cx simplesink.json \n``` ## Note The `cx` command is a custom script included in the tutorial development environment. This script runs the following equivalent request to the Kafka Connect REST API to create a new connector: ```bash curl -X POST -H \"Content-Type: application/json\" -d @simplesink.json http://connect:8083/connectors -w \"\\n\" \n``` Run the following command in the shell to check the status of the connectors: ```bash status \n``` If your sink connector started successfully, you should see the following output: ``` Kafka topics: ... The status of the connectors: sink  |  mongo-tutorial-sink  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSinkConnector Currently configured connectors [ \"mongo-tutorial-sink\" ] ... \n``` 3 ### Write Data to a Kafka Topic In the same shell, create a Python script to write data to a Kafka topic. ```bash nano kafkawrite.py \n``` Paste the following code into the file and save your changes: ```python from kafka import KafkaProducer import json from json import dumps p = KafkaProducer(bootstrap_servers = ['broker:29092'], value_serializer = lambda x:dumps(x).encode('utf-8')) data = {'name': 'roscoe'} p.send('Tutorial2.pets', value = data) p.flush() \n``` Run the Python script: ```bash python3 kafkawrite.py \n``` 4 ### View the Data in the MongoDB Collection In the same shell, connect to MongoDB using `mongosh`, the MongoDB shell by running the following command: ```bash mongosh \"mongodb://mongo1\" \n``` After you connect successfully, you should see the following MongoDB shell prompt: ``` rs0 [direct: primary] test> \n```\n\n ```javascript use Tutorial2 db.pets.find() \n``` You should see the following document returned as the result: ``` { _id: ObjectId(\"62659...\"), name: 'roscoe' } \n``` Exit the MongoDB shell by entering the command `exit`. 5 ### (Optional) Stop the Docker Containers After you complete this tutorial, free resources on your computer by stopping or removing Docker assets. You can choose to remove both the Docker containers and images, or exclusively the containers. If you remove the containers and images, you must download them again to restart your MongoDB Kafka Connector development environment, which is approximately 2.4 GB in size. If you exclusively remove the containers, you can reuse the images and avoid downloading most of the large files in the sample data pipeline. ## Tip ### More Tutorials If you plan to complete any more MongoDB Kafka Connector tutorials, consider removing only containers. If you don't plan to complete any more MongoDB Kafka Connector tutorials, consider removing containers and images. Select the tab that corresponds to the removal task you want to run. To restart the containers, follow the same steps required to start them in the [Tutorial Setup.](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-tutorial-setup-run-environment) ## Summary In this tutorial, you configured a sink connector to save data from a Kafka topic to a collection in a MongoDB cluster. ## Learn More Read the following resources to learn more about concepts mentioned in this tutorial: * [Sink Connector Configuration Properties](https://mongodb.com/docs/kafka-connector/current/sink-connector/configuration-properties/#std-label-kafka-sink-configuration-properties)\n* [Introduction to Kafka Connector Converters](https://mongodb.com/docs/kafka-connector/current/introduction/converters/#std-label-intro-converters)\n* [Kafka Connect REST API](https://developer.confluent.io/learn-kafka/kafka-connect/rest-api/) ## Note Send your tutorial feedback or ideas for future MongoDB Kafka Connector tutorials through this [feedback form.](https://tinyurl.com/mongokafkatutorialfeedback) ← [Getting Started with the MongoDB Kafka Source Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/source-connector/ \"Previous Section\")[Replicate Data with a Change Data Capture Handler](https://mongodb.com/docs/kafka-connector/current/tutorials/replicate-with-cdc/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Kafka Connector Tutorial Setup The tutorials in this section run on a development environment using Docker to package the dependencies and configurations you need to run the MongoDB Connector for Apache Kafka. Make sure you complete the development environment setup steps before proceeding to the tutorials. ## Requirements * Create or sign into your Docker account to download the Docker containers you need for this tutorial. To learn how to sign up for an account and install Docker Desktop, read the signup and download steps in the[Docker Hub Quickstart.](https://docs.docker.com/docker-hub/)\n* A terminal app and shell. For MacOS users, use Terminal or a similar app. For Windows users, use PowerShell.\n* Optional. Install **git** to download the setup files. To learn how to install git, read the [Git Downloads](https://git-scm.com/downloads)page. ## Set Up Your Development Environment with Docker 1 ### Clone or Download the Tutorial Repository Next, clone the tutorial git repository with the following command: `| git clone https://github.com/mongodb-university/kafka-edu.git |\n| ------------------------------------------------------------- | ` If you do not have git installed, you can download the[zip archive](https://github.com/mongodb-university/kafka-edu/archive/refs/heads/main.zip)instead. 2 ### Run the Development Environment Select the tab that matches your OS for instructions on how to run the commands in this guide: Start the Docker image with the following command: ```bash docker-compose -p mongo-kafka up -d \n``` ## Note ### Port Mappings The sandbox maps the following services to ports on your host machine: * The sandbox MongoDB server maps to port `35001` on your host machine\n* The sandbox Kafka Connect JMX server maps to port `35000` on your host machine These ports must be free to start the sandbox. The \"mongo-kafka-base\" image creates a Docker container that includes all the services you need in the tutorial and runs them on a shared network called \"mongodb-kafka-base\\_localnet\" as shown in the following diagram: ![Diagram that shows the Docker compose containers in mongo-kafka-base](https://mongodb.com/docs/kafka-connector/current/includes/figures/mongo-kafka-base-container.png) When the command completes successfully, it outputs the following text: ``` ... Creating zookeeper ... done Creating broker    ... done Creating schema-registry ... done Creating connect         ... done Creating rest-proxy      ... done Creating mongo1          ... done Creating mongo1-setup    ... done \n``` 3 ### Verify the Successful Setup Confirm the development environment started normally by running the following commands: ```bash docker exec mongo1 status \n``` This command should output the following information if the Docker development environment was set up successfully: ``` Kafka topics:     \"topic\": \"docker-connect-status\",     \"topic\": \"docker-connect-offsets\",     \"topic\": \"docker-connect-configs\",     \"topic\": \"__consumer_offsets\", The status of the connectors: Currently configured connectors [] Version of MongoDB Connector for Apache Kafka installed: {\"class\":\"com.mongodb.kafka.connect.MongoSinkConnector\",\"type\":\"sink\",\"version\":\"1.8.0\"} {\"class\":\"com.mongodb.kafka.connect.MongoSourceConnector\",\"type\":\"source\",\"version\":\"1.8.0\"} \n``` Since you have not started the connectors, the status and configured list are empty. Your development environment setup is complete and you can proceed to the next step of the tutorial. ## Tip ### Connect To Development Environment MongoDB You can connect to the MongoDB server running in your development environment with the following connection string: ``` mongodb://localhost:35001/?directConnection=true \n``` ← [Tutorials](https://mongodb.com/docs/kafka-connector/current/tutorials/ \"Previous Section\")[Explore MongoDB Change Streams](https://mongodb.com/docs/kafka-connector/current/tutorials/explore-change-streams/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/explore-change-streams/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Explore MongoDB Change Streams On this page * [Explore Change Streams](#explore-change-streams)\n* [Summary](#summary)\n* [Learn More](#learn-more) Follow this tutorial to learn how to create a change stream on a MongoDB collection and observe the change events it creates. ## Explore Change Streams 1 ### Complete the Tutorial Setup Complete the steps in the [Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-kafka-tutorials-docker-setup) to start the the Confluent Kafka Connect and MongoDB environment. 2 ### Connect to the Docker Container Create two interactive shell sessions on the tutorial Docker Container, each in a separate window. Run the following command from a terminal to start an interactive shell. `| docker exec -it mongo1 /bin/bash |\n| -------------------------------- | ` We will refer to this interactive shell as **ChangeStreamShell1** throughout this tutorial. Run the following command in a second terminal to start an interactive shell: ```bash docker exec -it mongo1 /bin/bash \n``` We will refer to this interactive shell as **ChangeStreamShell2** throughout this tutorial. 3 ### Open a Change Stream In **ChangeStreamShell1**, create a Python script to open a change stream using the PyMongo driver. ```bash nano openchangestream.py \n``` Paste the following code into the file and save the changes: ```python import pymongo from bson.json_util import dumps client = pymongo.MongoClient('mongodb://mongo1') db = client.get_database(name='Tutorial1') with db.orders.watch() as stream:     print('\\nA change stream is open on the Tutorial1.orders namespace.  Currently watching ...\\n\\n')     for change in stream:         print(dumps(change, indent = 2)) \n``` Run the Python script: ```bash python3 openchangestream.py \n``` The script outputs the following message after it starts successfully: ```bash Change Stream is opened on the Tutorial1.orders namespace.  Currently watching ... \n``` 4 ### Trigger a Change Event In **ChangeStreamShell2**, connect to MongoDB using `mongosh`, the MongoDB shell, using the following command: ```bash mongosh \"mongodb://mongo1\" \n``` After you connect successfully, you should see the following MongoDB shell prompt: ``` rs0 [direct: primary] test> \n``` At the prompt, type the following commands: ```javascript use Tutorial1 db.orders.insertOne( { 'test' : 1 } ) \n``` After entering the preceding commands, switch to **ChangeStreamShell1** to view the change stream output, which should resemble the following: ``` {   \"_id\": {     \"_data\": \"826264...\"   },   \"operationType\": \"insert\",   \"clusterTime\": {     \"$timestamp\": {       \"t\": 1650754657,       \"i\": 1     }   },   \"wallTime\": {     \"$date\": \"2022-10-13T17:06:23.409Z\"   },   \"fullDocument\": {     \"_id\": {       \"$oid\": \"<_id value of document>\"     },     \"test\": 1   },   \"ns\": {     \"db\": \"Tutorial1\",     \"coll\": \"orders\"   },   \"documentKey\": {     \"_id\": {       \"$oid\": \"<_id value of document>\"     }   } } \n``` To stop the script, press `Ctrl+C`. By the end of this step, you've successfully triggered and observed a change stream event. 5 ### Open a Filtered Change Stream You can apply a filter to a change stream by passing it an aggregation pipeline. In **ChangeStreamShell1**, create a new Python script to open a filtered change stream using the PyMongo driver. ```bash nano pipeline.py \n``` Paste the following code into the file and save the changes: ```python import pymongo from bson.json_util import dumps\n\n db = client.get_database(name='Tutorial1') pipeline = [ { \"$match\": { \"$and\": [ { \"fullDocument.type\": \"temp\" }, { \"fullDocument.value\": { \"$gte\": 100 } } ] } } ] with db.sensors.watch(pipeline=pipeline) as stream:     print('\\nChange Stream is opened on the Tutorial1.sensors namespace.  Currently watching for values > 100...\\n\\n')     for change in stream:         print(dumps(change, indent = 2)) \n``` Run the Python script: ```python python3 pipeline.py \n``` The script outputs the following message after it starts successfully: ``` Change Stream is opened on the Tutorial1.sensors namespace.  Currently watching for values > 100... \n``` 6 ### Observe the Filtered Change Stream Return to your **ChangeStreamShell2** session which should be connected to MongoDB using `mongosh`. At the prompt, type the following commands: ```javascript use Tutorial1 db.sensors.insertOne( { 'type' : 'temp', 'value':101 } ) \n``` As indicated by the script output, the change stream creates a change event because it matches the following pipeline: ``` [ { \"$match\": { \"$and\": [ { \"fullDocument.type\": \"temp\" }, { \"fullDocument.value\": { \"$gte\": 100 } } ] } } ] \n``` Try inserting the following documents in in **ChangeStreamShell2** to verify the change stream only produces events when the documents match the filter: ```javascript db.sensors.insertOne( { 'type' : 'temp', 'value': 99 } ) db.sensors.insertOne( { 'type' : 'pressure', 'value': 22 } ) \n``` 7 ### (Optional) Stop the Docker Containers After you complete this tutorial, free resources on your computer by stopping or removing Docker assets. You can choose to remove both the Docker containers and images, or exclusively the containers. If you remove the containers and images, you must download them again to restart your MongoDB Kafka Connector development environment, which is approximately 2.4 GB in size. If you exclusively remove the containers, you can reuse the images and avoid downloading most of the large files in the sample data pipeline. ## Tip ### More Tutorials If you plan to complete any more MongoDB Kafka Connector tutorials, consider removing only containers. If you don't plan to complete any more MongoDB Kafka Connector tutorials, consider removing containers and images. Select the tab that corresponds to the removal task you want to run. To restart the containers, follow the same steps required to start them in the [Tutorial Setup.](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-tutorial-setup-run-environment) ## Summary In this tutorial, you created a change stream on MongoDB and observed the output. The MongoDB Kafka source connector reads the change events from a change stream that you configure, and writes them to a Kafka topic. To learn how to configure a change stream and Kafka topic for a source connector, proceed to the [Getting Started with the MongoDB Kafka Source Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/source-connector/#std-label-kafka-tutorial-source-connector)tutorial. ## Learn More Read the following resources to learn more about concepts mentioned in this tutorial: * [Change Streams and the Source Connector](https://mongodb.com/docs/kafka-connector/current/source-connector/fundamentals/change-streams/#std-label-kafka-source-change-streams)\n* [Modify Change Stream Output](https://www.mongodb.com/docs/manual/changeStreams/#modify-change-stream-output)\n* [MongoDB Shell (mongosh)](https://www.mongodb.com/docs/mongodb-shell/) ## Note Send your tutorial feedback or ideas for future MongoDB Kafka Connector tutorials through this [feedback form.](https://tinyurl.com/mongokafkatutorialfeedback) ← [Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/ \"Previous Section\")[Getting Started with the MongoDB Kafka Source Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/source-connector/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/tutorials/source-connector/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Getting Started with the MongoDB Kafka Source Connector On this page * [Get Started with the MongoDB Kafka Source Connector](#get-started-with-the-mongodb-kafka-source-connector)\n* [Summary](#summary)\n* [Learn More](#learn-more) Follow this tutorial to learn how to configure a MongoDB Kafka source connector to read data from a change stream and publish it to an Apache Kafka topic. ## Get Started with the MongoDB Kafka Source Connector 1 ### Complete the Tutorial Setup Complete the steps in the [Kafka Connector Tutorial Setup](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-kafka-tutorials-docker-setup) to start the the Confluent Kafka Connect and MongoDB environment. 2 ### Configure the Source Connector Create an interactive shell session on the tutorial Docker container downloaded for the Tutorial Setup using the following command: `| docker exec -it mongo1 /bin/bash |\n| -------------------------------- | ` Create a source configuration file called `simplesource.json` with the following command: ```bash nano simplesource.json \n``` Paste the following configuration information into the file and save your changes: ``` {   \"name\": \"mongo-simple-source\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",     \"connection.uri\": \"mongodb://mongo1\",     \"database\": \"Tutorial1\",     \"collection\": \"orders\"   } } \n``` Run the following command in the shell to start the source connector using the configuration file you created: ```bash cx simplesource.json \n``` ## Note The `cx` command is a custom script included in the tutorial development environment. This script runs the following equivalent request to the Kafka Connect REST API to create a new connector: ```bash curl -X POST -H \"Content-Type: application/json\" -d @simplesource.json http://connect:8083/connectors -w \"\\n\" \n``` Run the following command in the shell to check the status of the connectors: ```bash status \n``` If your source connector started successfully, you should see the following output: ``` Kafka topics: ... The status of the connectors: source  |  mongo-simple-source  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector Currently configured connectors [   \"mongo-simple-source\" ] ... \n``` 3 ### Create Change Events In the same shell, connect to MongoDB using `mongosh`, the MongoDB shell by running the following command: ```bash mongosh \"mongodb://mongo1\" \n``` After you connect successfully, you should see the following MongoDB shell prompt: ``` rs0 [direct: primary] test> \n``` At the prompt, type the following commands to insert a new document: ```javascript use Tutorial1 db.orders.insertOne( { 'order_id' : 1, 'item' : 'coffee' } ) \n``` Once MongoDB completes the insert command, you should receive an acknowledgment that resembles the following text: ``` {   acknowledged: true,   insertedId: ObjectId(\"627e7e...\") } \n``` Exit the MongoDB shell by entering the command `exit`. Check the status of your Kafka environment using the following command: ```bash status \n``` In the output of the preceding command, you should see the new topic that the source connector created after receiving the change event: ``` ... \"topic\": \"Tutorial1.orders\", ... \n``` Confirm the content of data on the new Kafka topic by running the following command: ```bash kc Tutorial1.orders \n``` ## Note The `kc` command is a helper script that outputs the content of a Kafka topic. You should see the following Kafka topic data, organized by \"Key\" and \"Value\" sections when you run the preceding command: From the \"Value\" section of the output, you can find the part of the`payload` that includes the `fullDocument` data as highlighted in the following formatted JSON document: ``` {   \"_id\": {     \"_data\": \"8262655A...\"   },\n\n   \"clusterTime\": {     \"$timestamp\": {       \"t\": 1650809557,       \"i\": 2     }   },   \"wallTime\": {     \"$date\": \"2022-10-13T17:06:23.409Z\"   },   \"fullDocument\": {     \"_id\": {       \"$oid\": \"62655a...\"     },     \"order_id\": 1,     \"item\": \"coffee\"   },   \"ns\": {     \"db\": \"Tutorial1\",     \"coll\": \"orders\"   },   \"documentKey\": {     \"_id\": {       \"$oid\": \"62655a...\"     }   } } \n``` 4 ### Reconfigure the Change Stream You can omit the metadata from the events created by the change stream by configuring it to only return the `fullDocument` field. Stop the connector using the following command: ```bash del mongo-simple-source \n``` ## Note The `del` command is a helper script that calls the Kafka Connect REST API to stop the connector and is equivalent to the following command: ```bash curl -X DELETE connect:8083/connectors/<parameter> \n``` Edit the source configuration file called `simplesource.json` with the following command: ```bash nano simplesource.json \n``` Remove the existing configuration, add the following configuration, and save the file: ``` {   \"name\": \"mongo-simple-source\",   \"config\": {     \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",     \"connection.uri\": \"mongodb://mongo1\",     \"publish.full.document.only\": true,     \"database\": \"Tutorial1\",     \"collection\": \"orders\"   } } \n``` Run the following command in the shell to start the source connector using the configuration file you updated: ```bash cx simplesource.json \n``` Connect to MongoDB using `mongosh` using the following command: ```bash mongosh \"mongodb://mongo1\" \n``` At the prompt, type the following commands to insert a new document: ```bash use Tutorial1 db.orders.insertOne( { 'order_id' : 2, 'item' : 'oatmeal' } ) \n``` Exit `mongosh` by running the following command: ```bash exit \n``` Confirm the content of data on the new Kafka topic by running the following command: ```bash kc Tutorial1.orders \n``` The `payload` field in the \"Value\" document should contain only the following document data: ``` { \"_id\": { \"$oid\": \"<your _id value>\" }, \"order_id\": 2, \"item\": \"oatmeal\" } \n``` 5 ### (Optional) Stop the Docker Containers After you complete this tutorial, free resources on your computer by stopping or removing Docker assets. You can choose to remove both the Docker containers and images, or exclusively the containers. If you remove the containers and images, you must download them again to restart your MongoDB Kafka Connector development environment, which is approximately 2.4 GB in size. If you exclusively remove the containers, you can reuse the images and avoid downloading most of the large files in the sample data pipeline. ## Tip ### More Tutorials If you plan to complete any more MongoDB Kafka Connector tutorials, consider removing only containers. If you don't plan to complete any more MongoDB Kafka Connector tutorials, consider removing containers and images. Select the tab that corresponds to the removal task you want to run. To restart the containers, follow the same steps required to start them in the [Tutorial Setup.](https://mongodb.com/docs/kafka-connector/current/tutorials/tutorial-setup/#std-label-tutorial-setup-run-environment) ## Summary In this tutorial, you started a source connector using different configurations to alter the change stream event data published to a Kafka topic. ## Learn More Read the following resources to learn more about concepts mentioned in this tutorial:\n\n ## Note Send your tutorial feedback or ideas for future MongoDB Kafka Connector tutorials through this [feedback form.](https://tinyurl.com/mongokafkatutorialfeedback) ← [Explore MongoDB Change Streams](https://mongodb.com/docs/kafka-connector/current/tutorials/explore-change-streams/ \"Previous Section\")[Getting Started with the MongoDB Kafka Sink Connector](https://mongodb.com/docs/kafka-connector/current/tutorials/sink-connector/ \"Next Section\") →",
  "https://www.mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/post-processors/": " [Docs Home](https://www.mongodb.com/docs/) → [MongoDB Kafka Connector](https://mongodb.com/docs/kafka-connector/current/) # Sink Connector Post Processors On this page * [Overview](#overview)\n* [How Post Processors Modify Data](#how-post-processors-modify-data)\n* [How to Specify Post Processors](#how-to-specify-post-processors)\n* [Prebuilt Post Processors](#prebuilt-post-processors)\n* [Configure the Document Id Adder Post Processor](#configure-the-document-id-adder-post-processor)\n* [Post Processor Examples](#post-processor-examples)\n* [Allow List and Block List Examples](#allow-list-and-block-list-examples)\n* [Allow List Projector Example](#allow-list-projector-example)\n* [Block List Projector Example](#block-list-projector-example)\n* [Projection Wildcard Pattern Matching Examples](#projection-wildcard-pattern-matching-examples)\n* [Field Renaming Examples](#field-renaming-examples)\n* [How to Create a Custom Post Processor](#how-to-create-a-custom-post-processor) ## Overview On this page, you can learn how to configure **post processors** in your MongoDB Kafka sink connector. Post processors modify sink records that the connector reads from a Kafka topic before the connector stores it in your MongoDB collection. A few examples of data modifications post processors can make include: * Set the document `_id` field to a custom value\n* Include or exclude message key or value fields\n* Rename fields You can use the prebuilt post processors included in the connector or implement your own. See the following sections for more information on post processors: * [How Post Processors Modify Data](#std-label-sink-post-processors-modify-data)\n* [How to Specify Post Processors](#std-label-sink-post-processors-specify)\n* [Prebuilt Post Processors](#std-label-sink-post-processors-prebuilt-list)\n* [Configure the Document Id Adder Post Processor](#std-label-sink-post-processors-document-id-adder)\n* [Post Processor Configuration Examples](#std-label-sink-post-processors-examples)\n* [Create a Custom Post Processor](#std-label-sink-post-processors-custom) ## How Post Processors Modify Data Post processors modify data read from a Kafka topic. The connector stores the message in a `SinkDocument` class which contains a representation of the Kafka `SinkRecord` key and value fields. The connector sequentially applies any post processors specified in the configuration and stores the result in a MongoDB collection. Post processors perform data modification tasks such as generating the document `_id` field, projecting message key or value fields, and renaming fields. You can use the prebuilt post processors included in the connector, or you can implement your own by extending the[PostProcessor](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/PostProcessor.java)class. ## Important ### Post Processors and Change Data Capture (CDC) Handlers You cannot apply a post processor to [CDC handler](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/change-data-capture/#std-label-sink-fundamentals-cdc-handler)event data. If you specify both, the connector logs a warning. ## How to Specify Post Processors You can specify one or more post processors in the `post.processor.chain`configuration setting as a comma-separated list. If you specify more than one, the connector applies them sequentially in which each post processor modifies the data output by the prior one. To ensure the documents the connector writes to MongoDB contain unique `_id`fields, it automatically adds the `DocumentIdAdder` post processor in the first position of the chain if you do not otherwise include it. The following example setting specifies that the connector should run the`KafkaMetaAdder` post processor first and then the`AllowListValueProjector` post processor on the output.\n\n ` ## Prebuilt Post Processors The following table contains a list of all the post processors included in the sink connector.\n\n ### Configure the Document Id Adder Post Processor The `DocumentIdAdder` post processor uses a **strategy** to determine how it should format the `_id` field in the MongoDB document. A strategy defines preset behavior that you can customize for your use case. You can specify a strategy for this post processor in the`document.id.strategy` setting as shown in the following example: ``` document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy \n``` The following table shows a list of the strategies you can use to configure the `DocumentIdAdder` post processor:\n\n #### Create a Custom Document Id Strategy If the built-in document id adder strategies do not cover your use case, you can define a custom document id strategy by following the steps below: 1. Create a Java class that implements the interface[IdStrategy](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/id/strategy/IdStrategy.java)and contains your custom configuration logic.\n2. Compile the class to a JAR file.\n3. Add the compiled JAR to the class path / plugin path for all your Kafka workers. For more information about plugin paths, see the[Confluent documentation.](https://docs.confluent.io/current/connect/managing/community.html)\n4. Update the `document.id.strategy` setting to the full class name of your custom class in all your Kafka workers. ## Note ### Selected strategy may have implications on delivery semantics BSON ObjectId or UUID strategies can only guarantee at-least-once delivery since the connector generates new ids on retries or when processing records again. Other strategies permit exactly-once delivery if you can guarantee the fields that form the document id are unique. For example implementations of the `IdStrategy` interface, see the source code directory that contains[id strategy implementations](https://github.com/mongodb/mongo-kafka/tree/master/src/main/java/com/mongodb/kafka/connect/sink/processor/id/strategy)packaged with the connector. ## Post Processor Examples This section shows examples of configuration and sample output of the following types of post processors: * [Allow List and Block List](#std-label-sink-allowlist-blocklist-examples)\n* [Projection Wildcard Pattern Matching](#std-label-sink-projection-wildcard-examples)\n* [Field Renaming](#std-label-sink-field-renamer-examples) ### Allow List and Block List Examples The **allow list** and **block list** projector post processors determine which fields to include and exclude from the output. When you use the **allow list** projector, the post processor only outputs data from the fields that you specify. When you use the **block list** projector, the post process only omits data from the fields that you specify. ## Note You can use the \".\" (dot) notation to reference nested fields in the record. You can also use the notation to reference fields of documents in an array. When you add a projector to your post processor chain, you must specify the projector type and whether to apply it to the key or value portion of the sink document. See the following sections for example projector configurations and output. ### Allow List Projector Example Suppose your Kafka record value documents resembled the following user profile data: ``` {   \"name\": \"Sally Kimball\",   \"age\": 10,   \"address\": {     \"city\": \"Idaville\",     \"country\": \"USA\"   },   \"hobbies\": [     \"reading\",     \"solving crime\"   ] } \n``` You can configure the `AllowList` value projector to store select data such as the \"name\", \"address.city\", and \"hobbies\" fields from your value documents using the following settings: ``` post.processor.chain=com.mongodb.kafka.connect.sink.processor.AllowListValueProjector value.projection.type=AllowList value.projection.list=name,address.city,hobbies \n``` After the post processor applies the projection, it outputs the following record: ``` {   \"name\": \"Sally Kimball\",   \"address\": {     \"city\": \"Idaville\"   },   \"hobbies\": [     \"reading\",     \"solving crime\"   ] } \n``` ### Block List Projector Example Suppose your Kafka record key documents resembled the following user identification data: ``` {   \"username\": \"user5983\",   \"registration\": {     \"date\": \"2021-09-13\",     \"source\": \"mobile\"   },   \"authToken\": {     \"alg\": \"HS256\",     \"type\": \"JWT\",\n\n   } } \n``` You can configure the `BlockList` key projector to omit the \"authToken\" and \"registration.source\" fields before storing the data with the following settings: ``` post.processor.chain=com.mongodb.kafka.connect.sink.processor.BlockListKeyProjector key.projection.type=BlockList key.projection.list=authToken,registration.source \n``` After the post processor applies the projection, it outputs the following record: ``` {   \"username\": \"user5983\",   \"registration\": {     \"date\": \"2021-09-13\",   } } \n``` ### Projection Wildcard Pattern Matching Examples This section shows how you can configure the projector post processors to match wildcard patterns to match field names. | Pattern | Description                                                        |\n| ------- | ------------------------------------------------------------------ |\n| \\*      | Matches any number of characters in the current level.             |\n| \\*\\*    | Matches any characters in the current level and all nested levels. | For the allow list and block list wildcard pattern matching examples in this section, refer to the following value document that contains weather measurements: ``` {   \"city\": \"Springfield\",   \"temperature\": {     \"high\": 28,     \"low\": 24,     \"units\": \"C\"   },   \"wind_speed_10m\": {     \"average\": 3,     \"units\": \"km/h\"   },   \"wind_speed_80m\": {     \"average\": 8,     \"units\": \"km/h\"   },   \"soil_conditions\": {     \"temperature\": {       \"high\": 22,       \"low\": 17,       \"units\": \"C\"     },     \"moisture\": {       \"average\": 340,       \"units\": \"mm\"     }   } } \n``` #### Allow List Wildcard Examples You can use the `*` wildcard to match multiple field names. The following example configuration matches the following fields: * The top-level field named \"city\"\n* The fields named \"average\" that are subdocuments of any top-level field that starts with the name \"wind\\_speed\". ``` post.processor.chain=com.mongodb.kafka.connect.sink.processor.AllowListValueProjector value.projection.type=AllowList value.projection.list=city,wind_speed*.average \n``` After the post processor applies the allow list projection, it outputs the following record: ``` {   \"city\": \"Springfield\",   \"wind_speed_10m\": {     \"average\": 3,   },   \"wind_speed_80m\": {     \"average\": 8,   } } \n``` You can use the `**` wildcard which matches objects at any level starting from the one at which you specify the wildcard. The following wildcard matching example projects any document that contains the field named \"low\". ``` post.processor.chain=com.mongodb.kafka.connect.sink.processor.AllowListValueProjector value.projection.type=AllowList value.projection.list=**.low \n``` The post processor that applies the projection outputs the following record: ``` {   \"temperature\": {     \"high\": 28,     \"low\": 24,     \"units\": \"C\"   },   \"soil_conditions\": {     \"temperature\": {       \"high\": 22,       \"low\": 17,       \"units\": \"C\"     }   } } \n``` #### Block List Wildcard Example You can use the wildcard patterns to match fields at a specific document level as shown in the following block list configuration example: ``` post.processor.chain=com.mongodb.kafka.connect.sink.processor.BlockListValueProjector value.projection.type=BlockList\n\n \n``` ``` {   \"city\": \"Springfield\",   \"temperature\": {     \"high\": 28,     \"low\": 24,     \"units\": \"C\"   },   \"wind_speed_10m\": {     \"average\": 3,     \"units\": \"km/h\"   },   \"wind_speed_80m\": {     \"average\": 8,     \"units\": \"km/h\"   },   \"soil_conditions\": {     \"moisture\": {       \"average\": 340,       \"units\": \"mm\"     }   } } \n``` ### Field Renaming Examples This section shows how you can configure the `RenameByMapping`and `RenameByRegex` field renamer post processors to update field names in a sink record. The field renaming settings specify the following: * Whether to update the key or value document in the record\n* The field names to update\n* The new field names You must specify `RenameByMapping` and `RenameByRegex` settings in a JSON array. You can specify nested fields by using either dot notation or pattern matching. The field renamer post processor examples use the following example sink record: **Key Document** ``` {   \"location\": \"Provence\",   \"date_month\": \"October\",   \"date_day\": 17 } \n``` **Value Document** ``` {   \"flapjacks\": {     \"purchased\": 598,     \"size\": \"large\"   } } \n``` #### Rename by Mapping Example The `RenameByMapping` post processor setting specifies one or more JSON objects that assign fields matching a string to a new name. Each object contains the text to match in the `oldName` element and the replacement text in the `newName` element as described in the table below. | Key Name | Description                                                                                                                                                |\n| -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| oldName  | Specifies whether to match fields in the key or value document and the field name to replace. The setting uses a \".\" character to separate the two values. |\n| newName  | Specifies the replacement field name for all matches of the field.                                                                                         | The following example property matches the \"location\" field of a key document and renames it to \"country\": ``` field.renamer.mapping=[{\"oldName\":\"key.location\", \"newName\":\"country\"}] \n``` This setting instructs the `RenameByMapping` post processor to transform the [original key document](#std-label-rename-example-key-document) to the following document: ``` {   \"country\": \"Provence\",   \"date_month\": \"October\",   \"date_day\": 17 } \n``` You can perform a similar field name assignment for value documents by specifying the value document with the appended field name in the `oldName`field as follows: ``` field.renamer.mapping=[{\"oldName\":\"value.flapjacks\", \"newName\":\"crepes\"}] \n``` This setting instructs the `RenameByMapping` post processor to transform the [original value document](#std-label-rename-example-value-document) to the following document: ``` {   \"crepes\": {     \"purchased\": 598,     \"size\": \"large\"   } } \n```\n\n ``` field.renamer.mapping=[{ \"oldName\":\"key.location\", \"newName\":\"city\" }, { \"oldName\":\"value.crepes\", \"newName\":\"flapjacks\" }] \n``` #### Rename by Regular Expression The `RenameByRegex` post processor setting specifies the field names and text patterns that it should match, and replacement values for the matched text. You can specify one or more renaming expressions in JSON objects containing the fields described in the following table: | Key Name | Description                                                                                               |\n| -------- | --------------------------------------------------------------------------------------------------------- |\n| regexp   | Contains a regular expression that matches fields to perform the replacement.                             |\n| pattern  | Contains a regular expression that matches on the text to replace.                                        |\n| replace  | Contains the replacement text for all matches of the regular expression you defined in the pattern field. | The following example setting instructs the post processor to perform the following: * Match any field names in the key document that start with \"date\". In the set of matching fields, replace all text that matches the pattern `_`with the `-` character.\n* Match any field names in the value document that are subdocuments of`crepes`. In the set of matching fields, replace all text that matches the pattern `purchased` with `quantity`. ``` field.renamer.regexp=[{\"regexp\":\"^key\\\\.date.*$\",\"pattern\":\"_\",\"replace\":\"-\"}, {\"regexp\":\"^value\\\\.crepes\\\\..*\",\"pattern\":\"purchased\",\"replace\":\"quantity\"}] \n``` When the connector applies the post processor to the [example key document](#std-label-rename-example-key-document)and the [example value document](#std-label-rename-example-value-document), it outputs the following: **Key Document** ``` {   \"location\": \"Provence\",   \"date-month\": \"October\",   \"date-day\": 17 } \n``` **Value Document** ``` {   \"crepes\": {     \"quantity\": 598,     \"size\": \"large\"   } } \n``` ## Warning ### The renamer post processors do not overwrite existing field names The target field names you set in your renamer post processors to may result in duplicate field names in the same document. To avoid this, the post processor skips renaming when it would duplicate an existing field name at the same level of the document. ## How to Create a Custom Post Processor If the built-in post processors do not cover your use case, you can create a custom post processor class using the following steps: 1. Create a Java class that extends the[PostProcessor](https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/PostProcessor.java)abstract class.\n2. Override the `process()` method in your class. You can update the`SinkDocument`, a BSON representation of the sink record key and value fields, and access the original Kafka `SinkRecord` in your method.\n3. Compile the class to a JAR file.\n4. Add the compiled JAR to the class path / plugin path for all your Kafka workers. For more information about plugin paths, see the Confluent documentation on[Manually Installing Community Connectors.](https://docs.confluent.io/current/connect/managing/community.html)\n5. Add your post processor full class name to the post processor chain configuration.\n\n ← [Write Model Strategies](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/write-strategies/ \"Previous Section\")[Error Handling](https://mongodb.com/docs/kafka-connector/current/sink-connector/fundamentals/error-handling-strategies/ \"Next Section\") →"
}