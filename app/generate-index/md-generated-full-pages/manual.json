{
  "https://www.mongodb.com/docs/manual/reference/": " Docs Home → MongoDB Manual \nREFERENCE \nCollationDescription of collation fields as well as supported languages and associated locales for collation.Configuration File OptionsFull documentation of the configuration file and available run-time operations.Connection String URI FormatThe complete specification of the MongoDB connection string format that the drivers use to describe connections to MongoDB deployments.Database CommandsDocumentation of all MongoDB database commands operations, syntax, and use.Default MongoDB PortList of default ports used by MongoDB.Default MongoDB Read Concerns/Write ConcernsList of default read and write concerns.Exit Codes and StatusesDetails the codes and statuses that MongoDB returns when exiting.Explain ResultsDocumentation on information returned from explain operations.GlossaryA glossary of common terms and concepts specific to MongoDB.Log MessagesDescribes the components of log messages.MongoDB Cluster ParametersDocumentation of all mongod and mongos parameters that are available in the setClusterParameter.MongoDB Limits and ThresholdsA list of important limits and thresholds imposed by MongoDB.MongoDB Package ComponentsDocumentation of mongod and mongos and all other tools distributed with MongoDB.MongoDB Server ParametersDocumentation of all mongod and mongos parameters that are available in the setParameter (command) and setParameter run-time interface.MongoDB Wire ProtocolDescription of the MongoDB Wire Protocol.mongosh MethodsDocumentation of JavaScript methods and helpers in mongosh.OperatorsDocumentation of query, update, projection, and aggregation framework operators.Stable APIDescribes the Stable API.Server SessionsDescribes server sessions.System CollectionsDescribes the collections that MongoDB reserves for internal use.mongoDocumentation of JavaScript methods and helpers in the legacy mongo shell. \nTIP \nSEE ALSO: The following pages contain additional reference material:      * Data Model Examples and Patterns  * Sharding Reference  * Replication Reference  * Security Reference ←  FAQ: MongoDB DiagnosticsCollation → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/about/": " Docs Home → MongoDB Manual \nABOUT MONGODB DOCUMENTATION \nOn this page    \n * License\n   \n * Man Pages\n * Version and Revisions\n * Report an Issue or Make a Change Request\n * Contribute to the Documentation The MongoDB Manual contains comprehensive documentation on MongoDB. This page describes the manual's licensing, editions, and versions, and describes how to make a change request and how to contribute to the manual. \nLICENSE \nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License © MongoDB, Inc. 2008-2022 \nMAN PAGES \nIn addition to the MongoDB Manual, you can access the MongoDB Man Pages, which are also distributed with the official MongoDB Packages. \nVERSION AND REVISIONS \nThis version of the manual reflects version 6.0 of MongoDB. See the MongoDB Documentation Project Page for an overview of all editions and output formats of the MongoDB Manual. You can see the full revision history and track ongoing improvements and additions for all versions of the manual from its GitHub repository. The most up-to-date, current, and stable version of the manual is always available at \"https://www.mongodb.com/docs/manual/\". \nREPORT AN ISSUE OR MAKE A CHANGE REQUEST \nTo report an issue with this manual or to make a change request, file a ticket at the MongoDB DOCS Project on Jira. \nCONTRIBUTE TO THE DOCUMENTATION \nThe entire documentation source for this manual is available in the mongodb/docs repository, which is one of the MongoDB project repositories on GitHub. To contribute to the documentation, you can open a GitHub account, fork the mongodb/docs repository, make a change, and issue a pull request. In order for the documentation team to accept your change, you must complete the MongoDB Contributor Agreement. You can clone the repository by issuing the following command at your system shell: git clone git://github.com/mongodb/docs.git  \nABOUT THE DOCUMENTATION PROCESS \nThe MongoDB Manual uses Sphinx, a sophisticated documentation engine built upon Python Docutils. The original reStructured Text files, as well as all necessary Sphinx extensions and build tools, are available in the same repository as the documentation. For more information on the MongoDB documentation process, see the Meta Documentation. If you have any questions, please feel free to open a Jira Case. What is MongoDB? → On this page  * License\n * Man Pages\n * Version and Revisions\n * Report an Issue or Make a Change Request\n * Contribute to the Documentation Share Feedback\n",
  "https://www.mongodb.com/docs/manual/faq/": " Docs Home → MongoDB Manual \nFREQUENTLY ASKED QUESTIONS \nFAQ pages in this section:      * FAQ: MongoDB Fundamentals  * FAQ: Indexes  * FAQ: Concurrency  * FAQ: Sharding with MongoDB  * FAQ: Replication and Replica Sets  * FAQ: MongoDB Storage  * FAQ: MongoDB Diagnostics ←  FAQ: MongoDB StorageFAQ: MongoDB Fundamentals → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/meta/410/": " Docs Home → MongoDB Manual \nFILE DELETED \nThe URL you requested has been deleted. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/meta/404/": " Docs Home → MongoDB Manual \nFILE NOT FOUND \nThe URL you requested does not exist or has been removed. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/create-indexes-to-support-queries/": " Docs Home → MongoDB Manual \nCREATE INDEXES TO SUPPORT YOUR QUERIES \nOn this page    \n * Create a Single-Key Index if All Queries Use the Same, Single Key\n   \n * Create Compound Indexes to Support Several Different Queries\n * Create Indexes to Support Text Search\n * Index Use and Collation An index supports a query when the index contains all the fields scanned by the query. The query scans the index and not the collection. Creating indexes that support queries results in greatly increased query performance. This document describes strategies for creating indexes that support queries. \nCREATE A SINGLE-KEY INDEX IF ALL QUERIES USE THE SAME, SINGLE KEY \nIf you only ever query on a single key in a given collection, then you need to create just one single-key index for that collection. For example, you might create an index on category in the product collection: db.products.createIndex( { \"category\": 1 } )  \nCREATE COMPOUND INDEXES TO SUPPORT SEVERAL DIFFERENT QUERIES \nIf you sometimes query on only one key and at other times query on that key combined with a second key, then creating a compound index is more efficient than creating a single-key index. MongoDB will use the compound index for both queries. For example, you might create an index on both category and item. db.products.createIndex( { \"category\": 1, \"item\": 1 } )  This allows you both options. You can query on just category, and you also can query on category combined with item. A single compound index on multiple fields can support all the queries that search a \"prefix\" subset of those fields. \nEXAMPLE The following index on a collection: { x: 1, y: 1, z: 1 }  Can support queries that the following indexes support: { x: 1 }{ x: 1, y: 1 }  There are some situations where the prefix indexes may offer better query performance: for example if z is a large array. The { x: 1, y: 1, z: 1 } index can also support many of the same queries as the following index: { x: 1, z: 1 }  Also, { x: 1, z: 1 } has an additional use. Given the following query: db.collection.find( { x: 5 } ).sort( { z: 1} )  The { x: 1, z: 1 } index supports both the query and the sort operation, while the { x: 1, y: 1, z: 1 } index only supports the query. For more information on sorting, see Use Indexes to Sort Query Results. Starting in version 2.6, MongoDB can use index intersection to fulfill queries. The choice between creating compound indexes that support your queries or relying on index intersection depends on the specifics of your system. See Index Intersection and Compound Indexes for more details. \nCREATE INDEXES TO SUPPORT TEXT SEARCH \nFor data hosted on MongoDB Atlas, you can support full-text search with Atlas Search indexes. To learn more, see Create an Atlas Search Index. For self-managed (non-Atlas) deployments, MongoDB provides a text index type that supports searching for string content in a collection. To learn more about self-managed text indexes, see Text Indexes. \nINDEX USE AND COLLATION \nTo use an index for string comparisons, an operation must also specify the same collation. That is, an index with a collation cannot support an operation that performs string comparisons on the indexed fields if the operation specifies a different collation. For example, the collection myColl has an index on a string field category with the collation locale \"fr\". db.myColl.createIndex( { category: 1 }, { collation: { locale: \"fr\" } } )  The following query operation, which specifies the same collation as the index, can use the index: db.myColl.find( { category: \"cafe\" } ).collation( { locale: \"fr\" } )  However, the following query operation, which by default uses the \"simple\" binary collator, cannot use the index: db.myColl.find( { category: \"cafe\" } )  For a compound index where the index prefix keys are not strings, arrays, and embedded documents, an operation that specifies a different collation can still use the index to support comparisons on the index prefix keys. For example, the collection myColl has a compound index on the numeric fields score and price and the string field category; the index is created with the collation locale \"fr\" for string comparisons:\n\n  The following operations, which use \"simple\" binary collation for string comparisons, can use the index: db.myColl.find( { score: 5 } ).sort( { price: 1 } )db.myColl.find( { score: 5, price: { $gt: NumberDecimal( \"10\" ) } } ).sort( { price: 1 } )  The following operation, which uses \"simple\" binary collation for string comparisons on the indexed category field, can use the index to fulfill only the score: 5 portion of the query: db.myColl.find( { score: 5, category: \"cafe\" } ) \n←  The ESR (Equality, Sort, Range) RuleUse Indexes to Sort Query Results → On this page  * Create a Single-Key Index if All Queries Use the Same, Single Key\n * Create Compound Indexes to Support Several Different Queries\n * Create Indexes to Support Text Search\n * Index Use and Collation Share Feedback\n",
  "https://www.mongodb.com/docs/manual/changeStreams/": " Docs Home → MongoDB Manual \nCHANGE STREAMS \nOn this page    \n * Availability\n   \n * Connect\n * Watch a Collection, Database, or Deployment\n * Open A Change Stream\n * Modify Change Stream Output\n * Lookup Full Document for Update Operations\n * Resume a Change Stream\n * Use Cases\n * Access Control\n * Event Notification\n * Collation\n * Change Streams and Orphan Documents\n * Change Streams with Document Pre- and Post-Images Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog. Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them. Because change streams use the aggregation framework, applications can also filter for specific changes or transform the notifications at will. Starting in MongoDB 5.1, change streams are optimized, providing more efficient resource utilization and faster execution of some aggregation pipeline stages. \nAVAILABILITY \nChange streams are available for replica sets and sharded clusters:      * Storage Engine.\n   \n   The replica sets and sharded clusters must use the WiredTiger storage engine. Change streams can also be used on deployments that employ MongoDB's encryption-at-rest feature.  * Replica Set Protocol Version.\n   \n   The replica sets and sharded clusters must use replica set protocol version 1 (pv1).  * Read Concern \"majority\" Enablement.\n   \n   Starting in MongoDB 4.2, change streams are available regardless of the \"majority\" read concern support; that is, read concern majority support can be either enabled (default) or disabled to use change streams.\n   \n   In MongoDB 4.0 and earlier, change streams are available only if \"majority\" read concern support is enabled (default). \nCONNECT \nConnections for a change stream can either use DNS seed lists with the +srv connection option or by listing the servers individually in the connection string. If the driver loses the connection to a change stream or the connection goes down, it attempts to reestablish a connection to the change stream through another node in the cluster that has a matching read preference. If the driver cannot find a node with the correct read preference, it throws an exception. For more information, see Connection String URI Format. \nWATCH A COLLECTION, DATABASE, OR DEPLOYMENT \nYou can open change streams against: Target\nDescription\nA collection You can open a change stream cursor for a single collection (except system collections, or any collections in the admin, local, and config databases). The examples on this page use the MongoDB drivers to open and work with a change stream cursor for a single collection. See also the mongosh method db.collection.watch(). A database Starting in MongoDB 4.0, you can open a change stream cursor for a single database (excluding admin, local, and config database) to watch for changes to all its non-system collections. For the MongoDB driver method, refer to your driver documentation. See also the mongosh method db.watch(). A deployment Starting in MongoDB 4.0, you can open a change stream cursor for a deployment (either a replica set or a sharded cluster) to watch for changes to all non-system collections across all databases except for admin, local, and config. For the MongoDB driver method, refer to your driver documentation. See also the mongosh method Mongo.watch(). \nNOTE \nCHANGE STREAM EXAMPLES The examples on this page use the MongoDB drivers to illustrate how to open a change stream cursor for a collection and work with the change stream cursor. \nOPEN A CHANGE STREAM \nTo open a change stream:  * For a replica set, you can issue the open change stream operation from any of the data-bearing members.  * For a sharded cluster, you must issue the open change stream operation from the mongos. The following example opens a change stream for a collection and iterates over the cursor to retrieve the change stream documents. [1] ---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the examples on this page. ----------------------------------------  \nTo retrieve the data change event from the cursor, iterate the change stream cursor. For information on the change stream event, see Change Events. While the connection to the MongoDB deployment remains open, the cursor remains open until one of the following occurs:  * The cursor is explicitly closed.  * An invalidate event occurs.\n\n \nNOTE The lifecycle of an unclosed cursor is language-dependent. [1] Starting in MongoDB 4.0, you can specify a startAtOperationTime to open the cursor at a particular point in time. If the specified starting point is in the past, it must be in the time range of the oplog. \nMODIFY CHANGE STREAM OUTPUT \n---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the examples on this page. ----------------------------------------   TIP The _id field of the change stream event document act as the resume token. Do not use the pipeline to modify or remove the change stream event's _id field. Starting in MongoDB 4.2, change streams will throw an exception if the change stream aggregation pipeline modifies an event's _id field. See Change Events for more information on the change stream response document format. \nLOOKUP FULL DOCUMENT FOR UPDATE OPERATIONS \nBy default, change streams only return the delta of fields during the update operation. However, you can configure the change stream to return the most current majority-committed version of the updated document. ---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the examples on this page. ----------------------------------------   NOTE If there are one or more majority-committed operations that modified the updated document after the update operation but before the lookup, the full document returned may differ significantly from the document at the time of the update operation. However, the deltas included in the change stream document always correctly describe the watched collection changes that applied to that change stream event. See Change Events for more information on the change stream response document format. \nRESUME A CHANGE STREAM \nChange streams are resumable by specifying a resume token to either resumeAfter or startAfter when opening the cursor. \nRESUMEAFTER FOR CHANGE STREAMS \nYou can resume a change stream after a specific event by passing a resume token to resumeAfter when opening the cursor. See Resume Tokens for more information on the resume token. \nIMPORTANT  * The oplog must have enough history to locate the operation associated with the token or the timestamp, if the timestamp is in the past.  * You cannot use resumeAfter to resume a change stream after an invalidate event (for example, a collection drop or rename) closes the stream. Starting in MongoDB 4.2, you can use startAfter to start a new change stream after an invalidate event.   STARTAFTER FOR CHANGE STREAMS \nNew in version 4.2. You can start a new change stream after a specific event by passing a resume token to startAfter when opening the cursor. Unlike resumeAfter, startAfter can resume notifications after an invalidate event by creating a new change stream. See Resume Tokens for more information on the resume token. \nIMPORTANT  * The oplog must have enough history to locate the operation associated with the token or the timestamp, if the timestamp is in the past. \nRESUME TOKENS \nThe resume token is available from multiple sources: Source\nDescription\nChange Events\nEach change event notification includes a resume token on the _id field.\nAggregation The $changeStream aggregation stage includes a resume token on the cursor.postBatchResumeToken field. This field only appears when using the aggregate command. Get More\nThe getMore command includes a resume token on the cursor.postBatchResumeToken field. Changed in version 4.2: Starting in MongoDB 4.2, change streams will throw an exception if the change stream aggregation pipeline modifies an event's _id field. \nTIP MongoDB provides a \"snippet\", an extension to mongosh, that decodes hex-encoded resume tokens. You can install and run the resumetoken snippet from mongosh: snippet install resumetokendecodeResumeToken('<RESUME TOKEN>')  You can also run resumetoken from the command line (without using mongosh) if npm is installed on your system: npx mongodb-resumetoken-decoder <RESUME TOKEN>  See the following for more details on:  * resumetoken  * using snippets in mongosh. RESUME TOKENS FROM CHANGE EVENTS \nChange event notifications include a resume token on the _id field:\n\n RESUME TOKENS FROM AGGREGATE \nWhen using the aggregate command, the $changeStream aggregation stage includes a resume token on the cursor.postBatchResumeToken field: {   \"cursor\": {      \"firstBatch\": [],      \"postBatchResumeToken\": {         \"_data\": \"8263515EAC000000022B0429296E1404\"      },      \"id\": Long(\"4309380460777152828\"),      \"ns\": \"test.names\"   },   \"ok\": 1,   \"$clusterTime\": {      \"clusterTime\": Timestamp({ \"t\": 1666277036, \"i\": 1 }),      \"signature\": {         \"hash\": Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),         \"keyId\": Long(\"0\")      }   },   \"operationTime\": Timestamp({ \"t\": 1666277036, \"i\": 1 })} RESUME TOKENS FROM GETMORE \nThe getMore command also includes a resume token on the cursor.postBatchResumeToken field: {   \"cursor\": {      \"nextBatch\": [],      \"postBatchResumeToken\": {         \"_data\": \"8263515979000000022B0429296E1404\"      },      \"id\": Long(\"7049907285270685005\"),      \"ns\": \"test.names\"   },   \"ok\": 1,   \"$clusterTime\": {      \"clusterTime\": Timestamp( { \"t\": 1666275705, \"i\": 1 } ),      \"signature\": {         \"hash\": Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),         \"keyId\": Long(\"0\")      }   },   \"operationTime\": Timestamp({ \"t\": 1666275705, \"i\": 1 })} \nUSE CASES \nChange streams can benefit architectures with reliant business systems, informing downstream systems once data changes are durable. For example, change streams can save time for developers when implementing Extract, Transform, and Load (ETL) services, cross-platform synchronization, collaboration functionality, and notification services. \nACCESS CONTROL \nFor deployments enforcing Authentication and authorization:  * To open a change stream against specific collection, applications must have privileges that grant changeStream and find actions on the corresponding collection.\n   \n   { resource: { db: <dbname>, collection: <collection> }, actions: [ \"find\", \"changeStream\" ] }\n   \n     * To open a change stream on a single database, applications must have privileges that grant changeStream and find actions on all non-system collections in the database.\n   \n   { resource: { db: <dbname>, collection: \"\" }, actions: [ \"find\", \"changeStream\" ] }\n   \n     * To open a change stream on an entire deployment, applications must have privileges that grant changeStream and find actions on all non-system collections for all databases in the deployment.\n   \n   { resource: { db: \"\", collection: \"\" }, actions: [ \"find\", \"changeStream\" ] }\n   \n    \nEVENT NOTIFICATION \nChange streams only notify on data changes that have persisted to a majority of data-bearing members in the replica set. This ensures that notifications are triggered only by majority-committed changes that are durable in failure scenarios. For example, consider a 3-member replica set with a change stream cursor opened against the primary. If a client issues an insert operation, the change stream only notifies the application of the data change once that insert has persisted to a majority of data-bearing members. If an operation is associated with a transaction, the change event document includes the txnNumber and the lsid. \nCOLLATION \nStarting in MongoDB 4.2, change streams use simple binary comparisons unless an explicit collation is provided. In earlier versions, change streams opened on a single collection (db.collection.watch()) would inherit that collection's default collation.\n\n \nStarting in MongoDB 5.3, during range migration, change stream events are not generated for updates to orphaned documents. \nCHANGE STREAMS WITH DOCUMENT PRE- AND POST-IMAGES \nStarting in MongoDB 6.0, you can use change stream events to output the version of a document before and after changes (the document pre- and post-images):  * The pre-image is the document before it was replaced, updated, or deleted. There is no pre-image for an inserted document.  * The post-image is the document after it was inserted, replaced, or updated. There is no post-image for a deleted document.  * Enable changeStreamPreAndPostImages for a collection using db.createCollection(), create, or collMod. Pre- and post-images are not available for a change stream event if the images were:  * Not enabled on the collection at the time of a document update or delete operation.  * Removed after the pre- and post-image retention time set in expireAfterSeconds.\n   \n   * The following example sets expireAfterSeconds to 100 seconds:\n     \n     use admindb.runCommand( {   setClusterParameter:      { changeStreamOptions: { preAndPostImages: { expireAfterSeconds: 100 } } }} )\n     \n     \n   \n   * The following example returns the current changeStreamOptions settings, including expireAfterSeconds:\n     \n     db.adminCommand( { getClusterParameter: \"changeStreamOptions\" } )\n     \n     \n   \n   * Setting expireAfterSeconds to off uses the default retention policy: pre- and post-images are retained until the corresponding change stream events are removed from the oplog.\n   \n   * If a change stream event is removed from the oplog, then the corresponding pre- and post-images are also deleted regardless of the expireAfterSeconds pre- and post-image retention time. Additional considerations:  * Enabling pre- and post-images consumes storage space and adds processing time. Only enable pre- and post-images if you need them.  * Limit the change stream event size to less than 16 megabytes. To limit the event size, you can:\n   \n   * Limit the document size to 8 megabytes. You can request pre- and post-images simultaneously in the change stream output if other change stream event fields like updateDescription are not large.\n   \n   * Request only post-images in the change stream output for documents up to 16 megabytes if other change stream event fields like updateDescription are not large.\n   \n   * Request only pre-images in the change stream output for documents up to 16 megabytes if:\n     \n     * document updates affect only a small fraction of the document structure or content, and\n     \n     * do not cause a replace change event. A replace event always includes the post-image.  * To request a pre-image, you set fullDocumentBeforeChange to required or whenAvailable in db.collection.watch(). To request a post-image, you set fullDocument using the same method.  * Pre-images are written to the config.system.preimages collection.\n   \n   * The config.system.preimages collection may become large. To limit the collection size, you can set expireAfterSeconds time for the pre-images as shown earlier.\n   \n   * Pre-images are removed asynchronously by a background process. \nIMPORTANT \nBACKWARD-INCOMPATIBLE FEATURE Starting in MongoDB 6.0, if you are using document pre- and post-images for change streams, you must disable changeStreamPreAndPostImages for each collection using the collMod command before you can downgrade to an earlier MongoDB version. \nTIP \nSEE ALSO:  * For change stream events and output, see Change Events.  * To watch a collection for changes, see db.collection.watch().  * For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. ←  Config DatabaseChange Streams Production Recommendations → On this page\n\n Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/resync-replica-set-member/": " Docs Home → MongoDB Manual \nRESYNC A MEMBER OF A REPLICA SET \nOn this page    \n * Procedures A replica set member becomes \"stale\" when its replication process falls so far behind that the primary overwrites oplog entries the member has not yet replicated. The member cannot catch up and becomes \"stale.\" When this occurs, you must completely resynchronize the member by removing its data and performing an initial sync. This tutorial addresses both resyncing a stale member and creating a new member using seed data from another member, both of which can be used to restore a replica set member. When syncing a member, choose a time when the system has the bandwidth to move a large amount of data. Schedule the synchronization during a time of low usage or during a maintenance window. MongoDB provides two options for performing an initial sync:      * Restart the mongod with an empty data directory and let MongoDB's normal initial syncing feature restore the data. This is the more simple option but may take longer to replace the data.\n   \n   See Automatically Sync a Member.  * Restart the machine with a copy of a recent data directory from another member in the replica set. This procedure can replace the data more quickly but requires more manual steps.\n   \n   See Sync by Copying Data Files from Another Member. \nPROCEDURES  NOTE To prevent changing the write quorum, never rotate more than one replica set member at a time. \nAUTOMATICALLY SYNC A MEMBER  WARNING During initial sync, mongod removes the contents of the dbPath directory. This procedure relies on MongoDB's regular process for Replica Set Syncing. This stores the current data on the member. For an overview of MongoDB initial sync process, see the Replica Set Syncing section. Initial sync operations can impact the other members of the set and create additional traffic to the primary. The syncing member requires another member of the set that is accessible and up to date. If the instance has no data, you can follow the Add Members to a Replica Set or Replace a Replica Set Member procedure to add a new member to a replica set. You can also force a mongod that is already a member of the set to perform an initial sync by restarting the instance without the contents of the dbPath directory:  1. Stop the member's mongod instance. To ensure a clean shutdown, use the db.shutdownServer() method from mongosh or on Linux systems, the mongod --shutdown option.  2. (Optional) Make a backup of all data and sub-directories from the member's dbPath directory. If a full backup is not required, consider backing up just the diagnostic.data directory to preserve potentially-useful troubleshooting data in the event of an issue. See Full Time Diagnostic Data Capture for more information.  3. Delete all data and sub-directories from the member's dbPath directory.  4. Restart the mongod process. At this point, the mongod performs an initial sync. The length of the initial sync process depends on the size of the database and the network latency between members of the replica set. \nSYNC BY COPYING DATA FILES FROM ANOTHER MEMBER \nThis approach \"seeds\" a new or stale member using the data files from an existing member of the replica set. The data files must be sufficiently recent to allow the new member to catch up with the oplog. Otherwise the member would need to perform an initial sync. COPY THE DATA FILES \nYou can capture the data files as either a snapshot or a direct copy. However, in most cases you cannot copy data files from a running mongod instance to another because the data files will change during the file copy operation. \nIMPORTANT If copying data files, ensure that your copy includes the content of the local database. You cannot use a mongodump backup for the data files: only a snapshot backup. For approaches to capturing a consistent snapshot of a running mongod instance, see the MongoDB Backup Methods documentation. SYNC THE MEMBER \nAfter you have copied the data files from the \"seed\" source, start the mongod instance with a new members[n]._id and allow it to apply all operations from the oplog until it reflects the current state of the replica set. To see the current status of the replica set, use rs.printSecondaryReplicationInfo() or rs.status(). ←  Force a Member to Become PrimaryConfigure Replica Set Tag Sets → On this page  * Procedures Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/backup-with-filesystem-snapshots/": " Docs Home → MongoDB Manual \nBACK UP AND RESTORE WITH FILESYSTEM SNAPSHOTS \nOn this page    \n * Snapshots Overview\n   \n * Considerations\n * Back Up and Restore Using LVM on Linux\n * Back up Instances with Journal Files on Separate Volume or without Journaling This document describes a procedure for creating backups of MongoDB systems using system-level tools, such as LVM or storage appliance, as well as the corresponding restoration strategies. These filesystem snapshots, or \"block-level\" backup methods, use system level tools to create copies of the device that holds MongoDB's data files. These methods complete quickly and work reliably, but require additional system configuration outside of MongoDB. \nTIP \nSEE ALSO:      * MongoDB Backup Methods  * Back Up and Restore with MongoDB Tools \nSNAPSHOTS OVERVIEW \nSnapshots work by creating pointers between the live data and a special snapshot volume. These pointers are theoretically equivalent to \"hard links.\" As the working data diverges from the snapshot, the snapshot process uses a copy-on-write strategy. As a result, the snapshot only stores modified data. After making the snapshot, you mount the snapshot image on your file system and copy data from the snapshot. The resulting backup contains a full copy of all data. \nCONSIDERATIONS  WIREDTIGER STORAGE ENGINE \nMongoDB 3.2 added support for volume-level back up of MongoDB instances using the WiredTiger storage engine when the MongoDB instance's data files and journal files reside on separate volumes. However, to create a coherent backup, the database must be locked and all writes to the database must be suspended during the backup process. Prior to MongoDB 3.2, creating volume-level backups of MongoDB instances using WiredTiger required that the data files and journal reside on the same volume. \nENCRYPTED STORAGE ENGINE (MONGODB ENTERPRISE ONLY) \nFor encrypted storage engines that use AES256-GCM encryption mode, AES256-GCM requires that every process use a unique counter block value with the key. For encrypted storage engine configured with AES256-GCM cipher:  * Restoring from Hot BackupStarting in 4.2, if you restore from files taken via \"hot\" backup (i.e. the mongod is running), MongoDB can detect \"dirty\" keys on startup and automatically rollover the database key to avoid IV (Initialization Vector) reuse.\n * Restoring from Cold Backup\n   \n   However, if you restore from files taken via \"cold\" backup (i.e. the mongod is not running), MongoDB cannot detect \"dirty\" keys on startup, and reuse of IV voids confidentiality and integrity guarantees.\n   \n   Starting in 4.2, to avoid the reuse of the keys after restoring from a cold filesystem snapshot, MongoDB adds a new command-line option --eseDatabaseKeyRollover. When started with the --eseDatabaseKeyRollover option, the mongod instance rolls over the database keys configured with AES256-GCM cipher and exits. \nTIP  * In general, if using filesystem based backups for MongoDB Enterprise 4.2+, use the \"hot\" backup feature, if possible.  * For MongoDB Enterprise versions 4.0 and earlier, if you use AES256-GCM encryption mode, do not make copies of your data files or restore from filesystem snapshots (\"hot\" or \"cold\"). \nVALID DATABASE AT THE TIME OF SNAPSHOT \nThe database must be valid when the snapshot takes place. This means that all writes accepted by the database need to be fully written to disk: either to the journal or to data files. If there are writes that are not on disk when the backup occurs, the backup will not reflect these changes. For the WiredTiger storage engine, the data files reflect a consistent state as of the last checkpoint. Checkpoints occur with every 2 GB of data or every minute. \nENTIRE DISK IMAGE \nSnapshots create an image of an entire disk image. Unless you need to back up your entire system, consider isolating your MongoDB data files, journal (if applicable), and configuration on one logical disk that doesn't contain any other data. Alternately, store all MongoDB data files on a dedicated device so that you can make backups without duplicating extraneous data. \nSITE FAILURE PRECAUTION \nEnsure that you copy data from snapshots onto other systems. This ensures that data is safe from site failures. \nNO INCREMENTAL BACKUPS\n\n \nSNAPSHOTS WITH JOURNALING \nIf your mongod instance has journaling enabled, then you can use any kind of file system or volume/block level snapshot tool to create backups. If you manage your own infrastructure on a Linux-based system, configure your system with LVM to provide your disk packages and provide snapshot capability. You can also use LVM-based setups within a cloud/virtualized environment. \nNOTE Running LVM provides additional flexibility and enables the possibility of using snapshots to back up MongoDB. \nSNAPSHOTS WITH AMAZON EBS IN A RAID 10 CONFIGURATION \nIf your deployment depends on Amazon's Elastic Block Storage (EBS) with RAID configured within your instance, it is impossible to get a consistent state across all disks using the platform's snapshot tool. As an alternative, you can do one of the following:  * Flush all writes to disk and create a write lock to ensure consistent state during the backup process.\n   \n   If you choose this option see Back up Instances with Journal Files on Separate Volume or without Journaling.  * Configure LVM to run and hold your MongoDB data files on top of the RAID within your system.\n   \n   If you choose this option, perform the LVM backup operation described in Create a Snapshot. \nBACK UP AND RESTORE USING LVM ON LINUX \nThis section provides an overview of a simple backup process using LVM on a Linux system. While the tools, commands, and paths may be (slightly) different on your system the following steps provide a high level overview of the backup operation. \nNOTE Only use the following procedure as a guideline for a backup system and infrastructure. Production backup systems must consider a number of application specific requirements and factors unique to specific environments. \nCREATE A SNAPSHOT \nChanged in version 3.2: Starting in MongoDB 3.2, for the purpose of volume-level backup of MongoDB instances using WiredTiger, the data files and the journal are no longer required to reside on a single volume. To create a snapshot with LVM, issue a command as root in the following format: lvcreate --size 100M --snapshot --name mdb-snap01 /dev/vg0/mongodb  This command creates an LVM snapshot (with the --snapshot option) named mdb-snap01 of the mongodb volume in the vg0 volume group. This example creates a snapshot named mdb-snap01 located at /dev/vg0/mdb-snap01. The location and paths to your systems volume groups and devices may vary slightly depending on your operating system's LVM configuration. The snapshot has a cap of at 100 megabytes, because of the parameter --size 100M. This size does not reflect the total amount of the data on the disk, but rather the quantity of differences between the current state of /dev/vg0/mongodb and the creation of the snapshot (i.e. /dev/vg0/mdb-snap01.) \nWARNING Ensure that you create snapshots with enough space to account for data growth, particularly for the period of time that it takes to copy data out of the system or to a temporary image. If your snapshot runs out of space, the snapshot image becomes unusable. Discard this logical volume and create another. The snapshot will exist when the command returns. You can restore directly from the snapshot at any time or by creating a new logical volume and restoring from this snapshot to the alternate image. While snapshots are great for creating high quality backups quickly, they are not ideal as a format for storing backup data. Snapshots typically depend and reside on the same storage infrastructure as the original disk images. Therefore, it's crucial that you archive these snapshots and store them elsewhere. \nARCHIVE A SNAPSHOT \nAfter creating a snapshot, mount the snapshot and copy the data to separate storage. Your system might try to compress the backup images as you move them offline. Alternatively, take a block level copy of the snapshot image, such as with the following procedure: umount /dev/vg0/mdb-snap01dd if=/dev/vg0/mdb-snap01 | gzip > mdb-snap01.gz  The above command sequence does the following:  * Ensures that the /dev/vg0/mdb-snap01 device is not mounted. Never take a block level copy of a filesystem or filesystem snapshot that is mounted.\n\n \nRESTORE A SNAPSHOT \nTo restore a snapshot created with LVM, issue the following sequence of commands: lvcreate --size 1G --name mdb-new vg0gzip -d -c mdb-snap01.gz | dd of=/dev/vg0/mdb-newmount /dev/vg0/mdb-new /srv/mongodb  The above sequence does the following:  * Creates a new logical volume named mdb-new, in the /dev/vg0 volume group. The path to the new device will be /dev/vg0/mdb-new.\n   \n   \n   WARNING\n   \n   This volume will have a maximum size of 1 gigabyte. The original file system must have had a total size of 1 gigabyte or smaller, or else the restoration will fail.\n   \n   Change 1G to your desired volume size.  * Uncompresses and unarchives the mdb-snap01.gz into the mdb-new disk image.  * Mounts the mdb-new disk image to the /srv/mongodb directory. Modify the mount point to correspond to your MongoDB data file location, or other location as needed. \nNOTE The restored snapshot will have a stale mongod.lock file. If you do not remove this file from the snapshot, and MongoDB may assume that the stale lock file indicates an unclean shutdown. If you're running with storage.journal.enabled enabled, and you do not use db.fsyncLock(), you do not need to remove the mongod.lock file. If you use db.fsyncLock() you will need to remove the lock. \nRESTORE DIRECTLY FROM A SNAPSHOT \nTo restore a backup without writing to a compressed gz file, use the following sequence of commands: umount /dev/vg0/mdb-snap01lvcreate --size 1G --name mdb-new vg0dd if=/dev/vg0/mdb-snap01 of=/dev/vg0/mdb-newmount /dev/vg0/mdb-new /srv/mongodb  \nNOTE New in version 3.6: All MongoDB collections have UUIDs by default. When MongoDB restores collections, the restored collections retain their original UUIDs. When restoring a collection where no UUID was present, MongoDB generates a UUID for the restored collection. For more information on collection UUIDs, see Collections. \nREMOTE BACKUP STORAGE \nYou can implement off-system backups using the combined process and SSH. This sequence is identical to procedures explained above, except that it archives and compresses the backup on a remote system using SSH. Consider the following procedure: umount /dev/vg0/mdb-snap01dd if=/dev/vg0/mdb-snap01 | ssh username@example.com gzip > /opt/backup/mdb-snap01.gzlvcreate --size 1G --name mdb-new vg0ssh username@example.com gzip -d -c /opt/backup/mdb-snap01.gz | dd of=/dev/vg0/mdb-newmount /dev/vg0/mdb-new /srv/mongodb  \nBACK UP INSTANCES WITH JOURNAL FILES ON SEPARATE VOLUME OR WITHOUT JOURNALING \nChanged in version 3.2: Starting in MongoDB 3.2, for the purpose of volume-level backup of MongoDB instances using WiredTiger, the data files and the journal are no longer required to reside on a single volume. However, the database must be locked and all writes to the database must be suspended during the backup process to ensure the consistency of the backup. If your mongod instance is either running without journaling or has the journal files on a separate volume, you must flush all writes to disk and lock the database to prevent writes during the backup process. If you have a replica set configuration, then for your backup use a secondary which is not receiving reads (i.e. hidden member). 1 \nFLUSH WRITES TO DISK AND LOCK THE DATABASE TO PREVENT FURTHER WRITES. \nTo flush writes to disk and to \"lock\" the database, issue the db.fsyncLock() method in mongosh: db.fsyncLock(); \n2 \nPERFORM THE BACKUP OPERATION DESCRIBED IN CREATE A SNAPSHOT. \n3 \nAFTER THE SNAPSHOT COMPLETES, UNLOCK THE DATABASE.\n\n db.fsyncUnlock(); \n←  MongoDB Backup MethodsBack Up and Restore with MongoDB Tools → On this page  * Snapshots Overview\n * Considerations\n * Back Up and Restore Using LVM on Linux\n * Back up Instances with Journal Files on Separate Volume or without Journaling Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes/": " Docs Home → MongoDB Manual \nCALCULATE DISTANCE USING SPHERICAL GEOMETRY  WARNING For spherical queries, use the 2dsphere index result. The use of 2d index for spherical queries may lead to incorrect results, such as the use of the 2d index for spherical queries that wrap around the poles. The 2d index supports queries that calculate distances on a Euclidean plane (flat surface). The index also supports the following query operators and command that calculate distances using spherical geometry: \nNOTE While basic queries using spherical distance are supported by the 2d index, consider moving to a 2dsphere index if your data is primarily longitude and latitude.      * $nearSphere  * $centerSphere  * $near  * $geoNear pipeline stage with the spherical: true option \nIMPORTANT The aforementioned operations use radians for distance. Other spherical query operators do not, such as $geoWithin. For spherical query operators to function properly, you must convert distances to radians, and convert from radians to the distances units used by your application. To convert:  * distance to radians: divide the distance by the radius of the sphere (e.g. the Earth) in the same units as the distance measurement.  * radians to distance: multiply the radian measure by the radius of the sphere (e.g. the Earth) in the units system that you want to convert the distance to. The equatorial radius of the Earth is approximately 3,963.2 miles or 6,378.1 kilometers. The following query would return documents from the places collection within the circle described by the center [ -74, 40.74 ] with a radius of 100 miles: db.places.find( { loc: { $geoWithin: { $centerSphere: [ [ -74, 40.74 ] ,                                                     100 / 3963.2 ] } } } )  \nNOTE \nIMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.  * Valid longitude values are between -180 and 180, both inclusive.  * Valid latitude values are between -90 and 90, both inclusive. ←  2d Index InternalsgeoHaystack Indexes → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/convert-sharded-cluster-to-replica-set/": " Docs Home → MongoDB Manual \nCONVERT SHARDED CLUSTER TO REPLICA SET \nOn this page    \n * Convert a Cluster with a Single Shard into a Replica Set\n   \n * Convert a Sharded Cluster into a Replica Set This tutorial describes the process for converting a sharded cluster to a non-sharded replica set. To convert a replica set into a sharded cluster Convert a Replica Set to a Sharded Cluster. See the Sharding documentation for more information on sharded clusters. \nCONVERT A CLUSTER WITH A SINGLE SHARD INTO A REPLICA SET \nIn the case of a sharded cluster with only one shard, that shard contains the full data set. Use the following procedure to convert that cluster into a non-sharded replica set:       1. Reconfigure the application to connect to the primary member of the replica set hosting the single shard that system will be the new replica set.  2. Remove the --shardsvr option from your mongod.\n    \n    \n    TIP\n    \n    Changing the --shardsvr option will change the port that mongod listens for incoming connections on. The single-shard cluster is now a non-sharded replica set that will accept read and write operations on the data set. You may now decommission the remaining sharding infrastructure. \nCONVERT A SHARDED CLUSTER INTO A REPLICA SET \nUse the following procedure to transition from a sharded cluster with more than one shard to an entirely new replica set.  1. With the sharded cluster running, deploy a new replica set in addition to your sharded cluster. The replica set must have sufficient capacity to hold all of the data files from all of the current shards combined. Do not configure the application to connect to the new replica set until the data transfer is complete.  2. Stop all writes to the sharded cluster. You may reconfigure your application or stop all mongos instances. If you stop all mongos instances, the applications will not be able to read from the database. If you stop all mongos instances, start a temporary mongos instance on that applications cannot access for the data migration procedure.  3. Use mongodump and mongorestore to migrate the data from the mongos instance to the new replica set.\n    \n    \n    NOTE\n    \n    Not all collections on all databases are necessarily sharded. Do not solely migrate the sharded collections. Ensure that all databases and all collections migrate correctly.  4. Reconfigure the application to use the non-sharded replica set instead of the mongos instance. The application will now use the un-sharded replica set for reads and writes. You may now decommission the remaining unused sharded cluster infrastructure. ←  Back Up Cluster MetadataConvert a Replica Set to a Sharded Cluster → On this page  * Convert a Cluster with a Single Shard into a Replica Set\n * Convert a Sharded Cluster into a Replica Set Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/transparent-huge-pages/": " Docs Home → MongoDB Manual \nDISABLE TRANSPARENT HUGE PAGES (THP) \nOn this page    \n * Create a Service File\n   \n * Using tuned and ktune Transparent Huge Pages (THP) is a Linux memory management system that reduces the overhead of Translation Lookaside Buffer (TLB) lookups on machines with large amounts of memory by using larger memory pages. However, database workloads often perform poorly with THP enabled, because they tend to have sparse rather than contiguous memory access patterns. When running MongoDB on Linux, THP should be disabled for best performance. To ensure that THP is disabled before mongod starts, you should create a service file for your platform's initialization system that disables THP at boot. Instructions are provided below for both the systemd and the System V init initialization systems. Additionally, for RHEL / CentOS systems that make use of ktune and tuned performance profiles, you must create a custom tuned profile as well. \nCREATE A SERVICE FILE \nTo create a service file that disables THP, you will use the built-in initialization system for your platform. Recent versions of Linux tend to use systemd (which uses the systemctl command), while older versions of Linux tend to use System V init (which uses the service command). Refer to the documentation for your operating system for more information. Use the initialization system appropriate for your platform:   USING TUNED AND KTUNE  IMPORTANT If using tuned or ktune, you must also perform the steps in this section after creating the service file above. tuned and ktune are dynamic kernel tuning tools that can affect the transparent huge pages setting on your system. If you are using tuned / ktune on your RHEL / CentOS system while running mongod, you must create a custom tuned profile to ensure that THP remains disabled. \nRED HAT/CENTOS 6 \n1 CREATE A NEW PROFILE. \nCreate a new profile from an existing profile by copying the relevant directory. This example uses the virtual-guest profile as the base, and uses virtual-guest-no-thp as the new profile: sudo cp -r /etc/tune-profiles/virtual-guest /etc/tune-profiles/virtual-guest-no-thp \n2 EDIT KTUNE.SH. \nEdit /etc/tune-profiles/virtual-guest-no-thp/ktune.sh and change the set_transparent_hugepages setting to the following: set_transparent_hugepages never \n3 ENABLE THE NEW PROFILE. \nEnable the new profile: sudo tuned-adm profile virtual-guest-no-thp  \nRED HAT/CENTOS 7 AND 8 \n1 CREATE A NEW PROFILE. \nCreate a new directory to hold the custom tuned profile. This example inherits from the existing virtual-guest profile, and uses virtual-guest-no-thp as the new profile: sudo mkdir /etc/tuned/virtual-guest-no-thp \n2 EDIT TUNED.CONF. \nCreate and edit /etc/tuned/virtual-guest-no-thp/tuned.conf so that it contains the following: [main]include=virtual-guest\n[vm]transparent_hugepages=never  This example inherits from the existing virtual-guest profile. Select the profile most appropriate for your system. 3 ENABLE THE NEW PROFILE. \nEnable the new profile: sudo tuned-adm profile virtual-guest-no-thp \n←  Database Profiler OutputManage Sharded Cluster Health with Health Managers → On this page  * Create a Service File\n * Using tuned and ktune Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/upgrade-to-enterprise-sharded-cluster/": " Docs Home → MongoDB Manual \nUPGRADE TO MONGODB ENTERPRISE (SHARDED CLUSTER) \nOn this page    \n * Consideration\n   \n * Download Enterprise Binaries\n * Procedure MongoDB Enterprise provides various features not available in the MongoDB Community edition, such as:      * In-Memory Storage Engine  * Auditing  * Kerberos Authentication  * LDAP Proxy Authentication and LDAP Authorization  * Encryption at Rest The following steps outline the procedure to upgrade a sharded cluster from the MongoDB Community Edition to the MongoDB Enterprise Edition. For example, the steps can be used to upgrade MongoDB 6.0 Community to MongoDB 6.0 Enterprise. \nCONSIDERATION  WARNING Do not use these instructions to upgrade to another release version. To upgrade release versions, refer to the appropriate release upgrade instructions, such as Upgrade to MongoDB 6.0. \nDOWNLOAD ENTERPRISE BINARIES \nDepending on your operating system, you can install the MongoDB Enterprise binaries by using a package manager or by downloading the binaries manually.   PROCEDURE \nTo minimize downtime, you can upgrade from MongoDB Community to Enterprise Edition using a \"rolling\" upgrade by upgrading the members individually while the other members are available. 1 \nDISABLE THE BALANCER. \nConnect mongosh to a mongos instance in the sharded cluster, and run sh.stopBalancer() to disable the balancer: sh.stopBalancer()  \nNOTE If a migration is in progress, the system will complete the in-progress migration before stopping the balancer. You can run sh.isBalancerRunning() to check the balancer's current state. To verify that the balancer is disabled, run sh.getBalancerState(), which returns false if the balancer is disabled: sh.getBalancerState()  Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0, sh.stopBalancer() also disables auto-splitting for the sharded cluster. For more information on disabling the balancer, see Disable the Balancer. 2 \nUPGRADE THE CONFIG SERVERS. \n 1. Upgrade the secondary members of the replica set one at a time:\n    \n    a. Shut down the secondary mongod instance.\n    \n    b. Restart the member with the Enterprise mongod, specifying the same configuration options (e.g. same data directory, configuration file, etc.).\n    \n    c. Wait for the member to recover to SECONDARY state before upgrading the next secondary member. To check the member's state, issue rs.status() in mongosh.\n    \n    Repeat for each remaining secondary member.  2. Step down the replica set primary.\n    \n    Connect mongosh to the primary and use rs.stepDown() to step down the primary and force an election of a new primary:\n    \n    rs.stepDown()\n    \n      3. When rs.status() shows that the primary has stepped down and another member is primary, upgrade the stepped-down primary:\n    \n    a. Shut down the stepped-down primary.\n    \n    b. Restart with the Enterprise mongod, specifying the same configuration options (e.g. same data directory, configuration file, etc.). 3 \nUPGRADE THE SHARDS. \nUpgrade the shards one at a time. For each shard replica set:  1. Upgrade the secondary members of the replica set one at a time:\n    \n    a. Shut down the secondary mongod instance.\n    \n    b. Restart the member with the Enterprise mongod, specifying the same configuration options (e.g. same data directory, configuration file, etc.).\n    \n    c. Wait for the member to recover to SECONDARY state before upgrading the next secondary member. To check the member's state, issue rs.status() in mongosh.\n    \n    Repeat for each remaining secondary member.\n\n  3. When rs.status() shows that the primary has stepped down and another member is primary, upgrade the stepped-down primary:\n    \n    a. Shut down the stepped-down primary.\n    \n    b. Restart with the Enterprise mongod, specifying the same configuration options (e.g. same data directory, configuration file, etc.). 4 \nUPGRADE THE MONGOS INSTANCES. \nFor each mongos instance, shutdown mongos and restart with the Enterprise mongos, specifying the same configuration options. 5 \nRE-ENABLE THE BALANCER. \nUsing mongosh, connect to a mongos in the cluster and run sh.startBalancer() to re-enable the balancer: sh.startBalancer()  Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0, sh.startBalancer() also enables auto-splitting for the sharded cluster. For more inforamtion on the balancer, see Enable the Balancer. \nIMPORTANT Before using any Enterprise features, ensure that all members have been upgraded to Enterprise edition. ←  Upgrade to MongoDB Enterprise (Replica Set)Verify Integrity of MongoDB Packages → On this page  * Consideration\n * Download Enterprise Binaries\n * Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/configure-scram-client-authentication/": " Docs Home → MongoDB Manual \nUSE SCRAM TO AUTHENTICATE CLIENTS \nOn this page    \n * Procedure\n   \n * Next Steps The following procedure sets up SCRAM for client authentication on a standalone mongod instance. To use SCRAM authentication for replica sets or sharded clusters, see Deploy Replica Set With Keyfile Authentication. \nPROCEDURE \n1 \nSTART MONGODB WITHOUT ACCESS CONTROL \nStart a standalone mongod instance without access control. Open a terminal and run the following command as the mongod user: mongod --port 27017 --dbpath /var/lib/mongodb  The mongod instance in this tutorial uses port 27017 and the /var/lib/mongodb data directory. The tutorial assumes that the /var/lib/mongodb directory exists and is the default dbPath. You may specify a different data directory or port as needed. \nTIP When mongod starts, it creates some system files in the /var/lib/mongodb directory. To ensure the system files have the correct ownership, follow this tutorial as the mongod user. If you start mongod as the root user you will have to update file ownership later. 2 \nCONNECT TO THE INSTANCE \nOpen a new terminal and connect to the database deployment with mongosh: mongosh --port 27017  If you are connecting to a different deployment, specify additional command line options, such as --host, as needed to connect. 3 \nCREATE THE USER ADMINISTRATOR  IMPORTANT \nLOCALHOST EXCEPTION You can create the user administrator either before or after enabling access control. If you enable access control before creating any user, MongoDB provides a localhost exception which allows you to create a user administrator in the admin database. Once created, you must authenticate as the user administrator to create additional users. Using mongosh:       1. switch to the admin database  2. add the myUserAdmin user with the userAdminAnyDatabase and readWriteAnyDatabase roles\": use admindb.createUser(  {    user: \"myUserAdmin\",    pwd: passwordPrompt(), // or cleartext password    roles: [      { role: \"userAdminAnyDatabase\", db: \"admin\" },      { role: \"readWriteAnyDatabase\", db: \"admin\" }    ]  })  \nTIP The passwordPrompt() method prompts you to enter the password. You can also specify your password directly as a string. We recommend to use the passwordPrompt() method to avoid the password being visible on your screen and potentially leaking the password to your shell history. The userAdminAnyDatabase role allows this user to:  * create users  * grant or revoke roles from users  * create or modify customs roles You can assign your user additional built-in roles or user-defined roles as needed. The database where you create the user, in this example admin, is the user's authentication database. Although the user needs to authenticate to this database, the user can have roles in other databases. The user's authentication database doesn't limit the user's privileges. 4 \nRE-START THE MONGODB INSTANCE WITH ACCESS CONTROL \nShut down the mongod instance. Using mongosh, issue the following command: db.adminCommand( { shutdown: 1 } )  Exit mongosh. Start the mongod with access control enabled.  * If you start the mongod from the command line, add the --auth command line option:\n   \n   mongod --auth --port 27017 --dbpath /var/lib/mongodb\n   \n     * If you start the mongod using a configuration file, add the security.authorization configuration file setting:\n   \n   security:    authorization: enabled\n   \n    Clients that connect to this instance must now authenticate themselves and can only perform actions as determined by their assigned roles. \nIMPORTANT \nLOCALHOST EXCEPTION You can create users either before or after enabling access control. If you enable access control before creating any user, MongoDB provides a localhost exception which allows you to create a user administrator in the admin database. Once created, you must authenticate as the user administrator to create additional users. 5 \nCONNECT AND AUTHENTICATE AS THE USER ADMINISTRATOR \nUsing mongosh, you can:   NEXT STEPS \nTo use SCRAM authentication for replica sets or sharded clusters, see Deploy Replica Set With Keyfile Authentication.\n\n On this page  * Procedure\n * Next Steps Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/insert-documents/": " Docs Home → MongoDB Manual \nINSERT DOCUMENTS \nOn this page    \n * Insert a Single Document\n   \n * Insert Multiple Documents\n * Insert Behavior ---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the examples on this page. ---------------------------------------- This page provides examples of insert operations in MongoDB. \nNOTE \nCREATING A COLLECTION If the collection does not currently exist, insert operations will create the collection. \nINSERT A SINGLE DOCUMENT     To retrieve the document that you just inserted, query the collection:   INSERT MULTIPLE DOCUMENTS \n---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the examples on this page. ----------------------------------------      INSERT BEHAVIOR  COLLECTION CREATION \nIf the collection does not currently exist, insert operations will create the collection. \n_ID FIELD \nIn MongoDB, each document stored in a collection requires a unique _id field that acts as a primary key. If an inserted document omits the _id field, the MongoDB driver automatically generates an ObjectId for the _id field. This also applies to documents inserted through update operations with upsert: true. \nATOMICITY \nAll write operations in MongoDB are atomic on the level of a single document. For more information on MongoDB and atomicity, see Atomicity and Transactions \nWRITE ACKNOWLEDGEMENT \nWith write concerns, you can specify the level of acknowledgement requested from MongoDB for write operations. For details, see Write Concern.  ←  MongoDB CRUD OperationsInsert Methods → On this page  * Insert a Single Document\n * Insert Multiple Documents\n * Insert Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/model-referenced-one-to-many-relationships-between-documents/": " Docs Home → MongoDB Manual \nMODEL ONE-TO-MANY RELATIONSHIPS WITH DOCUMENT REFERENCES \nOn this page    \n * Overview\n   \n * Pattern \nOVERVIEW \nThis page describes a data model that uses references between documents to describe one-to-many relationships between connected data. \nPATTERN \nConsider the following example that maps publisher and book relationships. The example illustrates the advantage of referencing over embedding to avoid repetition of the publisher information. Embedding the publisher document inside the book document would lead to repetition of the publisher data, as the following documents show: {   title: \"MongoDB: The Definitive Guide\",   author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ],   published_date: ISODate(\"2010-09-24\"),   pages: 216,   language: \"English\",   publisher: {              name: \"O'Reilly Media\",              founded: 1980,              location: \"CA\"            }}\n{   title: \"50 Tips and Tricks for MongoDB Developer\",   author: \"Kristina Chodorow\",   published_date: ISODate(\"2011-05-06\"),   pages: 68,   language: \"English\",   publisher: {              name: \"O'Reilly Media\",              founded: 1980,              location: \"CA\"            }}  To avoid repetition of the publisher data, use references and keep the publisher information in a separate collection from the book collection. When using references, the growth of the relationships determine where to store the reference. If the number of books per publisher is small with limited growth, storing the book reference inside the publisher document may sometimes be useful. Otherwise, if the number of books per publisher is unbounded, this data model would lead to mutable, growing arrays, as in the following example: {   name: \"O'Reilly Media\",   founded: 1980,   location: \"CA\",   books: [123456789, 234567890, ...]}\n{    _id: 123456789,    title: \"MongoDB: The Definitive Guide\",    author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ],    published_date: ISODate(\"2010-09-24\"),    pages: 216,    language: \"English\"}\n{   _id: 234567890,   title: \"50 Tips and Tricks for MongoDB Developer\",   author: \"Kristina Chodorow\",   published_date: ISODate(\"2011-05-06\"),   pages: 68,   language: \"English\"}  To avoid mutable, growing arrays, store the publisher reference inside the book document: {   _id: \"oreilly\",   name: \"O'Reilly Media\",   founded: 1980,   location: \"CA\"}\n{   _id: 123456789,   title: \"MongoDB: The Definitive Guide\",   author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ],   published_date: ISODate(\"2010-09-24\"),   pages: 216,   language: \"English\",   publisher_id: \"oreilly\"}\n{   _id: 234567890,   title: \"50 Tips and Tricks for MongoDB Developer\",   author: \"Kristina Chodorow\",   published_date: ISODate(\"2011-05-06\"),   pages: 68,   language: \"English\",   publisher_id: \"oreilly\"} \n←  Model One-to-Many Relationships with Embedded DocumentsModel Tree Structures → On this page  * Overview\n * Pattern Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/build-indexes-on-sharded-clusters/": " Docs Home → MongoDB Manual \nROLLING INDEX BUILDS ON SHARDED CLUSTERS \nOn this page    \n * Considerations\n   \n * Prerequisites\n * Procedure\n * Additional Information Index builds can impact sharded cluster performance. By default, MongoDB 4.4 and later build indexes simultaneously on all data-bearing replica set members. Index builds on sharded clusters occur only on those shards which contain data for the collection being indexed. For workloads which cannot tolerate performance decrease due to index builds, consider using the following procedure to build indexes in a rolling fashion. Rolling index builds take at most one shard replica set member out at a time, starting with the secondary members, and builds the index on that member as a standalone. Rolling index builds require at least one replica set election per shard. \nCONSIDERATIONS  UNIQUE INDEXES \nTo create unique indexes using the following procedure, you must stop all writes to the collection during this procedure. If you cannot stop all writes to the collection during this procedure, do not use the procedure on this page. Instead, build your unique index on the collection by issuing db.collection.createIndex() on the mongos for a sharded cluster. \nOPLOG SIZE \nEnsure that your oplog is large enough to permit the indexing or re-indexing operation to complete without falling too far behind to catch up. See the oplog sizing documentation for additional information. \nPREREQUISITES \nFor building unique indexes\n      1. To create unique indexes using the following procedure, you must stop all writes to the collection during the index build. Otherwise, you may end up with inconsistent data across the replica set members. If you cannot stop all writes to the collection, do not use the following procedure to create unique indexes.\n    \n    \n    WARNING\n    \n    If you cannot stop all writes to the collection, do not use the following procedure to create unique indexes.  2. Before creating the index, validate that no documents in the collection violate the index constraints. If a collection is distributed across shards and a shard contains a chunk with duplicate documents, the create index operation may succeed on the shards without duplicates but not on the shard with duplicates. To avoid leaving inconsistent indexes across shards, you can issue the db.collection.dropIndex() from a mongos to drop the index from the collection. \nPROCEDURE  IMPORTANT The following procedure to build indexes in a rolling fashion applies to sharded clusters deployments, and not replica set deployments. For the procedure for replica sets, see Rolling Index Builds on Replica Sets instead. \nA. STOP THE BALANCER \nConnect mongosh to a mongos instance in the sharded cluster, and run sh.stopBalancer() to disable the balancer: [1] sh.stopBalancer()  \nNOTE If a migration is in progress, the system will complete the in-progress migration before stopping the balancer. To verify that the balancer is disabled, run sh.getBalancerState(), which returns false if the balancer is disabled: sh.getBalancerState()  [1] Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes.In MongoDB versions earlier than 6.0, sh.stopBalancer() also disables auto-splitting for the sharded cluster. \nB. DETERMINE THE DISTRIBUTION OF THE COLLECTION \nFrom mongosh connected to the mongos, refresh the cached routing table for that mongos to avoid returning stale distribution information for the collection. Once refreshed, run db.collection.getShardDistribution() for the collection you wish to build the index. For example, if you want to create an ascending index on the records collection in the test database: db.adminCommand( { flushRouterConfig: \"test.records\" } );db.records.getShardDistribution();  The method outputs the shard distribution. For example, consider a sharded cluster with 3 shards shardA, shardB, and shardC and the db.collection.getShardDistribution() returns the following:\n\n From the output, you only build the indexes for test.records on shardA and shardC. \nC. BUILD INDEXES ON THE SHARDS THAT CONTAIN COLLECTION CHUNKS \nFor each shard that contains chunks for the collection, follow the procedure to build the index on the shard. C1. STOP ONE SECONDARY AND RESTART AS A STANDALONE \nFor an affected shard, stop the mongod process associated with one of its secondary. Restart after making the following configuration updates:  \n[2](1, 2) By running the mongod on a different port, you ensure that the other members of the replica set and all clients will not contact the member while you are building the index. C2. BUILD THE INDEX \nConnect directly to the mongod instance running as a standalone on the new port and create the new index for this instance. For example, connect mongosh to the instance, and use the db.collection.createIndex() method to create an ascending index on the username field of the records collection: db.records.createIndex( { username: 1 } )  C3. RESTART THE PROGRAM MONGOD AS A REPLICA SET MEMBER \nWhen the index build completes, shutdown the mongod instance. Undo the configuration changes made when starting as a standalone to return to its original configuration and restart. \nIMPORTANT Be sure to remove the skipShardingConfigurationChecks parameter and disableLogicalSessionCacheRefresh parameter. For example, to restart your replica set shard member:  \nAllow replication to catch up on this member. C4. REPEAT THE PROCEDURE FOR THE REMAINING SECONDARIES FOR THE SHARD \nOnce the member catches up with the other members of the set, repeat the procedure one member at a time for the remaining secondary members for the shard:  1. C1. Stop One Secondary and Restart as a Standalone  2. C2. Build the Index  3. C3. Restart the Program mongod as a Replica Set Member C5. BUILD THE INDEX ON THE PRIMARY \nWhen all the secondaries for the shard have the new index, step down the primary for the shard, restart it as a standalone using the procedure described above, and build the index on the former primary:  1. Use the rs.stepDown() method in mongosh to step down the primary. Upon successful stepdown, the current primary becomes a secondary and the replica set members elect a new primary.  2. C1. Stop One Secondary and Restart as a Standalone  3. C2. Build the Index  4. C3. Restart the Program mongod as a Replica Set Member \nD. REPEAT FOR THE OTHER AFFECTED SHARDS \nOnce you finish building the index for a shard, repeat C. Build Indexes on the Shards That Contain Collection Chunks for the other affected shards. \nE. RESTART THE BALANCER \nOnce you finish the rolling index build for the affected shards, restart the balancer. Connect mongosh to a mongos instance in the sharded cluster, and run sh.startBalancer(): [3] sh.startBalancer()  [3] Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes.In MongoDB versions earlier than 6.0, sh.startBalancer() also enables auto-splitting for the sharded cluster. \nADDITIONAL INFORMATION \nA sharded collection has an inconsistent index if the collection does not have the exact same indexes (including the index options) on each shard that contains chunks for the collection. Although inconsistent indexes should not occur during normal operations, inconsistent indexes can occur, such as:  * When a user is creating an index with a unique key constraint and one shard contains a chunk with duplicate documents. In such cases, the create index operation may succeed on the shards without duplicates but not on the shard with duplicates.  * When a user is creating an index across the shards in a rolling manner but either fails to build the index for an associated shard or incorrectly builds an index with different specification. Starting in MongoDB 4.4 (and 4.2.6), the config server primary periodically checks for index inconsistencies across the shards for sharded collections. To configure these periodic checks, see enableShardedIndexConsistencyCheck and shardedIndexConsistencyCheckIntervalMS. The command serverStatus returns the field shardedIndexConsistency to report on index inconsistencies when run on the config server primary.\n\n ←  Rolling Index Builds on Replica SetsIndex Intersection → On this page  * Considerations\n * Prerequisites\n * Procedure\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/model-data-for-atomic-operations/": " Docs Home → MongoDB Manual \nMODEL DATA FOR ATOMIC OPERATIONS \nOn this page    \n * Pattern Although MongoDB supports multi-document transactions for replica sets (starting in version 4.0) and sharded clusters (starting in version 4.2), for many scenarios, the denormalized data model, as discussed on this page, will continue to be optimal for your data and use cases. \nPATTERN \nIn MongoDB, a write operation on a single document is atomic. For fields that must be updated together, embedding the fields within the same document ensures that the fields can be updated atomically. For example, consider a situation where you need to maintain information on books, including the number of copies available for checkout as well as the current checkout information. The available copies of the book and the checkout information should be in sync. As such, embedding the available field and the checkout field within the same document ensures that you can update the two fields atomically. {    _id: 123456789,    title: \"MongoDB: The Definitive Guide\",    author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ],    published_date: ISODate(\"2010-09-24\"),    pages: 216,    language: \"English\",    publisher_id: \"oreilly\",    available: 3,    checkout: [ { by: \"joe\", date: ISODate(\"2012-10-15\") } ]}  Then to update with new checkout information, you can use the db.collection.updateOne() method to atomically update both the available field and the checkout field: db.books.updateOne (   { _id: 123456789, available: { $gt: 0 } },   {     $inc: { available: -1 },     $push: { checkout: { by: \"abc\", date: new Date() } }   })  The operation returns a document that contains information on the status of the operation: { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }  The matchedCount field shows that 1 document matched the update condition, and modifiedCount shows that the operation updated 1 document. If no document matched the update condition, then matchedCount and modifiedCount would be 0 and would indicate that you could not check out the book. ←  Model Specific Application ContextsModel Data to Support Keyword Search → On this page  * Pattern Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/project-fields-from-query-results/": " Docs Home → MongoDB Manual \nPROJECT FIELDS TO RETURN FROM QUERY \nOn this page    \n * Return All Fields in Matching Documents\n   \n * Return the Specified Fields and the _id Field Only\n * Suppress _id Field\n * Return All But the Excluded Fields\n * Return Specific Fields in Embedded Documents\n * Suppress Specific Fields in Embedded Documents\n * Projection on Embedded Documents in an Array\n * Project Specific Array Elements in the Returned Array\n * Additional Considerations ---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the following examples. ---------------------------------------- By default, queries in MongoDB return all fields in matching documents. To limit the amount of data that MongoDB sends to applications, you can include a projection document to specify or restrict fields to return.    RETURN ALL FIELDS IN MATCHING DOCUMENTS   The following example returns all fields from all documents in the inventory collection where the status equals \"A\":  \nThe operation corresponds to the following SQL statement: SELECT * from inventory WHERE status = \"A\"  \nRETURN THE SPECIFIED FIELDS AND THE _ID FIELD ONLY \nA projection can explicitly include several fields by setting the <field> to 1 in the projection document. The following operation returns all documents that match the query. In the result set, only the item, status and, by default, the _id fields return in the matching documents.  \nThe operation corresponds to the following SQL statement: SELECT _id, item, status from inventory WHERE status = \"A\"  \nSUPPRESS _ID FIELD \nYou can remove the _id field from the results by setting it to 0 in the projection, as in the following example:  \nThe operation corresponds to the following SQL statement: SELECT item, status from inventory WHERE status = \"A\"  \nNOTE With the exception of the _id field, you cannot combine inclusion and exclusion statements in projection documents. \nRETURN ALL BUT THE EXCLUDED FIELDS \nInstead of listing the fields to return in the matching document, you can use a projection to exclude specific fields. The following example which returns all fields except for the status and the instock fields in the matching documents:   NOTE With the exception of the _id field, you cannot combine inclusion and exclusion statements in projection documents. \nRETURN SPECIFIC FIELDS IN EMBEDDED DOCUMENTS \nYou can return specific fields in an embedded document. Use the dot notation to refer to the embedded field and set to 1 in the projection document. The following example returns:      * The _id field (returned by default),  * The item field,  * The status field,  * The uom field in the size document. The uom field remains embedded in the size document.  \nStarting in MongoDB 4.4, you can also specify embedded fields using the nested form, e.g. { item: 1, status: 1, size: { uom: 1 } }. \nSUPPRESS SPECIFIC FIELDS IN EMBEDDED DOCUMENTS \nYou can suppress specific fields in an embedded document. Use the dot notation to refer to the embedded field in the projection document and set to 0. The following example specifies a projection to exclude the uom field inside the size document. All other fields are returned in the matching documents:  \nStarting in MongoDB 4.4, you can also specify embedded fields using the nested form, e.g. { size: { uom: 0 } }. \nPROJECTION ON EMBEDDED DOCUMENTS IN AN ARRAY \nUse dot notation to project specific fields inside documents embedded in an array. The following example specifies a projection to return:  * The _id field (returned by default),  * The item field,  * The status field,  * The qty field in the documents embedded in the instock array.   PROJECT SPECIFIC ARRAY ELEMENTS IN THE RETURNED ARRAY   \nADDITIONAL CONSIDERATIONS \nStarting in MongoDB 4.4, MongoDB enforces additional restrictions with regards to projections. See Projection Restrictions for details. \nTIP \nSEE ALSO:  * Projection  * Query Documents ←  Query an Array of Embedded DocumentsQuery for Null or Missing Fields → On this page  * Return All Fields in Matching Documents\n * Return the Specified Fields and the _id Field Only\n * Suppress _id Field\n * Return All But the Excluded Fields\n * Return Specific Fields in Embedded Documents\n * Suppress Specific Fields in Embedded Documents\n * Projection on Embedded Documents in an Array\n * Project Specific Array Elements in the Returned Array\n * Additional Considerations Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/iterate-a-cursor/": " Docs Home → MongoDB Manual \nITERATE A CURSOR IN MONGOSH \nOn this page    \n * Manually Iterate the Cursor\n   \n * Iterator Index\n * Cursor Behaviors\n * Cursor Information The db.collection.find() method returns a cursor. To access the documents, you need to iterate the cursor. However, in mongosh, if the returned cursor is not assigned to a variable using the var keyword, then the cursor is automatically iterated up to 20 times [1] to print up to the first 20 documents in the results. The following examples describe ways to manually iterate the cursor to access the documents or to use the iterator index. \nMANUALLY ITERATE THE CURSOR \nIn mongosh, when you assign the cursor returned from the find() method to a variable using the var keyword, the cursor does not automatically iterate. You can call the cursor variable in the shell to iterate up to 20 times [1] and print the matching documents, as in the following example: var myCursor = db.users.find( { type: 2 } );\nmyCursor  You can also use the cursor method next() to access the documents, as in the following example: var myCursor = db.users.find( { type: 2 } );\nwhile (myCursor.hasNext()) {   print(tojson(myCursor.next()));}  As an alternative print operation, consider the printjson() helper method to replace print(tojson()): var myCursor = db.users.find( { type: 2 } );\nwhile (myCursor.hasNext()) {   printjson(myCursor.next());}  You can use the cursor method forEach() to iterate the cursor and access the documents, as in the following example: var myCursor =  db.users.find( { type: 2 } );\nmyCursor.forEach(printjson);  See JavaScript cursor methods and your driver documentation for more information on cursor methods. [1](1, 2) You can set the DBQuery.shellBatchSize attribute to change the number of documents from the default value of 20. \nITERATOR INDEX \nIn mongosh, you can use the toArray() method to iterate the cursor and return the documents in an array, as in the following: var myCursor = db.inventory.find( { type: 2 } );var documentArray = myCursor.toArray();var myDocument = documentArray[3];  The toArray() method loads into RAM all documents returned by the cursor; the toArray() method exhausts the cursor. Additionally, some Drivers provide access to the documents by using an index on the cursor (i.e. cursor[index]). This is a shortcut for first calling the toArray() method and then using an index on the resulting array. Consider the following example: var myCursor = db.users.find( { type: 2 } );var myDocument = myCursor[1];  The myCursor[1] is equivalent to the following example: myCursor.toArray() [1];  \nCURSOR BEHAVIORS  CURSORS OPENED WITHIN A SESSION \nStarting in MongoDB 5.0 (and 4.4.8), cursors created within a client session close when the corresponding server session ends with the killSessions command, if the session times out, or if the client has exhausted the cursor. By default, server sessions have an expiration timeout of 30 minutes. To change the value, set the localLogicalSessionTimeoutMinutes parameter when starting up mongod. \nCURSORS OPENED OUTSIDE OF A SESSION \nCursors that aren't opened under a session automatically close after 10 minutes of inactivity, or if client has exhausted the cursor. To override this behavior in mongosh, you can use the cursor.noCursorTimeout() method: var myCursor = db.users.find().noCursorTimeout();  After setting the noCursorTimeout option, you must either close the cursor manually with cursor.close() or by exhausting the cursor's results. See your driver documentation for information on setting the noCursorTimeout option. \nCURSOR ISOLATION \nAs a cursor returns documents, other operations may interleave with the query. \nCURSOR BATCHES \nThe MongoDB server returns the query results in batches. The amount of data in the batch will not exceed the maximum BSON document size. To override the default size of the batch, see batchSize() and limit().\n\n find() and aggregate() operations have an initial batch size of 101 documents by default. Subsequent getMore operations issued against the resulting cursor have no default batch size, so they are limited only by the 16 megabyte message size. For queries that include a sort operation without an index, the server must load all the documents in memory to perform the sort before returning any results. As you iterate through the cursor and reach the end of the returned batch, if there are more results, cursor.next() will perform a getMore operation to retrieve the next batch. To see how many documents remain in the batch as you iterate the cursor, you can use the objsLeftInBatch() method, as in the following example: var myCursor = db.inventory.find();\nvar myFirstDocument = myCursor.hasNext() ? myCursor.next() : null;\nmyCursor.objsLeftInBatch();  \nCURSOR INFORMATION \nThe db.serverStatus() method returns a document that includes a metrics field. The metrics field contains a metrics.cursor field with the following information:      * number of timed out cursors since the last server restart  * number of open cursors with the option DBQuery.Option.noTimeout set to prevent timeout after a period of inactivity  * number of \"pinned\" open cursors  * total number of open cursors Consider the following example which calls the db.serverStatus() method and accesses the metrics field from the results and then the cursor field from the metrics field: db.serverStatus().metrics.cursor  The result is the following document: {   \"timedOut\" : <number>   \"open\" : {      \"noTimeout\" : <number>,      \"pinned\" : <number>,      \"total\" : <number>   }}  \nTIP \nSEE ALSO: db.serverStatus() ←  Query for Null or Missing FieldsUpdate Documents → On this page  * Manually Iterate the Cursor\n * Iterator Index\n * Cursor Behaviors\n * Cursor Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/configure-windows-netsh-firewall/": " Docs Home → MongoDB Manual \nCONFIGURE WINDOWS NETSH FIREWALL FOR MONGODB \nOn this page    \n * Overview\n   \n * Patterns\n * Manage and Maintain Windows Firewall Configurations On Windows Server systems, the netsh program provides methods for managing the Windows Firewall. These firewall rules make it possible for administrators to control what hosts can connect to the system, and limit risk exposure by limiting the hosts that can connect to a system. This document outlines basic Windows Firewall configurations. Use these approaches as a starting point for your larger networking organization. For a detailed over view of security practices and risk management for MongoDB, see Security. \nTIP \nSEE ALSO: Windows Firewall documentation from Microsoft. \nOVERVIEW \nWindows Firewall processes rules in an ordered determined by rule type, and parsed in the following order:       1. Windows Service Hardening  2. Connection security rules  3. Authenticated Bypass Rules  4. Block Rules  5. Allow Rules  6. Default Rules By default, the policy in Windows Firewall allows all outbound connections and blocks all incoming connections. Given the default ports of all MongoDB processes, you must configure networking rules that permit only required communication between your application and the appropriate exe and exe instances. The configuration changes outlined in this document will create rules which explicitly allow traffic from specific addresses and on specific ports, using a default policy that drops all traffic that is not explicitly allowed. You can configure the Windows Firewall with using the netsh command line tool or through a windows application. On Windows Server 2008 this application is Windows Firewall With Advanced Security in Administrative Tools. On previous versions of Windows Server, access the Windows Firewall application in the System and Security control panel. The procedures in this document use the netsh command line tool. \nPATTERNS \nThis section contains a number of patterns and examples for configuring Windows Firewall for use with MongoDB deployments. If you have configured different ports using the port configuration setting, you will need to modify the rules accordingly. \nTRAFFIC TO AND FROM MONGOD.EXE INSTANCES \nThis pattern is applicable to all exe instances running as standalone instances or as part of a replica set. The goal of this pattern is to explicitly allow traffic to the exe instance from the application server. netsh advfirewall firewall add rule name=\"Open mongod port 27017\" dir=in action=allow protocol=TCP localport=27017  This rule allows all incoming traffic to port 27017, which allows the application server to connect to the exe instance. Windows Firewall also allows enabling network access for an entire application rather than to a specific port, as in the following example: netsh advfirewall firewall add rule name=\"Allowing mongod\" dir=in action=allow program=\" C:\\Program Files\\MongoDB\\Server\\3.4\\bin\\mongod.exe\"  You can allow all access for a exe server, with the following invocation: netsh advfirewall firewall add rule name=\"Allowing mongos\" dir=in action=allow program=\" C:\\Program Files\\MongoDB\\Server\\3.4\\bin\\mongos.exe\"  \nTRAFFIC TO AND FROM MONGOS.EXE INSTANCES \nexe instances provide query routing for sharded clusters. Clients connect to exe instances, which behave from the client's perspective as exe instances. In turn, the exe connects to all exe instances that are components of the sharded cluster. Use the same Windows Firewall command to allow traffic to and from these instances as you would from the exe instances that are members of the replica set. netsh advfirewall firewall add rule name=\"Open mongod shard port 27018\" dir=in action=allow protocol=TCP localport=27018  \nTRAFFIC TO AND FROM A MONGODB CONFIG SERVER \nConfiguration servers, host the config database that stores metadata for sharded clusters. Each production cluster has three configuration servers, initiated using the mongod --configsvr option. [1] Configuration servers listen for connections on port 27019. As a result, add the following Windows Firewall rules to the config server to allow incoming and outgoing connection on port 27019, for connection to the other config servers. netsh advfirewall firewall add rule name=\"Open mongod config svr port 27019\" dir=in action=allow protocol=TCP localport=27019  Additionally, config servers need to allow incoming connections from all of the exe instances in the cluster and all exe instances in the cluster. Add rules that resemble the following:\n\n  Replace <ip-address> with the addresses of the exe instances and the shard exe instances. [1] You also can run a config server by using the configsvr value for the clusterRole setting in a configuration file. \nTRAFFIC TO AND FROM A MONGODB SHARD SERVER \nFor shard servers, running as mongod --shardsvr [2] Because the default port number is 27018 when running with the shardsvr value for the clusterRole setting, you must configure the following Windows Firewall rules to allow traffic to and from each shard: netsh advfirewall firewall add rule name=\"Open mongod shardsvr inbound\" dir=in action=allow protocol=TCP remoteip=<ip-address> localport=27018netsh advfirewall firewall add rule name=\"Open mongod shardsvr outbound\" dir=out action=allow protocol=TCP remoteip=<ip-address> localport=27018  Replace the <ip-address> specification with the IP address of all exe instances. This allows you to permit incoming and outgoing traffic between all shards including constituent replica set members to:  * all exe instances in the shard's replica sets.  * all exe instances in other shards. [3] Furthermore, shards need to be able make outgoing connections to:  * all exe instances.  * all exe instances in the config servers. Create a rule that resembles the following, and replace the <ip-address> with the address of the config servers and the exe instances: netsh advfirewall firewall add rule name=\"Open mongod config svr outbound\" dir=out action=allow protocol=TCP remoteip=<ip-address> localport=27018  [2] You can also specify the shard server option with the shardsvr value for the clusterRole setting in the configuration file. Shard members are also often conventional replica sets using the default port. [3] All shards in a cluster need to be able to communicate with all other shards to facilitate chunk and balancing operations. \nPROVIDE ACCESS FOR MONITORING SYSTEMS \nThe mongostat diagnostic tool, when running with the --discover needs to be able to reach all components of a cluster, including the config servers, the shard servers, and the exe instances. Changed in version 3.6: MongoDB 3.6 removes the deprecated HTTP interface and REST API to MongoDB. \nMANAGE AND MAINTAIN WINDOWS FIREWALL CONFIGURATIONS \nThis section contains a number of basic operations for managing and using netsh. While you can use the GUI front ends to manage the Windows Firewall, all core functionality is accessible is accessible from netsh. \nDELETE ALL WINDOWS FIREWALL RULES \nTo delete the firewall rule allowing exe traffic: netsh advfirewall firewall delete rule name=\"Open mongod port 27017\" protocol=tcp localport=27017\nnetsh advfirewall firewall delete rule name=\"Open mongod shard port 27018\" protocol=tcp localport=27018  \nLIST ALL WINDOWS FIREWALL RULES \nTo return a list of all Windows Firewall rules: netsh advfirewall firewall show rule name=all  \nRESET WINDOWS FIREWALL \nTo reset the Windows Firewall rules: netsh advfirewall reset  \nBACKUP AND RESTORE WINDOWS FIREWALL RULES \nTo simplify administration of larger collection of systems, you can export or import firewall systems from different servers) rules very easily on Windows: Export all firewall rules with the following command: netsh advfirewall export \"C:\\temp\\MongoDBfw.wfw\"  Replace \"C:\\temp\\MongoDBfw.wfw\" with a path of your choosing. You can use a command in the following form to import a file created using this operation: netsh advfirewall import \"C:\\temp\\MongoDBfw.wfw\" \n←  Configure Linux iptables Firewall for MongoDBImplement Field Level Redaction → On this page  * Overview\n * Patterns\n * Manage and Maintain Windows Firewall Configurations Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/enforce-keyfile-access-control-in-existing-replica-set/": " Docs Home → MongoDB Manual \nUPDATE REPLICA SET TO KEYFILE AUTHENTICATION \nOn this page    \n * Overview\n   \n * Considerations\n * Enforce Keyfile Access Control on Existing Replica Set\n * x.509 Internal Authentication \nOVERVIEW \nEnforcing access control on an existing replica set requires configuring:      * Security between members of the replica set using Internal Authentication, and  * Security between connecting clients and the replica set using User Access Controls. For this tutorial, each member of the replica set uses the same internal authentication mechanism and settings. Enforcing internal authentication also enforces user access control. To connect to the replica set, clients like mongosh need to use a user account. See Users. \nCLOUD MANAGER AND OPS MANAGER \nIf Cloud Manager or Ops Manager is managing your deployment, see the Cloud Manager manual or the Ops Manager manual for enforcing access control. \nCONSIDERATIONS  IMPORTANT To avoid configuration updates due to IP address changes, use DNS hostnames instead of IP addresses. It is particularly important to use a DNS hostname instead of an IP address when configuring replica set members or sharded cluster members. Use hostnames instead of IP addresses to configure clusters across a split network horizon. Starting in MongoDB 5.0, nodes that are only configured with an IP address will fail startup validation and will not start. \nIP BINDING \nMongoDB binaries, mongod and mongos, bind to localhost by default. \nOPERATING SYSTEM \nThis tutorial uses the mongod programs. Windows users should use the exe program instead. \nKEYFILE SECURITY \nKeyfiles are bare-minimum forms of security and are best suited for testing or development environments. For production environments we recommend using x.509 certificates. \nUSERS \nThis tutorial covers creating the minimum number of administrative users on the admin database only. For the user authentication, the tutorial uses the default SCRAM authentication mechanism. Challenge-response security mechanisms are best suited for testing or development environments. For production environments, we recommend using x.509 certificates or LDAP Proxy Authentication (available for MongoDB Enterprise only) or Kerberos Authentication (available for MongoDB Enterprise only). For details on creating users for specific authentication mechanism, refer to the specific authentication mechanism pages. See ➤ Configure Role-Based Access Control for best practices for user creation and management. \nDOWNTIME \nThe following procedure for enforcing access control requires downtime. For a procedure that does not require downtime, see Update Replica Set to Keyfile Authentication (No Downtime) instead. \nENFORCE KEYFILE ACCESS CONTROL ON EXISTING REPLICA SET \n1 \nCREATE A KEYFILE. \nWith keyfile authentication, each mongod instances in the replica set uses the contents of the keyfile as the shared password for authenticating other members in the deployment. Only mongod instances with the correct keyfile can join the replica set. \nNOTE Starting in MongoDB 4.2, keyfiles for internal membership authentication use YAML format to allow for multiple keys in a keyfile. The YAML format accepts content of:  * a single key string (same as in earlier versions),  * multiple key strings (each string must be enclosed in quotes), or  * sequence of key strings. The YAML format is compatible with the existing single-key keyfiles that use the text file format. A key's length must be between 6 and 1024 characters and may only contain characters in the base64 set. All members of the replica set must share at least one common key. \nNOTE On UNIX systems, the keyfile must not have group or world permissions. On Windows systems, keyfile permissions are not checked. You can generate a keyfile using any method you choose. For example, the following operation uses openssl to generate a complex pseudo-random 1024 character string to use as a shared password. It then uses chmod to change file permissions to provide read permissions for the file owner only: openssl rand -base64 756 > <path-to-keyfile>chmod 400 <path-to-keyfile>  See Keyfiles for additional details and requirements for using keyfiles. 2 \nCOPY THE KEYFILE TO EACH REPLICA SET MEMBER. \nCopy the keyfile to each server hosting the replica set members. Ensure that the user running the mongod instances is the owner of the file and can access the keyfile. Avoid storing the keyfile on storage mediums that can be easily disconnected from the hardware hosting the mongod instances, such as a USB drive or a network attached storage device. 3 \nSHUT DOWN ALL MEMBERS OF THE REPLICA SET.\n\n To shut down a mongod, connect each mongod using mongosh and issue the db.shutdownServer() on the admin database: use admindb.shutdownServer()  At the end of this step, all members of the replica set should be offline. 4 \nRESTART EACH MEMBER OF THE REPLICA SET WITH ACCESS CONTROL ENFORCED. \nRestart each mongod in the replica set with either the security.keyFile configuration file setting or the --keyFile command-line option. Running mongod with the --keyFile command-line option or the security.keyFile configuration file setting enforces both Internal/Membership Authentication and Role-Based Access Control. CONFIGURATION FILE \nIf using a configuration file, set  * security.keyFile to the keyfile's path, and  * replication.replSetName to the replica set name. Include additional options as required for your configuration. For instance, if you wish remote clients to connect to your deployment or your deployment members are run on different hosts, specify the net.bindIp setting. For more information, see Localhost Binding Compatibility Changes. security:  keyFile: <path-to-keyfile>replication:  replSetName: <replicaSetName>net:   bindIp: localhost,<hostname(s)|ip address(es)>  Start the mongod using the configuration file: mongod --config <path-to-config-file>  For more information on the configuration file, see configuration options. COMMAND LINE \nIf using the command line options, start the mongod with the following options:  * --keyFile set to the keyfile's path, and  * --replSet set to the replica set name. Include additional options as required for your configuration. For instance, if you wish remote clients to connect to your deployment or your deployment members are run on different hosts, specify the --bind_ip. For more information, see Localhost Binding Compatibility Changes. mongod --keyFile <path-to-keyfile> --replSet <replicaSetName> --bind_ip localhost,<hostname(s)|ip address(es)>  \nIMPORTANT To avoid configuration updates due to IP address changes, use DNS hostnames instead of IP addresses. It is particularly important to use a DNS hostname instead of an IP address when configuring replica set members or sharded cluster members. Use hostnames instead of IP addresses to configure clusters across a split network horizon. Starting in MongoDB 5.0, nodes that are only configured with an IP address will fail startup validation and will not start. For more information on command-line options, see the mongod reference page. 5 \nCONNECT TO THE PRIMARY USING THE LOCALHOST INTERFACE. \nConnect mongosh to one of the mongod instances over the localhost interface. You must run mongosh on the same physical machine as the mongod instance. Use rs.status() to identify the primary replica set member. If you are connected to the primary, continue to the next step. If not, connect mongosh to the primary over the localhost interface. \nIMPORTANT \nYOU MUST CONNECT TO THE PRIMARY BEFORE PROCEEDING. \n6 \nCREATE THE USER ADMINISTRATOR.  IMPORTANT After you create the first user, the localhost exception is no longer available. The first user must have privileges to create other users, such as a user with the userAdminAnyDatabase. This ensures that you can create additional users after the Localhost Exception closes. If at least one user does not have privileges to create users, once the localhost exception closes you may be unable to create or modify users with new privileges, and therefore unable to access necessary operations. Add a user using the db.createUser() method. The user should have at minimum the userAdminAnyDatabase role on the admin database. You must be connected to the primary to create users. The following example creates the user fred with the userAdminAnyDatabase role on the admin database. \nIMPORTANT Passwords should be random, long, and complex to ensure system security and to prevent or delay malicious access. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell.\n\n  Enter the password when prompted. See Database User Roles for a full list of built-in roles and related to database administration operations. 7 \nAUTHENTICATE AS THE USER ADMINISTRATOR. \nAuthenticate to the admin database. In mongosh, use db.auth() to authenticate. For example, the following authenticate as the user administrator fred: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. db.getSiblingDB(\"admin\").auth(\"fred\", passwordPrompt()) // or cleartext password  Alternatively, connect a new mongosh instance to the primary replica set member using the -u <username>, -p <password>, and the --authenticationDatabase parameters. mongosh -u \"fred\" -p  --authenticationDatabase \"admin\"  If you do not specify the password to the -p command-line option, mongosh prompts for the password. 8 \nCREATE THE CLUSTER ADMINISTRATOR (OPTIONAL). \nThe cluster administrator user has the clusterAdmin role, which grants access to replication operations. Create a cluster administrator user and assign the clusterAdmin role in the admin database: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. db.getSiblingDB(\"admin\").createUser(  {    \"user\" : \"ravi\",    \"pwd\" : passwordPrompt(),     // or cleartext password    roles: [ { \"role\" : \"clusterAdmin\", \"db\" : \"admin\" } ]  })  Enter the password when prompted. See Cluster Administration Roles for a full list of built-in roles related to replica set operations. 9 \nCREATE ADDITIONAL USERS (OPTIONAL). \nCreate users to allow clients to connect and interact with the replica set. See Database User Roles for basic built-in roles to use in creating read-only and read-write users. You may also want additional administrative users. For more information on users, see Users. \nX.509 INTERNAL AUTHENTICATION \nFor details on using x.509 for internal authentication, see Use x.509 Certificate for Membership Authentication. To upgrade from keyfile internal authentication to x.509 internal authentication, see Upgrade from Keyfile Authentication to x.509 Authentication. ←  Deploy Replica Set With Keyfile AuthenticationUpdate Replica Set to Keyfile Authentication (No Downtime) → On this page  * Overview\n * Considerations\n * Enforce Keyfile Access Control on Existing Replica Set\n * x.509 Internal Authentication Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/upgrade-cluster-to-ssl/": " Docs Home → MongoDB Manual \nUPGRADE A CLUSTER TO USE TLS/SSL \nThe MongoDB server supports listening for both TLS/SSL encrypted and unencrypted connections on the same TCP port. This allows upgrades of MongoDB clusters to use TLS/SSL encrypted connections. \nNOTE MongoDB disables support for TLS 1.0 encryption on systems where TLS 1.1+ is available. For more details, see Disable TLS 1.0. \nPROCEDURE (USING TLS SETTINGS)  IMPORTANT A full description of TLS/SSL, PKI (Public Key Infrastructure) certificates, and Certificate Authority is beyond the scope of this document. This page assumes prior knowledge of TLS/SSL as well as access to valid certificates. To upgrade from a MongoDB cluster using no TLS/SSL encryption to one using only TLS/SSL encryption, use the following rolling upgrade process. \nNOTE The procedures in this section use the tls settings/option (Available in MongoDB 4.2). For procedures using their ssl aliases, see Procedure (Using ssl Settings). The tls settings/options provide identical functionality as the ssl options since MongoDB has always supported TLS 1.0 and later.       1. For each node of a cluster, start the node with the command-line option --tlsMode or the configuration file option net.tls.mode set to allowTLS. The allowTLS setting allows the node to accept both TLS/SSL and non-TLS/non-SSL incoming connections. Its connections to other servers do not use TLS/SSL. Include other TLS/SSL options [2] as well as any other options that are required for your specific configuration.\n    \n    \n    NOTE\n    \n    mongod and mongos bind to localhost by default. If the members of your deployment are run on different hosts or if you wish remote clients to connect to your deployment, you must specify --bind_ip or net.bindIp.\n    \n    For example:\n    \n    \n    \n    \n    Upgrade all nodes of the cluster to these settings.  2. Switch all clients to use TLS/SSL. See TLS/SSL Configuration for Clients.  3. For each node of a cluster, use the setParameter command to update the tlsMode to preferTLS. [1] With preferTLS as its net.tls.mode, the node accepts both TLS/SSL and non-TLS/non-SSL incoming connections, and its connections to other servers use TLS/SSL. For example:\n    \n    db.adminCommand( { setParameter: 1, tlsMode: \"preferTLS\" } )\n    \n    \n    \n    Upgrade all nodes of the cluster to these settings.\n    \n    At this point, all connections should be using TLS/SSL.  4. For each node of the cluster, use the setParameter command to update the tlsMode to requireTLS. [1] With requireTLS as its net.tls.mode, the node will reject any non-TLS/non-SSL connections. For example:\n    \n    db.adminCommand( { setParameter: 1, tlsMode: \"requireTLS\" } )\n    \n      5. After the upgrade of all nodes, edit the configuration file with the appropriate TLS/SSL settings to ensure that upon subsequent restarts, the cluster uses TLS/SSL. \nPROCEDURE (USING SSL SETTINGS)  IMPORTANT A full description of TLS/SSL, PKI (Public Key Infrastructure) certificates, and Certificate Authority is beyond the scope of this document. This page assumes prior knowledge of TLS/SSL as well as access to valid certificates. To upgrade from a MongoDB cluster using no TLS/SSL encryption to one using only TLS/SSL encryption, use the following rolling upgrade process. \nNOTE The procedures in this section use the ssl settings/option. For procedures using their tls aliases (Available in MongoDB 4.2), see Procedure (Using tls Settings). The tls settings/options provide identical functionality as the ssl options since MongoDB has always supported TLS 1.0 and later.\n\n  2. Switch all clients to use TLS/SSL. See TLS/SSL Configuration for Clients.  3. For each node of a cluster, use the setParameter command to update the sslMode to preferSSL. [1] With preferSSL as its net.ssl.mode, the node accepts both TLS/SSL and non-TLS/non-SSL incoming connections, and its connections to other servers use TLS/SSL. For example:\n    \n    db.adminCommand( { setParameter: 1, sslMode: \"preferSSL\" } )\n    \n    \n    \n    Upgrade all nodes of the cluster to these settings.\n    \n    At this point, all connections should be using TLS/SSL.  4. For each node of the cluster, use the setParameter command to update the sslMode to requireSSL. [1] With requireSSL as its net.ssl.mode, the node will reject any non-TLS/non-SSL connections. For example:\n    \n    db.adminCommand( { setParameter: 1, sslMode: \"requireSSL\" } )\n    \n      5. After the upgrade of all nodes, edit the configuration file with the appropriate TLS/SSL settings to ensure that upon subsequent restarts, the cluster uses TLS/SSL. [1](1, 2, 3, 4) As an alternative to using the setParameter command, you can also restart the nodes with the appropriate TLS/SSL options and values. [2](1, 2) Starting in MongoDB 4.0, you can use system SSL certificate stores for Windows and macOS. To use the system SSL certificate store, use:\n * net.tls.certificateSelector (or the command-line option --tlsCertificateSelector) instead of net.tls.certificateKeyFile (or the command-line option``--certificateKeyFile``).\n * net.ssl.certificateSelector (or the command-line option --sslCertificateSelector) instead of net.ssl.PEMKeyFile (or the command-line option``--sslPEMKeyFile``). When using the system SSL certificate store, OCSP (Online Certificate Status Protocol) is used to validate the revocation status of certificates. ←  TLS/SSL Configuration for ClientsConfigure MongoDB for FIPS → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/split-chunks-in-sharded-cluster/": " Docs Home → MongoDB Manual \nSPLIT CHUNKS IN A SHARDED CLUSTER \nBy default, MongoDB potentially splits a chunk only when migrating data belonging to it. However, you may want to split chunks manually if you have a large amount of data in your cluster and very few chunks, as is the case after deploying a cluster using existing data. To split chunks manually, use the split command with either fields middle or find. mongosh provides the helper methods sh.splitFind() and sh.splitAt(). splitFind() splits the chunk that contains the first document returned that matches this query into two equally sized chunks. You must specify the full namespace (i.e. \"<database>.<collection>\") of the sharded collection to splitFind(). The query in splitFind() does not need to use the shard key, though it nearly always makes sense to do so. \nEXAMPLE The following command splits the chunk that contains the value of 63109 for the zipcode field in the people collection of the records database: sh.splitFind( \"records.people\", { \"zipcode\": \"63109\" } )  Use splitAt() to split a chunk in two, using the queried document as the lower bound in the new chunk: \nEXAMPLE The following command splits the chunk that contains the value of 63109 for the zipcode field in the people collection of the records database. sh.splitAt( \"records.people\", { \"zipcode\": \"63109\" } )  \nNOTE splitAt() does not necessarily split the chunk into two equally sized chunks. The split occurs at the location of the document matching the query, regardless of where that document is in the chunk. \nTIP \nSEE ALSO: Empty Collection ←  Create Ranges in a Sharded ClusterMerge Chunks in a Sharded Cluster → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/install-mongodb-community-with-docker/": " Docs Home → MongoDB Manual \nINSTALL MONGODB COMMUNITY WITH DOCKER \nOn this page    \n * About This Task\n   \n * Before You Begin\n * Procedure You can run MongoDB community Edition as a Docker container using the official MongoDB Community image. Using a Docker image for running your MongoDB deployment is useful to:      * Stand up a deployment quickly.  * Help manage configuration files.  * Test different features on multiple versions of MongoDB. \nABOUT THIS TASK \nThis page describes the Docker install instructions for MongoDB Community edition. The MongoDB Enterprise Docker image in conjunction with the MongoDB Kubernetes Operator are recommended for production deployments. For enterprise instructions, see Install MongoDB Enterprise with Docker. This procedure uses the official mongo community image, which is maintained by MongoDB. A full description of Docker is beyond the scope of this documentation. This page assumes prior knowledge of Docker. \nBEFORE YOU BEGIN \nBefore you can run a MongoDB Community Docker container, you must install Docker . \nPROCEDURE \n1 \nPULL THE MONGODB DOCKER IMAGE \ndocker pull mongodb/mongodb-community-server \n2 \nRUN THE IMAGE AS A CONTAINER \ndocker run --name mongo -d mongodb/mongodb-community-server:latest  \nNOTE \nINSTALL A SPECIFIC VERSION OF MONGODB To install a specific version of MongoDB, specify the version after the : in the Docker run command. Docker pulls and runs the specified version. For example, to run MongoDB 5.0: docker run --name mongo -d mongodb/mongodb-community-server:5.0-ubi8  For a full list of available MongoDB Community Server images, see: the official Docker Hub page. 3 \nCHECK THAT THE CONTAINER IS RUNNING \nTo check the status of your Docker container, run the following command: docker container ls  The output from the ls command lists the following fields that describe the running container:  * Container ID  * Image  * Command  * Created  * Status  * Port  * Names CONTAINER ID   IMAGE                                       COMMAND                  CREATED         STATUS         PORTS       NAMESc29db5687290   mongodb/mongodb-community-server:5.0-ubi8   \"docker-entrypoint.s…\"   4 seconds ago   Up 3 seconds   27017/tcp   mongo 4 \nCONNECT TO THE MONGODB DEPLOYMENT WITH MONGOSH \nOpen an interactive container instance of mongo and connect to the deployment with mongosh. docker exec -it mongo mongosh \n5 \nVALIDATE YOUR DEPLOYMENT \nTo confirm your MongoDB instance is running, run the Hello command: db.runCommand(   {      hello: 1   })  The result of this command returns a document describing your mongod deployment: {   isWritablePrimary: true,   topologyVersion: {      processId: ObjectId(\"63c00e27195285e827d48908\"),      counter: Long(\"0\")},   maxBsonObjectSize: 16777216,   maxMessageSizeBytes: 48000000,   maxWriteBatchSize: 100000,   localTime: ISODate(\"2023-01-12T16:51:10.132Z\"),   logicalSessionTimeoutMinutes: 30,   connectionId: 18,   minWireVersion: 0,   maxWireVersion: 20,   readOnly: false,   ok: 1} ←  Install MongoDB Community on Windows using msiexec.exeInstall MongoDB Enterprise → On this page  * About This Task\n * Before You Begin\n * Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/build-a-2d-index/": " Docs Home → MongoDB Manual \nCREATE A 2D INDEX \nOn this page    \n * Define Location Range for a 2d Index\n   \n * Define Location Precision for a 2d Index To build a geospatial 2d index, use the db.collection.createIndex() method and specify 2d. Use the following syntax: db.<collection>.createIndex( { <location field> : \"2d\" ,                               <additional field> : <value> } ,                             { <index-specification options> } )  The 2d index uses the following optional index-specification options: { min : <lower bound> , max : <upper bound> ,  bits : <bit precision> }  \nDEFINE LOCATION RANGE FOR A 2D INDEX \nBy default, a 2d index assumes longitude and latitude and has boundaries of -180 inclusive and 180 non-inclusive. If documents contain coordinate data outside of the specified range, MongoDB returns an error. \nIMPORTANT The default boundaries allow applications to insert documents with invalid latitudes greater than 90 or less than -90. The behavior of geospatial queries with such invalid points is not defined. On 2d indexes you can change the location range. You can build a 2d geospatial index with a location range other than the default. Use the min and max options when creating the index. Use the following syntax: db.collection.createIndex( { <location field> : \"2d\" } ,                           { min : <lower bound> , max : <upper bound> } )  \nDEFINE LOCATION PRECISION FOR A 2D INDEX \nBy default, a 2d index on legacy coordinate pairs uses 26 bits of precision, which is roughly equivalent to 2 feet or 60 centimeters of precision using the default range of -180 to 180. Precision is measured by the size in bits of the geohash values used to store location data. You can configure geospatial indexes with up to 32 bits of precision. Index precision does not affect query accuracy. The actual grid coordinates are always used in the final query processing. Advantages to lower precision are a lower processing overhead for insert operations and use of less space. An advantage to higher precision is that queries scan smaller portions of the index to return results. To configure a location precision other than the default, use the bits option when creating the index. Use following syntax: db.<collection>.createIndex( {<location field> : \"<index type>\"} ,                             { bits : <bit precision> } )  For information on the internals of geohash values, see Calculation of Geohash Values for 2d Indexes. ←  2d IndexesQuery a 2d Index → On this page  * Define Location Range for a 2d Index\n * Define Location Precision for a 2d Index Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/upgrade-keyfile-to-x509/": " Docs Home → MongoDB Manual \nUPGRADE FROM KEYFILE AUTHENTICATION TO X.509 AUTHENTICATION \nOn this page    \n * Upgrade Procedures (Using tls Options)\n   \n * Upgrade Procedures (Using ssl Options) To upgrade clusters that are currently using keyfile authentication to x.509 authentication, use the following rolling upgrade processes. \nNOTE MongoDB disables support for TLS 1.0 encryption on systems where TLS 1.1+ is available. For more details, see Disable TLS 1.0. \nUPGRADE PROCEDURES (USING TLS OPTIONS)  NOTE Starting in version 4.2, MongoDB provides net.tls settings (and corresponding --tls command-line options) that corresponds to the net.ssl settings (and their corresponding --ssl command-line options). The new tls settings/options provide identical functionality as the ssl settings/options since MongoDB has always supported TLS 1.0 and later. The procedures in this section use the tls settings/options. For procedures using the ssl settings/options, see Upgrade Procedures (Using ssl Options). \nUPGRADE (USING TLS OPTIONS) CLUSTERS USING TLS/SSL AND KEYFILE  NOTE The procedure uses the tls settings/options. For procedures using the ssl settings/options, see Clusters Currently Using TLS/SSL (Using ssl Options). For clusters using TLS/SSL and keyfile authentication, to upgrade to x.509 cluster authentication, use the following rolling upgrade process:       1. For each member of a cluster, add the following settings to the configuration file and restart:\n    \n    * security.clusterAuthModeSet to sendKeyFile. With this setting value, each node continues to send its keyfile to authenticate itself as a member. However, each node can receive either a keyfile or an x.509 certificate from other members to authenticate those members.\n    * net.tls.clusterFileSet to the appropriate path of the node's certificate key file for membership authentication. The mongod / mongos presents this file to other members of the cluster to identify itself as a member.\n    \n    Include other TLS/SSL options and any other options as appropriate for your specific configuration.\n    \n    For example:\n    \n    net:   tls:      mode: requireTLS      certificateKeyFile: /etc/ssl/mongodb.pem      CAFile: /etc/ssl/caToValidateReceivedCertificates.pem      clusterFile: \"/etc/ssl/myReplMembershipCertificateKeyFile.pem\"security:   clusterAuthMode: sendKeyFile   keyFile: /my/securely/located/membershipkeyreplication:   replSetName: myReplicaSetnet:   bindIp: localhost,mongodb0.example.net   port: 27017systemLog:   destination: file   path: \"/var/log/mongodb/mongod.log\"   logAppend: truestorage:   dbPath: \"/var/lib/mongodb\"processManagement:   fork: true\n    \n    \n    \n    Update all nodes of the cluster to include the security.clusterAuthMode and net.tls.clusterFile settings before continuing.  2. Connect to each node and use the setParameter command to update the clusterAuthMode to sendX509. [1]\n    \n    db.adminCommand( { setParameter: 1, clusterAuthMode: \"sendX509\" } )\n    \n    \n    \n    With sendX509, each node sends its net.tls.clusterFile to authenticate itself as a member. However, each node can receive either a keyfile or an x.509 certificate from other members to authenticate those members.\n    \n    Upgrade all nodes of the cluster to this setting before continuing.\n\n  4. After the upgrade of all nodes, edit the configuration file with the appropriate x.509 settings to ensure that upon subsequent restarts, the cluster uses x.509 authentication. For example:\n    \n    net:   tls:      mode: requireTLS      certificateKeyFile: /etc/ssl/mongodb.pem      CAFile: /etc/ssl/caToValidateReceivedCertificates.pem      clusterFile: \"/etc/ssl/myReplMembershipCertificateKeyFile.pem\"security.clusterAuthMode: x509replication:   replSetName: myReplicaSetnet:   bindIp: localhost,mongodb0.example.net   port: 27017systemLog:   destination: file   path: \"/var/log/mongodb/mongod.log\"   logAppend: truestorage:   dbPath: \"/var/lib/mongodb\"processManagement:   fork: true\n    \n     \nTIP \nSEE ALSO: You can also configure mongod and mongos using command-line options instead of the configuration file:  * For mongod, see: - --tlsMode - --tlsCertificateKeyFile - --tlsCAFile - --tlsClusterFile - --clusterAuthMode  * For mongos, see: - --tlsMode - --tlsCertificateKeyFile - --tlsCAFile - --tlsClusterFile - --clusterAuthMode \nUPDATE (USING TLS OPTIONS) CLUSTERS USING KEYFILE BUT NOT TLS/SSL  NOTE The procedure uses the tls options. For procedures using the ssl settings/options, see Clusters Currently Not Using TLS/SSL (Using ssl Options). For clusters using keyfile authentication but not TLS/SSL, to upgrade to x.509 membership authentication and TLS/SSL connections:\n\n  2. Connect to each node and use the setParameter command [1] to:\n    \n    a. Update the tlsMode to preferSSL. With the tlsMode set to preferTLS, the node accepts both TLS/SSL and non-TLS/non-SSL incoming connections, and its outgoing connections use TLS/SSL.\n    \n    b. Update the clusterAuthMode to sendX509. With the clusterAuthMode set to sendX509, each node sends its net.tls.clusterFile to authenticate itself as a member. However, each node continues to accept either a keyfile or an x.509 certificate from other members to authenticate those members.\n    \n    db.adminCommand( { setParameter: 1, tlsMode: \"preferTLS\" } );db.adminCommand( { setParameter: 1, clusterAuthMode: \"sendX509\" } );\n    \n    \n    \n    Upgrade all nodes of the cluster to these settings before continuing.  3. After the upgrade of all nodes, edit the configuration file with the appropriate TLS/SSL and x.509 settings to ensure that upon subsequent restarts, the cluster uses x.509 authentication.\n    \n    net:   tls:      mode: preferTLS      certificateKeyFile: /etc/ssl/mongodb.pem      clusterFile: \"/etc/ssl/myReplMembershipCertificateKeyFile.pem\"      CAFile: /etc/ssl/caToValidateReceivedCertificates.pemsecurity:   clusterAuthMode: sendX509replication:   replSetName: myReplicaSetnet:   bindIp: localhost,mongodb0.example.net   port: 27017systemLog:   destination: file   path: \"/var/log/mongodb/mongod.log\"   logAppend: truestorage:   dbPath: \"/var/lib/mongodb\"processManagement:   fork: true\n    \n    \n    \n    \n    NOTE\n    \n    At this point,\n    \n     * The nodes in the cluster use TLS/SSL connections. However, the nodes can accept non-TLS/SSL connections from client applications.\n    \n     * The nodes send their x.509 certificates for membership authentication, but can accept either x.509 certificates or keyfiles from other nodes to verify the other nodes' membership.\n    \n    To enforce TLS/SSL connections from client applications as well as only accept x.509 for membership authentication, see next step.\n\n \nTIP \nSEE ALSO: You can also configure mongod and mongos using command-line options instead of the configuration file:  * For mongod, see: - --tlsMode - --tlsCertificateKeyFile - --tlsCAFile - --tlsClusterFile - --clusterAuthMode  * For mongos, see: - --tlsMode - --tlsCertificateKeyFile - --tlsCAFile - --tlsClusterFile - --clusterAuthMode \nUPGRADE PROCEDURES (USING SSL OPTIONS)  NOTE Starting in version 4.2, MongoDB provides net.tls settings (and corresponding command-line options) that corresponds to the net.ssl settings (and their corresponding command-line options). The tls settings/options provide identical functionality as the ssl settings/options since MongoDB has always supported TLS 1.0 and later. The procedures in this section use the ssl options. For procedures using the tls options, see Upgrade Procedures (Using tls Options). \nCLUSTERS CURRENTLY USING TLS/SSL (USING SSL OPTIONS)  NOTE The procedure uses the ssl options. For procedures using the tls options (available starting in MongoDB 4.2), see Clusters Currently Using TLS/SSL (Using ssl Options). For clusters using TLS/SSL and keyfile authentication, to upgrade to x.509 cluster authentication, use the following rolling upgrade process:  1. For each node of a cluster, start the node with the option --clusterAuthMode set to sendKeyFile and the option --sslClusterFile set to the appropriate path of the node's certificate. Include other TLS/SSL options as well as any other options that are required for your specific configuration. For example:\n    \n    mongod --replSet <name> --sslMode requireSSL --clusterAuthMode sendKeyFile --sslClusterFile <path to membership certificate and key PEM file> --sslPEMKeyFile <path to TLS/SSL Certificate and key PEM file>  --sslCAFile <path to root CA PEM file> --bind_ip localhost,<hostname(s)|ip address(es)>\n    \n    \n    \n    With this setting, each node continues to use its keyfile to authenticate itself as a member. However, each node can now accept either a keyfile or an x.509 certificate from other members to authenticate those members. Upgrade all nodes of the cluster to this setting.  2. Then, for each node of a cluster, connect to the node and use the setParameter command to update the clusterAuthMode to sendX509. [1] For example,\n    \n    db.adminCommand( { setParameter: 1, clusterAuthMode: \"sendX509\" } )\n    \n    \n    \n    With this setting, each node uses its x.509 certificate, specified with the --sslClusterFile option in the previous step, to authenticate itself as a member. However, each node continues to accept either a keyfile or an x.509 certificate from other members to authenticate those members. Upgrade all nodes of the cluster to this setting.  3. Optional but recommended. Finally, for each node of the cluster, connect to the node and use the setParameter command to update the clusterAuthMode to x509 to only use the x.509 certificate for authentication. [1] For example:\n    \n    db.adminCommand( { setParameter: 1, clusterAuthMode: \"x509\" } )\n    \n      4. After the upgrade of all nodes, edit the configuration file with the appropriate x.509 settings to ensure that upon subsequent restarts, the cluster uses x.509 authentication. See --clusterAuthMode for the various modes and their descriptions. \nCLUSTERS CURRENTLY NOT USING TLS/SSL (USING SSL OPTIONS)  NOTE The procedure uses the ssl options. For procedures using the tls options (available starting in MongoDB 4.2), see Update (Using tls Options) Clusters Using Keyfile but Not TLS/SSL. For clusters using keyfile authentication but not TLS/SSL, to upgrade to x.509 authentication, use the following rolling upgrade process:\n\n  2. Then, for each node of a cluster, connect to the node and use the setParameter command to update the sslMode to preferSSL and the clusterAuthMode to sendX509. [1] For example:\n    \n    db.adminCommand( { setParameter: 1, sslMode: \"preferSSL\", clusterAuthMode: \"sendX509\" } )\n    \n    \n    \n    With the sslMode set to preferSSL, the node accepts both TLS/SSL and non-TLS/non-SSL incoming connections, and its outgoing connections use TLS/SSL.\n    \n    With the clusterAuthMode set to sendX509, each node uses its x.509 certificate, specified with the --sslClusterFile option in the previous step, to authenticate itself as a member. However, each node continues to accept either a keyfile or an x.509 certificate from other members to authenticate those members.\n    \n    Upgrade all nodes of the cluster to these settings.  3. Optional but recommended. Finally, for each node of the cluster, connect to the node and use the setParameter command to update the sslMode to requireSSL and the clusterAuthMode to x509. [1] For example:\n    \n    db.adminCommand( { setParameter: 1, sslMode: \"requireSSL\", clusterAuthMode: \"x509\" } )\n    \n    \n    \n    With the sslMode set to requireSSL, the node only uses TLS/SSLs connections.\n    \n    With the clusterAuthMode set to x509, the node only uses the x.509 certificate for authentication.  4. After the upgrade of all nodes, edit the configuration file with the appropriate TLS/SSL and x.509 settings to ensure that upon subsequent restarts, the cluster uses x.509 authentication. See --clusterAuthMode for the various modes and their descriptions. [1](1, 2, 3, 4, 5, 6, 7) As an alternative to using the setParameter command, you can also restart the nodes with the appropriate TLS/SSL and x509 options and values. ←  Use x.509 Certificate for Membership AuthenticationRolling Update of x.509 Cluster Certificates that Contain New DN → On this page  * Upgrade Procedures (Using tls Options)\n * Upgrade Procedures (Using ssl Options) Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/sharding-high-availability-writes/": " Docs Home → MongoDB Manual \nDISTRIBUTED LOCAL WRITES FOR INSERT ONLY WORKLOADS \nOn this page    \n * Scenario\n   \n * Procedure MongoDB Tag Aware Sharding allows administrators to control data distribution in a sharded cluster by defining ranges of the shard key and tagging them to one or more shards. This tutorial uses Zones along with a multi-datacenter sharded cluster deployment and application-side logic to support distributed local writes, as well as high write availability in the event of a replica set election or datacenter failure. Changed in version 4.0.3: By defining the zones and the zone ranges before sharding an empty or a non-existing collection, the shard collection operation creates chunks for the defined zone ranges as well as any additional chunks to cover the entire range of the shard key values and performs an initial chunk distribution based on the zone ranges. This initial creation and distribution of chunks allows for faster setup of zoned sharding. After the initial distribution, the balancer manages the chunk distribution going forward.See Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection for an example. \nIMPORTANT The concepts discussed in this tutorial require a specific deployment architecture, as well as application-level logic. These concepts require familiarity with MongoDB sharded clusters, replica sets, and the general behavior of zones. This tutorial assumes an insert-only or insert-intensive workload. The concepts and strategies discussed in this tutorial are not well suited for use cases that require fast reads or updates. \nSCENARIO \nConsider an insert-intensive application, where reads are infrequent and low priority compared to writes. The application writes documents to a sharded collection, and requires near-constant uptime from the database to support its SLAs or SLOs. The following represents a partial view of the format of documents the application writes to the database: {   \"_id\" : ObjectId(\"56f08c447fe58b2e96f595fa\"),   \"message_id\" : 329620,   \"datacenter\" : \"alfa\",   \"userid\" : 123,   ...}{   \"_id\" : ObjectId(\"56f08c447fe58b2e96f595fb\"),   \"message_id\" : 578494,   \"datacenter\" : \"bravo\",   \"userid\" : 456,   ...}{   \"_id\" : ObjectId(\"56f08c447fe58b2e96f595fc\"),   \"message_id\" : 689979,   \"datacenter\" : \"bravo\",   \"userid\" : 789,   ...}  \nSHARD KEY \nThe collection uses the { datacenter : 1, userid : 1 } compound index as the shard key. The datacenter field in each document allows for creating a tag range on each distinct datacenter value. Without the datacenter field, it would not be possible to associate a document with a specific datacenter. The userid field provides a high cardinality and low frequency component to the shard key relative to datacenter. See Choosing a Shard Key for more general instructions on selecting a shard key. \nARCHITECTURE \nThe deployment consists of two datacenters, alfa and bravo. There are two shards, shard0000 and shard0001. Each shard is a replica set with three members. shard0000 has two members on alfa and one priority 0 member on bravo. shard0001 has two members on bravo and one priority 0 member on alfa.  \nTAGS \nThis application requires one tag per datacenter. Each shard has one tag assigned to it based on the datacenter containing the majority of its replica set members. There are two tag ranges, one for each datacenter. alfa Datacenter Tag shards with a majority of members on this datacenter as alfa. Create a tag range with:      * a lower bound of { \"datacenter\" : \"alfa\", \"userid\" : MinKey },  * an upper bound of { \"datacenter\" : \"alfa\", \"userid\" : MaxKey }, and  * the tag alfa bravo Datacenter Tag shards with a majority of members on this datacenter as bravo. Create a tag range with:  * a lower bound of { \"datacenter\" : \"bravo\", \"userid\" : MinKey },\n\n  * the tag bravo \nNOTE The MinKey and MaxKey values are reserved special values for comparisons Based on the configured tags and tag ranges, mongos routes documents with datacenter : alfa to the alfa datacenter, and documents with datacenter : bravo to the bravo datacenter. \nWRITE OPERATIONS \nIf an inserted or updated document matches a configured tag range, it can only be written to a shard with the related tag. MongoDB can write documents that do not match a configured tag range to any shard in the cluster. \nNOTE The behavior described above requires the cluster to be in a steady state with no chunks violating a configured tag range. See the following section on the balancer for more information. \nBALANCER \nThe balancer migrates the tagged chunks to the appropriate shard. Until the migration, shards may contain chunks that violate configured tag ranges and tags. Once balancing completes, shards should only contain chunks whose ranges do not violate its assigned tags and tag ranges. Adding or removing tags or tag ranges can result in chunk migrations. Depending on the size of your data set and the number of chunks a tag range affects, these migrations may impact cluster performance. Consider running your balancer during specific scheduled windows. See Schedule the Balancing Window for a tutorial on how to set a scheduling window. \nAPPLICATION BEHAVIOR \nBy default, the application writes to the nearest datacenter. If the local datacenter is down, or if writes to that datacenter are not acknowledged within a set time period, the application switches to the other available datacenter by changing the value of the datacenter field before attempting to write the document to the database. The application supports write timeouts. The application uses Write Concern to set a timeout for each write operation. If the application encounters a write or timeout error, it modifies the datacenter field in each document and performs the write. This routes the document to the other datacenter. If both datacenters are down, then writes cannot succeed. See Resolve Write Failure. The application periodically checks connectivity to any data centers marked as \"down\". If connectivity is restored, the application can continue performing normal write operations. Given the switching logic, as well as any load balancers or similar mechanisms in place to handle client traffic between datacenters, the application cannot predict which of the two datacenters a given document was written to. To ensure that no documents are missed as a part of read operations, the application must perform broadcast queries by not including the datacenter field as a part of any query. The application performs reads using a read preference of nearest to reduce latency. It is possible for a write operation to succeed despite a reported timeout error. The application responds to the error by attempting to re-write the document to the other datacenter - this can result in a document being duplicated across both datacenters. The application resolves duplicates as a part of the read logic. \nSWITCHING LOGIC \nThe application has logic to switch datacenters if one or more writes fail, or if writes are not acknowledged within a set time period. The application modifies the datacenter field based on the target datacenter's tag to direct the document towards that datacenter. For example, an application attempting to write to the alfa datacenter might follow this general procedure:  1. Attempt to write document, specifying datacenter : alfa.  2. On write timeout or error, log alfa as momentarily down.  3. Attempt to write same document, modifying datacenter : bravo.  4. On write timeout or error, log bravo as momentarily down.  5. If both alfa and bravo are down, log and report errors. See Resolve Write Failure. \nPROCEDURE  CONFIGURE SHARD TAGS \nYou must be connected to a mongos associated with the target sharded cluster in order to proceed. You cannot create tags by connecting directly to a shard replica set member. 1 TAG EACH SHARD. \nTag each shard in the alfa data center with the alfa tag. sh.addShardTag(\"shard0000\", \"alfa\")  Tag each shard in the bravo data center with the bravo tag. sh.addShardTag(\"shard0001\", \"bravo\")  You can review the tags assigned to any given shard by running sh.status(). 2 DEFINE RANGES FOR EACH TAG. \nDefine the range for the alfa database and associate it to the alfa tag using the sh.addTagRange() method. This method requires:  * The full namespace of the target collection.  * The inclusive lower bound of the range.  * The exclusive upper bound of the range.\n\n sh.addTagRange(   \"<database>.<collection>\",  { \"datacenter\" : \"alfa\", \"userid\" : MinKey },  { \"datacenter\" : \"alfa\", \"userid\" : MaxKey },   \"alfa\")  Define the range for the bravo database and associate it to the bravo tag using the sh.addTagRange() method. This method requires:  * The full namespace of the target collection.  * The inclusive lower bound of the range.  * The exclusive upper bound of the range.  * The name of the tag. sh.addTagRange(   \"<database>.<collection>\",  { \"datacenter\" : \"bravo\", \"userid\" : MinKey },  { \"datacenter\" : \"bravo\", \"userid\" : MaxKey },   \"bravo\")  The MinKey and MaxKey values are reserved special values for comparisons. MinKey always compares as less than every other possible value, while MaxKey always compares as greater than every other possible value. The configured ranges capture every user for each datacenter. 3 REVIEW THE CHANGES. \nThe next time the balancer runs, it migrates data across the shards respecting the configured zones. Once balancing finishes, the shards tagged as alfa should only contain documents with datacenter : alfa, while shards tagged as bravo should only contain documents with datacenter : bravo. You can review the chunk distribution by running sh.status(). \nRESOLVE WRITE FAILURE \nWhen the application's default datacenter is down or inaccessible, the application changes the datacenter field to the other datacenter. For example, the application attempts to write the following document to the alfa datacenter by default: {   \"_id\" : ObjectId(\"56f08c447fe58b2e96f595fa\"),   \"message_id\" : 329620,   \"datacenter\" : \"alfa\",   \"userid\" : 123,   ...}  If the application receives an error on attempted write, or if the write acknowledgement takes too long, the application logs the datacenter as unavailable and alters the datacenter field to point to the bravo datacenter. {   \"_id\" : ObjectId(\"56f08c457fe58b2e96f595fb\"),   \"message_id\" : 329620,   \"datacenter\" : \"bravo\",   \"userid\" : 123,   ...}  The application periodically checks the alfa datacenter for connectivity. If the datacenter is reachable again, the application can resume normal writes. \nNOTE It is possible that the original write to datacenter : alfa succeeded, especially if the error was related to a timeout. If so, the document with message_id : 329620 may now be duplicated across both datacenters. Applications must resolve duplicates as a part of read operations. \nRESOLVE DUPLICATE DOCUMENTS ON READS \nThe application's switching logic allows for potential document duplication. When performing reads, the application resolves any duplicate documents on the application layer. The following query searches for documents where the userid is 123. Note that while userid is part of the shard key, the query does not include the datacenter field, and therefore does not perform a targeted read operation. db.collection.find( { \"userid\" : 123 } )  The results show that the document with message_id of 329620 has been inserted into MongoDB twice, probably as a result of a delayed write acknowledgement. {  \"_id\" : ObjectId(\"56f08c447fe58b2e96f595fa\"),  \"message_id\" : 329620  \"datacenter\" : \"alfa\",  \"userid\" : 123,  data : {...}}{  \"_id\" : ObjectId(\"56f08c457fe58b2e96f595fb\"),  \"message_id\" : 329620  \"datacenter\" : \"bravo\",  \"userid\" : 123,  ...}  The application can either ignore the duplicates, taking one of the two documents, or it can attempt to trim the duplicates until only a single document remains. One method for trimming duplicates is to use the ObjectId.getTimestamp() method to extract the timestamp from the _id field. The application can then keep either the first document inserted, or the last document inserted. This assumes the _id field uses the MongoDB ObjectId().\n\n ISODate(\"2016-03-22T00:05:24Z\")  Using getTimestamp() on the document with ObjectId(\"56f08c457fe58b2e96f595fb\") returns: ISODate(\"2016-03-22T00:05:25Z\") \n←  Segmenting Data by Application or CustomerDistribute Collections Using Zones → On this page  * Scenario\n * Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/query-documents/": " Docs Home → MongoDB Manual \nQUERY DOCUMENTS \nOn this page    \n * Select All Documents in a Collection\n   \n * Specify Equality Condition\n * Specify Conditions Using Query Operators\n * Specify AND Conditions\n * Specify OR Conditions\n * Specify AND as well as OR Conditions\n * Additional Query Tutorials\n * Behavior ---------------------------------------- ➤ Use the Select your language drop-down menu in the upper-right to set the language of the following examples. ----------------------------------------    SELECT ALL DOCUMENTS IN A COLLECTION    This operation uses a filter predicate of {}, which corresponds to the following SQL statement: SELECT * FROM inventory   \nSPECIFY EQUALITY CONDITION   The following example selects from the inventory collection all documents where the status equals \"D\":  \nThis operation uses a filter predicate of { status: \"D\" }, which corresponds to the following SQL statement: SELECT * FROM inventory WHERE status = \"D\"   \nSPECIFY CONDITIONS USING QUERY OPERATORS   The following example retrieves all documents from the inventory collection where status equals either \"A\" or \"D\":   NOTE Although you can express this query using the $or operator, use the $in operator rather than the $or operator when performing equality checks on the same field. The operation uses a filter predicate of { status: { $in: [ \"A\", \"D\" ] } }, which corresponds to the following SQL statement: SELECT * FROM inventory WHERE status in (\"A\", \"D\")  Refer to the Query and Projection Operators document for the complete list of MongoDB query operators. \nSPECIFY AND CONDITIONS \nA compound query can specify conditions for more than one field in the collection's documents. Implicitly, a logical AND conjunction connects the clauses of a compound query so that the query selects the documents in the collection that match all the conditions. The following example retrieves all documents in the inventory collection where the status equals \"A\" and qty is less than ($lt) 30:  \nThe operation uses a filter predicate of { status: \"A\", qty: { $lt: 30 } }, which corresponds to the following SQL statement: SELECT * FROM inventory WHERE status = \"A\" AND qty < 30  See comparison operators for other MongoDB comparison operators. \nSPECIFY OR CONDITIONS \nUsing the $or operator, you can specify a compound query that joins each clause with a logical OR conjunction so that the query selects the documents in the collection that match at least one condition. The following example retrieves all documents in the collection where the status equals \"A\" or qty is less than ($lt) 30:  \nThe operation uses a filter predicate of { $or: [ { status: 'A' }, { qty: { $lt: 30 } } ] }, which corresponds to the following SQL statement: SELECT * FROM inventory WHERE status = \"A\" OR qty < 30  \nNOTE Queries that use comparison operators are subject to Type Bracketing. \nSPECIFY AND AS WELL AS OR CONDITIONS \nIn the following example, the compound query document selects all documents in the collection where the status equals \"A\" and either qty is less than ($lt) 30 or item starts with the character p:  \nThe operation uses a filter predicate of: {   status: 'A',   $or: [     { qty: { $lt: 30 } }, { item: { $regex: '^p' } }   ]}  which corresponds to the following SQL statement: SELECT * FROM inventory WHERE status = \"A\" AND ( qty < 30 OR item LIKE \"p%\")  \nNOTE MongoDB supports regular expressions $regex queries to perform string pattern matches. \nADDITIONAL QUERY TUTORIALS \nFor additional query examples, see:      * Query on Embedded/Nested Documents  * Query an Array  * Query an Array of Embedded Documents  * Project Fields to Return from Query  * Query for Null or Missing Fields \nBEHAVIOR  CURSOR   \nREAD ISOLATION \nFor reads to replica sets and replica set shards, read concern allows clients to choose a level of isolation for their reads. For more information, see Read Concern. \nQUERY RESULT FORMAT \nWhen you run a find operation with a MongoDB driver or mongosh, the command returns a cursor that manages query results. The query results are not returned as an array of documents. To learn how to iterate through documents in a cursor, refer to your driver's documentation. If you are using mongosh, see Iterate a Cursor in mongosh.  ←  Insert MethodsQuery on Embedded/Nested Documents → On this page\n\n Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/force-member-to-be-primary/": " Docs Home → MongoDB Manual \nFORCE A MEMBER TO BECOME PRIMARY \nOn this page    \n * Overview\n   \n * Consideration\n * Procedures \nOVERVIEW \nYou can force a replica set member to become primary by giving it a higher members[n].priority value than any other member in the set. Optionally, you also can force a member never to become primary by setting its members[n].priority value to 0, which means the member can never seek election as primary. For more information, see Priority 0 Replica Set Members. For more information on priorities, see members[n].priority. \nCONSIDERATION \nA majority of the configured members of a replica set must be available for a set to reconfigure a set or elect a primary. See Replica Set Elections for more information. \nPROCEDURES  NOTE Changed in version 4.0.2: If the parameter enableElectionHandoff is true (default), when a primary steps down from rs.stepDown() (or the replSetStepDown command without the force: true), the stepped-down primary nominates an eligible secondary to call an election immediately. Otherwise, secondaries can wait up to settings.electionTimeoutMillis before calling an election. The stepped down primary does not wait for the effects of the handoff. For more information, see enableElectionHandoff. \nFORCE A MEMBER TO BE PRIMARY BY SETTING ITS PRIORITY HIGH \nThis procedure assumes your current primary is m1.example.net and that you'd like to instead make m3.example.net primary. The procedure also assumes you have a three-member replica set with the configuration below. For more information on configurations, see Replica Set Configuration Use. This procedure assumes this configuration: {    \"_id\" : \"rs\",    \"version\" : 7,    \"members\" : [        {            \"_id\" : 0,            \"host\" : \"m1.example.net:27017\"        },        {            \"_id\" : 1,            \"host\" : \"m2.example.net:27017\"        },        {            \"_id\" : 2,            \"host\" : \"m3.example.net:27017\"        }    ]} \n      1. In a mongosh session that is connected to the primary, use the following sequence of operations to make m3.example.net the primary:\n    \n    cfg = rs.conf()cfg.members[0].priority = 0.5cfg.members[1].priority = 0.5cfg.members[2].priority = 1rs.reconfig(cfg)\n    \n    \n    \n    The last statement calls rs.reconfig() with the modified configuration document to configure m3.example.net to have a higher members[n].priority value than the other mongod instances.\n    \n    The following sequence of events occur:\n    \n    * m3.example.net and m2.example.net sync with m1.example.net (typically within 10 seconds).\n    \n    * m1.example.net sees that it no longer has highest priority and, in most cases, steps down. m1.example.net does not step down if m3.example.net's sync is far behind. In that case, m1.example.net waits until m3.example.net is within 10 seconds of its optime and then steps down. This minimizes the amount of time with no primary following failover.\n    \n    * The step down forces an election in which m3.example.net becomes primary based on its priority setting.\n\n \nFORCE A MEMBER TO BE PRIMARY USING DATABASE COMMANDS \nConsider a replica set with the following members:  * mdb0.example.net - the current primary.  * mdb1.example.net - a secondary.  * mdb2.example.net - a secondary . To force a member to become primary use the following procedure:  1. In mongosh, run rs.status() to ensure your replica set is running as expected.  2. In a mongosh session that is connected to the mongod instance running on mdb2.example.net, freeze mdb2.example.net so that it does not attempt to become primary for 120 seconds.\n    \n    rs.freeze(120)\n    \n      3. In a mongosh session that is connected to the mongod running on mdb0.example.net, step down this instance that the mongod is not eligible to become primary for 120 seconds:\n    \n    rs.stepDown(120)\n    \n    \n    \n    mdb1.example.net becomes primary.\n    \n    \n    NOTE\n    \n    During the transition, there is a short window where the set does not have a primary. For more information, consider the rs.freeze() and rs.stepDown() methods that wrap the replSetFreeze and replSetStepDown commands. ←  Perform Maintenance on Replica Set MembersResync a Member of a Replica Set → On this page  * Overview\n * Consideration\n * Procedures Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/model-data-for-schema-versioning/": " Docs Home → MongoDB Manual \nMODEL DATA FOR SCHEMA VERSIONING \nOn this page    \n * Overview\n   \n * Schema Versioning Pattern\n * Example\n * Use Cases \nOVERVIEW \nDatabase schemas occasionally need to be updated. For example, a schema designed to hold user contact information may need to be updated to include new methods of communication as they become popular, such as Twitter or Skype. You can use MongoDB's flexible schema model, which supports differently shaped documents in the same collection, to gradually update your collection's schema. As you update your schema model, the Schema Versioning pattern allows you to track these updates with version numbers. Your application code can use version numbers to identify and handle differently shaped documents without downtime. \nSCHEMA VERSIONING PATTERN \nTo implement the Schema Versioning pattern, add a schema_version (or similarly named) field to your schema the first time that you modify your schema. Documents that use the new schema should have a schema_version of 2 to indicate that they adhere to the second iteration of your schema. If you update your schema again, increment the schema_version. Your application code can use a document's schema_version, or lack thereof, to conditionally handle documents. Use the latest schema to store new information in the database. \nEXAMPLE \nThe following example iterates upon the schema for documents in the users collection. In the first iteration of this schema, a record includes galactic_id, name, and phone fields: // users collection\n{    \"_id\": \"<ObjectId>\",    \"galactic_id\": 123,    \"name\": \"Anakin Skywalker\",    \"phone\": \"503-555-0000\",}  In the next iteration, the schema is updated to include more information in a different shape: // users collection\n{    \"_id\": \"<ObjectId>\",    \"galactic_id\": 123,    \"name\": \"Darth Vader\",    \"contact_method\": {        \"work\": \"503-555-0210\",        \"home\": \"503-555-0220\",        \"twitter\": \"@realdarthvader\",        \"skype\": \"AlwaysWithYou\"    },    \"schema_version\": \"2\"}  Adding a schema_version means that an application can identify documents shaped for the new schema and handle them accordingly. The application can still handle old documents if schema_version does not exist on the document. For example, consider an application that finds a user's phone number(s) by galactic_id. Upon being given a galactic_id, the application needs to query the database: db.users.find( { galactic_id: 123 } );  After the document is returned from the database, the application checks to see whether the document has a schema_version field.      * If it does not have a schema_version field, the application passes the returned document to a dedicated function that renders the phone field from the original schema.  * If it does have a schema_version field, the application checks the schema version. In this example, the schema_version is 2 and the application passes the returned document to a dedicated function that renders the new contact_method.work and contact_method.home fields. Using the schema_version field, application code can support any number of schema iterations in the same collection by adding dedicated handler functions to the code. \nUSE CASES \nThe Schema Versioning pattern is ideal for any one or a combination of the following cases:  * Application downtime is not an option  * Updating documents may take hours, days, or weeks of time to complete  * Updating documents to the new schema version is not a requirement The Schema Versioning pattern helps you better decide when and how data migrations will take place relative to traditional, tabular databases. ←  Model Data to Support Keyword SearchModel Monetary Data → On this page  * Overview\n * Schema Versioning Pattern\n * Example\n * Use Cases Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/deploy-shard-cluster/": " Docs Home → MongoDB Manual \nDEPLOY A SHARDED CLUSTER \nOn this page    \n * Overview\n   \n * Considerations\n * Procedure \nOVERVIEW \nThis tutorial involves creating a new sharded cluster that consists of a mongos, the config server replica set, and two shard replica sets. \nCONSIDERATIONS  CONNECTIVITY \nEach member of a sharded cluster must be able to connect to all other members in the cluster. This includes all shards and config servers. Ensure that network and security systems, including all interface and firewalls, allow these connections. \nHOSTNAMES AND CONFIGURATION  IMPORTANT To avoid configuration updates due to IP address changes, use DNS hostnames instead of IP addresses. It is particularly important to use a DNS hostname instead of an IP address when configuring replica set members or sharded cluster members. Use hostnames instead of IP addresses to configure clusters across a split network horizon. Starting in MongoDB 5.0, nodes that are only configured with an IP address will fail startup validation and will not start. LOCALHOST DEPLOYMENTS \nIf you use either localhost or its IP address as the hostname portion of any host identifier, you must use that identifier as the host setting for any other MongoDB component in the cluster. For example, the sh.addShard() method takes a host parameter for the hostname of the target shard. If you set host to localhost, you must then use localhost as the host for all other shards in the cluster. \nSECURITY \nThis tutorial does not include the required steps for configuring Internal/Membership Authentication or Role-Based Access Control. In production environments, sharded clusters should employ at minimum x.509 security for internal authentication and client access. \nPROCEDURE  CREATE THE CONFIG SERVER REPLICA SET \nThe following steps deploys a config server replica set. For a production deployment, deploy a config server replica set with at least three members. For testing purposes, you can create a single-member replica set. \nNOTE The config server replica set must not use the same name as any of the shard replica sets. For this tutorial, the config server replica set members are associated with the following hosts: Config Server Replica Set Member\nHostname\nMember 0\ncfg1.example.net\nMember 1\ncfg2.example.net\nMember 2\ncfg3.example.net 1 START EACH MEMBER OF THE CONFIG SERVER REPLICA SET. \nWhen starting each mongod, specify the mongod settings either via a configuration file or the command line.  2 CONNECT TO ONE OF THE CONFIG SERVERS. \nConnect mongosh to one of the config server members. mongosh --host <hostname> --port <port> \n3 INITIATE THE REPLICA SET. \nFrom mongosh, run the rs.initiate() method. rs.initiate() can take an optional replica set configuration document. In the replica set configuration document, include:      * The _id set to the replica set name specified in either the replication.replSetName or the --replSet option.  * The configsvr field set to true for the config server replica set.  * The members array with a document per each member of the replica set. \nIMPORTANT Run rs.initiate() on just one and only one mongod instance for the replica set. rs.initiate(  {    _id: \"myReplSet\",    configsvr: true,    members: [      { _id : 0, host : \"cfg1.example.net:27019\" },      { _id : 1, host : \"cfg2.example.net:27019\" },      { _id : 2, host : \"cfg3.example.net:27019\" }    ]  })  See Replica Set Configuration for more information on replica set configuration documents. Once the config server replica set (CSRS) is initiated and up, proceed to creating the shard replica sets. \nCREATE THE SHARD REPLICA SETS \nFor a production deployment, use a replica set with at least three members. For testing purposes, you can create a single-member replica set. \nNOTE Shard replica sets must not use the same name as the config server replica set. For each shard, use the following steps to create the shard replica set: 1 START EACH MEMBER OF THE SHARD REPLICA SET. \nWhen starting each mongod, specify the mongod settings either via a configuration file or the command line.  2\n\n \nConnect mongosh to one of the replica set members. mongosh --host <hostname> --port <port> \n3 INITIATE THE REPLICA SET. \nFrom mongosh, run the rs.initiate() method. rs.initiate() can take an optional replica set configuration document. In the replica set configuration document, include:  * The _id field set to the replica set name specified in either the replication.replSetName or the --replSet option.  * The members array with a document per each member of the replica set. The following example initiates a three member replica set. \nIMPORTANT Run rs.initiate() on just one and only one mongod instance for the replica set. rs.initiate(  {    _id : \"myReplSet\",    members: [      { _id : 0, host : \"s1-mongo1.example.net:27018\" },      { _id : 1, host : \"s1-mongo2.example.net:27018\" },      { _id : 2, host : \"s1-mongo3.example.net:27018\" }    ]  })  \nSTART A MONGOS FOR THE SHARDED CLUSTER \nStart a mongos using either a configuration file or a command line parameter to specify the config servers.  \nAt this point, your sharded cluster consists of the mongos and the config servers. You can now connect to the sharded cluster using mongosh. \nCONNECT TO THE SHARDED CLUSTER \nConnect mongosh to the mongos. Specify the host and port on which the mongos is running: mongosh --host <hostname> --port <port>  Once you have connected mongosh to the mongos, continue to the next procedure to add shards to the cluster. \nADD SHARDS TO THE CLUSTER \nIn a mongosh session that is connected to the mongos, use the sh.addShard() method to add each shard to the cluster. The following operation adds a single shard replica set to the cluster: sh.addShard( \"<replSetName>/s1-mongo1.example.net:27018,s1-mongo2.example.net:27018,s1-mongo3.example.net:27018\")  Repeat these steps until the cluster includes all desired shards. \nSHARD A COLLECTION \nTo shard a collection, connect mongosh to the mongos and use the sh.shardCollection() method. \nNOTE \nSHARDING AND INDEXES If the collection already contains data, you must create an index that supports the shard key before sharding the collection. If the collection is empty, MongoDB creates the index as part of sh.shardCollection(). MongoDB provides two strategies to shard collections:  * Hashed sharding uses a hashed index of a single field as the shard key to partition data across your sharded cluster.\n   \n   sh.shardCollection(\"<database>.<collection>\", { <shard key field> : \"hashed\" } )\n   \n     * Range-based sharding can use multiple fields as the shard key and divides data into contiguous ranges determined by the shard key values.\n   \n   sh.shardCollection(\"<database>.<collection>\", { <shard key field> : 1, ... } )\n   \n    \nSHARD KEY CONSIDERATIONS \nYour selection of shard key affects the efficiency of sharding, as well as your ability to take advantage of certain sharding features such as zones. To learn how to choose an effective shard key, see Choose a Shard Key. Starting in version 4.0, mongosh provides the method convertShardKeyToHashed(). This method uses the same hashing function as the hashed index and can be used to see what the hashed value would be for a key. \nTIP \nSEE ALSO:  * For hashed sharding shard keys, see Hashed Sharding Shard Key  * For ranged sharding shard keys, see Shard Key Selection ←  Ranged ShardingZones → On this page  * Overview\n * Considerations\n * Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/faq/indexes/": " Docs Home → MongoDB Manual \nFAQ: INDEXES \nOn this page    \n * How do I create an index?\n   \n * How does an index build affect database performance?\n * How do I see what indexes exist on a collection?\n * How can I see if a query uses an index?\n * How do I determine which fields to index?\n * How can I see the size of an index?\n * How do write operations affect indexes? This document addresses some common questions regarding MongoDB indexes. For more information on indexes, see Indexes. \nHOW DO I CREATE AN INDEX? \nTo create an index on a collection, use the db.collection.createIndex() method. Creating an index is an administrative operation. In general, applications should not call db.collection.createIndex() on a regular basis. \nNOTE Index builds can impact performance; see How does an index build affect database performance?. Administrators should consider the performance implications before building indexes. \nHOW DOES AN INDEX BUILD AFFECT DATABASE PERFORMANCE? \nMongoDB index builds against a populated collection require an exclusive read-write lock against the collection. Operations that require a read or write lock on the collection must wait until the mongod releases the lock. Changed in version 4.2.      * For feature compatibility version (fcv) \"4.2\", MongoDB uses an optimized build process that only holds the exclusive lock at the beginning and end of the index build. The rest of the build process yields to interleaving read and write operations.  * For feature compatibility version (fcv) \"4.0\", the default foreground index build process holds the exclusive lock for the entire index build. background index builds do not take an exclusive lock during the build process. For more information on the index build process, see Index Builds on Populated Collections. Index builds on replica sets have specific performance considerations and risks. See Index Builds in Replicated Environments for more information. To minimize the impact of building an index on replica sets, including shard replica sets, use a rolling index build procedure as described in Rolling Index Builds on Replica Sets. To return information on currently running index creation operations, see Active Indexing Operations. To kill a running index creation operation on a primary or standalone mongod, use db.killOp(). The partially built index will be deleted. You cannot terminate a replicated index build on secondary members of a replica set. You must first drop the index on the primary. The secondaries will replicate the drop operation and drop the indexes after the index build completes. All further replication blocks behind the index build and drop. \nHOW DO I SEE WHAT INDEXES EXIST ON A COLLECTION? \nTo list a collection's indexes, use the db.collection.getIndexes() method. \nHOW CAN I SEE IF A QUERY USES AN INDEX? \nTo inspect how MongoDB processes a query, use the explain() method. \nHOW DO I DETERMINE WHICH FIELDS TO INDEX? \nA number of factors determine which fields to index, including selectivity, the support for multiple query shapes, and size of the index. For more information, see Operational Considerations for Indexes and Indexing Strategies. \nHOW CAN I SEE THE SIZE OF AN INDEX? \nThe db.collection.stats() includes an indexSizes document which provides size information for each index on the collection. Depending on its size, an index may not fit into RAM. An index fits into RAM when your server has enough RAM available for both the index and the rest of the working set. When an index is too large to fit into RAM, MongoDB must read the index from disk, which is a much slower operation than reading from RAM. In certain cases, an index does not need to fit entirely into RAM. For details, see Indexes that Hold Only Recent Values in RAM. \nHOW DO WRITE OPERATIONS AFFECT INDEXES? \nWrite operations may require updates to indexes:  * If a write operation modifies an indexed field, MongoDB updates all indexes that have the modified field as a key. Therefore, if your application is write-heavy, indexes might affect performance. ←  FAQ: MongoDB FundamentalsFAQ: Concurrency → On this page  * How do I create an index?\n * How does an index build affect database performance?\n * How do I see what indexes exist on a collection?\n * How can I see if a query uses an index?\n * How do I determine which fields to index?\n * How can I see the size of an index?\n * How do write operations affect indexes? Share Feedback\n",
  "https://www.mongodb.com/docs/manual/tutorial/manage-indexes/": " Docs Home → MongoDB Manual \nMANAGE INDEXES \nOn this page    \n * View Existing Indexes\n   \n * Remove Indexes\n * Modify an Index\n * Find Inconsistent Indexes Across Shards This page shows how to manage existing indexes. For instructions on creating indexes, refer to the specific index type pages. \nVIEW EXISTING INDEXES   \nREMOVE INDEXES  TIP \nHIDE AN INDEX BEFORE DROPPING IT If you drop an index that is actively used in production, your application may incur a performance degradation. Before you drop an index, you can evaluate the potential impact of the drop by hiding the index. Hidden indexes are not used to support queries. If you hide an index and observe substantial negative performance impact, consider keeping and unhiding the index so queries can resume using it.   MODIFY AN INDEX   \nMINIMIZE PERFORMANCE IMPACT WITH A TEMPORARY INDEX \nIf you drop an index that is actively used in production, your application may incur a performance degradation. To ensure queries can still use an index during modification, you can create a temporary, redundant index that contains the same fields as the modified index. EXAMPLE \nThis example creates a new index and modifies that index to make it unique. 1 CREATE A SITEANALYTICS COLLECTION WITH AN INDEX ON THE URL FIELD \nRun this command: db.siteAnalytics.createIndex( { \"url\": 1 } )  The command returns the name of the index: url_1 2 CREATE A TEMPORARY INDEX THAT CONTAINS THE URL FIELD \nRun this command: db.siteAnalytics.createIndex( { \"url\": 1, \"dummyField\": 1 } )  The command returns the name of the index: url_1_dummyField_1 This temporary index lets you safely drop the original { \"url\": 1 } index without impacting performance. 3 DROP THE ORIGINAL INDEX \nRun this command: db.siteAnalytics.dropIndex( { \"url\": 1 } )  The command returns: { nIndexesWas: 3, ok: 1 } 4 RECREATE THE { \"URL\": 1 } INDEX WITH THE UNIQUE PROPERTY \nRun this command: db.siteAnalytics.createIndex( { \"url\": 1 }, { \"unique\": true } )  The command returns the name of the index: url_1 The url_1 index is recreated and you can drop the temporary index without impacting performance. Queries on the url field can use the new unique index. 5 DROP THE TEMPORARY INDEX \nRun this command: db.siteAnalytics.dropIndex( { \"url\": 1, \"dummyField\": 1 } )  The command returns: { nIndexesWas: 3, ok: 1 } 6 CONFIRM THAT THE INDEX WAS UPDATED \nTo view the indexes on the siteAnalytics collection, run this command: db.siteAnalytics.getIndexes()  The command returns these indexes, indicating that the url_1 index is now unique: [  { v: 2, key: { _id: 1 }, name: '_id_' },  { v: 2, key: { url: 1 }, name: 'url_1', unique: true }] \nFIND INCONSISTENT INDEXES ACROSS SHARDS \nA sharded collection has an inconsistent index if the collection does not have the exact same indexes (including the index options) on each shard that contains chunks for the collection. Although inconsistent indexes should not occur during normal operations, inconsistent indexes can occur , such as:      * When a user is creating an index with a unique key constraint and one shard contains a chunk with duplicate documents. In such cases, the create index operation may succeed on the shards without duplicates but not on the shard with duplicates.  * When a user is creating an index across the shards in a rolling manner (i.e. manually building the index one by one across the shards) but either fails to build the index for an associated shard or incorrectly builds an index with different specification. Starting in MongoDB 4.4 (and 4.2.6), the config server primary, by default, checks for index inconsistencies across the shards for sharded collections, and the command serverStatus, when run on the config server primary, returns the field shardedIndexConsistency field to report on the number of sharded collections with index inconsistencies. If shardedIndexConsistency reports any index inconsistencies, you can run the following pipeline for your sharded collections until you find the inconsistencies. \nNOTE The following pipeline is for MongoDB 4.2.4 and above.\n\n  2. Run the aggregation pipeline for the sharded collection to test. For example, to test if the sharded collection test.reviews has inconsistent indexes across its associated shards:\n    \n    db.getSiblingDB(\"test\").reviews.aggregate(pipeline)\n    \n    \n    \n    If the collection has inconsistent indexes, the aggregation for that collection returns details regarding the inconsistent indexes:\n    \n    { \"missingFromShards\" : [ \"shardB\" ], \"inconsistentProperties\" : [ ], \"indexName\" : \"page_1_score_1\" }{ \"missingFromShards\" : [ ], \"inconsistentProperties\" : [ { \"k\" : \"expireAfterSeconds\", \"v\" : 60 }, { \"k\" : \"expireAfterSeconds\", \"v\" : 600 } ], \"indexName\" : \"reviewDt_1\" }\n    \n    \n    \n    The returned documents indicate two inconsistencies for the sharded collection test.reviews:\n    \n    1. An index named page_1_score_1 is missing from the collection on shardB.\n    \n    2. An index named reviewDt_1 has inconsistent properties across the collection's shards, specifically, the expireAfterSeconds properties differ. To resolve the inconsistency where an index is missing from the collection on a particular shard(s), You can either:  * Perform a rolling index build for the collection on the affected shard(s).\n   \n   -OR-  * Issue an index build db.collection.createIndex() from a mongos instance. The operation only builds the collection's index on the shard(s) missing the index. To resolve where the index properties differ across the shards, Drop the incorrect index from the collection on the affected shard(s) and rebuild the index. To rebuild the index, you can either:  * Perform a rolling index build for the collection on the affected shard.\n   \n   -OR-  * Issue an index build db.collection.createIndex() from a mongos instance. The operation only builds the collection's index on the shard(s) missing the index. Alternatively, if the inconsistency is the expireAfterSeconds property, you can run the collMod command to update the number of seconds instead of dropping and rebuilding the index. ←  Index IntersectionMeasure Index Use → On this page  * View Existing Indexes\n * Remove Indexes\n * Modify an Index\n * Find Inconsistent Indexes Across Shards Share Feedback\n",
  "https://www.mongodb.com/docs/manual/includes/featureCompatibility-caveat/": " Docs Home → MongoDB Manual Enabling these backwards-incompatible features can complicate the downgrade process since you must remove any persisted backwards-incompatible features before you downgrade. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.4-downgrade-replica-set/": " Docs Home → MongoDB Manual \nDOWNGRADE 3.4 REPLICA SET TO 3.2 \nOn this page    \n * Downgrade Path\n   \n * Create Backup\n * Prerequisites\n * Procedure Before you attempt any downgrade, familiarize yourself with the content of this document. \nDOWNGRADE PATH \nOnce upgraded to 3.4, you cannot downgrade to a 3.2.7 or earlier version. You can only downgrade to a 3.2.8 or later version. \nCREATE BACKUP \nOptional but Recommended. Create a backup of your database. \nPREREQUISITES \nBefore downgrading the binaries, you must downgrade the feature compatibility version and remove any 3.4 features incompatible with 3.2 or earlier versions as generally outlined below. These steps are necessary only if featureCompatibilityVersion has ever been set to \"3.4\". \n1. DOWNGRADE FEATURE COMPATIBILITY VERSION \n      1. Connect a mongo shell to the primary.  2. Downgrade the featureCompatibilityVersion to \"3.2\".\n    \n    db.adminCommand({setFeatureCompatibilityVersion: \"3.2\"})\n    \n    \n    \n    This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command on the primary as the operation is idempotent. \n2. REMOVE VIEWS \nIf you have defined any views, drop the views before downgrading MongoDB 3.4 to 3.2.  1. Connect a mongo shell to the primary.  2. To find views, you can run the following in the mongo shell:\n    \n    db.adminCommand(\"listDatabases\").databases.forEach(function(d){   let mdb = db.getSiblingDB(d.name);   mdb.getCollectionInfos({type: \"view\"}).forEach(function(c){      print(mdb[c.name]);   });});\n    \n    \n    \n    In each database that contains views, drop the system.views collection to drop all views in that database.\n    \n    If running with access control, you must have privileges to drop the system.views collection for the database. See Create a Role to Drop system.views Collection across Databases. \n3. REMOVE COLLATION OPTION FROM COLLECTIONS AND INDEXES \nIf you have defined any non-\"simple\" collation for a collection or an index, remove the collection or index before downgrading MongoDB 3.4 to 3.2.  1. Connect a mongo shell to the primary.  2. To find collections with collation specifications, you can run the following in the mongo shell:\n    \n    db.adminCommand(\"listDatabases\").databases.forEach(function(d){   let mdb = db.getSiblingDB(d.name);   mdb.getCollectionInfos( { \"options.collation\": { $exists: true } } ).forEach(function(c){      print(mdb[c.name]);   });});\n    \n    \n    \n    You can migrate the content of the collection to a new collection without the collation specification (one way is via the aggregation pipeline stage $out).  3. To find indexes with collation specification, you can run the following in the mongo shell:\n    \n    db.adminCommand(\"listDatabases\").databases.forEach(function(d){   let mdb = db.getSiblingDB(d.name);   mdb.getCollectionInfos().forEach(function(c){      let currentCollection = mdb.getCollection(c.name);      currentCollection.getIndexes().forEach(function(i){         if (i.collation){            printjson(i);         }      });   });});\n    \n    \n    \n    Drop the indexes with a collation specification. After the downgrade, recreate the dropped indexes.\n\n \n 1. Connect a mongo shell to the primary.  2. Convert any data of decimal type. In versions of MongoDB earlier than 3.4, operations against documents that contain decimal type may fail. For some possible conversion options, see Model Monetary Data.\n    \n    To detect the presence of decimal, you can run db.collection.validate(true) against the collections which may contain decimal data.\n    \n    db.collection.validate(true) reports on decimal data only when featureCompatibilityVersion is \"3.2\". \n5. DOWNGRADE INDEX VERSIONS \nIf you have v: 2 indexes (i.e. the default version for indexes created in MongoDB 3.4 if featureCompatibilityVersion: \"3.4\"), reindex the collection to recreate all indexes on the collection as v: 1 before downgrading MongoDB.  1. Connect a mongo shell to the primary.  2. To find indexes with v: 2, you can run the following in the mongo shell:\n    \n    db.adminCommand(\"listDatabases\").databases.forEach(function(d){   let mdb = db.getSiblingDB(d.name);   mdb.getCollectionInfos().forEach(function(c){      let currentCollection = mdb.getCollection(c.name);      currentCollection.getIndexes().forEach(function(i){         if (i.v === 2){            printjson(i);         }      });   });});\n    \n     Repeat the process on secondary members of the replica set as the reindex operation does not propagate to the secondaries. \nTIP If connecting a mongo shell to a secondary member, use db.getMongo() to allow reads from secondaries. \nPROCEDURE \n1 \nDOWNLOAD THE LATEST 3.2 BINARIES. \nUsing either a package manager or a manual download, get the latest release in the 3.2 series. If using a package manager, add a new repository for the 3.2 binaries, then perform the actual downgrade process. Once upgraded to 3.4, you cannot downgrade to a 3.2.7 or earlier version. You can only downgrade to a 3.2.8 or later version. 2 \nDOWNGRADE SECONDARY MEMBERS OF THE REPLICA SET. \nDowngrade each secondary member of the replica set, one at a time:  a. Shut down the mongod. See Stop mongod Processes for instructions on safely terminating mongod processes.  b. Replace the 3.4 binary with the 3.2 binary and restart.  c. Wait for the member to recover to SECONDARY state before downgrading the next secondary. To check the member's state, use the rs.status() method in the mongo shell. 3 \nSTEP DOWN THE PRIMARY. \nUse rs.stepDown() in the mongo shell to step down the primary and force the normal failover procedure. rs.stepDown()  rs.stepDown() expedites the failover procedure and is preferable to shutting down the primary directly. 4 \nREPLACE AND RESTART FORMER PRIMARY MONGOD. \nWhen rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state, shut down the previous primary and replace the mongod binary with the 3.2 binary and start the new instance. ←  Downgrade 3.4 Standalone to 3.2Downgrade 3.4 Sharded Cluster to 3.2 → On this page  * Downgrade Path\n * Create Backup\n * Prerequisites\n * Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.6-compatibility/": " Docs Home → MongoDB Manual \nCOMPATIBILITY CHANGES IN MONGODB 3.6 \nOn this page    \n * Localhost Binding Compatibility Changes\n   \n * Shard Replica Set\n * HTTP Interface and REST API\n * Tools Changes\n * Array Operation Compatibility Changes\n * Update Operation Changes\n * Platform Support\n * General Compatibility Changes\n * Backwards Incompatible Features The following 3.6 changes can affect the compatibility with older versions of MongoDB. \nLOCALHOST BINDING COMPATIBILITY CHANGES  WARNING Before binding to a non-localhost (e.g. publicly accessible) IP address, ensure you have secured your cluster from unauthorized access. For a complete list of security recommendations, see Security Checklist. At minimum, consider enabling authentication and hardening network infrastructure. MongoDB binaries, mongod and mongos, bind to localhost by default. If the net.ipv6 configuration file setting or the --ipv6 command line option is set for the binary, the binary additionally binds to the localhost IPv6 address. By default mongod and mongos that are bound to localhost only accept connections from clients that are running on the same computer. This binding behavior includes mongosh and other members of your replica set or sharded cluster. Remote clients cannot connect to binaries that are bound only to localhost. To override the default binding and bind to other IP addresses, use the net.bindIp configuration file setting or the --bind_ip command-line option to specify a list of hostnames or IP addresses. \nWARNING Starting in MongDB 5.0, split horizon DNS nodes that are only configured with an IP address fail startup validation and report an error. See disableSplitHorizonIPCheck. For example, the following mongod instance binds to both the localhost and the hostname My-Example-Associated-Hostname, which is associated with the IP address 198.51.100.1: mongod --bind_ip localhost,My-Example-Associated-Hostname  In order to connect to this instance, remote clients must specify the hostname or its associated IP address 198.51.100.1: mongosh --host My-Example-Associated-Hostname\nmongosh --host 198.51.100.1  To bind to all IPv4 addresses, you can specify the bind ip address of 0.0.0.0. To bind to all IPv4 and IPv6 addresses, you can specify the bind ip address of ::,0.0.0.0 or alternatively, use the new net.bindIpAll setting or the new command-line option --bind_ip_all. \nSHARD REPLICA SET \nStarting in 3.6, shards must be replica sets. To upgrade your sharded cluster to version 3.6, the shard servers must be running as a replica set. To convert an existing shard standalone instance to a shard replica set, see Convert a Shard Standalone to a Shard Replica Set. \nHTTP INTERFACE AND REST API \nMongoDB 3.6 removes the deprecated HTTP interface and REST API to MongoDB. Configuration Settings\nmongod/mongos option\nnet.http.enabled\nnet.http.JSONPEnabled\nnet.http.port\nnet.http.RESTInterfaceEnabled\nhttpinterface\nnohttpinterface\njsonp\nrest \nTOOLS CHANGES \nMongoDB 3.6 removes the deprecated mongooplog tool. \nARRAY OPERATION COMPATIBILITY CHANGES  $TYPE: \"ARRAY\" BEHAVIOR CHANGE \nStarting in 3.6, $type: \"array\" and $type: 4 expressions match array fields that contain any element type. In earlier versions, $type : \"array\" only matches array fields that contain nested arrays. For example, a collection named c contains two documents: { \"_id\": 1, \"a\": [ 1, 2, 3 ] },{ \"_id\": 2, \"a\": [ 1, 2, [ 3, 4 ] ] }  The following operation queries by type on field a: db.c.find( { \"a\": { $type : \"array\" } } )  Starting in 3.6, the query returns both documents in the collection because the $type query can now detect that field a is itself an array. { \"_id\": 1, \"a\": [ 1, 2, 3 ] },{ \"_id\": 2, \"a\": [ 1, 2, [ 3, 4 ] ] }  In 3.4 and earlier versions of MongoDB, the query only returns those documents in which the array field a contains an element of BSON type array.\n\n  If upgrading from a MongoDB 3.4.x deployment that has partial indexes whose partialFilterExpression includes a $type : \"array\" or $type : 4 expression, you must rebuild these indexes after upgrading to avoid conflicting $type : 'array' semantics. For more information on the $type: \"array\" expression, see Querying by Array Type. \nARRAY SORT BEHAVIOR \nStarting in 3.6, when sorting a field containing an array, MongoDB orders the field with the lowest-valued element of the array first for ascending sorts and the highest-valued element of the array first for descending sorts. A sort no longer takes the query predicate into account when choosing the array element which will act as the sort key. This behavior change applies to both the find command and the aggregation pipeline. As a result of this change, applications that currently sort by an array field may experience a different sort order. \nIMPORTANT As a result of changes to sorting behavior on array fields in MongoDB 4.4, when you sort on an array indexed with a multikey index, the query plan includes a blocking sort stage, unless:      * The index boundaries for all sort fields are [MinKey, MaxKey], and  * No boundaries for any multikey-indexed field have the same path prefix as the sort pattern. FIND METHOD SORTING \nA sort key is the array element MongoDB uses during the sorting process to compare and ultimately order documents containing an array. In an ascending sort, documents containing arrays with the lowest-valued sort keys are ordered first. Likewise, in a descending sort, documents containing arrays with the highest-valued sort keys are ordered first. In MongoDB 3.4 and earlier, a sort by an array field took into account the query predicate when determining the sort key. For example, a collection coll has the following documents: { _id: 0, a: [-3, -2, 2, 3] }{ _id: 1, a: [ 5, -4 ] }  Consider following sort operation on the array field a of the collection: db.coll.find({a: {$gte: 0}}).sort({a: 1});  In MongoDB 3.6, the sort operation no longer takes into account the query predicate when determining its sort key. As a result, the sort key is the lowest-valued element for each document:  * -3 for the document with _id: 0 and  * -4 for the document with _id: 1. The operation returns the documents in the following order: { \"_id\" : 1, \"a\" : [ 5, -4 ] }{ \"_id\" : 0, \"a\" : [ -3, -2, 2, 3 ] }  Previous MongoDB versions use the lowest-valued array element that matches the query predicate of {$gte: 0} as the sort key:  * 2 for the document with _id: 0 and  * 5 for the document with _id: 1, and would return the documents in the following order: { _id: 0, a: [-3, -2, 2, 3] }{ _id: 1, a: [ 5, -4 ] }  AGGREGATE METHOD SORTING \nIn MongoDB 3.6, when sorting array fields with the db.collection.aggregate() method, only a single array element is used as the sort key. Consider the following example: // Documents.{ \"_id\" : 1, \"timestamps\" : [ ISODate(\"2017-07-15T15:31:01Z\"), ISODate(\"2017-07-21T18:31:01Z\") ] }{ \"_id\" : 0, \"timestamps\" : [ ISODate(\"2017-07-21T15:31:01Z\"), ISODate(\"2017-07-21T13:31:01Z\") ] }\n// Query.db.c.aggregate([{$sort: {timestamps: -1}}])  For a descending sort, the most recent time in the array is used as the sort key: 3:31 PM on July 21 for the document with _id: 0 and 5:31 PM on July 21 for the document with _id: 1. Since the sort is descending, these keys are then ordered from most recent to least recent, resulting in the document with _id: 1 sorting before the document with _id: 0. Before 3.6, the entire array is used as the sort key for aggregation sorts. The array sort keys are compared element-wise to determine the sort order of the result set. \nEXAMPLE\n\n  Prior to 3.6, the sort keys are [3, 1, 5] and [3, 4, 0] respectively. Since the first array elements are equal, the second array element breaks the tie and the document with _id: 0 sorts first. For more information on sorting with the Aggregation Pipeline, see $sort. SORTING WITH A COMPOUND SORT PATTERN ON MULTIPLE ARRAY FIELDS WITH AGGREGATE \nIn MongoDB 3.6, when sorting in an aggregation pipeline, MongoDB can no longer sort documents which contain parallel arrays in the fields being sorted on. Arrays are considered parallel if they are sibling elements of the BSON object. Sort keys involving nested arrays are not considered parallel, nor are sort keys which share the same array as a prefix. \nNOTE This behavior has always existed for sorting with find, but now in 3.6 find and aggregate share the same semantics. \nEXAMPLE A collection contains the following document: {a: [ {b: [1, 2]}, {b: [3, 4]} ]}  The following aggregation succeeds because the sort is performed on a nested array: db.coll.aggregate([{$sort: {\"a.b\": 1}}])  \nEXAMPLE Similarly, if a collection contains the following document: {a: [{b: 1, c: 1}, {b: 2, c: 2}]}  The following aggregation succeeds because the sort keys share the same array as a prefix: db.coll.aggregate([{$sort: {\"a.b\": 1, \"a.c\": 1}}])  \nEXAMPLE However, in a collection that contains the following documents: { _id: 1, a: [ 1, 2 ], b: [ 1, 2 ]}{ _id: 2, a: [ -3, 5 ], b: 0 }{ _id: 3, a: [ -6, 12 ], b: 100 }  The following sort operation fails: db.coll.aggregate([ { $sort: {a: 1, b: 1} } ])  MongoDB cannot sort on both the a and b fields because in the document with _id : 1 , the sibling fields a and b are both arrays. As a result, MongoDB encounters a parallel array during sort key generation and returns an error. \nUPDATE OPERATION CHANGES  NEW FIELDS IN UPDATES \nStarting in MongoDB 3.6, new fields added through update operations are appended in lexicographic order. For example, a collection coll has the following document: { _id: 0, x: 0 }  The following update operation adds two new fields to the document: db.coll.update({_id: 0}, {$set: {b: 0, a: 0}})  Starting in 3.6, MongoDB appends the new fields in lexicographic order. The updated document would be {_id : 0, x: 0, a: 0, b: 0}. Earlier versions of MongoDB append the new fields in order of appearance in the update document. The updated document would be {_id : 0, x: 0, b: 0, a: 0}. \nFIELDS CONFLICTING WITH ARRAYFILTERS IDENTIFIERS \nStarting in MongoDB 3.6, fields that conflict with arrayFilters identifiers can no longer be updated. For example, a collection coll has the following document: { _id: 0, x: { \"$[]\": 0 } }  The following update succeeds in earlier versions of MongoDB: db.coll.update({_id: 0}, {$set: {\"x.$[]\": 1}})  In MongoDB 3.6 the update fails since the field name \"$[]\" conflicts with arrayFilters identifier syntax. For more information on arrayFilters see Specify arrayFilters for Array Update Operations. \nNOTE The new update behaviors apply only when featureCompatibilityVersion is set to 3.6. \nSTRICTER VALIDATION OF $POP ARGUMENTS \nStarting in 3.6 [1], $pop operator performs a stricter validation of its argument to require either:  * -1 to remove the first element of an array, or  * 1 to remove the last element in an array. In earlier versions, $pop allowed:  * any value less than 0 to remove the first element of an array, and  * any value greater than or equal to 0 as well as non-numeric value to remove the last element in an array. [1] Affects MongoDB deployments with featureCompatibilityVersion set to 3.6; e.g. new 3.6 deployments, upgraded deployments that have set featureCompatibilityVersion to 3.6. \nREMOVE $PUSHALL UPDATE OPERATOR\n\n Instead of $pushAll, use the $push operator with the $each modifier. For example: db.students.update(   { name: \"joe\" },   { $push: { scores: { $each: [ 90, 92, 85 ] } } })  \nPLATFORM SUPPORT \n * MongoDB 3.6 discontinues support for versions of Windows prior to Windows Server 2008 R2 and Windows 7.  * MongoDB 3.6 is not tested on APFS, the new filesystem in macOS 10.13 and may encounter errors. \nGENERAL COMPATIBILITY CHANGES  MONGODB-CR DEPRECATION \nAs of MongoDB 3.6, MONGODB-CR authentication mechanism is deprecated. If you have not upgraded your MONGODB-CR authentication schema to SCRAM, see Upgrade to SCRAM. \nARBITER AND PRIORITY \nArbiters have priority 0. \nDEPRECATE MASTER-SLAVE REPLICATION \nMongoDB 3.6 deprecates master-slave replication. \n--NOJOURNAL OPTION WITH WIREDTIGER \nIn version 3.6, the --nojournal option is deprecated for replica set members using the WiredTiger storage engine. Replica set members which use the WiredTiger storage engine should not use the --nojournal option. For more information about journaling, see Manage Journaling. \nAGGREGATE COMMAND AND RESULTS \nMongoDB 3.6 removes the option for the aggregate command to return its results as a single document. If you run the aggregate command, you must include either the cursor option or the explain option. Rather than run the aggregate command directly, most users should use the db.collection.aggregate() helper provided in the mongo shell or the equivalent helper in their driver. These helpers return a cursor unless using the explain option. \nAGGREGATION DATE TO STRING COERCION \nStarting in 3.6, a date coerced to a string in an aggregation expression will include milliseconds and is appended with the letter 'Z'. \nEXAMPLE // Documents.{_id: 0, d: ISODate(\"2017-10-18T20:04:27.978Z\")}{_id: 1, d: ISODate(\"2017-10-18T20:04:28.192Z\")}\n// Query.db.coll.aggregate({$project: {d: {$toLower: \"$d\"}}})  Prior to 3.6, this would return the dates as d: \"2017-10-18t20:04:27\" and d: \"2017-10-18t20:04:28\" respectively. In 3.6, the results include the milliseconds and letter 'Z': d: \"2017-10-18t20:04:27.978z\" and d: \"2017-10-18t20:04:28.192z\". The change applies to the following aggregation operators:  * $concat  * $substr  * $substrBytes  * $substrCP  * $strcasecmp  * $toLower  * $toUpper \nREMOVE DIAGNOSTIC LOGGING COMMAND AND OPTION \nMongoDB 3.6 removes the deprecated diagLogging command and mongod --diaglog option. Instead, use mongoreplay to capture, replay, and profile commands sent to your MongoDB deployment. \nVALIDATE OPERATION \nStarting in MongoDB 3.6, for the WiredTiger storage engine, only the full validation process will force a checkpoint and flush all in-memory data to disk before verifying the on-disk data. In previous versions, the data validation process for the WT storage engine always forces a checkpoint. For more information on the validate operation, see the validate command and the db.collection.validate() method. \nINDEXES NAMED * \nStarting in 3.6, you cannot specify * as the index name during index creation nor can you delete indexes named * by specifying the index keys. To delete existing indexes named *, delete the index before upgrading. To rename them, delete and recreate the index. \nDEPRECATED OPTIONS \nChanged in version 3.6.1.  * MongoDB 3.6.1 deprecates the snapshot query option.\n   \n   For MMAPv1, use hint() on the { _id: 1} index instead to prevent a cursor from returning a document more than once if an intervening write operation results in a move of the document.\n   \n   For other storage engines, use hint() with { $natural : 1 } instead.\n\n [2] Starting in version 4.0, MongoDB offers transactions. \nBACKWARDS INCOMPATIBLE FEATURES \nThe following 3.6 features require that featureCompatibilityVersion be set to \"3.6\":  * UUID for collections  * $jsonSchema document validation  * Change Streams  * Chunk aware secondaries  * View definitions, document validators, and partial index filters that use 3.6 query features  * Sessions and retryable writes  * Users and roles with authenticationRestrictions 3.6 deployments have the following default featureCompatibilityVersion values: Deployments\nfeatureCompatibilityVersion\nFor new 3.6 deployments\n\"3.6\"\nFor deployments upgraded from 3.4\n\"3.4\" until you setFeatureCompatibilityVersion to \"3.6\". To view the featureCompatibilityVersion, see Get FeatureCompatibilityVersion. If you need to downgrade from 3.6, you must remove data related to all persisted incompatible features from your database before downgrading the binaries. See the 3.6 downgrade procedures. ←  3.6 ChangelogUpgrade a Standalone to 3.6 → On this page  * Localhost Binding Compatibility Changes\n * Shard Replica Set\n * HTTP Interface and REST API\n * Tools Changes\n * Array Operation Compatibility Changes\n * Update Operation Changes\n * Platform Support\n * General Compatibility Changes\n * Backwards Incompatible Features Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/4.0-compatibility/": " Docs Home → MongoDB Manual \nCOMPATIBILITY CHANGES IN MONGODB 4.0 \nOn this page    \n * Remove Support for MONGODB-CR\n   \n * Deprecate MMAPv1\n * x.509 Authentication Certificate Restrictions\n * Replica Sets\n * Sharded Clusters\n * 4.0 Feature Compatibility\n * General\n * Disable TLS 1.0\n * AES-GCM\n * mongo Shell\n * Removed Binary and Deprecated Fields/Commands The following 4.0 changes can affect the compatibility with older versions of MongoDB. \nREMOVE SUPPORT FOR MONGODB-CR \nStarting in version 4.0, MongoDB removes support for the deprecated MongoDB Challenge-Response (MONGODB-CR) authentication mechanism. Since version 3.0, MongoDB has not supported the creation of MONGODB-CR users unless the deployment had been upgraded from a 2.6 or earlier deployment that already had MONGODB-CR users and had not upgraded the authentication schema. If your deployment has user credentials stored in MONGODB-CR schema, you must upgrade to Salted Challenge Response Authentication Mechanism (SCRAM) before you upgrade to version 4.0. For information on upgrading to SCRAM, see Upgrade to SCRAM. \nREMOVAL OF AUTHSCHEMAUPGRADE COMMAND \nMongoDB 4.0 removes the authSchemaUpgrade command . The command, available in MongoDB 3.0 through MongoDB 3.6, supports the upgrade process for systems with MONGODB-CR users to SCRAM users. If your deployment has user credentials stored in MONGODB-CR schema, you must upgrade to Salted Challenge Response Authentication Mechanism (SCRAM) before you upgrade to version 4.0. For information on upgrading to SCRAM, see Upgrade to SCRAM. \nREMOVE MONGODB-CR SUPPORT FROM DB.COPYDATABASE() AND COPYDB \nThe method db.copyDatabase() cannot copy from a mongod instance that enforces MONGODB-CR authentication. The command copydb cannot copy from a mongod instance that enforces MONGODB-CR authentication. In conjunction with this change, MongoDB 4.0 removes the copydbgetnonce command. \nDEPRECATE MMAPV1 \nStarting in version 4.0, MongoDB deprecates the MMAPv1 storage engine. To change your MMAPv1 storage engine deployment to WiredTiger Storage Engine, see:      * Change Standalone to WiredTiger  * Change Replica Set to WiredTiger  * Change Sharded Cluster to WiredTiger \nX.509 AUTHENTICATION CERTIFICATE RESTRICTIONS \nStarting in MongoDB 4.2, if you specify --tlsAllowInvalidateCertificates or net.tls.allowInvalidCertificates: true when using x.509 authentication, an invalid certificate is only sufficient to establish a TLS connection but it is insufficient for authentication. If you are using invalid certificates to perform x.509 authentication, update your certificates to valid certificates. For example, you may sign your existing certificates with a trusted CA, or if using a custom CA, specify that CA using net.ssl.CAFile. \nREPLICA SETS  REMOVE PV0 FOR REPLICA SETS \nStarting in version 4.0, MongoDB removes the deprecated replica set protocol version 0 pv0. Before upgrading to MongoDB 4.0, you must upgrade to pv1. To upgrade to pv1, connect a mongo shell to the replica set primary and perform the following sequence of operations: cfg = rs.conf();cfg.protocolVersion=1;rs.reconfig(cfg);  To reduce the likelihood of w:1 rollbacks, you can also reconfigure the replica set to a higher settings.catchUpTimeoutMillis setting. For more information on pv1, see Replica Set Protocol Version. \nREMOVE MASTER-SLAVE REPLICATION \nMongoDB 4.0 removes support for the deprecated master-slave replication. Before you can upgrade to MongoDB 4.0, if your deployment uses master-slave replication, you must upgrade to a replica set. To convert from master-slave replication to a replica set, see Convert a Master-Slave Deployment to a Replica Set. \nJOURNALING AND REPLICA SETS \nStarting in MongoDB 4.0, you cannot specify --nojournal option or storage.journal.enabled: false for replica set members that use the WiredTiger storage engine. \nINDEX BUILDS AND REPLICA SETS\n\n \nROLLBACK LIMIT \nMongoDB 4.0 removes the limit on the amount of data that can be rolled back. In previous versions, a mongod instance will not roll back more than 300 megabytes of data and requires manual intervention if more than 300 megabytes of data need to be rolled back. Starting in MongoDB 4.0, the rollback time limit defaults to 1 day and is configurable using the new parameter rollbackTimeLimitSecs. In earlier versions, the rollback time limit is not configurable and is set to 30 minutes. \nSHARDED CLUSTERS \nmongos uses \"majority\" write concern for the following operations that affect the sharded cluster metadata: Command\nMethod\nNote\naddShard\nsh.addShard() create\ndb.createCollection() drop\ndb.collection.drop() dropDatabase\ndb.dropDatabase()\nChanged in MongoDB 3.6\nenableSharding\nsh.enableSharding() movePrimary \nrenameCollection\ndb.collection.renameCollection() shardCollection\nsh.shardCollection() removeShard \nsetFeatureCompatibilityVersion  \n4.0 FEATURE COMPATIBILITY \nSome features in 4.0 require not just the 4.0 binaries but the featureCompatibilityVersion set to 4.0. These features include:  * SCRAM-SHA-256  * New type conversion operators and enhancements  * Multi-document transactions  * $dateToString option changes  * New change stream methods  * Change stream resume token data type change \nGENERAL \n * The geospatial query operators $near and $nearSphere now supports querying on sharded collections.  * For the create command (and the mongo shell db.createCollection() method), you cannot set the option autoIndexId to false when creating collections in databases other than the local database.  * When authentication is enabled, running the listDatabases command without the listDatabases action privilege returns a list of all databases on which the user running the command has the find action privilege. In previous versions, running the command without the listDatabases action resulted in an Unauthorized response.  * The default value of taskExecutorPoolSize changed from 0 to 1. On Linux, to restore the previous behavior for a 4.0 deployment, set taskExecutorPoolSize to 0 and AsyncRequestsSenderUseBaton to false.  * MongoDB 4.0 removes the ability to set transportLayer and net.transportLayer to legacy for mongod and mongos instances. The transportLayer setting is automatically set to asio and cannot be modified.  * Starting in MongoDB 4.0, the reIndex command and its helper db.collection.reIndex() take a Global exclusive (W) lock and will block other operations until it finishes.  * If the value specified for fields other than year, isoYear, and timezone is outside the valid range, $dateFromParts carries or subtracts the difference from other date parts to calculate the date. In previous versions, values that exceed the range result in an error.  * Changed behavior of the killCursors privilege action. Prior to MongoDB 4.0, a user could kill any cursor if they knew that cursor's ID. As of MongoDB 4.0, the killCursors privilege grants the user the ability to kill any cursor associated with a currently authenticated user. If the user does not have permission to kill a cursor, killCursors returns an error.  * MongoDB 4.0 adds a killAnyCursor privilege action that grants the user permission to kill any cursor for the specified collection.  * The mongos binary will crash when attempting to connect to mongod instances whose feature compatibility version (fCV) is greater than that of the mongos. For example, you cannot connect a MongoDB 4.0 version mongos to a 4.2 sharded cluster with fCV set to 4.2. You can, however, connect a MongoDB 4.0 version mongos to a 4.2 sharded cluster with fCV set to 4.0.  * Starting in 4.0, MongoDB resolves localhost IP address as configured instead of assuming 127.0.0.1. \nCURSOR.MIN() AND CURSOR.MAX() \nIf you use max() with min() to specify a range, the bound specified by max() must be greater than the bound specified by min(). In previous versions, the bounds could be equal but would scan no index entries, always resulting in an empty result set. \nDISABLE TLS 1.0 \nMongoDB binaries (mongod, mongos, and mongo) disables support for TLS 1.0 encryption on systems where TLS 1.1+ is available. If you need to support TLS 1.0:\n\n  * For mongos instances, you can specify none to net.ssl.disabledProtocols or --sslDisabledProtocols none.  * For the mongo shell, you can specify --sslDisabledProtocols none.\n   \n   The --sslDisabledProtocols option is available for the mongo shell in:\n   \n   * MongoDB version 4.0+\n   \n   * MongoDB version 3.6.5+\n   \n   * MongoDB version 3.4.15+ On macOS, to connect mongo shell version 3.6.4 or earlier to a MongoDB 4.0+ deployment requires explicit enabling of TLS 1.0. \nAES-GCM \nMongoDB Enterprise on Windows no longer supports AES256-GCM. This cipher is now available only on Linux. \nMONGO SHELL  SHOW COLLECTIONS \nThe mongo shell, show collections is equivalent to: db.runCommand( { listCollections: 1.0, authorizedCollections: true, nameOnly: true } )   * For users with the required access, show collections lists the non-system collections for the database.  * For users without the required access, show collections lists only the collections for which the users has privileges. When a version 4.0 mongo shell is connected to an earlier version MongoDB deployment that does not support authorizedCollections and nameOnly options,  * A user must have the required access to run listCollections.  * If a user does not have required access and runs show collections, MongoDB uses the authenticatedUserPrivileges field returned by connectionStatus to return an approximate list of collections for the user. \nDB.GETCOLLECTIONNAMES() \nThe db.getCollectionNames() is equivalent to: db.runCommand( { listCollections: 1.0, authorizedCollections: true, nameOnly: true } )   * For users with the required access (privileges that grant listCollections action on the database), the method lists the names of all collections for the database.  * For users without the required access, the method lists only the collections for which the users has privileges. For example, if a user has find on a specific collection in a database, the method would return just that collection. \nREMOVED BINARY AND DEPRECATED FIELDS/COMMANDS  MONGOPERF \nMongoDB 4.0 removes the mongoperf binary. \nCOPYDB AND CLONE COMMANDS \nMongoDB 4.0 deprecates the copydb and the clone commands and their mongo shell helpers db.copyDatabase() and db.cloneDatabase(). As alternatives, users can use mongodump and mongorestore (with the mongorestore options --nsFrom and --nsTo) or write a script using the drivers. For example, to copy the test database from a local instance running on the default port 27017 to the examples database on the same instance, you can:  1. Use mongodump to dump the test database to an archive mongodump-test-db:\n    \n    mongodump --archive=\"mongodump-test-db\" --db=test\n    \n      2. Use mongorestore with --nsFrom and --nsTo to restore (with database name change) from the archive:\n    \n    mongorestore --archive=\"mongodump-test-db\" --nsFrom='test.*' --nsTo='examples.*'\n    \n     \nTIP Include additional options as necessary, such as to specify the uri or host, username, password and authentication database. Alternatively, instead of using an archive file, you can mongodump the test database to the standard output stream and pipe into mongorestore: mongodump --archive --db=test | mongorestore --archive  --nsFrom='test.*' --nsTo='examples.*'  \nPARAMETERS \n * MongoDB removes the obsolete logUserIds parameter. Use Auditing instead. \n$ISOLATED OPERATOR \nMongoDB drops support for the $isolated operator. If you have an existing partial index that includes the $isolated operator or a view that includes a $isolated operator, recreate the index or view without the operator in the definition before upgrading. Instead of the $isolated operator, use transactions instead. \nGEONEAR COMMAND \nMongoDB 4.0 deprecates the geoNear command. Use one of the following operations instead.  * $geoNear aggregation stage.\n\n  * $nearSphere query operator. \nMAXSCAN OPTION \nMongoDB deprecates the option maxScan for the find command and the mongo shell helper cursor.maxScan(). Instead, use maxTimeMS option or the helper cursor.maxTimeMS(). \nOUTPUT FIELD CHANGES \n * The following fields returned from replSetGetStatus are deprecated:\n   \n   * replSetGetStatus.syncingTo\n   \n   * replSetGetStatus.members[n].syncingTo\n   \n   Use replSetGetStatus.syncSourceHost and replSetGetStatus.members[n].syncSourceHost instead.  * The $currentOp aggregation stage, the currentOp command, and db.currentOp() helper no longer return the threadId field in their outputs.  * The serverStatus command now always returns 0 for the asserts.warning field. ←  Release Notes for MongoDB 4.0Upgrade a Standalone to 4.0 → On this page  * Remove Support for MONGODB-CR\n * Deprecate MMAPv1\n * x.509 Authentication Certificate Restrictions\n * Replica Sets\n * Sharded Clusters\n * 4.0 Feature Compatibility\n * General\n * Disable TLS 1.0\n * AES-GCM\n * mongo Shell\n * Removed Binary and Deprecated Fields/Commands Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/5.0/": " Docs Home → MongoDB Manual \nRELEASE NOTES FOR MONGODB 5.0 \nOn this page    \n * Patch Releases\n   \n * Time Series Collections\n * Aggregation\n * Auditing\n * Capped Collections\n * Change Streams\n * Indexes\n * Removed Commands\n * Replica Sets\n * Security\n * Sharded Clusters\n * Shell Changes\n * Snapshots\n * Transactions\n * Naming Changes\n * General Improvements\n * Platform Support\n * Changes Affecting Compatibility\n * Upgrade Procedures\n * Download\n * Known Issues\n * Report an Issue \nNOTE MongoDB 5.0 Released Jul 13, 2021 \nPATCH RELEASES  5.0.15 - FEB 27, 2023 \n     * SERVER-54900 Blocking networking calls can delay sync-source resolution indefinitely  * SERVER-72416 The find and findAndModify projection code does not honor the collection level collation  * SERVER-71759 dataSize command doesn't yield  * SERVER-72222 MapReduce with single reduce optimization fails when merging results in sharded cluster  * WT-9268 Delay deletion of the history store record to reconciliation  * All JIRA issues closed in 5.0.15  * 5.0.15 Changelog \n5.0.14 - NOV 21, 2022 \nIssues fixed:  * SERVER-68477 Improve NaN-handling for expireAfterSeconds TTL index parameter  * SERVER-66289 $out incorrectly throws BSONObj size error on v5.0.8  * SERVER-61185 Use prefix_search for unique index lookup  * SERVER-68115 Bug fix for \"elemMatchRootLength > 0\" invariant trigger  * SERVER-68139 Resharding command fails if the projection sort is bigger than 100MB  * All JIRA issues closed in 5.0.14  * 5.0.14 Changelog \n5.0.13 - SEP 29, 2022 \nIssues fixed:  * SERVER-69611 Set the -ffp-contract=off compiler option by default  * SERVER-69220 refineCollectionShardKey permits toggling current shard key fields between range-based and hashed, leading to data inconsistency  * SERVER-67650 Resharding recipient can return remainingOperationTimeEstimatedSecs=0 when the oplog applier hasn't caught up with the oplog fetcher  * SERVER-68094 Resharding with custom generated _id fails with projection error  * WT-9870 Fix updating pinned timestamp whenever oldest timestamp is updated during recovery  * All JIRA issues closed in 5.0.13  * 5.0.13 Changelog \n5.0.12 - SEP 05, 2022 \nIssues fixed:  * SERVER-68925 Reintroduce check table logging settings at startup (revert SERVER-43664)  * SERVER-63852 getThreadName() should not crash  * SERVER-60958 Avoid server hang in chunk migration when step-down event occurs  * SERVER-65382 AutoSplitVector should not use clientReadable to reorder shard key fields  * SERVER-63843 Don't allow recursive doLog in synchronous signal handlers  * All JIRA issues closed in 5.0.12  * 5.0.12 Changelog \n5.0.11 - AUG 19, 2022 \nIssues fixed:  * SERVER-68511 MovePrimary update of config.databases entry must use dotted fields notation  * SERVER-61321 Improve handling of large/NaN values for text index version  * SERVER-60607 Improve handling of large/NaN values for geo index version  * SERVER-68628 Retrying a failed resharding operation after a primary failover can lead to server crash or lost writes  * SERVER-68522 Prevent 5.0 binary from starting in FCV 4.4 with misconfigured TTL index  * WT-9500 Fix RTS to use cell time window instead of key/value timestamps of HS update  * All JIRA issues closed in 5.0.11  * 5.0.11 Changelog \n5.0.10 - JULY 29, 2022  WARNING MongoDB version 5.0.10 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-66418 Bad projection created during dependency analysis due to string order assumption  * SERVER-65821 Deadlock during setFCV when there are prepared transactions that have not persisted commit/abort decision\n\n  * SERVER-63971 Change server parameter to default to read-your-writes behavior after 2PC transaction  * SERVER-66433 Backport deadline waiting for overlapping range deletion to finish to pre-v5.1 versions  * All JIRA issues closed in 5.0.10  * 5.0.10 Changelog \n5.0.9 - MAY 31, 2022  WARNING MongoDB version 5.0.9 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-65636 Remove limits on number of LDAP connections per host  * SERVER-65137 Detect namespace changes when refreshing Collection after yielding  * SERVER-64822 Sharding an empty collection releases the critical section too early  * SERVER-62175 Mongos fails to attach RetryableWrite Error Label For Command Interrupted In _parseCommand  * WT-9096 Fix search near returning wrong key/value sometimes when key doesn't exist  * All JIRA issues closed in 5.0.9  * 5.0.9 Changelog \n5.0.8 - APRIL 25, 2022  WARNING MongoDB version 5.0.8 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-63531 commitQuorum incorrectly includes buildIndexes:false nodes and error message incorrectly says that only voting nodes are eligible  * SERVER-63387 1 StreamingCursor should return backup blocks in the order they were retrieved from the WiredTiger backup cursor  * SERVER-62229 Fix invariant when applying index build entries while recoverFromOplogAsStandalone=true  * SERVER-61879 Refreshes to recover migrations must never join ongoing refreshes  * WT-8924 Don't check against on disk time window if there is an insert list when checking for conflicts in row-store  * All JIRA issues closed in 5.0.8  * 5.0.8 Changelog \n5.0.7 - APRIL 11, 2022  WARNING MongoDB version 5.0.7 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-64517 RecoverableCriticalSection is not properly recovered on startup  * SERVER-64403 Find queries with SORT_MERGE collation-encode the missing sort attribute  * SERVER-63742 Default topology time in shard can lead to infinite refresh in shard registry  * SERVER-60412 Host memory limit check does not honor cgroups v2  * WT-7922 Handle missing WiredTiger version file  * All JIRA issues closed in 5.0.7  * 5.0.7 Changelog \n5.0.6 - JANUARY 31, 2022  WARNING MongoDB version 5.0.6 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * WT-8395 Inconsistent data after upgrade from 4.4.3 and 4.4.4 to 4.4.8+ and 5.0.2+  * SERVER-62245 MigrationRecovery must not assume that only one migration needs to be recovered  * SERVER-61427 Unique index builds can cause a loss of availability during commit due to checking many false duplicates  * SERVER-61194 Prevent time-series bucket OID reuse with coarse granularity  * SERVER-60310 OCSP response validation should not consider statuses of irrelevant certificates  * All JIRA issues closed in 5.0.6  * 5.0.6 Changelog \n5.0.5 - DECEMBER 6, 2021  WARNING MongoDB version 5.0.5 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-61483 Resharding coordinator fails to recover abort decision on step-up, attempts to commit operation as success, leading to data inconsistency  * SERVER-59858 Add observability for tasks scheduled on the reactor thread  * SERVER-51329 Unexpected non-retryable error when shutting down a mongos server  * WT-8163 Consider more eviction scenarios to give up checkpoint-cleanup  * WT-7912 Fix prefix search near optimisation to handle scenarios where the key range is split across pages.\n\n  * 5.0.5 Changelog \n5.0.4 - NOV 15, 2021  WARNING MongoDB version 5.0.4 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-60326 Windows Server fails to start when X509 certificate has empty subject name  * SERVER-59876 Large delays in returning from libcrypto.so while establishing egress connections  * SERVER-59456 Start the LDAPReaper threadpool  * SERVER-59226 Deadlock when stepping down with a profile session marked as uninterruptible  * SERVER-59074 Do not acquire storage tickets just to set/wait on oplog visibility  * All JIRA issues closed in 5.0.4  * 5.0.4 Changelog \n5.0.3 - SEP 21, 2021  WARNING MongoDB version 5.0.3 is not recommended for production use in sharded clusters due to critical issue SERVER-68511, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-57667: Improve processing speed for resharding's collection cloning pipeline  * SERVER-57630: Enable SSL_OP_NO_RENEGOTIATION on Ubuntu 18.04 when running against OpenSSL 1.1.1  * WT-8005: Fix a prepare commit bug that could leave the history store entry unresolved  * WT-7995: Fix the global visibility that it cannot go beyond checkpoint visibility  * WT-7984: Fix a bug that could cause a checkpoint to omit a page of data  * All JIRA issues closed in 5.0.3  * 5.0.3 Changelog \n5.0.2 - AUG 4, 2021  WARNING MongoDB version 5.0.2 is not recommended for production use due to critical issues SERVER-68511, WT-7984, and WT-7995, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-58936: Unique index constraints may not be enforced  * SERVER-57756: Race between concurrent stepdowns and applying transaction oplog entry  * SERVER-54729: MongoDB Enterprise Debian/Ubuntu packages should depend on libsasl2-modules and libsasl2-modules-gssapi-mit  * SERVER-47372: config.cache collections can remain even after collection has been dropped  * WT-6729: Quiesce eviction prior running rollback to stable's active transaction check  * All JIRA issues closed in 5.0.2  * 5.0.2 Changelog \n5.0.1 - JUL 22, 2021  WARNING MongoDB version 5.0.1 is not recommended for production use due to critical issues SERVER-68511, SERVER-58936, WT-7984, and WT-7995, fixed in later versions. Use the latest available patch release version. Issues fixed:  * SERVER-58489: Collection creation stuck in an infinite writeConflictRetry loop when having a duplicate name as a view  * SERVER-58171: Changing time-series granularity does not update view definition  * All JIRA issues closed in 5.0.1  * 5.0.1 Changelog \n5.0.0 - JUL 13, 2021  WARNING MongoDB version 5.0.0 is not recommended for production use due to critical issues SERVER-68511, SERVER-58936, WT-7984, and WT-7995, fixed in later versions. Use the latest available patch release version. The rest of this page provides the 5.0.0 release notes: \nTIME SERIES COLLECTIONS \nMongoDB 5.0 introduces time series collections which efficiently store sequences of measurements over a period of time. Compared to normal collections, storing time series data in time series collections improves query efficiency and reduces disk usage for your data and indexes. \nAGGREGATION  NEW AGGREGATION OPERATORS \nMongoDB 5.0 introduces the following aggregation operators: Operator\nDescription\n$count $count (aggregation accumulator) provides a count of all documents when used in the existing pipeline $group (aggregation) stage and the new MongoDB 5.0 $setWindowFields stage. \nNOTE \nDISAMBIGUATION The $count (aggregation accumulator) is distinct from the $count (aggregation) pipeline stage.\n\n \nWINDOW OPERATORS \nMongoDB 5.0 introduces the $setWindowFields pipeline stage, allowing you to perform operations on a specified span of documents in a collection, known as a window. The operation returns the results based on the chosen window operator. For example, you can use the $setWindowFields stage to output the:  * Difference in sales between two documents in a collection.  * Sales rankings.  * Cumulative sales totals.  * Analysis of complex time series information without exporting the data to an external database. \nGENERAL AGGREGATION IMPROVEMENTS \n$EXPR OPERATOR: COMPARISON OPERATORS USE INDEXES \nStarting in MongoDB 5.0, the $eq, $lt, $lte, $gt, and $gte operators placed in an $expr operator can use indexes to improve performance. $IFNULL EXPRESSION ACCEPTS MULTIPLE INPUT EXPRESSIONS \nStarting in MongoDB 5.0, you can specify multiple input expressions for the $ifNull expression before returning a replacement expression. LET OPTION FOR AGGREGATION \nStarting in MongoDB 5.0, the aggregate command and db.collection.aggregate() helper method have a let option to specify a list of variables that can be used elsewhere in the aggregation pipeline. This allows you to improve command readability by separating the variables from the query text. $LOOKUP STAGE: CONCISE CORRELATED SUBQUERIES \nStarting in MongoDB 5.0, an aggregation pipeline $lookup stage supports concise correlated subqueries that improve joins between collections. $LOOKUP STAGE: UNCORRELATED SUBQUERIES \nStarting in MongoDB 5.0, for an uncorrelated subquery in a $lookup pipeline stage containing a $sample stage, the $sampleRate operator, or the $rand operator, the subquery is always run again if repeated. Previously, depending on the subquery output size, either the subquery output was cached or the subquery was run again. See Perform an Uncorrelated Subquery with $lookup. \nAUDITING  RUNTIME AUDIT FILTER CONFIGURATION \nMongoDB 5.0 adds the ability to configure auditing filters at runtime. Operator\nDescription\nauditConfigPollingFrequencySecs\nDefines the polling interval for checking audit configuration\ngetAuditConfig\nRetrieves audit configurations from mongod and mongos.\nsetAuditConfig\nSets new audit configurations for mongod and mongos instances at runtime. \nGENERAL AUDITING UPDATES \nStarting in MongoDB 5.0:  * System event auditing has:\n   \n   * An additional uuid field and other audit message enhancements.\n   \n   * New audit message types: clientMetadata, directAuthMutation, logout, and startup.\n   \n   * Additional information and logging scenarios for these existing audit message types: authCheck, authenticate, createCollection, createIndex, and dropCollection.  * DDL operations auditing on secondaries has changes. See Audit Events and Filter. \nCAPPED COLLECTIONS  CAPPED COLLECTION DELETES ARE REPLICATED TO SECONDARIES \nStarting in MongoDB 5.0, the implicit delete operations for replica set capped collections are processed by the primary and replicated to the secondary members. \nEXPLICIT DELETES ALLOWED ON CAPPED COLLECTIONS \nStarting in MongoDB 5.0.7, you can delete documents from capped collections using delete methods. \nCHANGE STREAMS  CHANGE EVENTS OUTPUT \nStarting in MongoDB 5.0, Change Events contain the field updateDescription.truncatedArrays to record array truncations. \nINDEXES  PARTIAL INDEXES BEHAVIOR CHANGE \nStarting in MongoDB 5.0, multiple partial indexes can be created using the same key pattern as long as the partialFilterExpression fields do not express equivalent filters. In earlier versions of MongoDB, creating multiple partial indexes is not allowed when using the same key pattern with different partialFilterExpressions. \nUNIQUE SPARSE INDEX BEHAVIOR CHANGE \nStarting in MongoDB 5.0, unique sparse and unique non-sparse indexes with the same key pattern can exist on a single collection. See unique sparse index creation \nCANNOT DROP READY INDEXES DURING IN-PROGRESS INDEX BUILDS \nThe db.collection.dropIndexes() command cannot drop ready indexes if there are any in-progress index builds.  * In versions 4.4.0-4.4.4 of MongoDB, this logic was not true due to a bug.\n\n \nWhen run on a MongoDB deployment, db.collection.validate() attempts to fix multikey metadata inconsistencies of standalone deployments. \nREMOVAL OF GEOHAYSTACK INDEX AND THE GEOSEARCH COMMAND \nMongoDB 5.0 removes the deprecated geoHaystack index and geoSearch command. Use a 2d index with $geoNear or one of the supported geospatial query operators instead. Upgrading your MongoDB instance to 5.0 and setting featureCompatibilityVersion to 5.0 will delete any pre-existing geoHaystack indexes. \nNEW ERROR MESSAGES \nThe db.collection.createIndex() and db.collection.createIndexes() operations have new error messages when options are specified incorrectly. \nINTERRUPTED INDEX BUILDS \nIf a node in a replica set is cleanly shutdown or rolls back during an index build, the index build progress is now saved to disk. When the server restarts, index creation resumes from the saved position. \nREINDEX BEHAVIOR CHANGE \nStarting in MongoDB 5.0, the reIndex command and the db.collection.reIndex() shell method may only be run on standalone instances. \nREMOVED COMMANDS \nStarting in MongoDB 5.0, these database commands and mongo shell helper methods are removed: Removed Command\nAlternative\ndb.collection.ensureIndex()\ndb.collection.createIndex()\ndb.resetError()\nNot available\nresetError\nNot available\nshardConnPoolStats\nconnPoolStats\nunsetSharding\nNot available\ngeoSearch\n$geoNear or one of the supported geospatial query operators \nREPLICA SETS  NON-TRANSACTIONAL READS ON CONFIG.TRANSACTIONS \nStarting in MongoDB 5.0, non-transaction reads are not allowed on the config.transactions collection with the following read concerns and options:  * \"snapshot\"  * \"majority\" and the afterClusterTime option is set  * When using a MongoDB Driver and \"majority\" within a causally consistent session \nHELLO COMMAND \nStarting in MongoDB 5.0 (and 4.4.2, 4.2.10, 4.0.21, and 3.6.21), the hello command and the db.hello() method were introduced as replacements for the isMaster command and the db.isMaster() method. The new topology metric connections.exhaustHello tracks this in connections. \nQUIESCE PERIOD \nStarting in MongoDB 5.0, mongod and mongos enter a quiesce period to allow any ongoing database operations to complete before shutting down. \nLIMIT REMOVED FOR MEMBERS[N]._ID VALUES \nStarting in MongoDB 5.0, the members[n]._id field may be any integer value greater than or equal to 0. Previously, this value was limited to an integer between 0 and 255 inclusive. \nENABLEMAJORITYREADCONCERN IS NOT CONFIGURABLE \nStarting in MongoDB 5.0, enableMajorityReadConcern and --enableMajorityReadConcern cannot be changed and are always set to true due to storage engine improvements. In earlier versions of MongoDB, enableMajorityReadConcern and --enableMajorityReadConcern are configurable and can be set to false to prevent storage cache pressure from immobilizing a deployment with a three-member primary-secondary-arbiter (PSA) architecture. If you are using a three-member primary-secondary-arbiter (PSA) architecture, consider the following:  * The write concern \"majority\" can cause performance issues if a secondary is unavailable or lagging. For advice on how to mitigate these issues, see Mitigate Performance Issues with PSA Replica Set.  * If you are using a global default \"majority\" and the write concern is less than the size of the majority, your queries may return stale (not fully replicated) data. \nENHANCED THREAD POOL TIMEOUT CONTROL \nStarting in MongoDB 5.0, you can use the new replWriterMinThreadCount server parameter to configure the timeout of idle threads in the thread pool for parallel execution of replication operations. When replWriterMinThreadCount is configured with a value less than replWriterThreadCount, idle threads above replWriterMinThreadCount are timed out. \nRECONFIGURING PSA REPLICA SETS\n\n \nLIMIT SYNC SOURCE RE-EVALUATIONS \nmaxNumSyncSourceChangesPerHour determines how many sync source changes can happen per hour before the node temporarily stops re-evaluating a sync source. This parameter will not prevent a node from starting to sync from another node if it doesn't have a sync source. \nENABLEOVERRIDECLUSTERCHAININGSETTING SERVER PARAMETER \nStarting in MongoDB 5.0.2 (and 4.2.16 and 4.4.8), you can set the new enableOverrideClusterChainingSetting server parameter to true to allow secondary members to replicate data from other secondary members even if settings.chainingAllowed is false. \nSECURITY  SUPPORT FOR ONLINE CERTIFICATE ROTATION \nStarting in MongoDB 5.0, you may now rotate the following TLS certificates on demand without first needing to stop your running mongod or mongos instance:  * TLS Certificates  * CRL (Certificate Revocation List) files (on Linux and Windows platforms)  * CA (Certificate Authority) files To rotate these certificates, replace the certificate files on your filesystem with updated versions, then use the rotateCertificates command or the db.rotateCertificates() shell method to trigger certificate rotation. Rotating certificates in this manner does not require downtime, and does not drop any active remote connections. See Online Certificate Rotation for full details. \nSUPPORT FOR CONFIGURING TLS 1.3 CIPHER SUITES \nMongoDB 5.0 introduces the opensslCipherSuiteConfig parameter to enable configuration of the supported cipher suites OpenSSL should permit when using TLS 1.3 encryption. \nTLS CONNECTION X509 CERTIFICATE STARTUP WARNING \nStarting in MongoDB 5.0, mongod and mongos now issue a startup warning when their certificates do not include a Subject Alternative Name attribute. The following platforms do not support common name validation:  * iOS 13 and higher  * MacOS 10.15 and higher  * Go 1.15 and higher Clients using these platforms will not authenticate to MongoDB servers that use x.509 certificates whose hostnames are specified by CommonName attributes. \nAPPLYOPS PRIVILEGE ACTION \nMongoDB 5.0 introduces the applyOps privilege action which is inherited by dbAdminAnyDatabase. The applyOps action permits users to run the applyOps database command. \nSHARDED CLUSTERS  RESHARDING \nThe ideal shard key allows MongoDB to distribute documents evenly throughout the cluster while facilitating common query patterns. A suboptimal shard key can lead to performance or scaling issues due to uneven data distribution. Starting in MongoDB 5.0, you can use the reshardCollection command to change the shard key for a collection to change the distribution of your data across your cluster. \nCURRENTOP REPORTS ONGOING RESHARDING OPERATIONS \nStarting in MongoDB 5.0, the $currentOp aggregation stage (and the currentOp command and db.currentOp() shell method) include additional information about the status of ongoing resharding operations for the resharding coordinator and the donor and recipient shards. \nDB.CURRENTOP METHOD NOW USES AGGREGATION STAGE IN MONGOSH \nStarting in MongoDB 5.0, the $currentOp aggregation stage is used when running the helper method db.currentOp() with mongosh. \nMONGOS / MONGOD CONNECTION POOL \nStarting in MongoDB 5.0 (also available starting in 4.4.5 and 4.2.13), MongoDB adds the parameter option \"automatic\" as the new default for the ShardingTaskExecutorPoolReplicaSetMatching. When set for a mongos, the instance follows the behavior specified for the \"matchPrimaryNode\" option. When set for a mongod, the instance follows the behavior specified for the \"disabled\" option. \nRENAMECOLLECTION COMPATIBLE WITH SHARDED COLLECTIONS \nStarting in MongoDB 5.0, you can use the renameCollection command to change the name of a sharded collection. When renaming a sharded or unsharded collection in a sharded cluster, the source and target collections are exclusively locked on every shard. Subsequent operations on the source and target collections must wait until the rename operation completes. \nMOVEPRIMARY ERROR MESSAGE FOR WRITES DURING OPERATION \nStarting in MongoDB 5.0, when using the movePrimary command to remove a shard from a sharded cluster, writes to the original shard will generate an error message.\n\n \nStarting in MongoDB 5.0, documents in the config.changelog collection for split and merge operations contain an owningShard field. The owningShard field shows the shardId of the shard that owns the chunks that were split or merged. The owningShard field helps identify shards where split or merge operations frequently occur. \nMAXCATCHUPPERCENTAGEBEFOREBLOCKINGWRITES SERVER PARAMETER \nStarting in MongoDB 5.0 (and 4.4.7, 4.2.15, 4.0.26), you can set the maxCatchUpPercentageBeforeBlockingWrites to specify the maximum allowed percentage of data not yet migrated during a moveChunk operation when compared to the total size (in MBs) of the chunk being transferred. This parameter can affect the behavior of:  * moveChunk commands that are run manually.  * Load balancer functionality, which automatically runs multiple moveChunk commands to evenly distribute chunks across shards. See Sharded Cluster Balancer. \nSHELL CHANGES  NEW MONGODB SHELL: MONGOSH \nThe mongo shell has been deprecated in MongoDB v5.0. The replacement shell is mongosh. The legacy mongo shell will be removed in a future release. Shell packaging also changes in MongoDB v5.0. Refer to the installation instructions for further details. \nSHELL SUPPORT FOR GCP AND AZURE KMS PROVIDERS \nStarting in MongoDB 5.0 (and MongoDB 4.4.5), the Google Cloud Platform KMS and Azure Key Vault are supported in both mongosh and the legacy mongo shell as Key Management Service (KMS) providers for Client-Side Field Level Encryption. Using a KMS, you can centrally and securely store Customer Master Keys (CMKs), which are used to encrypt and decrypt data encryption keys as part of the client-side field level encryption workflow. In addition, a configured KMS allows for the use of How CSFLE Decrypts Documents of data fields when used with MongoDB Enterprise. To learn more, see Configure a KMS provider using mongosh. \nSNAPSHOTS  EXTENDED SUPPORT FOR READ CONCERN \"SNAPSHOT\" \nStarting in MongoDB 5.0, read concern \"snapshot\" is supported for some read operations outside of multi-document transactions on primaries and secondaries. \nMINSNAPSHOTHISTORYWINDOWINSECONDS SERVER PARAMETER \nStarting in MongoDB 5.0, you can use the minSnapshotHistoryWindowInSeconds parameter to control how long WiredTiger keeps the snapshot history. \nTRANSACTIONS  COORDINATECOMMITRETURNIMMEDIATELYAFTERPERSISTINGDECISION PARAMETER \nThe server parameter coordinateCommitReturnImmediatelyAfterPersistingDecision controls when transaction commit decisions are returned to the client. The parameter was introduced in MongDB 5.0 with a default value of true. In MongoDB 6.0 and 5.0.10 the default value changes to false. When coordinateCommitReturnImmediatelyAfterPersistingDecision is false, the shard transaction coordinator waits for all members to acknowledge a multi-document transaction commit before returning the commit decision to the client. \nNAMING CHANGES \nStarting in February 2022, the \"Versioned API\" terminology was changed to \"Stable API\". All concepts and features remain the same with this naming change. \nGENERAL IMPROVEMENTS  EXECUTION PLAN STATISTICS FOR QUERY WITH $LOOKUP PIPELINE STAGE \nMongoDB 5.0 adds execution plan statistics for queries that use a $lookup pipeline stage. \nIMPROVED HANDLING OF ($) AND (.) IN FIELD NAMES \nMongoDB 5.0 adds improved support for field names that are ($) prefixed or that contain (.) characters. The validation rules for storing data have been updated to make it easier to work with data sources that use these characters. \nCLUSTER WIDE DEFAULT WRITE CONCERN \nStarting in MongoDB 5.0, once the Cluster Wide Write Concern (CWWC) is set via the setDefaultRWConcern command the write concern cannot be unset. \nIMPLICIT DEFAULT WRITE CONCERN \nStarting in MongoDB 5.0, the implicit default write concern is w: majority. However, special considerations are made for deployments containing arbiters:  * The voting majority of a replica set is 1 plus half the number of voting members, rounded down. If the number of data-bearing voting members is not greater than the voting majority, the default write concern is { w: 1 }.\n\n Specifically, MongoDB uses the following formula to determine the default write concern: if [ (#arbiters > 0) AND (#non-arbiters <= majority(#voting-nodes)) ]    defaultWriteConcern = { w: 1 }else    defaultWriteConcern = { w: \"majority\" } For example, consider the following deployments and their respective default write concerns: Non-Arbiters\nArbiters\nVoting Nodes\nMajority of Voting Nodes\nImplicit Default Write Concern\n2\n1\n3\n2\n{ w: 1 }\n4\n1\n5\n3\n{ w: \"majority\" }  * In the first example:\n   \n   * There are 2 non-arbiters and 1 arbiter for a total of 3 voting nodes.\n   \n   * The majority of voting nodes (1 plus half of 3, rounded down) is 2.\n   \n   * The number of non-arbiters (2) is equal to the majority of voting nodes (2), resulting in an implicit write concern of { w: 1 }.  * In the second example:\n   \n   * There are 4 non-arbiters and 1 arbiter for a total of 5 voting nodes.\n   \n   * The majority of voting nodes (1 plus half of 5, rounded down) is 3.\n   \n   * The number of non-arbiters (4) is greater than the majority of voting nodes (3), resulting in an implicit write concern of { w: \"majority\" }. The { w: \"majority\" } default write concern provides a stronger durability guarantee in the event of an election, or if replica set members become unavailable. The { w: \"majority\" } write concern may impact performance since writes will only be acknowledged once a calculated majority of replica set members have executed and persisted the write to disk. If your application relies on performance-sensitive writes, you can use the setDefaultRWConcern command to explicitly set the default write concern for improved performance at the cost of data durability guarantees. You can also set the write concern at the individual operation level for performance-critical writes. See your driver documentation for details. \nMONGOSSHUTDOWNTIMEOUTMILLISFORSIGNALEDSHUTDOWN PARAMETER \nStarting in MongoDB 5.0, the new parameter mongosShutdownTimeoutMillisForSignaledShutdown specifies the time in milliseconds to wait for any ongoing database operations to complete before initiating a shutdown of mongos. \nCONFIGURABLE ZSTD COMPRESSION LEVEL \nMongoDB 5.0 introduces the zstdCompressionLevel configuration file option which allows for configurable compression levels when blockCompressor is set to zstd. \nLOCK-FREE READ OPERATIONS \nStarting in MongoDB 5.0, the following read operations are not blocked when another operation holds an exclusive (X) write lock on the collection:  * find  * count  * distinct  * aggregate  * mapReduce  * listCollections  * listIndexes When writing to a collection, mapReduce and aggregate hold an intent exclusive (IX) lock. Therefore, if an exclusive X lock is already held on a collection, mapReduce and aggregate write operations are blocked. \nSCHEMA VALIDATION FAILURES EXPLAINED \nMongoDB 5.0 adds detailed explanations when a document fails schema validation. \nREPAIR OPTION IN VALIDATE COMMAND \nStarting in MongoDB 5.0, the validate command and db.collection.validate() helper method have a new repair option for repairing a collection that has inconsistencies. The validate command and db.collection.validate() helper method also return a new repaired boolean value that is true if the collection was repaired. \nVALIDATE COMMAND REPORTS DOCUMENT SCHEMA VIOLATIONS \nStarting in MongoDB 5.0, validate and db.collection.validate() validates documents in a collection. The commands report if any schema validation rules are violated. \nREPAIR OPTION IN MONGOD \nStarting in MongoDB 5.0, the --repair option for mongod validates the collections to find any inconsistencies and fixes them if possible, which avoids rebuilding the indexes. See the --repair option for usage and limitations. \nCORRUPTRECORDS FIELD IN VALIDATION OUTPUT \nStarting in MongoDB 5.0, the validate command and db.collection.validate() helper method return a new corruptRecords field that contains an array of RecordId values for corrupt documents. \nMAXVALIDATEMEMORYUSAGEMB SERVER PARAMETER\n\n \nFINDCHUNKSONCONFIGTIMEOUTMS SERVER PARAMETER \nStarting in MongoDB 5.0, you can use the findChunksOnConfigTimeoutMS parameter to change the timeout for find operations on chunks. \nDATABASE PROFILER FILTER OPTION \nStarting in MongoDB 5.0, you can set a filter option for the database profiler to determine which operations are profiled and logged. You can use the filter expression in place of the slowms and sampleRate profiler options. See:  * Database Profiler  * db.setProfilingLevel()  * profile \nLOG CHANGES TO DATABASE PROFILER SETTINGS \nStarting in MongoDB 5.0 (also available starting in 4.4.2, and 4.2.12), changes made to the database profiler level, slowms, sampleRate, or filter using the profile command or db.setProfilingLevel() wrapper method are recorded in the log file. \nINDEPENDENT LOG ROTATION FOR SERVER AND AUDIT LOGS \nStarting in MongoDB 5.0, when auditing is enabled, you may now rotate the server and audit logs independently using the logRotate command. Previously, logRotate would rotate the two logs together. \nREMOTE FIELD IN SLOW OPERATION LOGS \nStarting in MongoDB 5.0, slow operation log messages include a remote field specifying client IP address. \nREMOTEOPWAITMILLIS LOG FIELD \nStarting in MongoDB 5.0, you can use the remoteOpWaitMillis log field to obtain the wait time for results from shards. \nRESOLVEDVIEWS LOG FIELD FOR SLOW QUERIES ON VIEWS \nStarting in MongoDB 5.0, log messages for slow queries on views include a resolvedViews field that contains the view details. \nDEFINE VARIABLES USING THE LET OPTION \nStarting in MongoDB 5.0, the following commands have a let option to define a list of variables. This allows you to improve command readability by separating the variables from the query text.  * find command  * findAndModify command and corresponding db.collection.findAndModify() shell helper  * update command and corresponding db.collection.update() shell helper  * delete command  * db.collection.remove() shell helper The update command also has a c field to define a list of variables. \nSUPPORT FOR USERNAME TO LDAP DN MAPPING BY DEFAULT \nStarting in MongoDB 5.0, the userToDNMapping configuration file option and the --ldapUserToDNMapping command line option for mongod / mongos and mongoldap now map the authenticated username as the LDAP DN by default if an empty mapping document (i.e. an empty string or empty array) is specified to the option. Previously, providing an empty mapping document would cause mapping to fail. \nADDITIONAL DBSTATS FREE SPACE STATISTICS \nStarting in MongoDB 5.0, the dbStats command outputs these additional statistics:  * Free space allocated to collections (freeStorageSize)  * Free space allocated to indexes (indexFreeStorageSize)  * Total free space allocated to collections and indexes (totalFreeStorageSize) \nSERVERSTATUS OUTPUT CHANGE \nserverStatus includes the following new fields in its output: Aggregation Metrics  * metrics.commands.update.pipeline (Also available in 4.4.2+, 4.2.11+)  * metrics.commands.update.arrayFilters (Also available in 4.4.2+, 4.2.11+)  * metrics.commands.findAndModify.pipeline (Also available in 4.4.2+, 4.2.11+)  * metrics.commands.findAndModify.arrayFilters (Also available in 4.4.2+, 4.2.11+)  * metrics.dotsAndDollarsFields  * metrics.operatorCounters API Version Metrics  * metrics.apiVersions Replication Metrics  * metrics.repl.reconfig.numAutoReconfigsForRemovalOfNewlyAddedFields Read Concern Counters  * readConcernCounters, which reports on the read concern level specified by query operations (readConcernCounters replaces opReadConcernCounters)  * readConcernCounters now has the following new fields:\n   \n   * readConcernCounters.nonTransactionOps.noneInfo\n   \n   * readConcernCounters.transactionOps.noneInfo Write Concern Counters\n\n Number of Threaded Connections  * connections.threaded, which reports the number of incoming connections from clients that are assigned to threads that service client requests Resharding Statistics  * shardingStatistics.resharding, which reports statistics about resharding operations Service Executor Metrics  * network.serviceExecutors, which reports on the service executors that run operations for client requests Cursor Metrics  * metrics.cursor.moreThanOneBatch, which reports the total number of cursors that have returned more than one batch (additional batches are retrieved using the getMore command)  * metrics.cursor.totalOpened, which reports the total number of cursors that have been opened Security Counter  * security.authentication.saslSupportedMechsReceived, which reports the number of times a hello request includes a valid hello.saslSupportedMechs field. Repl  * repl now includes a primaryOnlyServices document that contains additional information about services that only run on replica set primaries. \nPLAN CACHE DEBUG INFO SIZE LIMIT \nStarting in MongoDB 5.0 (and 4.4.3, 4.2.12, 4.0.23, and 3.6.23), the plan cache will save full plan cache entries only if the cumulative size of the plan caches for all collections is lower than 0.5 GB. When the cumulative size of the plan caches for all collections exceeds this threshold, additional plan cache entries are stored without certain debug information. The estimated size in bytes of a plan cache entry is available in the output of $planCacheStats. \nCLOSURE OF INACTIVE CURSORS OPENED WITHIN A SESSION \nStarting in MongoDB 5.0 (and 4.4.8), cursors created within a client session close when the corresponding server session ends with the killSessions command, if the session times out, or if the client has exhausted the cursor. See Iterate a Cursor in mongosh. \nNEW VALIDATEDBMETADATA COMMAND \nMongoDB 5.0 adds the validateDBMetadata command. The validateDBMetadata command checks that the stored metadata of a database or a collection is valid within a particular API version. \nPLATFORM SUPPORT  MINIMUM MICROARCHITECTURE \nMongoDB 5.0 introduces the following minimum microarchitecture requirements: CPU\nMinimum Supported Microarchitecture\nIntel x86_64 MongoDB 5.0 requires one of:  * Intel Sandy Bridge or later Core processor, or  * Intel Tiger Lake or later Celeron or Pentium processor. AMD x86_64\nMongoDB 5.0 requires AMD Bulldozer or later.\nARM arm64\nMongoDB 5.0 requires ARMv8.2-A or later. MongoDB v5.0 is not supported on x86_64 or arm64 platforms that do not meet these minimum microarchitecture requirements. See x86_64 Platform Support for more information. \nREMOVED PLATFORMS \nMongoDB 5.0 removes support for the following platforms:  * macOS 10.13  * RHEL 7 / CentOS 7 / Oracle 7 on the PPC64LE architecture  * SLES 12 on the s390x architecture  * Ubuntu 18.04 on the PPC64LE and s390x architectures See Platform Support for the full list of platforms and architectures supported in MongoDB 5.0. \nCHANGES AFFECTING COMPATIBILITY \nSome changes can affect compatibility and may require user actions. For a detailed list of compatibility changes, see Compatibility Changes in MongoDB 5.0. \nUPGRADE PROCEDURES  IMPORTANT \nFEATURE COMPATIBILITY VERSION To upgrade to MongoDB 5.0 from a 4.4 deployment, the 4.4 deployment must have featureCompatibilityVersion set to 4.4. To check the version: db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )  To upgrade to MongoDB 5.0, refer to the upgrade instructions specific to your MongoDB deployment:  * Upgrade a Standalone to 5.0  * Upgrade a Replica Set to 5.0  * Upgrade a Sharded Cluster to 5.0 If you need guidance on upgrading to 5.0, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nDOWNLOAD \nTo download MongoDB 5.0, go to the MongoDB Download Center. \nTIP \nSEE ALSO:  * All Third Party License Notices. \nKNOWN ISSUES\n\n \nREPORT AN ISSUE \nTo report an issue, see https://github.com/mongodb/mongo/wiki/Submit-Bug-Reports for instructions on how to file a JIRA ticket for the MongoDB server or one of the related projects. ←  5.1 ChangelogCompatibility Changes in MongoDB 5.0 → On this page  * Patch Releases\n * Time Series Collections\n * Aggregation\n * Auditing\n * Capped Collections\n * Change Streams\n * Indexes\n * Removed Commands\n * Replica Sets\n * Security\n * Sharded Clusters\n * Shell Changes\n * Snapshots\n * Transactions\n * Naming Changes\n * General Improvements\n * Platform Support\n * Changes Affecting Compatibility\n * Upgrade Procedures\n * Download\n * Known Issues\n * Report an Issue Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.4-upgrade-standalone/": " Docs Home → MongoDB Manual \nUPGRADE A STANDALONE TO 3.4 \nOn this page    \n * Upgrade Recommendations and Checklists\n   \n * Download 3.4 Binaries\n * Upgrade Process\n * Additional Upgrade Procedures \nNOTE      * Starting in version 3.4.21, MongoDB 3.4-series removes support for Ubuntu 16.04 POWER/PPC64LE.  * For earlier MongoDB Enterprise versions that support Ubuntu 16.04 POWER/PPC64LE:\n   \n   Due to a lock elision bug present in older versions of the glibc package on Ubuntu 16.04 for POWER, you must upgrade the glibc package to at least glibc 2.23-0ubuntu5 before running MongoDB. Systems with older versions of the glibc package will experience database server crashes and misbehavior due to random memory corruption, and are unsuitable for production deployments of MongoDB \nIMPORTANT Before you attempt any upgrade, please familiarize yourself with the content of this document. If you need guidance on upgrading to 3.4, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nUPGRADE RECOMMENDATIONS AND CHECKLISTS \nWhen upgrading, consider the following: \nUPGRADE VERSION PATH \nTo upgrade an existing MongoDB deployment to 3.4, you must be running a 3.2-series release. To upgrade from a version earlier than the 3.2-series, you must successively upgrade major releases until you have upgraded to 3.2-series. For example, if you are running a 3.0-series, you must 3.2 before you can upgrade to 3.4. \nCHECK DRIVER COMPATIBILITY \nBefore you upgrade MongoDB, check that you're using a MongoDB 3.4-compatible driver. Consult the driver documentation for your specific driver to verify compatibility with MongoDB 3.4. Upgraded deployments that run on incompatible drivers might encounter unexpected or undefined behavior. \nPREPAREDNESS \nBefore beginning your upgrade, see the Compatibility Changes in MongoDB 3.4 document to ensure that your applications and deployments are compatible with MongoDB 3.4. Resolve the incompatibilities in your deployment before starting the upgrade. Before upgrading MongoDB, always test your application in a staging environment before deploying the upgrade to your production environment. \nDOWNGRADE CONSIDERATION \nOnce upgraded to 3.4, you cannot downgrade to a 3.2.7 or earlier version. You can only downgrade to a 3.2.8 or later version. The following steps outline the procedure to upgrade a standalone mongod from version 3.2 to 3.4. \nDOWNLOAD 3.4 BINARIES  VIA PACKAGE MANAGER \nIf you installed MongoDB from the MongoDB apt, yum, dnf, or zypper repositories, you should upgrade to 3.4 using your package manager. Follow the appropriate 3.4 installation instructions for your Linux system. This will involve adding a repository for the new release, then performing the actual upgrade process. \nMANUALLY \nIf you have not installed MongoDB using a package manager, you can manually download the MongoDB binaries from the MongoDB Download Center. See 3.4 installation instructions for more information. \nUPGRADE PROCESS \n1 \nREPLACE EXISTING 3.2 BINARIES WITH THE 3.4 BINARIES. \nShut down your mongod instance. Replace the existing binary with the 3.4 mongod binary and restart mongod. 2 \nENABLE BACKWARDS-INCOMPATIBLE 3.4 FEATURES. \nAt this point, you can run the 3.4 binaries without the 3.4 features that are incompatible with 3.2. To enable these 3.4 features, set the feature compatibility version to 3.4. \nWARNING Enabling these backwards-incompatible features can complicate the downgrade process. For details, see Remove 3.4 Incompatible Features. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features. Run the setFeatureCompatibilityVersion command against the admin database: db.adminCommand( { setFeatureCompatibilityVersion: \"3.4\" } )   This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command as the operation is idempotent. \nADDITIONAL UPGRADE PROCEDURES\n\n  * To upgrade a sharded cluster, see Upgrade a Sharded Cluster to 3.4. ←  Compatibility Changes in MongoDB 3.4Upgrade a Replica Set to 3.4 → On this page  * Upgrade Recommendations and Checklists\n * Download 3.4 Binaries\n * Upgrade Process\n * Additional Upgrade Procedures Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/6.0-downgrade-replica-set/": " Docs Home → MongoDB Manual \nDOWNGRADE 6.0 REPLICA SET TO 5.0 \nOn this page    \n * Downgrade Path\n   \n * Access Control\n * Prerequisites\n * Downgrade Procedure Before you attempt a downgrade, familiarize yourself with the content in this page. \nDOWNGRADE PATH  IMPORTANT Before you upgrade or downgrade a replica set, ensure all replica set members are running. If you do not, the upgrade or downgrade will not complete until all members are started. If you need to downgrade from 6.0, downgrade to the latest patch release of 5.0. MongoDB only supports single-version downgrades. You cannot downgrade to a release that is multiple versions behind your current release. You may downgrade a 6.0-series to a 5.0-series deployment, however, further downgrading that 5.0-series deployment to a 4.4-series deployment is not supported. \nACCESS CONTROL \nIf your replica set has access control enabled, your downgrade user privileges must include privileges to list and manage indexes across databases. A user with root role has the required privileges. \nPREREQUISITES \nBefore you begin the downgrade procedure, you must complete the following prerequisite steps. \n1. CREATE BACKUP \nOptional but Recommended. Create a backup of your database. To learn how to create a backup, see MongoDB Backup Methods. \n2. REMOVE BACKWARD-INCOMPATIBLE FEATURES \nTo downgrade from 6.0 to 5.0, you must remove 6.0 features that are incompatible with 5.0. For a list of incompatible features and how to remove them, see Downgrade Considerations. \n3. DOWNGRADE FEATURE COMPATIBILITY VERSION (FCV) \nTo downgrade the fCV of your replica set:       a. Ensure that no initial sync is in progress. Running the setFeatureCompatibilityVersion command while an initial sync is in progress causes the initial sync to restart.  b. Ensure that no nodes have a newlyAdded field in their replica set configuration. Run the following command on each node in your replica set to verify this:\n    \n    use localdb.system.replset.find( { \"members.newlyAdded\" : { $exists : true } } );\n    \n    \n    \n    The newlyAdded field only appears in a node's replica set configuration document during and shortly after an initial sync.  c. Ensure that no replica set member is in the ROLLBACK or RECOVERING state.  d. Use mongosh to connect to your primary.  e. Downgrade the featureCompatibilityVersion to \"5.0\".\n    \n    db.adminCommand( { setFeatureCompatibilityVersion: \"5.0\" } )\n    \n    \n    \n    The setFeatureCompatibilityVersion command performs writes to an internal system collection and is idempotent. If the command does not complete successfully, retry the command on the primary.  f. To ensure that all members of the replica set have the updated featureCompatibilityVersion, connect to each replica set member and check the featureCompatibilityVersion:\n    \n    db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )\n    \n    \n    \n    All members should return a result that includes:\n    \n    \"featureCompatibilityVersion\" : { \"version\" : \"5.0\" }\n    \n    \n    \n    If any member returns a featureCompatibilityVersion of \"6.0\", wait for the member to return version \"5.0\" before proceeding. For more information on the returned featureCompatibilityVersion value, see Get FeatureCompatibilityVersion. \nDOWNGRADE PROCEDURE  WARNING Before proceeding with the downgrade procedure, ensure that all replica set members, including delayed replica set members, have the prerequisite changes. To do that, check the featureCompatibilityVersion and the remove the incompatible features for each node before downgrading. 1 \nDOWNLOAD THE LATEST 5.0 BINARIES. \nUsing either a package manager or a manual download, get the latest release in the 5.0 series. If using a package manager, add a new repository for the 5.0 binaries, then perform the actual downgrade process. \nIMPORTANT\n\n If you need to downgrade from 6.0, downgrade to the latest patch release of 5.0. 2 \nREPLACE THE 6.0 BINARIES WITH THE DOWNLOADED 5.0 BINARIES. \nEnsure that the 5.0 binaries are in your System PATH. To confirm your binary version, run the following command: mongod --version  The command output should indicate a 5.0-series release. 3 \nDOWNGRADE SECONDARY MEMBERS OF THE REPLICA SET. \nDowngrade each secondary member of the replica set, one at a time: 1 SHUT DOWN THE MEMBER. \nTo shut down the mongod process, use mongosh to connect to the secondary and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 RESTART THE MEMBER. \nTo start a mongod process, run the following command: mongod --dbpath </path-to-data-folder>  To learn more about starting a mongod process, see Start mongod Processes. 3 WAIT FOR THE MEMBER TO ENTER THE SECONDARY STATE. \nBefore downgrading the next secondary, wait for the member to recover to the SECONDARY state. To check the member's state, use the rs.status() method in mongosh. 4 AFTER THE MEMBER IS IN SECONDARY STAGE, DOWNGRADE THE NEXT SECONDARY. \n4 \nDOWNGRADE THE ARBITER REPLICA SET MEMBER, IF ANY. \nSkip this step if the replica set does not include an arbiter. Downgrade the arbiter member of the replica set: 1 SHUT DOWN THE MEMBER. \nTo shutdown the arbiter, use mongosh to connect to the arbiter and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 DELETE THE CONTENTS OF THE ARBITER DATA DIRECTORY. \nTo find the data directory of the arbiter mongod, check either the storage.dbPath configuration setting or --dbpath command line option. Run the following command: rm -rf /path/to/mongodb/datafiles/* \n3 RESTART THE ARBITER. \nTo start a mongod process, run the following command: mongod --dbpath </path-to-mongodb-datafiles>  To learn more about starting a mongod process, see Start mongod Processes. 4 WAIT FOR THE MEMBER TO ENTER THE ARBITER STATE. \nBefore downgrading the primary, wait for the member to recover to the ARBITER state. To check the member's state, use the rs.status() method in mongosh. 5 \nDOWNGRADE THE PRIMARY. \n1 STEP DOWN THE PRIMARY. \nIn mongosh, use rs.stepDown() to step down the primary and force the normal failover procedure. rs.stepDown() \n2 VERIFY THAT THE PRIMARY HAS STEPPED DOWN. \nRun the following command: rs.status()  Verify that the primary has stepped down and another member has assumed the PRIMARY state. 3 REPLACE AND RESTART THE FORMER PRIMARY MONGOD. \n1 SHUT DOWN THE MEMBER. \nTo perform a shutdown of the primary, use mongosh to connect to the primary and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 RESTART MONGOD WITH THE 5.0 BINARY. \nTo start a mongod process, run the following command: mongod --dbpath </path-to-mongodb-datafiles>  To learn more about starting a mongod process, see Start mongod Processes. ←  Downgrade 6.0 Standalone to 5.0Downgrade 6.0 Sharded Cluster to 5.0 → On this page  * Downgrade Path\n * Access Control\n * Prerequisites\n * Downgrade Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/4.2-downgrade-replica-set/": " Docs Home → MongoDB Manual \nDOWNGRADE 4.2 REPLICA SET TO 4.0 \nOn this page    \n * Downgrade Path\n   \n * Considerations\n * Create Backup\n * Access Control\n * Prerequisites\n * Procedure Before you attempt any downgrade, familiarize yourself with the content of this document. \nDOWNGRADE PATH  IMPORTANT Before you upgrade or downgrade a replica set, ensure all replica set members are running. If you do not, the upgrade or downgrade will not complete until all members are started. If you need to downgrade from 4.2, downgrade to the latest patch release of 4.0. \nTIP If you downgrade,      * On Windows, downgrade to version 4.0.12 or later version. You cannot downgrade to a 4.0.11 or earlier version.  * On Linux/macOS, if you are running change streams and want to seamlessly resume change streams, downgrade to 4.0.7 or later versions. \nCONSIDERATIONS \nStarting in MongoDB 4.2, change streams are available regardless of the \"majority\" read concern support; that is, read concern majority support can be either enabled (default) or disabled to use change streams. In MongoDB 4.0 and earlier, change streams are available only if \"majority\" read concern support is enabled (default). Once you downgrade to 4.0-series, change streams will be disabled if you have disabled read concern \"majority\". \nCREATE BACKUP \nOptional but Recommended. Create a backup of your database. \nACCESS CONTROL \nIf your replica set has access control enabled, your downgrade user privileges must include privileges to list and manage indexes across databases. A user with root role has the required privileges. \nPREREQUISITES \nTo downgrade from 4.2 to 4.0, you must remove incompatible features that are persisted and/or update incompatible configuration settings. These include: \n1. DOWNGRADE FEATURE COMPATIBILITY VERSION (FCV)  TIP  * Ensure that no initial sync is in progress. Running setFeatureCompatibilityVersion command while an initial sync is in progress will cause the initial sync to restart.  * Ensure that no replica set member is in ROLLBACK or RECOVERING state. To downgrade the featureCompatibilityVersion of your replica set:  1. Connect a mongo shell to the primary.  2. Downgrade the featureCompatibilityVersion to \"4.0\".\n    \n    db.adminCommand({setFeatureCompatibilityVersion: \"4.0\"})\n    \n    \n    \n    The setFeatureCompatibilityVersion command performs writes to an internal system collection and is idempotent. If for any reason the command does not complete successfully, retry the command on the primary.  3. To ensure that all members of the replica set reflect the updated featureCompatibilityVersion, connect to each replica set member and check the featureCompatibilityVersion:\n    \n    db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )\n    \n    \n    \n    All members should return a result that includes:\n    \n    \"featureCompatibilityVersion\" : { \"version\" : \"4.0\" }\n    \n    \n    \n    If any member returns a featureCompatibilityVersion of \"4.2\", wait for the member to reflect version \"4.0\" before proceeding. \nNOTE Arbiters do not replicate the admin.system.version collection. Because of this, arbiters always have a Feature Compatibility Version equal to the downgrade version of the binary, regardless of the FCV value of the replica set. For example, an arbiter in a MongoDB 4.2 cluster, has an FCV value of 4.0. For more information on the returned featureCompatibilityVersion value, see Get FeatureCompatibilityVersion. \n2. REMOVE FCV 4.2 PERSISTED FEATURES \nThe following steps are necessary only if fCV has ever been set to \"4.2\". Remove all persisted 4.2 features that are incompatible with 4.0. 2A. INDEX KEY SIZE \nStarting in MongoDB 4.2, for featureCompatibilityVersion (fCV) set to \"4.2\" or greater, MongoDB removes the Index Key Limit. For fCV set to \"4.0\", the limit still applies.\n\n 2B. INDEX NAME LENGTH \nStarting in MongoDB 4.2, for featureCompatibilityVersion (fCV) set to \"4.2\" or greater, MongoDB removes the Index Name Length. For fCV set to \"4.0\", the limit still applies. If you have an index with a name that exceeds the Index Name Length once fCV is set to \"4.0\", drop and recreate the index with a shorter name. db.collection.dropIndex( <name | index specification> )\ndb.collection.createIndex(   { <index specification> },   { name: <shorter name> }}  \nTIP \nSEE:  * db.collection.dropIndex()  * db.collection.createIndex() 2C. UNIQUE INDEX VERSION \nWith featureCompatibilityVersion (fCV) \"4.2\", MongoDB uses a new internal format for unique indexes that is incompatible with MongoDB 4.0. The new internal format applies to both existing unique indexes as well as newly created/rebuilt unique indexes. If fCV has ever been set to \"4.2\", use the following script to drop and recreate all unique indexes. \nTIP Perform this operation after you have resolved any index key size and index name length issues first. Script // A script to rebuild unique indexes after downgrading fcv 4.2 to 4.0.// Run this script to drop and recreate unique indexes// for backwards compatibility with 4.0.\ndb.adminCommand(\"listDatabases\").databases.forEach(function(d){   let mdb = db.getSiblingDB(d.name);\n   mdb.getCollectionInfos( { type: \"collection\" } ).forEach(function(c){      let currentCollection = mdb.getCollection(c.name);\n      currentCollection.getIndexes().forEach(function(idx){         if (idx.unique){            print(\"Dropping and recreating the following index:\" + tojson(idx))\n            assert.commandWorked(mdb.runCommand({dropIndexes: c.name, index: idx.name}));\n            let res = mdb.runCommand({ createIndexes: c.name, indexes: [idx] });            if (res.ok !== 1)               assert.commandWorked(res);         }      });   });});  2D. REMOVE USER_1_DB_1 SYSTEM UNIQUE INDEX \nIn addition, if you have enabled access control, you must also remove the system unique index user_1_db_1 on the admin.system.users collection. If fCV has ever been set to \"4.2\", use the following command to drop the user_1_db_1 system unique index: db.getSiblingDB(\"admin\").getCollection(\"system.users\").dropIndex(\"user_1_db_1\")  The user_1_db_1 index will automatically be rebuilt when starting the server with the 4.0 binary in the procedure below. 2E. REMOVE WILDCARD INDEXES \nFor featureCompatibilityVersion (fCV) set to \"4.2\", MongoDB supports creating Wildcard Indexes. You must drop all wildcard indexes before downgrading to fCV \"4.0\". Use the following script to drop and recreate all wildcard indexes:\n\n  \nIMPORTANT Downgrading to fCV \"4.0\" during an in-progress wildcard index build does not automatically drop or kill the index build. The index build can complete after downgrading to fcv \"4.0\", resulting in a valid wildcard index on the collection. Starting the 4.0 binary against against that data directory will result in startup failures. Use db.currentOp() to check for any in-progress wildcard index builds. Once any in-progress wildcard index builds complete, run the script to drop them before downgrading to fCV \"4.0\". 2F. VIEW DEFINITIONS/COLLECTION VALIDATION DEFINITIONS THAT INCLUDE 4.2 OPERATORS \nBefore downgrading the binaries, modify read-only view definitions and collection validation definitions that include the 4.2 operators, such as $set, $unset, $replaceWith.  * For the $set stage, use the $addFields stage instead.  * For the $replaceWith stage, use the $replaceRoot stage instead.  * For the $unset stage, use the $project stage instead. You can modify a view either by:  * dropping the view (db.myview.drop() method) and recreating the view (db.createView() method) or  * using the collMod command. You can modify the colleciton validation expressions by:  * using the collMod command. \n3. UPDATE TLS-PREFIXED CONFIGURATION \nStarting in MongoDB 4.2, MongoDB adds \"tls\"-prefixed options as aliases for the \"ssl\"-prefixed options. If your deployments or clients use the \"tls\"-prefixed options, replace with the corresponding \"ssl\"-prefixed options for the mongod, the mongos, and the mongo shell and drivers. \n4. PREPARE DOWNGRADE FROM ZSTD COMPRESSION \nZSTD DATA COMPRESSION  IMPORTANT If you also use zstd Journal Compression, perform these steps after you perform the prerequisite steps for the journal compressor. The zstd compression library is available starting in version 4.2. For any member that has data stored using zstd compression, the downgrade procedure will require an initial sync for that member. To prepare:  1. Create a new empty data directory for the mongod instance. This directory will be used in the downgrade procedure below.\n    \n    \n    IMPORTANT\n    \n    Ensure that the user account running mongod has read and write permissions for the new directory.  2. If you use a configuration file, update the file to prepare for the downgrade procedure:\n    \n    a. Delete storage.wiredTiger.collectionConfig.blockCompressor to use the default compressor (snappy) or set to another 4.0 supported compressor.\n    \n    b. Update storage.dbPath to the new data directory.\n    If you use command-line options instead, you will have to update the options in the procedure below. Repeat for any other members that used zstd compression. ZSTD JOURNAL COMPRESSION \nThe zstd compression library is available for journal data compression starting in version 4.2. For any member that uses zstd library for its journal compressor: If the member uses zstd for journal compression and data compression,  * If using a configuration file, delete storage.wiredTiger.engineConfig.journalCompressor to use the default compressor (snappy) or set to another 4.0 supported compressor.  * If using command-line options instead, you will have to update the options in the procedure below. If the member uses zstd for journal compression only, \nNOTE The following procedure involves restarting the replica member as a standalone without the journal.  1. Perform a clean shutdown of the mongod instance:\n    \n    db.getSiblingDB('admin').shutdownServer()\n    \n    \n\n  3. Restart the mongod instance:\n    \n    * If you are using a configuration file:\n      \n      mongod -f <path/to/myconfig.conf>\n      \n      \n    \n    * If you are using command-line options instead of a configuration file,\n      \n      * Include the --nojournal option\n      \n      * Remove any replication command-line options (such as --replSet):\n      \n      * Set parameter disableLogicalSessionCacheRefresh to true in the --setParameter option.\n      \n      mongod --nojournal --setParameter disableLogicalSessionCacheRefresh=true  ...\n      \n        4. Perform a clean shutdown of the mongod instance:\n    \n    db.getSiblingDB('admin').shutdownServer()\n    \n    \n    \n    Confirm that the process is no longer running.  5. Update the configuration file to prepare to restart as a replica set member with the new journal compressor:\n    \n    * Remove the storage.journal.enabled setting.\n    \n    * Uncomment the replication settings for your deployment.\n    \n    * Remove the disableLogicalSessionCacheRefresh parameter.\n    \n    * Remove storage.wiredTiger.engineConfig.journalCompressor setting to use the default journal compressor or specify a new value.\n    \n    For example:\n    \n    storage:   wiredTiger:      engineConfig:         journalCompressor: <newValue>replication:   replSetName: replA\n    \n    \n    \n    If you use command-line options instead of a configuration file, you will have to update the command-line options during the restart below.  6. Restart the mongod instance as a replica set member:\n    \n    * If you are using a configuration file:\n      \n      mongod -f <path/to/myconfig.conf>\n      \n      \n    \n    * If you are using command-line options instead of a configuration file:\n      \n      * Remove the --nojournal option.\n      \n      * Remove the --wiredTigerJournalCompressor command-line option to use the default journal compressor or update to a new value.\n      \n      * Include your replication command-line options as well as any additional options for your replica set member.\n      \n      * Remove the disableLogicalSessionCacheRefresh parameter.\n      \n      mongod --wiredTigerJournalCompressor <differentCompressor|none> --replSet ...\n      \n       \nNOTE If you encounter an unclean shutdown for a mongod during the downgrade procedure such that you need to use the journal files to recover, recover the instance using the 4.2 mongod and then retry the downgrade of the instance. ZSTD NETWORK COMPRESSION \nThe zstd compression library is available for network message compression starting in version 4.2. To prepare for the downgrade:  1. For any replica set member that uses zstd for network message compression and uses a configuration file, update the net.compression.compressors setting to prepare for the restart during the downgrade procedure.\n    \n    If you use command-line options instead, you will have to update the options in the procedure below.  2. For any client that specifies zstd in its URI connection string, update to remove zstd from the list.  3. For any mongo shell that specifies zstd in its --networkMessageCompressors, update to remove zstd from the list. \nIMPORTANT\n\n \n5. REMOVE CLIENT-SIDE FIELD LEVEL ENCRYPTION DOCUMENT VALIDATION KEYWORDS  IMPORTANT Remove client-side field level encryption code in applications prior to downgrading the server. MongoDB 4.2 adds support for enforcing client-side field level encryption as part of a collection's Specify JSON Schema Validation document validation. Specifically, the $jsonSchema object supports the encrypt and encryptMetadata keywords. MongoDB 4.0 does not support these keywords and fails to start if any collection specifies those keywords as part of its validation $jsonSchema. Use db.getCollectionInfos() on each database to identify collections specifying automatic field level encryption rules as part of the $jsonSchema validator. To prepare for downgrade, connect to the replica set primary and perform either of the following operations for each collection using the 4.0-incompatible keywords:  * Use collMod to modify the collection's validator and replace the $jsonSchema with a schema that contains only 4.0-compatible document validation syntax:\n   \n   db.runCommand({  \"collMod\" : \"<collection>\",  \"validator\" : {    \"$jsonSchema\" : { <4.0-compatible schema object> }  }})\n   \n   \n   \n   -or-  * Use collMod to remove the validator object entirely:\n   \n   db.runComand({ \"collMod\" : \"<collection>\", \"validator\" : {} })\n   \n    \nPROCEDURE  WARNING Before proceeding with the downgrade procedure, ensure that all replica set members, including delayed replica set members, reflect the prerequisite changes. That is, check the featureCompatibilityVersion and the removal of incompatible features for each node before downgrading. 1 \nDOWNLOAD THE LATEST 4.0 BINARIES. \nUsing either a package manager or a manual download, get the latest release in the 4.0 series. If using a package manager, add a new repository for the 4.0 binaries, then perform the actual downgrade process. \nIMPORTANT Before you upgrade or downgrade a replica set, ensure all replica set members are running. If you do not, the upgrade or downgrade will not complete until all members are started. If you need to downgrade from 4.2, downgrade to the latest patch release of 4.0. 2 \nDOWNGRADE SECONDARY MEMBERS OF THE REPLICA SET. \nDowngrade each secondary member of the replica set, one at a time:  a. Shut down the mongod. See Stop mongod Processes for additional ways to safely terminate mongod processes.\n    \n    db.adminCommand( { shutdown: 1 } )\n    \n      b. Replace the 4.2 binary with the 4.0 binary and restart.\n    \n    \n    NOTE\n    \n    If you use command-line options instead of a configuration file, update the command-line options as appropriate during the restart.\n    \n     * If your command-line options include \"tls\"-prefixed options, update to \"ssl\"-prefixed options.\n    \n     * If the mongod instance used zstd data compression,\n       \n       * Update --dbpath to the new directory (created during the prerequisites).\n       \n       * Remove --wiredTigerCollectionBlockCompressor to use the default snappy compressor (or, alternatively, explicitly set to a 4.0 supported compressor).\n    \n     * If the mongod instance used zstd journal compression,\n       \n       * Remove --wiredTigerJournalCompressor to use the default snappy compressor (or, alternatively, explicitly set to a 4.0 supported compressor).\n    \n     * If the mongod instance included zstd network message compression,\n       \n       * Remove --networkMessageCompressors to enable message compression using the default snappy,zlib compressors. Alternatively, explicitly specify the compressor(s).  c. Wait for the member to recover to SECONDARY state. To check the member's state, connect a mongo shell to the member and run rs.status() method.\n\n 3 \nDOWNGRADE ARBITER REPLICA SET MEMBER, IF ANY. \nSkip this step if the replica set does not include an arbiter. Downgrade the arbiter member of the replica set:  a. Shut down the mongod. See Stop mongod Processes for additional ways to safely terminate mongod processes.\n    \n    db.adminCommand( { shutdown: 1 } )\n    \n      b. Delete the arbiter data directory contents. The storage.dbPath configuration setting or --dbpath command line option specify the data directory of the arbiter mongod.\n    \n    rm -rf /path/to/mongodb/datafiles/*\n    \n      c. Replace the 4.2 binary with the 4.0 binary and restart.\n    \n    \n    NOTE\n    \n    If you use command-line options instead of a configuration file, update the command-line options as appropriate during the restart.\n    \n     * If your command-line options include \"tls\"-prefixed options, update to \"ssl\"-prefixed options.\n    \n     * If the mongod instance used zstd data compression,\n       \n       * Update --dbpath to the new directory (created during the prerequisites).\n       \n       * Remove --wiredTigerCollectionBlockCompressor to use the default snappy compressor (or, alternatively, explicitly set to a 4.0 supported compressor).\n    \n     * If the mongod instance used zstd journal compression,\n       \n       * Remove --wiredTigerJournalCompressor to use the default snappy compressor (or, alternatively, explicitly set to a 4.0 supported compressor).\n    \n     * If the mongod instance included zstd network message compression,\n       \n       * Remove --networkMessageCompressors to enable message compression using the default snappy,zlib compressors. Alternatively, explicitly specify the compressor(s).  d. Wait for the member to recover to ARBITER state. To check the member's state, connect a mongo shell to the member and run rs.status() method. 4 \nSTEP DOWN THE PRIMARY. \nUse rs.stepDown() in the mongo shell to step down the primary and force the normal failover procedure. rs.stepDown()  rs.stepDown() expedites the failover procedure and is preferable to shutting down the primary directly. 5 \nREPLACE AND RESTART THE FORMER PRIMARY. \nWhen rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state:  a. Shut down the previous primary.\n    \n    db.adminCommand( { shutdown: 1 } )\n    \n    \n\n ←  Downgrade 4.2 Standalone to 4.0Downgrade 4.2 Sharded Cluster to 4.0 → On this page  * Downgrade Path\n * Considerations\n * Create Backup\n * Access Control\n * Prerequisites\n * Procedure Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.6-changelog/": " Docs Home → MongoDB Manual \n3.6 CHANGELOG \nOn this page    \n * 3.6.23 Changelog\n   \n * 3.6.22 Changelog\n * 3.6.21 Changelog\n * 3.6.20 Changelog\n * 3.6.19 Changelog\n * 3.6.18 Changelog\n * 3.6.17 Changelog\n * 3.6.16 Changelog\n * 3.6.15 Changelog\n * 3.6.14 Changelog\n * 3.6.13 Changelog\n * 3.6.12 Changelog\n * 3.6.11 Changelog\n * 3.6.10 Changelog\n * 3.6.9 Changelog\n * 3.6.8 Changelog\n * 3.6.7 Changelog\n * 3.6.6 Changelog\n * 3.6.5 Changelog\n * 3.6.4 Changelog\n * 3.6.3 Changelog\n * 3.6.2 Changelog\n * 3.6.1 Changelog \n3.6.23 CHANGELOG  QUERY \nSERVER-40361 Reduce memory footprint of plan cache entries \nINTERNALS \n     * SERVER-43973 jsTestName() should return a unique name for each thread in the parallel suite  * SERVER-46686 Explain does not respect maxTimeMS  * SERVER-50267 Set output limit for 'rawMongoProgramOutput()'  * SERVER-52969 Disable Powercyle on non-master branches  * SERVER-53992 Remove calls to detect-outliers from performance tasks  * SERVER-54134 Ignore newer mongo versions on older branches of setup_multiversion_mongodb.py  * TOOLS-2803 [v3.6] Add --config option for password values \n3.6.22 CHANGELOG  SHARDING \nSERVER-53182 [v3.6] Omit \"versions\" from {shardingState: 1} command when not running as a --shardsvr \nREPLICATION \n * SERVER-33747 Arbiter tries to start data replication if cannot find itself in config after restart  * SERVER-52680 Removed node on startup stuck in STARTUP2 after being re-added into the replica set  * SERVER-53026 Secondary cannot restart replication  * SERVER-53345 Excuse arbiter_new_hostname.js from multiversion tests \nQUERY \nSERVER-32960 $mod has inconsistent rounding/truncation behavior \nSTORAGE \nSERVER-52902 Remove assertion !haveJournalFiles() at dur_journal.cpp:265 \nBUILD AND PACKAGING \n * SERVER-32437 Platform Support: add Amazon Linux 2  * SERVER-52854 Fix package test on debian based system in 3.6 \nINTERNALS \n * SERVER-50445 Return the value as double when NumberLong subtraction overflows in ExpressionSubtract  * SERVER-50891 mongod 3.6.20-rc2 core dumps if dbpath DNE or perms block  * SERVER-52654 new signing keys not generated by the monitoring-keys-for-HMAC thread  * SERVER-52806 deb install files assume systemd \n3.6.21 CHANGELOG  SHARDING \n * SERVER-51808 invariant failure: readConcern level != Available  * SERVER-51885 Blacklist balancing_sessions_collection.js from sharding_csrs_continuous_config_stepdown \nREPLICATION \n * SERVER-49986 Convert isMaster command to hello and add aliases  * SERVER-49987 Rename response fields if “hello” is sent on mongod  * SERVER-49988 Rename response fields if \"hello\" is sent on mongos  * SERVER-49991 Alias printSlaveReplicationInfo() shell helper  * SERVER-50116 Forbid oplog writes when replication is enabled  * SERVER-50640 listCommands should return \"isMaster\" alias in addition to \"hello\" command name \nQUERY \n * SERVER-51083 Problem with regex index bounds  * SERVER-51120 Find queries with SORT_MERGE incorrectly sort the results when the collation is specified \nSTORAGE \nSERVER-34243 listCollections should not require a MODE_S database lock \nOPERATIONS \nSERVER-26726 Check number of arguments for createIndex() and throw error if more than two arguments \nBUILD AND PACKAGING\n\n \nINTERNALS \n * SERVER-45624 Pre-split and distribute chunks of sessions collection  * SERVER-50123 Record number of physical cores on all platforms  * SERVER-50216 Adjust sys-perf frequencies  * SERVER-50605 Add {logMessage: \"msg\"} test-only command  * SERVER-50736 Make OpenSSL explicitly accept SNIs presented in ClientHello  * SERVER-50818 Coverity analysis defect 114987: Wrapper object use after free  * SERVER-51106 Make the isMaster command a derived class of hello  * SERVER-51303 Lookup stage followed by $match on type uses wrong field in matching condition  * TOOLS-2590 [v3.6] sslAllowInvalidHostnames bypass ssl/tls server certification validation entirely \n3.6.20 CHANGELOG  REPLICATION \n * SERVER-48967 Prevent replicated writes on empty namespaces on secondaries  * SERVER-49990 Alias setSlaveOk() and getSlaveOk() shell helpers  * SERVER-50039 Timing error in dbadmin.js test \nQUERY \n * SERVER-36910 Reduce severity or remove log message for PlanExecutor::DEAD error during find  * SERVER-39392 Invariant in PlanStage::dispose always evaluates as true  * SERVER-48993 explodeForSort can produce incorrect query plan \nAGGREGATION \nSERVER-40317 $facet execution has no limit on how much memory it can consume \nSTORAGE \nSERVER-49449 index_restart_secondary.js is not compatible with storage engines without support for persistence \nOPERATIONS \nSERVER-48244 Shell should not hard-code the allowed explain levels and let the server reject it instead \nBUILD AND PACKAGING \n * SERVER-33000 Platform Support: add Ubuntu 18.04  * SERVER-42042 Warn or fail early on dynamic builds with toolchain compiler and tcmalloc  * SERVER-49925 Dynamic builders should not combine the mongodbtoolchain with tcmalloc  * SERVER-50124 Pin lazy-object-proxy python module \nINTERNALS \n * SERVER-33229 Overload startParallelShell in auto_retry_on_network_error.js to connect after loading the auto_retry_on_network_error.js override  * SERVER-41600 IndexCatalog::refreshEntry should invalidate the index from the CollectionInfoCache  * SERVER-43233 Add ability to request only specific attribute(s) for the LDAP groups  * SERVER-45202 Improve Command alias infrastructure  * SERVER-47930 Ubuntu 20.04 - ldap_authz_authn.js fails to load test certificate  * SERVER-48709 signing key generator thread on config server not waken up as expected  * SERVER-49054 Server MSI should install Compass instead of Compass Community  * SERVER-49352 4.0 does not build with python 3.8  * SERVER-49404 Enforce additional checks in $arrayToObject  * SERVER-50180 Fix User lifetime management in AuthorizationManager::acquireUserForSessionRefresh \n3.6.19 CHANGELOG  SHARDING \n * SERVER-46487 The mongos routing for scatter/gather ops can have unbounded latency  * SERVER-47913 Redact sharding messages in split chunk and distributed lock acquisition \nREPLICATION \n * SERVER-45610 Some reads work while system is RECOVERING  * SERVER-47695 Write commands run by threads that can survive rollback can fail operationTime invariant in ServiceEntryPoint \nQUERY \n * SERVER-40805 Indicate the reason for replanning in the log file  * SERVER-47994 Fix for numerical overflow in GeoHash \nSTORAGE \nSERVER-46398 Suggest explicit dbpath when starting mongod on macOS and the default dbpath isn't found \nOPERATIONS \nSERVER-45295 Make sure that LDAP logs always contain context on AuthZN operation \nBUILD AND PACKAGING \n * SERVER-42042 Warn or fail early on dynamic builds with toolchain compiler and tcmalloc  * SERVER-48640 Update RHEL 6.2 AMI for package tests  * SERVER-48659 Update SLES 12 AMI for package tests  * SERVER-48681 Update Debian 8.1 AMI for package tests \nINTERNALS \n * SERVER-45367 When the Range Deleter is waiting for the open cursors, list these cursors in the logs.  * SERVER-45508 Invariant failure in getFieldsWithStringBounds with descending point ranges  * SERVER-47256 Update listed maintainer for repo package files\n\n  * SERVER-47798 Audit isMaster response validation for mongod and mongos  * SERVER-48032 Update references of community mailing-lists to MongoDB Developer Community Forums  * SERVER-48657 Use evergreen module to control signal processing in perf projects  * SERVER-49142 Validate correct field name in RoleName::parseFromBSON()  * SERVER-49335 publish_packages should use barque API key  * WT-5119 Birthmark records can be read as normal updates if reads race with checkpoints  * WT-5150 LAS sweep is not removing the entries that are no longer required  * WT-5196 Data mismatch failures with test/checkpoint after enabling LAS sweep  * WT-5376 WT_UPDATE.type field can race with visibility checks when returning key/value pairs  * WT-5587 Limit how many checkpoints are dropped by a subsequent checkpoint \n3.6.18 CHANGELOG  SECURITY \nSERVER-46834 Use monotonic time in UserCacheInvalidator \nSHARDING \n * SERVER-29153 Make sure replica set nodes agree on which node is primary before doing writes in ShardingTest initialization  * SERVER-33597 Make allow_partial_results.js, return_partial_shards_down.js start shards as replica sets  * SERVER-44130 Flip order of assert.lte arguments in logical_time_metadata.js  * SERVER-45770 Add to information contained in logfile about \"moveChunk.to\"  * SERVER-46466 Race with findAndModify retryable write and session migration \nREPLICATION \n * SERVER-35050 Don't abort collection clone due to negative document count  * SERVER-35437 Wait for secondary state after stepdown command in multi_rs.js  * SERVER-45178 Rollback via refetch can cause rollback to happen successfully w/o updating the rollback id. \nQUERY \n * SERVER-32903 Ambiguous field name error should be ignored during initial sync  * SERVER-45363 Issue with mongodb text indexes and weights when using wildcard specifier \nAGGREGATION \nSERVER-45418 DocumentSourceCursor batching memory accounting does not account for empty documents, leads to unbounded memory use for count-like aggregates \nSTORAGE \nSERVER-45289 Conditional jump or move depends on uninitialised value(s) in validate_adaptor.cpp \nOPERATIONS \n * SERVER-34199 \"timing\" section of serverStatus can have large impact on ftdc retention  * SERVER-46024 Collect /proc/vmstat swap statistics in FTDC \nBUILD AND PACKAGING \n * SERVER-45713 Run rhel7 push and publish tasks on large rhel70 distro  * SERVER-45732 Filter flags from net-snmp-config invocation more aggressively  * SERVER-46983 Upload repobuilding packages to correct URL  * SERVER-46996 all push/publish_packages tasks should run on small hosts \nINTERNALS \n * SERVER-30739 config_server_checks.js should assert if initial replSetConfig fails  * SERVER-37148 Remove mr_shard_version.js and mr_during_migrate.js in favor of MR FSM test  * SERVER-38119 Windows dump file naming does not account for dots in directory names  * SERVER-40047 Test for blocking queue should not test for non-guaranteed condition  * SERVER-40305 Add diagnostic logging to max_time_ms.js  * SERVER-42278 Log SockAddrs produced during LDAP connection establishment with manually derived sizes  * SERVER-42525 Single-node replica sets shouldn't wait for electable caught up secondaries during shutdown  * SERVER-43349 Incorrect serialization for $elemMatch $not $ne  * SERVER-44325 Add polyfill for certificate chain verification  * SERVER-44435 Allow x509 authorization to be selectively enabled based on the CA  * SERVER-44868 Initialsync logkeeper and logkeeper-short tests should pass snapshot ID and dataset link respectively to dsi(sys-perf)  * SERVER-45156 SockAddr constructor should take sockaddr, not sockaddr_storage  * SERVER-45472 Ensure RoleGraph can serialize authentication restrictions to BSON  * SERVER-45766 Remove \"requires\" from the server yaml  * SERVER-45867 Use latest version of signal processing  * SERVER-45935 [3.6] WT error handler incorrectly prints \"An unsupported journal format detected\"  * SERVER-46082 Use bin/analysis.py for performance projects\n\n  * SERVER-46754 Use new repobuilder service  * SERVER-46766 Teardown clusters after every perf run  * SERVER-46899 Fix tcmallocReleaseRate parameter in 4.0 and 3.6  * SERVER-47063 Upgrade static OpenSSL to 1.1.1e  * SERVER-47232 Ensure availability of Python ldap libraries \n3.6.17 CHANGELOG  REPLICATION \n * SERVER-34876 initial_sync_with_write_load.js needs to await initial sync completion before reading oplog  * SERVER-45396 fix the \"me\" field in isMaster responses when using splithorizon \nQUERY \nSERVER-42565 Aggregations and find commands sort missing fields differently \nAGGREGATION \n * SERVER-42756 $multiply operator may return with or w/o an error depending on whether pipeline optimisation is enabled  * SERVER-43764 Add more testing for special values in DoubleDoubleSummation  * SERVER-44174 $push and $addToSet should restrict memory usage  * SERVER-44733 Change stream should throw ChangeStreamFatalError if a single shard cannot be targeted for updateLookup  * SERVER-44869 Add query knob to control memory limit for $push and $addToSet \nSTORAGE \nSERVER-44796 Adjust nojournal startup warning to be more compelling \nBUILD AND PACKAGING \n * SERVER-37772 Platform Support: Add Community & Enterprise RHEL 8 x64  * SERVER-44641 Platform Support: Remove Enterprise RHEL 7 zSeries and SLES 12 zSeries from 3.6 \nINTERNALS \n * SERVER-34844 Relax expectations in apply_batches_totalMillis  * SERVER-35099 Increase wtimeout in get_last_error.js even more  * SERVER-35382 _isSelf command needs to be marked requiresAuth false  * SERVER-38002 Upgrade Pcre to at least 8.42  * SERVER-39131 Refactor storage engine lock file support  * SERVER-42697 Expose tcmalloc_release_rate via setParameter  * SERVER-43210 jstests/sharding/kill_sessions.js doesn't wait for operation to be killed.  * SERVER-43246 Add a log line for when a cursor is reaped due to logical session cleanup  * SERVER-44568 Update the embedded version of the server's timezone data file to tzdb-2019c  * SERVER-44828 Correct logic to re-compute dependencies after absorbing a $sort into query layer  * SERVER-45104 [v3.6] Fix spill_to_disk.js  * SERVER-45372 Blacklist versions without majority read concern for change_streams_update_lookup_shard_metadata_missing.js  * WT-4636 Fix strace in syscall test  * WT-5042 Reduce configuration parsing overhead from checkpoints  * WT-5120 Checkpoint hangs when reconciliation doesn't release the eviction generation  * WT-5135 Change lookaside file inserts to use cursor.insert  * WT-5218 Improve eviction to differentiate between clean and dirty pages with WT_CACHE_EVICT_NOKEEP readgen  * WT-5239 Fix syscall failure about metadata file open  * WT-5247 Ensure that only idempotent modify operations are logged  * WT-5277 Cursor key out-of-order detected in the lookaside file  * WT-5297 syscall.py fail on mongodb-4.0 \n3.6.16 CHANGELOG  SECURITY \nSERVER-44320 Allow zoned sharding commands to be authorized via actiontypes \nSHARDING \nSERVER-42914 Implement random chunk selection policy for balancer for use in concurrency_*_with_balancer workloads \nREPLICATION \n * SERVER-43193 Always disable document validation on secondaries  * SERVER-44675 server_status_metrics.js fails due to racy repl.buffer.count metric in serverStatus \nQUERY \nSERVER-44571 Documents involved in SERVER-44050 corruption scenario cannot be updated or deleted after upgrade \nSTORAGE \nSERVER-44584 (3.6) Rewriting updates as modifications incorrectly considers logging state \nBUILD AND PACKAGING \n * SERVER-37769 Platform Support: Add Community & Enterprise SLES 15 x64  * SERVER-44392 Platform Support: Remove zSeries ubuntu 16.04 \nINTERNALS \n * SERVER-35463 Mark listCommands as pre-auth  * SERVER-38002 Upgrade Pcre to at least 8.42\n\n  * SERVER-42961 Fix Split Horizon testing on SLES  * SERVER-43085 Regenerate all testing certificates with SHA-256 instead of SHA-1  * SERVER-44140 Use signal processing without DSI  * SERVER-44319 Skip keyfile check in replica_sets/auth1.js on windows  * SERVER-44322 Fail gracefully when the storage engine has failed to initialize on v3.6  * SERVER-44651 Update signal processing version  * SERVER-44727 detect-changes should not be called via run-dsi \n3.6.15 CHANGELOG  SECURITY \n * SERVER-43653 Upgrade static OpenSSL to 1.1.1d  * SERVER-43751 Recompute compressor manager message parameters \nSHARDING \n * SERVER-10456 get cursor logic used to find docs to clone (in migration) is not same as in removeRange  * SERVER-34760 Retries of _configsvrShardCollection may not send setShardVersion to primary shard  * SERVER-36159 Log whenever the gossiped config server opTime term changes  * SERVER-36222 call to ReplSetTest.restart in shard_identity_rollback.js is racy  * SERVER-36315 After stepdown the CSRS dist lock manager keeps trying to unlock locks  * SERVER-38205 Optimize splitVector for the jumbo-chunk case  * SERVER-42783 Migrations don't wait for majority replication of cloned documents if there are no transfer mods  * SERVER-42793 Autosplit size tracker random initial value is too low  * SERVER-44182 Blacklist major_version_check.js from sharding_last_stable suite in 3.6 \nREPLICATION \n * SERVER-37846 writeConcern can be satisfied with an arbiter if the write was committed  * SERVER-38685 Startup warning if In-Memory SE is used and writeConcernMajorityJournalDefault is true  * SERVER-38994 Step down on SIGTERM  * SERVER-39310 Check canServeReadsFor in getMores  * SERVER-40335 Don't wait for election handoff in ReplSetTest.stopSet()  * SERVER-40355 rs.config that contains an _id greater than the number of nodes will crash  * SERVER-42155 Indicate term mismatches when readConcern times out  * SERVER-43109 Explicitly state nodes in awaitReplication in do_not_advance_commit_point_beyond_last_applied_term.js  * SERVER-43673 Blacklist toggle_feature_compatibility.js from backup_restore.js on 3.6  * SERVER-43708 Wait for nodes to become secondary in last_vote.js and slave_delay_clean_shutdown.js \nQUERY \n * SERVER-39019 $elemMatch $ne serialization is incorrect, doesn't roundtrip  * SERVER-40382 Add a serverStatus metric to report plan cache memory consumption  * SERVER-41863 Make sleep command check that server clock has advanced before returning  * SERVER-43074 Do not use a global variable to encode 'multikeyPath' information when writing out catalog documents  * SERVER-43699 Find $mod can result in UB \nAGGREGATION \nSERVER-43034 Special values handling is inconsistent in DoubleDoubleSummation \nSTORAGE \n * SERVER-42441 renameCollectionForApplyOps should always rename the target out of the way if it exists  * SERVER-42652 Fix issue with rename collection  * SERVER-43908 Modify IndexConsistency hash-map keys to avoid hitting an invariant on duplicate index keys in KeyString form for different indexes  * SERVER-44188 [4.0] validate_tests should remove index entries by RecordId obtained from the cursor WIREDTIGER \n * SERVER-35271 4.0 rc0 upgrade from 3.6.5 logs \"unsupported WiredTiger file version\" error  * SERVER-39004 Introduce a quota mechanism for the overflow file  * SERVER-40575 Improve error message when starting MongoDB 3.6 on unclean shutdown of MongoDB 4.0 \nOPERATIONS \n * SERVER-28604 Log when a cursor is reaped for exceeding the timeout  * SERVER-40537 Log error code 261 for TooManyLogicalSessions to the mongodb.log  * SERVER-42257 Add new shell startup banner  * SERVER-43081 validate should report when an index's 'multikeyPaths' are set but the 'multikey' flag is false\n\n \nBUILD AND PACKAGING \nSERVER-42911 Rebuild mongodb by building.md and failed to build due to ModuleNotFoundError: No module named 'Cheetah' with MSVC on windows \nINTERNALS \n * SERVER-25025 Improve startup time when there are tens of thousands of collections/indexes on WiredTiger  * SERVER-42075 Add DSI module to perf.yml  * SERVER-42178 Disable Split Horizon HOSTALIAS testing on SLES  * SERVER-42706 Test seeding secondary without sessions or transactions table  * SERVER-42866 Trigger a logical session cache refresh on all shards in the ShardedCluster test fixture before running test  * SERVER-42953 ttl_repl_secondary_disabled.js should prevent elections  * SERVER-43151 Error in aggregation assertion at value.cpp:1368  * SERVER-43240 Add DSI and mongo-perf modules to MMAP build variants in v4.0 mongo  * SERVER-43634 Report different errors for missing scons and fail to import scons  * SERVER-43922 ldap_authz_lib.js uses runNonMongoProgram on 3.6  * SERVER-44064 Perform explicit cast on MessageCompressorManager parameter  * SERVER-44183 Failure to listen on an asio socket should be fatal  * SERVER-44184 De-duplicate resolved addresses on v3.6  * SERVER-44312 Specify evergreen auth in performance tests for signal processing  * WT-4869 Stop adding cache pressure when eviction is falling behind  * WT-4881 Soften the restrictions on re-entering reconciliation  * WT-4893 Fix a race between internal page child-page eviction checks and cursors in the tree  * WT-4898 Don't allow the eviction server to reconcile if it's busy  * WT-4956 Handle the case where 4 billion updates are made to a page without eviction  * WT-4957 Revert part of a change about when pages are queued for urgent eviction  * WT-5050 Assertion failure during urgent eviction of metadata page \n3.6.14 CHANGELOG  SECURITY \n * SERVER-38945 SSL performance regression  * SERVER-41069 Ability to disable authorization via x509 extensions  * SERVER-41441 Upgrade static OpenSSL to 1.1.1c \nSHARDING \n * SERVER-26531 jumbo flag in chunk can be cleared when the shard that owns it donates a chunk  * SERVER-36394 mongos should reset chunk size tracking information when autosplit = false or splitvector returns too few split points  * SERVER-36443 Long-running queries should not cause a build-up of unused ChunkManager objects  * SERVER-36469 shard_kill_and_pooling isn't querying the shard it's supposed to  * SERVER-41859 logical_time_metadata.js shouldn't assume lastApplied opTime equals $clusterTime  * SERVER-41867 CatalogCache::_scheduleDatabaseRefresh/_scheduleCollectionRefresh can try to grab _mutex it already owns  * SERVER-41869 Reverse mutex acquisition order in CatalogCache::_scheduleCollectionRefresh  * SERVER-42793 Autosplit size tracker random initial value is too low \nREPLICATION \n * SERVER-38659 Race condition in IsMasterIsFalseDuringStepdown unittest  * SERVER-40336 ReplicationCoordinatorImpl::_random isn't robust to replica set members being started at the same time  * SERVER-41036 Make ReadWriteAbility::_canAcceptNonLocalWrites AtomicWord<bool> to prevent torn reads.  * SERVER-42055 Only acquire a collection IX lock to write the lastVote document \nQUERY \n * SERVER-40134 Distinct command against a view can return incorrect results when the distinct path is multikey  * SERVER-41065 Make agg evaluate() thread safe by passing 'Variables' as a parameter  * SERVER-41829 findAndModify ignores filter expressions that are not objects \nSTORAGE \nWIREDTIGER \n * SERVER-41913 Avoid in-place modify operations for logged collections \nOPERATIONS \nSERVER-36099 FTDC for mongos is unworkably large for large installations \nBUILD AND PACKAGING \n * SERVER-36043 systemd unit for mongod starts before multi.user target  * SERVER-40563 Our init scripts check /proc/[pid]/stat should validate that (${procname}) is the process' command name.  * SERVER-41424 scons should fail early if invoked with wrong Python interpreter\n\n  * SERVER-41743 Update MSI inclusions for 3.6  * SERVER-42089 Platform Support: Remove Enterprise RHEL 6.7 zSeries from 3.4 and 3.6  * SERVER-42109 Add Ubuntu 14.04 to v3.6  * SERVER-42233 Bump Windows package dependencies  * SERVER-42603 Recent service file change may cause cyclic dependencies \nINTERNALS \n * SERVER-26626 In external_auth_WT JSTests, ensure that Saslauthd has fully started before continuing test  * SERVER-35067 Blacklist explain2.js from retryable writes jscore stepdown suite  * SERVER-37984 Upgrade yaml-cpp >= 0.6.0  * SERVER-38141 Incorrect usage of MONGO_CONFIG_HAS_SSL_SET_ECDH_AUTO prevents enabling of elliptic curve auto negotiation  * SERVER-39642 Decrement egress counter when scoped connection not returned to the pool  * SERVER-40156 Initial implementation of Split Horizon  * SERVER-40643 Add tests for replSetConfig and replSetInitiate with Split Horizon  * SERVER-40645 Add tests for SNI/TLS behavior of Split Horizon  * SERVER-40923 Remove npm test command from \"run jstestfuzz\" Evergreen function  * SERVER-40924 Add Evergreen task to sanity check fuzzer can parse JavaScript tests  * SERVER-41004 killSessions command can return CursorNotFound error for successful kill  * SERVER-41016 Calculate deltas for updates to large documents  * SERVER-41075 Remove 'mongo/repl/replication_coordinator.h' from op-context inclusion path  * SERVER-41148 FTDC is calling boost::filesystem without passing an error_code parameter  * SERVER-41184 Add metrics to indicate the amount of sharded updates done with only _id in query  * SERVER-41349 Add a log statement for slow dns resolution  * SERVER-41401 patch_files.txt doesn't distinguish between enterprise and community files  * SERVER-41442 Fix second race in umask.js  * SERVER-41546 SysV init scripts should verify PIDfile exists instead of failing  * SERVER-41624 Fix Split Horizons on Legacy Transport in v3.6  * SERVER-41680 Propagate ${branch_name} Evergreen expansion to fuzzer invocation  * SERVER-41753 Fix indexc.js to not depend on the clock always moving forward  * SERVER-41789 sys-perf: Use bootstrap.overrides in bootstrap.yml config file  * SERVER-41833 Update README license text  * SERVER-41967 Symlink /data to Z instead of C  * SERVER-42178 Disable Split Horizon HOSTALIAS testing on SLES  * SERVER-42228 LoggerRuntimeConfigError exceptions can lead to background dbhash thread running until Evergreen task times out  * WT-4352 Resolve birthmarks during eviction in more cases  * WT-4421 Add a way to calculate modify operations  * WT-4693 WT_CONNECTION::reconfigure should not require quiescence when downgraded  * WT-4706 Add a statistic to track the lookaside table size  * WT-4750 Sweep can remove active lookaside records when files are closed and re-opened  * WT-4759 Save a copy when an old overflow value is discarded  * WT-4768 Inconsistent data with lookaside eviction followed by sweep  * WT-4769 Don't discard active history for empty pages  * WT-4776 Modify operations should be equivalent to updates  * WT-4794 Mark lookaside history resolved in all paths  * WT-4803 Implement file_max configuration for Cache Overflow mechanism  * WT-4823 Add check for uninitialised lookaside resources  * WT-4848 Fix perf regression when calculating differences \n3.6.13 CHANGELOG  SECURITY \n * SERVER-40226 Upgrade static OpenSSL to 1.1.1b  * SERVER-40393 Disable SSL_MODE_RELEASE_BUFFERS in ASIO \nSHARDING \n * SERVER-36004 SessionUpdateTracker should ignore no-op entries for pre/post image oplogs  * SERVER-36457 mongos_rs_shard_failure_tolerance.js test should assert that the movePrimary command succeeds  * SERVER-39420 Remove in-memory boolean to indicate config.server.sessions collection set up  * SERVER-40136 The background key generator can remain disabled on FCV upgrade after a downgrade \nREPLICATION\n\n  * SERVER-37065 Report ismaster:false in isMaster command response immediately upon learning of a new term  * SERVER-39831 Never update commit point beyond last applied if learned from sync source  * SERVER-40628 Initial sync could fail under replsetprio1.js settings  * SERVER-40788 Improve logging around replication catchup  * SERVER-40977 mmapv1 and ephemeralForTest variants should exclude tests with requires_majority_read_concern tag  * SERVER-41081 do_not_advance_commit_point_beyond_last_applied_term.js must wait for Node E to reach stopReplProducerOnDocument failpoint  * SERVER-41247 do_not_advance_commit_point_beyond_last_applied_term.js must not run with periodic noop writer on  * SERVER-41342 read_committed_stale_history.js must perform continuous writes to ensure majority commit point propagates to secondary \nSTORAGE \n * SERVER-16571 Use Actual Memory Constraint vs. Total System Memory When They Differ  * SERVER-30356 Improve error reporting for validation  * SERVER-41213 Unique background index builds may produce inconsistent keys \nOPERATIONS \n * SERVER-39820 Add the client IP address to the successful authentication log message  * SERVER-40866 Use JSON.stringify() to serialize string in tojson() \nBUILD AND PACKAGING \n * SERVER-37765 Platform Support: Remove Ubuntu 14.04  * SERVER-37774 Platform Support: Remove Enterprise Ubuntu 16.04 PPCLE  * SERVER-40242 Update banner inclusions for vendored tools  * SERVER-40491 Update Debian 8 image for package test \nINTERNALS \n * SERVER-37382 printShardingStatus.js races with ShardingUptimeReporter thread  * SERVER-38984 Attach IDs to users  * SERVER-39746 System-Performance: Write project in runtime.yml  * SERVER-40052 Shutdown in mongos can trigger BatchWriteOp invariant  * SERVER-40165 generate wtimeout deadlines with the precise clock  * SERVER-40514 Race between BufferedHandler.close() and BufferedHandler.flush() leads to resmoke.py hanging with --log=buildlogger  * SERVER-40922 Add npm install command to \"run jstestfuzz\" Evergreen function  * SERVER-40932 Increase timeout for shutting down the ServiceExecutor  * SERVER-41088 Propagate Evergreen order field to Cedar for system perf  * SERVER-41103 Fix fuzzer task invocations  * SERVER-41169 Most powercycle testing for Linux was removed from Evergreen  * TOOLS-2068 mongodump oplog delay \n3.6.12 CHANGELOG  SHARDING \n * SERVER-35219 Regain MongoDB balancer performance with sessions  * SERVER-39847 Migrating session info can trigger fassert when destination shard has transaction history truncated by oplog  * SERVER-39932 SessionRuntimeInfo memory is not being released \nREPLICATION \n * SERVER-37255 replSetReconfig with concurrent election can trigger invariant  * SERVER-38722 CollectionCloner should handle QueryPlanKilled on collection drop  * SERVER-39490 opWriteConcernCounters can cause undefined behavior due to overflow  * SERVER-39641 Blacklist max_time_ms.js from read concern linearizable passthrough  * SERVER-39758 double_rollback.js should reliably wait for node 0 to complete rollback  * SERVER-40039 Increase assert.soon timeout in initial_sync_invalid_index_spec.js \nQUERY \nSERVER-38764 External sorter should use 64-bit integers for file offsets \nAGGREGATION \n * SERVER-39394 Views collation check should traverse through nested $lookup pipelines  * SERVER-39487 Aggregation operator $sqrt output for NumberDecimal is the same as $exp \nSTORAGE \n * SERVER-17010 Reduce file handle usage in File based Sorter  * SERVER-39773 decrease number of updates in rollback_wt_cache_full.js \nOPERATIONS \n * SERVER-37722 Quiet mode does not suppress connection end events  * SERVER-40131 Incorrect format string in Windows stacktrace generator.  * SERVER-40259 Include third party notices for ASIO and variant \nBUILD AND PACKAGING \nSERVER-36622 Package tests fail for newer Ubuntu \nTOOLS\n\n \nINTERNALS \n * SERVER-36231 Mongos write path doesn't look for retryable write concern errors  * SERVER-36437 The dbstats command should lock the database in MODE_IS instead of MODE_S  * SERVER-36750 blacklist memory.js on ppc64le in memory variants  * SERVER-37620 Improve watchdog_test reliability  * SERVER-38644 Guard against ephemeral files disappearing during umask test  * SERVER-38697 Powercycle kill by PID might try to kill the wrong process  * SERVER-38789 Accept connection failure on Windows in ssl_alert_reporting.js  * SERVER-39056 Further refine readWriteAnyDatabase  * SERVER-39058 Synchronize user set modification in AuthorizationSession with Client  * SERVER-39822 Improve watchdog_test.exe test reliability  * SERVER-40063 jstestfuzz_sharded_continuous_stepdown.yml is running with a 1-node CSRS on the 3.6 branch  * SERVER-40076 Tag JavaScript tests with the reason they're unable to run in Atlas  * WT-4324 Ensure checkpoints rewrite pages with data in the future  * WT-4615 Sync backup file before returning backup cursor \n3.6.11 CHANGELOG  SECURITY \nSERVER-36606 Remove size limits on BSON audit events \nSHARDING \n * SERVER-9043 Link flushRouterConfig on the config server and shards  * SERVER-31156 Admin command to update the chunk metadata for only one collection  * SERVER-36863 query_config.js assumes that config.mongos is always present  * SERVER-36901 sh.status() \"Last reported error:\" is actually 5th Latest error message  * SERVER-37339 Sharding state is set to initialized on Grid before sharding components are fully initialized  * SERVER-37624 Sessions never expire when you change value of localLogicalSessionTimeoutMinutes  * SERVER-38192 Ignore duplicate key error from refreshLogicalSessionCacheNow  * SERVER-38344 Early release of distributed database locks during initial collection sharding results in migration/split failures  * SERVER-38387 agg_out.js concurrency workload always fails to shard the output collection  * SERVER-38784 Blacklist verify_sessions_expiration_sharded.js from the sharding_last_stable_mongos_and_mixed_shards suite.  * SERVER-39030 Wrong info logged by splitVector \nREPLICATION \n * SERVER-32146 Log slow oplog entry application  * SERVER-35608 Invariant that term from lastAppliedOptime is never greater than our current term  * SERVER-37274 commands_that_accept_wc tests can give secondaries priority zero  * SERVER-37910 Create new serverStatus metric for number of operations applied on a secondary that's incremented at batch boundaries  * SERVER-37915 Replication doesn't update opsCounterRepl command on secondaries  * SERVER-38200 Fix ./jstests/replsets/initial_sync_oplog_hole.js to prevent using stale heartbeat info in the test.  * SERVER-38647 backup_restore_rolling.js can fail due to stepdown  * SERVER-38998 Create serverStatus metrics for readConcern and writeConcern  * SERVER-39142 ServerWriteConcernMetrics must lock _mutex before modifying members  * SERVER-39267 Guard opWriteConcernCounters behind flag  * SERVER-39286 server_write_concern_metrics.js must use requires_journaling tag \nQUERY \n * SERVER-38275 Handle explains without namespaces  * SERVER-39472 Backport changes to apply_ops_concurrent_non_atomic.js to account for HMAC key inserts in 3.6 and 4.0. \nAGGREGATION \n * SERVER-31098 Wrong ns in system.profile for aggregation query  * SERVER-39109 mongod crash: Invariant failure !_exec src/mongo/db/pipeline/document_source_cursor.cpp 295  * SERVER-39166 $graphLookup should force a pipeline to split in sharded cluster  * SERVER-39322 Backport timelib fix for incorrect snprintf invocation with static buffer \nSTORAGE \n * SERVER-35732 Hyphenated database names do not work with dbStats.fsUsedSize / TotalSize  * SERVER-38779 Build a mechanism to periodically cleanup old WT sessions from session cache  * SERVER-38801 Cope with large BSON blobs in queryable's listDirectory call. \nOPERATIONS\n\n \nBUILD AND PACKAGING \n * SERVER-33958 When using thin archives the --cache=nolinked mode should push thin archives to the cache  * SERVER-38416 Upgrade static OpenSSL to 1.1.1a  * SERVER-39483 Stripped binaries and .debug files should not be cached \nTOOLS \n * TOOLS-2030 mongodump does not dump system.js collections  * TOOLS-2109 Build Tools with Go 1.11 \nINTERNALS \n * SERVER-31755 Raise intermediate $lookup document size to 100MB, and make it configurable  * SERVER-35620 downloaded file integrity: use shasum instead of python script  * SERVER-38748 Background indexes created through applyOps should run on the command thread  * SERVER-38954 Increase query execution time for the test case in max_time_ms.js that expects it to hit the time limit  * SERVER-39009 Make bulldlogger credentials location configurable via environment variable  * SERVER-39031 Make SCons default --jobs to CPU count  * SERVER-39331 Remove StressTest from network_interface_asio_integration_test  * TOOLS-1906 Ignore mongorestore error \"x509 certificate routines:X509_STORE_add_cert:cert already in hash table\"  * TOOLS-2158 mongodump failing on Windows with \"error opening system CA store: Access is denied.\"  * TOOLS-2167 Add CGO flags for building on Windows  * TOOLS-2168 Add CGO flags for building on MacOS  * TOOLS-2210 Build with tools with Address Space Layout Randomised (ASLR) flags enabled  * WT-4483 Improve caching of small updates to large values \n3.6.10 CHANGELOG  SECURITY \n * SERVER-35212 URI connection does not default to admin database when no authSource is specified  * SERVER-37687 Capture libldap ErrorString in bind failure \nSHARDING \n * SERVER-36965 Ensure setUp writes in safe_secondary_reads_single_migration_suspend_range_deletion.js get propagated to the shard secondary  * SERVER-37051 ShardServerCatalogCacheLoader does not check the internal term after reading from the task queue  * SERVER-37080 Implement tuneable batch size for chunk migrations  * SERVER-37511 Logical session reaper and refresh threads should set up sessions collection immediately  * SERVER-37616 Implement tuneable batch size for the rangedeleter  * SERVER-37902 recovering_slaveok.js should assert all writes it makes succeed  * SERVER-37932 Remove erroneous part of coll_epoch_test1.js that attempts to test something that shouldn't work, but due to a bug in the test, the test passes  * SERVER-38050 The range deleter doesn't validate it is still operating on the same collection after the deletion loop  * SERVER-38062 Add assert.commandWorked() to commands in read_pref_cmd.js  * SERVER-38196 Coverity analysis defect 105128: Unchecked return value  * SERVER-38371 Session catalog migration will skip later sessions if a different earlier session encounters new transaction \nREPLICATION \n * SERVER-37500 Blacklist geo_s2ordering from replica_sets_kill_primary_jscore_passthrough suite  * SERVER-37557 Add startup warning about enableMajorityReadConcern and usage of arbiters  * SERVER-37935 Remove read concern \"majority\" overrides for change streams test suites  * SERVER-38024 initial_sync_oplog_hole test should be tagged as requiring document locking  * SERVER-38476 Increase timeout for clean_shutdown_oplog_state.js, again \nQUERY \n * SERVER-35455 QueryPlannerAccess should hold owned pointers by unique_ptr rather than raw pointer  * SERVER-37385 Change max_time_ms.js to tolerate 'Interrupted' error codes  * SERVER-38070 Infinite loop in aggregation expression  * SERVER-38164 $or pushdown optimization does not correctly handle $not within an $elemMatch  * SERVER-38601 Add regression test for memory leak during planning \nAGGREGATION \n * SERVER-37182 Different values when referencing whole object vs. a field of that object after $arrayToObject  * SERVER-37200 $match stage following $listSessions not working against mongos  * SERVER-37750 Optimized $sample stage does not yield  * SERVER-38843 Mapreduce should not create collections with duplicate IDs \nJAVASCRIPT\n\n  * SERVER-35061 Javascript sleep should always emit an error when interrupted \nSTORAGE \n * SERVER-29825 Do not allow rename from unreplicated to replicated DB or vice-versa  * SERVER-36873 ReplicationCoordinatorExternalStateImpl::shutdown() must not hold _threadMutex while waiting for _taskExecutor  * SERVER-36968 Rebuild interrupted indexes before checking AuthZN index presence  * SERVER-37408 Add afterClusterTime to initial sync collection scans  * SERVER-37524 In-Memory Storage Engine With Storage Watchdog Crashes The Server  * SERVER-37862 Reduce update ops in initial_sync_wt_cache_full.js  * SERVER-37930 Add test coverage for createIndexes inside nested applyOps  * SERVER-37931 lock_stats_suboperation_logs.js should ignore noise lock stats  * SERVER-38434 queryableBackupMode and wiredTigerEngineConfigString flags are incompatible  * SERVER-38498 decrease number of updates in rollback_wt_cache_full.js \nOPERATIONS \n * SERVER-33469 Make syslog log lines consistent with mongod log lines  * SERVER-35485 Mongo Shell does not accept compressors connection string argument  * SERVER-36977 Initial mongod.log is created using umask vs mode 600 \nBUILD AND PACKAGING \n * SERVER-35936 MongoDB Community Windows installer fails to install Compass  * SERVER-38421 Requirement on cryptography should reflect what is in the toolchain  * SERVER-38726 Add stable toolchain variables files \nTOOLS \n * TOOLS-1709 Set build version and git revision using -ldflags  * TOOLS-2149 Configure build outside Evergreen \nINTERNALS \n * SERVER-32424 Use WiredTiger cursor caching  * SERVER-34770 Retry on JavaScript execution interruptions in stepdown suites  * SERVER-35768 gssapiServiceName URL parameter does not work  * SERVER-36060 Make unit tests build cleanly on clang-6  * SERVER-36817 replSetFreeze command run by stepdown thread may fail when server is already primary  * SERVER-37120 Turn off linux-replSet-initialsync-logkeeper Build Variant for 3.4 and 3.6 branches  * SERVER-37241 Add testing to verify proper expiration of sessions in the sessions collection  * SERVER-37289 Use authenticated client to run the refreshLogicalSessionCacheNow command in resmoke sharded cluster fixture  * SERVER-37391 plan_cache_index_create.js should wait for index build start, rather than just createIndexes command start  * SERVER-37490 Increase the ConnectTimeout for powercycle  * SERVER-37562 Reduce all IX locks to IS locks in SessionsCollectionRS  * SERVER-37678 Update linter to enforce SSPL in header files  * SERVER-37913 coll_epoch_test1.js does not wait for the config server to replicate after dropping collections.  * SERVER-37916 Make legacy transport layer Sockets log hostnames in TLS version negotiations  * SERVER-38055 Mongod servers started through the shell do not respect TestData.enableMajorityReadConcern  * SERVER-38159 Blacklist umask nopassthrough test on mmapv1  * SERVER-38178 Buffer Overflow in data_builder.h when using terminated StringDatas  * SERVER-38230 Put secondary_reads_passthrough task in the right build variants  * SERVER-38303 Temporarily handle ipv6 failures due to s390x machine without ipv6 enabled  * SERVER-38390 Set requiresAuth to false for certain commands  * SERVER-38415 checkLog.containsWithCount does not reset count before each loop of assert.soon  * SERVER-38616 LDAPArrayIterator behaves incorrectly when initialized with an empty array  * TOOLS-1566 Should not include \"ssl\" tag for Linux 64 build  * TOOLS-1742 import cycle between util and testutil  * TOOLS-1996 Allow building tools from inside an ordinary GOPATH  * TOOLS-2099 Tools jstests failing on replica set shutdown  * TOOLS-2155 Set version/git-commit via ldflags in Evergreen and Server Evergreen  * TOOLS-2157 Update server vendoring  * WT-4333 WiredTiger cursor cache doesn't handle all possible locked handle states  * WT-4340 The cursor caching layer can incorrectly release too many handle locks  * WT-4343 Unlock when sleeping to allow other log threads to make progress  * WT-4411 Added connection statistic for current total of cached cursors\n\n  * WT-4438 Use more accurate statistics for cursor cache totals \n3.6.9 CHANGELOG  SECURITY \n * SERVER-35418 Allow specifying CAs for incoming and outgoing connections separately  * SERVER-37135 TLSVersionCounts needs to track and report TLS 1.3 \nSHARDING \n * SERVER-29160 Sharding commonly uses write concern timeouts of 15 seconds and these are timing out in migration related operations and causing BFs  * SERVER-31563 Reevaluate not_allowed_on_sharded_collection_cmd.js testing  * SERVER-31892 moveChunk with waitForDelete doesn't wait for majority write concern  * SERVER-35222 Crash on the config server at expired session cleanup  * SERVER-35238 Drop the collection after removing the partially written chunks for mapReduce in mongos_manual_intervention_actions.js  * SERVER-35763 lastWriteDate field can get out of sync between repl set nodes during migration  * SERVER-36831 LogicalSessionCache on mongos does not correctly report active operations  * SERVER-36850 Add a replication passthrough suite to detect errors in LogicalSessionsCache  * SERVER-37330 Add sharded passthrough suites to detect errors in LogicalSessionCache  * SERVER-37430 Destroy sharding task executors and AsyncRequestSenders after the PeriodicRunner is destroyed in mongod shutdown  * SERVER-37496 The balancer shouldn't register a shutdown task after the process is multithreaded  * SERVER-37631 Disable logical sessions if FCV is 3.4  * SERVER-37657 Report the offending oplog entries if a batch contains non-increasing transaction numbers  * SERVER-37735 [3.6] Ensure the full logical session ID is included in commands sent by the ShardingTaskExecutor \nREPLICATION \n * SERVER-20845 re-add replSetReconfig to auditing suite  * SERVER-25175 listIndexes shouldn't include in-progress background indexes  * SERVER-33383 Execution of replSetStepDown can race with unconditional stepdown via heartbeat response  * SERVER-34362 ReplicaSetCoordinatorExternalStateImpl::_dropAllTempCollections() needs to log failed db.  * SERVER-34868 A cursor with the \"exhaust\" option does not return documents inserted in the same txn  * SERVER-35239 AwaitData cursor must handle getMore from client with higher lastKnownCommittedOpTime  * SERVER-36503 Skip dry-run election during election handoff  * SERVER-36694 Do not hand off elections when in PV0  * SERVER-36746 A failed step down attempt shouldn't unconditionally reset LeaderMode to kMaster  * SERVER-36978 TaskRunner must ensure Client is initialized for thread before running tasks  * SERVER-37118 Coverity analysis defect 105000: Arguments in wrong order  * SERVER-37147 sessions_collection_auto_healing.js should use 2 node replica set  * SERVER-37152 Increase write concern timeouts for writes expected to succeed in tags.js  * SERVER-37227 Reintroduce enableMajorityReadConcern:false server parameter  * SERVER-37396 make rollback_auth.js more robust \nQUERY \n * SERVER-13946 Consider putting skip stages below fetch stages  * SERVER-32943 Query during background index build can lead to suboptimal cached plan  * SERVER-36435 Increase maxTimeMS timeout in awaitdata_getmore_cmd.js  * SERVER-36944 applyOps does not permit unknown field names when creating a v:1 index  * SERVER-36951 applyOps should work with a createIndexes command without a UUID  * SERVER-37058 Update with numeric field names inside an array can cause validation to fail  * SERVER-37132 Negation of $in with regex can incorrectly plan from the cache, leading to missing query results \nAGGREGATION \nSERVER-36993 mongod crash: Invariant failure indexedOr src/mongo/db/query/index_tag.cpp 237 \nJAVASCRIPT \nSERVER-37126 Invoke runSafely for all external implscope methods \nSTORAGE \n * SERVER-26854 LockStats for sub-operations should not include time for previous sub ops  * SERVER-34713 Progressively declining dropDatabase performance  * SERVER-34866 Blacklist/unblacklist tests in secondary_reads_passthrough suite  * SERVER-35657 Do not delay journal flushes when operations are waiting for oplog visibility  * SERVER-36879 write regression test for stuck cache issue during rollback\n\n  * SERVER-36969 initial_sync_wt_cache_full.js takes too long to complete on slow hosts  * SERVER-37313 FTDC collection blocked during foreground index build on secondary  * SERVER-37618 Capture all the logs in lock_stats_suboperation_logs.js  * SERVER-37749 replSetResizeOplog command does not validate argument \nOPERATIONS \n * SERVER-32064 A logical session ID should be included in all command requests from the mongo shell  * SERVER-33606 mongo shell startSession() should fail if the server doesn't support logical sessions  * SERVER-34864 String-valued fields should not participate in schema change detection in ftdc \nBUILD AND PACKAGING \n * SERVER-35737 install_compass fails on MacOS  * SERVER-37067 Upgrade static OpenSSL to 1.1.0i  * SERVER-37651 Update license files for MongoDB Community Edition  * SERVER-37754 Duplicate license headers in IDL files \nTOOLS \n * SERVER-30997 mongo cli --password is masked, but not when using mongodb:// connection string  * TOOLS-2102 Mongorestore does not check for errors decoding the oplog.bson file \nINTERNALS \n * SERVER-18985 setParameter should log at level 0  * SERVER-31570 Adjust mongobridge port allocations for easier debugging  * SERVER-32369 Invariant incorrectly with message stringifies its argument  * SERVER-33077 checkReplicatedDataHashes should provide more information if a collection doesn't exist  * SERVER-33470 Log archival message, even if successful, in hook_test_archival.py  * SERVER-34248 Investigate why function_string_representation.js started failing  * SERVER-34755 ignore missing collections when shutting down backup_restore.js  * SERVER-34916 Blacklist core/shell_connection_strings.js from retryable_writes_jscore_stepdown_passthrough  * SERVER-34986 CIDR Block That is Exempt from maxConns  * SERVER-35233 Powercycle remote collection validation does not skip views  * SERVER-35235 plan_cache_index_create.js should build initial index set in the foreground  * SERVER-35570 Improve robustness of backup_restore.js around dropping test database  * SERVER-35800 resmoke.py should retry getting a build_id and test_id from logkeeper  * SERVER-35818 provide a stdx::variant  * SERVER-36162 Powercycle - ensure internal crash command has been executed on the remote host  * SERVER-36250 Add support for optionally logging specific negotiated TLS versions  * SERVER-36301 build WT with HAVE_NO_CRC32_HARDWARE on RHEL 6.7 s390x  * SERVER-36451 ContinuousStepdown with killing nodes can hang due to not being able to start the primary  * SERVER-36721 list_local_sessions.js cannot run concurrently with refreshLogicalSessionCacheNow  * SERVER-36732 Wait for all secondaries to be up and electable in election handoff jstests  * SERVER-36747 Add a small time delay to \"jstests/ssl/ssl_client_certificate_warning_suppression.js\"  * SERVER-36756 Log the githash of the 10gen/jstestfuzz repository when the fuzzer's self-tests fail  * SERVER-36783 Run the secondary_reads_passthrough task of enterprise-rhel-62-64-bit-inmem on rhel62-large  * SERVER-36842 Core dump not generated after invariant failure in powercycle hosts  * SERVER-36919 Add server setParameter tlsWithholdClientCertificate (bool)  * SERVER-36964 Prevent secondaries in SessionsCollectionRS from attempting to set up the sessions collection.  * SERVER-36986 list_local_sessions.js expects non-existent session to exist when running concurrently with LogicalSessionsCache refresh  * SERVER-36987 ChunkVersion::minorVersion truncates to 16 bit  * SERVER-36988 awaitdata_getmore_cmd.js times out when run concurrently with the LogicalSessionCache refresh suite  * SERVER-37041 Update eval blacklists on older branches to account for differences in tests across branches  * SERVER-37064 Wrap \"mongod_flags\" onto multiple lines for readability  * SERVER-37130 Add TLS version counting to mongos  * SERVER-37149 Remove deadlock in ScheduleBeforeStartupTest\n\n  * SERVER-37393 Fix destructor race in ReplicaSetMonitorManager  * SERVER-37410 Add standalone passthrough suites to detect errors in LogicalSessionCache  * SERVER-37424 Sys-perf: change v4.0 branch batch time to once a week  * SERVER-37425 Longevity tests -- increase batch time to once a year  * SERVER-37437 mongo longevity tests failing  * SERVER-37467 Have collect_resource_info.py recover from transient errors.  * SERVER-37477 Disable TIG daily cron for update_test_lifecycle  * SERVER-37595 [3.6] Handle WT_TRY_SALVAGE error code gracefully  * SERVER-37599 Log exit code of shell-spawned processes  * SERVER-37701 Make SessionUpdateTracker include the uid portion of LogicalSessionId when tracking  * SERVER-37816 [3.6] Add --excludeWithAnyTags=requires_wiredtiger to ephemeralForTest variants  * WT-3276 Add recover=salvage to recover from a corrupted log file  * WT-3735 Add a workgen workload that generates a lot of page splits  * WT-3736 Add statistics to measure contention on lookaside cursor  * WT-3839 Document the undefined behavior when a range truncate overlaps with inserts  * WT-3856 Create a test that runs recovery to different points of time with schema operations  * WT-3879 Disallow checkpoint from evicting metadata pages  * WT-3894 Timestamp queue implementation and statistics improvements  * WT-3917 Enhance WT_CURSOR::reserve documentation around commit visibility  * WT-3943 Include full error message when a python test asserts  * WT-3955 Add verbose option to log more messages on error returns  * WT-3963 Add a schema intensive abort testing  * WT-3968 Use compression ratio to tune page sizes  * WT-4010 Simplify test/format timestamp handling.  * WT-4024 Fix a race between split and next/prev  * WT-4026 Add implementation for existing file extension configuration API  * WT-4048 Generalize timing_stress_for_test split functionality  * WT-4067 Enhance LSM to not pin as much history in cache  * WT-4090 Low priority reads  * WT-4101 Don't abort the eviction server during session verify when oldest_timestamp is held back  * WT-4104 Fix test/format failure during comparing data content with berkeley db  * WT-4111 Improve checkpoint scrubbing algorithm  * WT-4119 Avoid restarts updating / removing during a column store scan  * WT-4125 Ensure that subsequent checkpoints with stable timestamp don't read too much  * WT-4131 Rename lookaside to cache overflow  * WT-4133 Coverity 1393445, 1393446 Dereference before null check  * WT-4134 Rework assertion that we don't discard required history  * WT-4136 Add a new timing stress flag that yields during tree search  * WT-4138 Add an option to timeout waiting for space in the cache  * WT-4139 rename the cursor restart statistic to match implementation  * WT-4140 Cursor walk limits quick eviction page selection unnecessarily.  * WT-4141 Enhance checkpoint with timestamps to unblock eviction sooner  * WT-4144 Fix rollback_to_stable with lookaside history  * WT-4145 Only include the checkpoint timestamp during checkpoints  * WT-4146 Coverity 1393639, unused variable  * WT-4147 Log recovery should not ignore corruption outside of log records in a log file  * WT-4152 Save return value for later comparison in transaction code  * WT-4154 Surface the oldest read timestamp  * WT-4156 Add new wiredtiger_salvage top level API  * WT-4160 Restore performance when timestamps are not in use  * WT-4163 Lint  * WT-4168 Update upgrading documentation for 3.1.0 release  * WT-4169 Fix wt verify dump-pages failure  * WT-4171 Enabling tree walk timing stress causes excessive slowdown  * WT-4172 Add diagnostic hazard pointer checks in more places before freeing refs  * WT-4174 Do not access the lookaside file in rollback_to_stable when running with in_memory=true  * WT-4176 Expose a WT_SESSION.query_timestamp method  * WT-4177 Backup cursor open should force a log file switch  * WT-4178 Fixes for wt_btree_immediately_durable needed for in-memory  * WT-4179 Expose WiredTiger crc32c functions  * WT-4182 Use conservative approach for log checksum errors  * WT-4183 Extend verbose option to log more messages on error returns\n\n  * WT-4186 Log recovery should detect and report corruption within log records  * WT-4187 Coverity: unused value complaints  * WT-4188 Coverity: unchecked return value complaints  * WT-4189 Potential infinite loop in __async_flush_wait().  * WT-4191 Fix Coverity static analysis errors  * WT-4193 test/format snapshot-isolation search mismatch  * WT-4194 Improve fairness of eviction with multiple tables  * WT-4195 When encountering an illegal value, log the value that failed  * WT-4196 Make log corruption checking work regardless of the machine byte order  * WT-4198 Some supported MongoDB architectures don't support crc32 hardware  * WT-4199 Fix an incorrect report of log corruption  * WT-4201 Fix Coverity static analysis issues  * WT-4206 Fix error handling in cursor close routines  * WT-4207 Coverity #1394567: null pointer dereference  * WT-4208 tree walks can be interrupted by locked internal pages  * WT-4210 schema abort child process failing prematurely  * WT-4211 Add automated test for long running prepared transactions  * WT-4212 Update lookaside schema to handle prepared transactions  * WT-4213 Rename lock statistics that have redundant or misleading text  * WT-4215 Allow recovery of backup without salvage  * WT-4216 Use separate counters for page_swap yield and sleep  * WT-4218 Change eviction to evict prepared updates  * WT-4225 Automate a backup test that simulates volume snapshot via dd  * WT-4226 test/format LSM configurations can misconfigure prepare and timestamps  * WT-4229 Lint  * WT-4231 Fix ctags index of functions with attributes  * WT-4233 Change log corruption errors to warnings and truncate log  * WT-4234 Remove documentation mention of legacy tool statlog.py  * WT-4235 Fix workgen tracking of table state across workloads  * WT-4239 Don't allow checkpoints to perform insert-splits in the tree  * WT-4241 GNU-stack section should never be conditionally compiled out  * WT-4242 New log file extension Python test failure  * WT-4243 Fix lookaside sweep to not remove required entries  * WT-4246 Change transaction update list to support indirect references  * WT-4248 Fix checkpoints in schema_abort for slow machines  * WT-4249 Attempt to discard dirty page during verify operation  * WT-4251 Prepared updates cannot be discarded  * WT-4252 Btree debug functions can leak scratch buffers on error.  * WT-4253 Btree debug function to do blind reads doesn't handle row-store internal pages  * WT-4256 Loosen check during rollback_to_stable  * WT-4257 Don't assume timestamps from lookaside are aligned in memory  * WT-4259 Restore ref to the previous state rather than MEM when eviction fails  * WT-4261 Test salvage of out-of-sync metadata/turtle files  * WT-4262 Lock deleted children in eviction of internal pages \n3.6.8 CHANGELOG  SHARDING \n * SERVER-30841 Lower the amount of metadata refresh logging  * SERVER-33645 Determine if splitvector.js should be blacklisted from the retryable_writes_jscore_stepdown_suite  * SERVER-34204 Tailable cursor fails on getMore against a sharded cluster  * SERVER-34913 Blacklist splitvector.js from causally consistent suites  * SERVER-35092 ShardServerCatalogCacheLoader should have a timeout waiting for read concern  * SERVER-36332 CursorNotFound error in GetMore on a secondary with sessions  * SERVER-36741 ChunkManager::getShardIdsForRange invariants when max = MaxKey  * SERVER-36777 Excessive logging with sessions in FCV 3.4  * SERVER-37050 wait for replication in session_collection_auto_healing \nREPLICATION \n * SERVER-35941 Don't maintain full stable optime candidate list on secondaries in PV0  * SERVER-36128 ReplicationCoordinatorImpl::fillIsMasterForReplSet should return isMaster:false while in shutdown  * SERVER-36664 Use awaitReplication in read_committed_with_catalog_changes.js instead of getLastError  * SERVER-37010 Prevent unexpected elections in initial_sync4.js and initial_sync_rename_collection*.js \nQUERY \n * SERVER-26387 Replace noPassthrough/indexbg2.js with test that uses failpoints  * SERVER-34846 Covered index with collated field returns incorrect result when collation not involved in match or sort \nAGGREGATION\n\n \nSTORAGE \n * SERVER-34384 Passthrough test for secondary reads during oplog application  * SERVER-37002 dropping a collection with long index names via rename fails under MMAPv1 \nOPERATIONS \nSERVER-36479 Log redaction does not show planSummary when slow queries are logged \nBUILD AND PACKAGING \nSERVER-36884 Update curator version on stable branches \nINTERNALS \n * SERVER-32920 Avoid overriding read preference for the config server in passthrough tests.  * SERVER-34120 scoped connection not being returned to the pool  * SERVER-34144 Powercycle output improvements  * SERVER-34465 Add a testing parameter to choose a permanent sync source  * SERVER-34614 parallelTester should use a different connection for each new test  * SERVER-34923 Replace PeriodicRunnerASIO  * SERVER-35100 Do not log a Python stack trace when a hook dynamic test fails  * SERVER-35123 The periodic_runner_impl starts jobs immediately, unittests should reflect that  * SERVER-36001 Add requires_document_locking tag to noPassthrough/indexbg2.js  * SERVER-36149 Fix privileges on setFCV virtual namespace  * SERVER-36725 periodic_runner_impl_test should manually call tearDown  * SERVER-36743 Stop mongod before log ingestion in startup_logging.js  * SERVER-36947 Enable test commands for perf micro benchmarks  * SERVER-36980 Remove old aggregation fuzzer from evergreen  * WT-4218 Change eviction to evict prepared updates. \n3.6.7 CHANGELOG  SECURITY \n * SERVER-33857 Missing log redaction due to confusion with Command::redactForLogging()  * SERVER-35032 Non-SSL connections to requireSSL instances get non-informative log message in 3.6.4 \nSHARDING \n * SERVER-27725 Use batch insert when migrating chunks  * SERVER-32056 Increase maxExpireTime in tests to account for rounding  * SERVER-33237 Optimize the Range Deleter speed  * SERVER-33697 Provide sanity check on number of cached sessions  * SERVER-34843 Mongod can return operationTime greater than $clusterTime  * SERVER-34897 Introduce parameter to control whether MongoS should automatically retry failed find commands  * SERVER-35377 Operations on new clients get latest in-memory clusterTime as operationTime  * SERVER-35658 session migration is too sensitive to replica set primary elections  * SERVER-36041 Increase the size of changelog and actionlog  * SERVER-36132 Invariant that chunk migration is actually reflected in the metadata after successful commit  * SERVER-36232 Refresh after chunk migration commit may not see the committed metadata  * SERVER-36248 Do not reject sessions in FCV 3.4  * SERVER-36463 Bypass validation of a dummy signatures for isMaster on the unauthenticated connections \nREPLICATION \n * SERVER-28389 Pass CallbackCanceled error down to scatter-gather runner algorithm to process  * SERVER-32148 make NamespaceNotFound an acceptable error for emptyCapped and convertToCapped  * SERVER-32907 Turn heartbeat log verbosity back down in tests  * SERVER-33243 Improve logging when a node changes its sync source  * SERVER-34414 Creating a role with buildsIndexes:false node hits an fassert  * SERVER-34895 Stable timestamp can be set to timestamp not in oplog  * SERVER-34941 Stuck with cache full during recovery oplog application  * SERVER-34942 Stuck with cache full during oplog replay in initial sync  * SERVER-35058 Don't only rely on heartbeat to signal secondary positions in stepdown command  * SERVER-35113 Stable timestamp does not advance if lastApplied does not move forward, but all committed timestamp does, on single node RS  * SERVER-35200 Speed up failure detection in the OplogFetcher during steady state replication  * SERVER-35239 AwaitData cursor must handle getMore from client with higher lastKnownCommittedOpTime  * SERVER-35246 Ignore NamespaceNotFound errors when running collMod during checkReplicaSet in replsettest.js  * SERVER-35623 Send a replSetStepUp command to an eligible candidate on stepdown  * SERVER-35624 Enable election handoff by default and update affected tests  * SERVER-35754 Avoid network errors in stopSet() in catchup_takeover_one_high_priority.js  * SERVER-35766 Replication commands sent in candidate's new term can interrupt concurrent vote request\n\n  * SERVER-35962 buildindexes_false_with_system_indexes.js restarts nodes and should be tagged [requires_persistence]  * SERVER-36083 Add allowMajorityReadConcern:false evergreen variant  * SERVER-36225 Do not hold replication mutex when calling getMinValid  * SERVER-36234 Fix ./jstests/replsets/initial_sync_drop_collection.js to prevent using stale heartbeat info in the test. \nQUERY \n * SERVER-33245 mr.cpp can throw in an ON_BLOCK_EXIT handler when there's already an active exception  * SERVER-36239 MatchExpression parser query rule \nAGGREGATION \n * SERVER-35961 Remove uninitialized count variable in MapReduce command  * SERVER-35967 $sample with explain(true) hangs  * SERVER-36070 Aggregation with $out results in error when Auditing is enabled \nJAVASCRIPT \nSERVER-35986 Stop running eval command in parallel suite on older branches \nSTORAGE \n * SERVER-33191 Cache-full hangs on 3.6  * SERVER-34129 Don't hold database or collection locks during oplog truncation \nOPERATIONS \n * SERVER-34160 Mongo client run buffered command when terminated.  * SERVER-35180 Safeguard from setting operation session info values while in a direct client  * SERVER-35795 3.4 secondaries crashing after some time with 3.6 primary  * SERVER-36010 Change log messages for Windows stacktraces to use error() or severe() rather than log() \nBUILD AND PACKAGING \n * SERVER-35978 Fix lint on 3.6  * SERVER-36039 Support LibreSSL 2.7 on FreeBSD \nTOOLS \nTOOLS-2075 mongoreplay always replays to secondary \nINTERNALS \n * SERVER-26150 auth_utils.js authutils.assertAuthenticate should handle hangups better  * SERVER-32302 for-each loop over BSONObj is slower than explicit BSONObjIterator in MatchExpressionParser code  * SERVER-32371 features2.js should exclude clusterTime metadata  * SERVER-32639 Arbiters in standalone replica sets can't sign or validate clusterTime with auth on once FCV checks are removed  * SERVER-32792 Blacklist tests that assert on ismaster field of isMaster response in retryable_writes_jscore_stepdown_passthrough  * SERVER-33695 Include the loop name in the before and after recovery files in powertest.py  * SERVER-33841 Add hooks for the mutational fuzzer  * SERVER-34258 Error from mount_drives.sh on Windows  * SERVER-34558 Add SSL_version to client metadata logging  * SERVER-34793 Add call to BF suggestion server on failed task completion  * SERVER-35110 Log locations in set_feature_compatibility_version.js where fatal assertions/other errors are expected  * SERVER-35383 Increase electionTimeoutMillis for the ContinuousStepdown hook used in stepdown suites  * SERVER-35472 resmoke.py shouldn't fall back to stderr when logkeeper is unavailable  * SERVER-35668 Avoid moving concurrency tests to small instances on arm64  * SERVER-35724 Remote EC2 hosts which are not accessible via ssh should fail with system error  * SERVER-35858 Error in call BF Suggestion service  * SERVER-35861 Remove call to dashboard_gen.py in perf.yml  * SERVER-35946 Powercycle kill_mongod function should ensure the service is not in a running state  * SERVER-35993 read_concern_uninitiated_set restarts nodes so must not allow ephemeral storage  * SERVER-36223 Add hook for the fuzzer to not send lsid in the preamble on v3.6  * SERVER-36274 Re-enable --ycsb-throughput-analysis in sys-perf  * SERVER-36448 Disable election handoff in suites that use the ContinuousStepdown hook  * SERVER-36462 Add atlas user to runtime_secret.yml  * TOOLS-1991 Build tools with Go 1.10.1  * WT-4143 Use WiredTiger.turtle.set if it exists but WiredTiger.turtle does not \n3.6.6 CHANGELOG  SECURITY \n * SERVER-34822 RoleGraph update should ignore index creation on non-role collections  * SERVER-35322 external_auth tests should use unique kerberos credentials cache per test \nSHARDING\n\n  * SERVER-33081 Reset KeysCollectionManager during rollback properly  * SERVER-33327 Session::onMigrateCompletedOnPrimary should not update the lastWriteDate field  * SERVER-33538 mapReduce \"replace\" on a sharded output collection can lead to UUIDCatalog inconsistencies  * SERVER-33639 Concurrent writes against non-existent database can fail due to distlock acquisition timeout at createDatabase time  * SERVER-34363 addShard shard registry reload can cause mongos to terminate  * SERVER-34773 The TransactionReaper handler classes are not exception-safe  * SERVER-34833 Deadlock between the logical sessions' transaction reaper and MMAP V1 durability thread  * SERVER-34857 sharding_legacy_api_test is flaky due to reliance on the system clock  * SERVER-35006 Do not reset KeysCollectionManager  * SERVER-35345 Test that restarted mongoses are able to connect after FCV update  * SERVER-35609 Create initial chunk on primaryShard if only writing one chunk  * SERVER-35653 Revert initial chunk creation optimization  * SERVER-35745 _getNextSessionMods doesn't include oplogReplay flag to query new oplog created during migration \nREPLICATION \n * SERVER-5461 Add syncSourceHost field to replSetGetStatus output  * SERVER-29844 If node has a higher priority than the primary and is the most up to date, schedule the takeover that will execute faster.  * SERVER-31995 Logged initial sync statistics may exceed 16mb causing fassert  * SERVER-32382 Rollback can time out if oplog entries are large  * SERVER-32687 Ignore collection-not-found when fetching missing documents in sync_tail.cpp during initialSync  * SERVER-32935 improve exception handling in SyncTail::oplogApplication()  * SERVER-33638 CheckReplDBHash should ignore mapreduce incremental collections  * SERVER-33812 First initial sync oplog read batch fetched may be empty; do not treat as an error.  * SERVER-34102 Under PV1, ReplicationCoordinatorImpl::_handleTimePassing for a single node RS should start an election instead of auto-winning.  * SERVER-34110 Ignore NamespaceNotFound errors when refetching documents during rollback  * SERVER-34249 Oplog query on uninitiated replica set node can cause seg fault  * SERVER-34549 Tests refer to ReplSetTest.kDefaultTimeoutMS, which is undefined  * SERVER-34661 Return early when the vote request response has an error  * SERVER-34682 Old primary should vote yes and store the last vote after stepdown on learning of a higher term  * SERVER-34758 replSetGetStatus can deadlock with initialSyncer  * SERVER-34900 initial sync uses different batch limits from steady state replication  * SERVER-35087 \"./jstests/replsets/read_concern_uninitated_set.js\" and test should be disabled in evergreen for storage engines - ephemeralForTest & mmapv1.  * SERVER-35124 Stepdown suites with MMAP V1 often fail due to flushing mmaps taking long time  * SERVER-35249 Invariant that we apply to top of oplog on 3.6 replication recovery  * SERVER-35256 Do not treat it as an error if the first batch returned by an oplog query comes back empty in master-slave  * SERVER-35340 Reduce number of iterations in update_and_bulk_insert.js FSM workload  * SERVER-35405 Change default setting for replBatchLimitOperations \nQUERY \n * SERVER-33959 CursorManager attempts to dispose of aggregation cursors while holding partitioned lock, which leads to deadlock  * SERVER-34933 pcre verb support  * SERVER-35693 Parsing of $in takes quadratic time due to O(n^2) boost::flat_set constructor \nAGGREGATION \n * SERVER-34040 Disallow change stream on \"internal\" databases and system collections  * SERVER-35750 [3.6 only] Fix assert in change_stream_collation.js to be assert.soon() \nJAVASCRIPT \nSERVER-34515 Invariant failure _scope->exec( \"$arr = [];\", \"group clean up\", false, true, false , 2 * 1000) \nSTORAGE \n * SERVER-31679 Increase in disk i/o for writes to replica set  * SERVER-33078 convertToCapped size is not checked for float -> long long overflow\n\n  * SERVER-34829 Drop pending reaper must not delete the _dropPendingNamespaces entry until after the drop occurs  * SERVER-34863 Disable LSM testing on 3.2, 3.4 and 3.6 branches  * SERVER-35859 Disable rocksdb buildvariant WIREDTIGER \n * SERVER-33706 Remove warning message about Windows FS cache configuration \nOPERATIONS \n * SERVER-27230 Poor compression of diagnostic data when replica set is unhealthy  * SERVER-28830 cursorHandleFromId should verify that its not being called on a closed connection  * SERVER-32065 Add support for retryWrites option in mongo shell connection string format  * SERVER-33080 blacklist getLog2.js from the parallel suite  * SERVER-33546 Add numeric syncing field to replSetGetStatus members array  * SERVER-34141 Inconsistent appName in Shard  * SERVER-34159 Fix migration_destination_manager logging  * SERVER-35444 Heap stacks should not be included in serverStatus with heapprofilingenabled \nBUILD AND PACKAGING \n * SERVER-32999 Platform Support: remove Debian 7  * SERVER-33395 PPC64 little endian altivec optimizations are broken on newer gcc  * SERVER-34451 MongoDB installation on Windows error: setup wizard ended prematurely  * SERVER-35210 curator release versions should be populated from the project expansions \nTOOLS \n * TOOLS-2058 mongoreplay does not show OP_MSG commands  * TOOLS-2062 Support zlib compression in mongoreplay \nINTERNALS \n * SERVER-26884 Support archiving data files in Evergreen on test failure  * SERVER-29359 Enable SO_KEEPALIVE on egress server connections  * SERVER-31013 Make serverExitCodeMap useful to detect if server crashed on startup before connection established  * SERVER-31400 Record Linux netstat metrics in ftdc  * SERVER-31562 dump replica set oplogs at the end of every failed test  * SERVER-31830 Log values in invariant statements  * SERVER-32602 Multiversion tests are not running enterprise MongoDB on enterprise build variants  * SERVER-32688 FSM replication suites should give secondaries zero votes  * SERVER-32715 Add \"Connection refused\" to the list of _SSH_CONNECTION_ERRORS in remote_operations.py  * SERVER-32762 Module to upload files to S3  * SERVER-32763 Create archive.json and associate to task in evergreen.yml  * SERVER-32852 Capture FTDC data on failures of the concurrency suite  * SERVER-33009 AWS credential profiles in evergreen.yml  * SERVER-33144 Support archiving data files in Evergreen on test failure - Windows  * SERVER-33193 Enable test failure archive for specific tasks and hooks  * SERVER-33346 Pin a specific version of boto3 for use in etc/evergreen.yml scripts  * SERVER-33420 Add detection metafunctions to StatusWith  * SERVER-33512 Add PeriodicKillSecondaries hook to archive list  * SERVER-33553 OP_KILL_CURSORS fails on mongos: Unable to check out cursor for killCursor  * SERVER-33813 launch_evergreen_ec2_instance.sh should immediately fail if the call to aws_ec2.py is unsuccessful  * SERVER-33817 Powercycle test using kill mongod  * SERVER-33995 mongod crashes with nmap script  * SERVER-34075 powercycle_replication* must run replication recovery to observe canary documents  * SERVER-34345 Make it easier to provide validation rules for server parameters  * SERVER-34371 Stop ignoring errors when the test fixture fails to delete data files  * SERVER-34374 resmoke.py uses bytestrings for representing pathnames, leading to silently failing to clear the dbpath on Windows  * SERVER-34380 system_perf.yml: Remove the compile_proxy task  * SERVER-34481 Improve resiliency of awaitdata_getmore_cmd.js  * SERVER-34540 Remove sleeptest from dbtests  * SERVER-34563 Connect via SRV record fails if the returned record resides in subdomain  * SERVER-34735 Extract structured names from X.509 certificates  * SERVER-34810 Session cache refresh can erroneously kill cursors that are still in use  * SERVER-34851 Disallow index selection for identical min & max values on find  * SERVER-34865 Test archival fails when temporary files are removed\n\n  * SERVER-34936 Reduce batchtime for rocksdb buildvariant  * SERVER-34950 Use new sys-perf baseline 3.4.14-Baseline  * SERVER-34956 big_object1.js is not resilient to unexpected stepdowns  * SERVER-34996 Save console_output & console_screenshot from aws_ec2.py as artifacts in evergreen.yml  * SERVER-35051 Resmoke should stop the balancer before shutting down sharded clusters  * SERVER-35055 KeyedExecutor  * SERVER-35071 Split MMAPv1 tasks into separate variants in sys-perf  * SERVER-35101 Handle differently a badly formed bindIp argument  * SERVER-35108 Enable signal processing in system_perf.yml  * SERVER-35155 Fix jstests/ssl/x509_invalid.js using explicit X509 client names  * SERVER-35188 Typo in ServiceLiason* types  * SERVER-35190 resmoke.py runs lists of tests in alphabetical order by default  * SERVER-35207 Don't use json.get_history in system_perf.yml  * SERVER-35227 Delete embedded from the 3.6 branch  * SERVER-35229 Improve logging in apply_batch_only_goes_forward.js  * SERVER-35231 Invalid decimal continuation can trigger a dassert when decoding a KeyString  * SERVER-35232 Backport futures to 3.6  * SERVER-35348 Add silent option in the update_test_lifecycle task  * SERVER-35506 The Powercycle wait_for_mongod_shutdown function should ensure the mongod process is no longer running  * SERVER-35588 powertest.py should call replSetReconfigure command only after successful replSetGetConfig  * SERVER-35610 Refine LDAP options parsing  * SERVER-35706 Fix race in ctor for LogicalSessionCacheImpl  * SERVER-35834 Fix compile errors in dns_name_test on Microsoft compilers  * SERVER-35917 Blacklists tests which use resumeAfter from the sharded collections change streams passthrough  * WT-3698 Threads doing eviction can stall indefinitely when timestamps fail to advance  * WT-3892 Add timing_stress_for_test option for lookaside cleanup  * WT-3914 Add general documentation for prepared transactions  * WT-3937 Tune lookaside sweep to react to workload  * WT-4000 Avoid getting two snapshots when setting a read_timestamp  * WT-4004 test/format snapshot-isolation search mismatch failure  * WT-4016 Measure and improve lookaside performance with stable_timestamp set  * WT-4023 Add messages to a few places that return errors  * WT-4029 Bump the log file version  * WT-4033 Add ability to alter tables without taking exclusive access  * WT-4039 Move row-store missing-value support into the cell unpack code.  * WT-4044 Add an internal API to return if a generation is active  * WT-4049 Performance drop in long tests  * WT-4051 format configures too-small LSM caches.  * WT-4052 Free transaction resources on session reset  * WT-4055 format transaction prepare and logging configuration is incorrect  * WT-4056 New API to configure a minimum compatibility version on open  * WT-4057 round_to_oldest should establish txn snapshot after establishing rounded read timestamp  * WT-4058 Make slot switch quicker when io is slow  * WT-4059 Start lookaside sweep at full records  * WT-4060 Clean up __wt_getenv() error handling.  * WT-4061 Don't rollback during recovery due to cache pressure  * WT-4063 Update docs to make clear when log archiving is disabled.  * WT-4064 Relax checking of lookaside entry count  * WT-4069 Commit of a truncate can leave a page permanently locked  * WT-4070 WiredTiger recovery can checkpoint data referencing pieces of a transaction  * WT-4071 Run unit tests with timestamps disabled  * WT-4074 Lint fix: don't use error labels that do nothing other than return  * WT-4075 Allow timestamp_transaction after prepare  * WT-4076 test/format failure with cache stuck full of internal pages  * WT-4077 Transactional test on zSeries contains wrong LSN in checkpoint  * WT-4078 Splits can leak a page lock if memory allocation fails.  * WT-4079 confirm WiredTiger builds under gcc8  * WT-4080 gcc8 -Wparentheses complains about WiredTiger's __F() macro.\n\n  * WT-4086 Verify log file versions before opening logging subsystem  * WT-4087 Store current compatibility within WiredTiger  * WT-4091 Make timing_stress_test options for page split race conditions work.  * WT-4093 Temporarily disable test/format LSM and timestamp testing  * WT-4094 Understand variable throughput when running YCSB 50/50 workload  * WT-4098 Add new compatibility_max configuration option  * WT-4105 Optimize cache usage for update workload with history pinned  * WT-4110 test_timestamp_abort failed with missing records  * WT-4115 Valgrind error in est_wt4105_large_doc_small_upd  * WT-4116 Coverity #1393311 Copy-paste error  * WT-4117 Expose WiredTiger crc32c function  * WT-4120 Enhance test/format to dump the cache when timing out  * WT-4122 Ensure compatibility downgrade cleans up old log files  * WT-4127 Add common prefix for compatibility version errors  * WT-4128 Skip checkpoints while stable timestamp doesn't change \n3.6.5 CHANGELOG  SECURITY \n * SERVER-34418 Disable GCM encryption with ESE on OS X with OpenSSL  * SERVER-34477 Coverity analysis defect 103475: Invalid iterator comparison \nSHARDING \n * SERVER-32681 exact_shard_key_target.js moveChunk should wait for delete  * SERVER-33585 Do not return $clusterTime when no keys are available  * SERVER-33714 Downgrading FCV from 3.6 to 3.4 leaves an admin.system.keys collection on shards that on upgrade is orphaned and renamed without a UUID  * SERVER-33766 Secondary may not invalidate in-memory routing table cache after primary refresh  * SERVER-34098 Move chunk_manager_refresh_bm from unit test to perf microbenchmark  * SERVER-34347 Batch write with ranged query predicate mixed with single target triggers invariant on mongos  * SERVER-34508 Prevent race condition in Migration Destination Manager from swallowing an exception  * SERVER-34571 Invariant fails in destructor of MigrationChunkClonerSourceLegacy on shutdown  * SERVER-34586 Possible double mutex acquisition in ShardServerCatalogCacheLoader  * SERVER-34644 Disable DollarPrefixedFieldName checks for moveChunk/mergeChunks  * SERVER-34683 Downgrade replicaset from 3.6.4 to 3.4.14 fails due to the presence of config.system.sessions  * SERVER-34746 Segmentation fault when shard is started with --shardsvr before being added to a shard \nREPLICATION \n * SERVER-29966 Add invariant that lastOpApplied doesn't change during secondary batch application  * SERVER-30642 Raise election timeouts as a way to provide more stable replica set test topologies  * SERVER-32647 Retry connecting to replica set when given a seed node in Jepsen tests  * SERVER-33026 SyncSourceFeedback request doesn't have a timeout  * SERVER-33287 Create passthrough that kills the primary node  * SERVER-33475 Retried writes doesn't properly wait for writeConcern  * SERVER-33879 config.transactions is not updated during startup replication recovery  * SERVER-33956 A sequence of rename and create collections that do not arrive at the correct end state \nQUERY \n * SERVER-33154 {\"query\" => \"foo\"} is upconverted incorrectly  * SERVER-34389 Modify change_stream_collation test to avoid failure  * SERVER-34714 An $or query with children which are all trivially false incorrectly optimizes  * SERVER-34725 Group and count plan stages do not set the WorkingSetID output on PlanStage::DEAD state  * SERVER-34767 Randomized KeyString decode test can hit dassert in Decimal \nWRITE OPERATIONS \nSERVER-27534 All writing operations must fail if the term changes \nAGGREGATION \nSERVER-34399 $changeStream with invalid resume token crashes the server \nSTORAGE \n * SERVER-32989 repairDatabase can race with dropDatabase.  * SERVER-33743 Use all_committed to set lastApplied on primary nodes  * SERVER-34091 Oplog visibility rules can cause cappedTruncateAfter to erroneously skip record deletion in WiredTiger  * SERVER-34423 collMod interruption may cause invariant failure  * SERVER-34656 Add secondary reads sys-perf test to system_perf.yml WIREDTIGER\n\n \nOPERATIONS \n * SERVER-32876 Don't stall ftdc due to WT cache full  * SERVER-34665 The mongo shell should retry writes on a WriteConcernFailure error response from the server \nBUILD AND PACKAGING \n * SERVER-29463 Platform Support: add Debian 9 \"Stretch\"  * SERVER-44392 Platform Support: remove Ubuntu 16.04 \"Xenial\" on s390x architecture  * SERVER-32785 Integrate Google Benchmark into SCons  * SERVER-34149 Upgrade zlib to latest  * SERVER-34250 Update Tools builders in Evergreen to match upstream  * SERVER-34461 Source OpenSSL source tarball from boxes.10gen.com \nTOOLS \n * TOOLS-1765 mongoreplay crashes with out of memory recording from 8GB pcap file  * TOOLS-1776 mongoreplay hangs on open connection when finishing playback  * TOOLS-1780 Build tools with Go 1.8.x  * TOOLS-1948 Use Go-native TLS dialer on platforms with openssl 0.9.x  * TOOLS-1968 Backport - Need to update spacemonkeygo/openssl fork to support newer OpenSSL libraries  * TOOLS-1999 Use of --uri overrides use of --ssl  * TOOLS-2005 --quiet flag not honored on mongofiles \nINTERNALS \n * SERVER-25640 Have ReplSetTest run checkDBHashes() in stopSet()  * SERVER-29301 Upgrade MozJS to ESR 45.9.0  * SERVER-33199 Add Resmoke Benchmark suite  * SERVER-33200 Write a new test runner for Benchmark  * SERVER-33201 Add Benchmark resmoke testcase  * SERVER-33202 Add hook to enable resmoke.py to generate perf.json  * SERVER-33203 Write benchmark canary tests  * SERVER-33204 Modify Evergreen.yml to support Benchmark  * SERVER-33329 Server and Shell do not emit TLS \"protocol_version\" alert messages  * SERVER-33398 Add the new finer grained sys-perf tasks  * SERVER-33438 burn_in_tests.py fails if there are new or modified files and build/benchmarks.txt doesn't exist  * SERVER-33491 Fix benchmark.h compile with -fdirectives-only  * SERVER-33497 Remove the --options option to resmoke.py  * SERVER-33560 Remove deprecated Fixture function declarations from Google Benchmark  * SERVER-33636 wrap transaction_reaper.js writes in an assert.writeOK  * SERVER-33643 Add convenience function to get num cpu cores in benchmarks  * SERVER-33734 Improve jsTestLog filterability  * SERVER-33746 Pipe in additional Evergreen options to resmoke.py  * SERVER-33805 Change Jepsen tasks running in Evergreen to use mongobridge  * SERVER-33938 Increase replication timeout in CheckReplDBHash hook  * SERVER-34210 Fix display of benchmarks with multiple arguments  * SERVER-34218 FieldRef::parse does not completely initialize the FieldRef  * SERVER-34237 Expose means for shell to disable TLS 1.0  * SERVER-34390 Make OS X binaries speak TLS 1.2  * SERVER-34412 powercycle* tasks set timeout_secs in wrong location of task definition  * SERVER-34444 Reestablish a new connection to LocalToRemoteOperations after crash in powercycle.py  * SERVER-34476 Fix OpenSSL linking on FreeBSD  * SERVER-34582 AsyncRequestsSender can block network threads during construction  * SERVER-34605 Change Jepsen tasks to run on the ubuntu1604-build distro in Evergreen  * SERVER-34651 Performance regression on secondary application with retryable batched writes  * SERVER-34657 Add secondary reads YCSB test to system_perf.yml  * SERVER-34667 data_consistency_checks.js incorrectly assumes that the config.mongos collection always exists  * SERVER-34742 Stop running ssl_cert_password.js on OS X  * SERVER-34823 Thread name and connection number are not synchronized in TransportLayerLegacy  * SERVER-34827 retryable writes not in sys-perf-3.6 project  * SERVER-34834 Wait for replication of FCV document before downgrading binaries in clear_and_reinstate_keys_rs.js  * SERVER-34929 Fix malformed preprocessor macro check\n\n  * TOOLS-1978 tools fail eslint testing  * TOOLS-2003 Drop SUSE11 from Evergreen builds  * WT-3851 Optimize wt_compare* routines with NEON instructions for ARM platform  * WT-3886 Identify statistics relevant for prepared transactions  * WT-3910 libwiredtiger-3.0.1.dylib loading failure on OS X  * WT-3959 Recovery timestamp set on restart scenarios need addressing  * WT-3998 Fix a bug where stable timestamp was ignored on shutdown  * WT-4006 Add support for stress timing configurations to test/format  * WT-4009 Create fast path for cursor caching with \"overwrite=false\"  * WT-4012 Fix lookaside entry counters  * WT-4014 If eviction walk is interrupted, clean up the queue.  * WT-4015 Enhance schema06 unit test to test drops  * WT-4017 When evicting during a checkpoint, avoid splits  * WT-4019 Change test/format to test transaction prepare less often  * WT-4027 Yield cursor operations between restarted search/traverse  * WT-4028 Don't check for a modified page without holding the ref locked  * WT-4031 on-page zero-length row-store values can be discarded from checkpoints  * WT-4032 parent pages can be evicted while being split.  * WT-4034 Re-entering eviction can result in checkpoint corruption  * WT-4035 Truncate information discarded while active  * WT-4036 Fix Coverity false positive: out-of-bounds access.  * WT-4037 WT_REF structures freed while still in use  * WT-4042 Access data handles safely during cursor reopen  * WT-4045 Don't retry fsync calls after EIO failure \n3.6.4 CHANGELOG  SHARDING \n * SERVER-16802 Order of balancer chunk moves depends on order of config.collections  * SERVER-28670 Add sharding metadata refresh metrics section to serverStatus  * SERVER-28981 Sharding balancer prefers shards in a specific order when moving chunks  * SERVER-29807 RangeDeleter should log when its about to wait for majority replication  * SERVER-32210 Stepping down recipient shard's primary while migrating session information can fassert  * SERVER-32604 prevent adding a binary last-stable shard if the cluster is upgrading or downgrading  * SERVER-32885 Overlap chunk clone application on the donor with fetching documents from the recipient  * SERVER-32886 Unnecessary sleeps during chunk migration  * SERVER-33189 Unblacklist sharding tests marked as requiring already released last-stable versions  * SERVER-33763 3.6 drivers fail to communicate with 3.6 sharded clusters running at FCV 3.4  * SERVER-33869 make $changeStream on shards check shardVersion  * SERVER-33971 Nodes in MongoDB sharded cluster crashes with Invariant failure oplogEntry.getWallClockTime() \nREPLICATION \n * SERVER-29946 Increase heartbeat rate when a secondary has no sync source  * SERVER-30283 PingStats::hit() should not set _numFailuresSinceLastStart to integer max  * SERVER-31399 repl.apply.batches.totalMillis does not record the time spent applying batches  * SERVER-31666 catchup_takeover_one_high_priority should use initiateWithAnyNodeAsPrimary  * SERVER-31669 Increase the election timeout in case 3 of catchup.js  * SERVER-32776 Refresh rollback ID cache on increment  * SERVER-33448 Relax time diff requirements in apply_batches_totalMillis.js  * SERVER-33618 Initial sync should consider applyOps a CrudOpType \nQUERY \n * SERVER-31293 Don't consider readPreference \"primary\" as equivalent to 'slaveOk' in OP_QUERY find path  * SERVER-33302 Missing log redaction for a few failure paths  * SERVER-33542 Using maxTime() on MongoDB 3.4 and 3.6 does not yield the same error code \nSTORAGE \n * SERVER-32058 Abort of catalog meta-data updates may lead to cache inconsistency  * SERVER-32453 WTKVEngine::getAllIdents mishandles cursor->next return code.  * SERVER-32574 Repairing the local database can cause the WT oplog manager thread to permanently exit.  * SERVER-32641 Some artificial find may crash debug builds of the server\n\n  * SERVER-33086 renameCollection target should never have identical UUID with a different database  * SERVER-33087 Fix the use of dropTarget in renameCollection  * SERVER-33122 add option to disable cursor caching to speed up dropDatabase and collection drop  * SERVER-33233 Don't stall ftdc due to running out of tickets  * SERVER-33854 Fix applyOps field handling  * SERVER-34024 Disable WiredTiger cursor cache by default. \nOPERATIONS \n * SERVER-12644 Move note about noprealloc hurting performance to startup warnings  * SERVER-30114 Monitor cumulative time spent in tcmalloc spin lock  * SERVER-30567 Break out serviceExecutorTaskStats by task type  * SERVER-32498 \"desc\" field in currentOp output no longer contains the thread name used in log messages  * SERVER-33227 Using the method connect in a script will as a side effect update global db. \nBUILD AND PACKAGING \n * SERVER-31535 Platform Support: remove Ubuntu 12.04 builds  * SERVER-32923 Platform Support: remove SLES11 builds  * SERVER-33406 Reduce concurrency for s390x compile and compile_all builds to avoid OOM failures  * SERVER-33779 Remove tvOS and iOS variants from the v3.6 branch  * SERVER-33936 3.6 nightly builds not available for download \nINTERNALS \n * SERVER-30609 Investigate need for markThreadIdle with adaptive service executor  * SERVER-30979 Run the fuzzer with CSRS primary stepdowns  * SERVER-31181 ServiceExecutorAdaptive maxLatencyMicros should be greater than the minimum timer resolution of the OS  * SERVER-31198 Run the concurrency suite with shard stepdowns  * SERVER-31266 Treat queued task reporting for adaptive service executor consistently  * SERVER-31452 Run fuzzer with shard stepdowns  * SERVER-31556 Fix ContinuousStepdown logger  * SERVER-31917 RollbackTest fixture should add more log messages describing the actions it is taking  * SERVER-31999 Race condition in awaitdata_getmore_cmd.js  * SERVER-32034 Replica Set primary becomes unresponsive with adaptive Service Executor  * SERVER-32063 mongos responds with \"Unauthorized: there are no users authenticated\" to an isMaster request using an lsid  * SERVER-32126 validate() should do basic sanity checks for UUIDs  * SERVER-32241 applyOps reports success even when a nested applyOps fails.  * SERVER-32445 config.transactions table can get out of sync when the TransactionReaper remove entries  * SERVER-32474 resmoke.py logging output from data consistency hooks twice to logkeeper  * SERVER-32477 ASIOSession fails to gracefully teardown sockets  * SERVER-32528 Use fixed version of curator for remote EC2 instances in evergreen.yml  * SERVER-32583 Sys-perf update bootstrap variables to use proper ConfigDict Names  * SERVER-32600 setup_multiversion_mongodb.py may download non-x86_64 binaries on x86_64 platforms  * SERVER-32798 Add duroff variant for MMAPv1  * SERVER-33129 getMinimumTimerResolution() returns maximum resolution on Windows  * SERVER-33150 Race between creating a system.profile collection and assigning UUIDs to non-replicated collections on clean startup  * SERVER-33224 Use windows-64-vs2015-small distros for powercycle tasks  * SERVER-33346 Pin a specific version of boto3 for use in etc/evergreen.yml scripts  * SERVER-33424 Change update_test_lifecycle.py script to use OAuth when authenticating to JIRA  * SERVER-33437 Decorable::declareDecorationWithOwner passes wrong owner pointer to its decorations  * SERVER-33467 Do not free memory returned by ldap_get_option(LDAP_OPT_ERROR_STRING) on Windows  * SERVER-33483 HTTP detection no longer working  * SERVER-33484 Code coverage can't distinguish between files with identical names  * SERVER-33502 Blacklist or rewrite jsCore tests that use the getLastError command from the parallel suite  * SERVER-33520 Enable background refreshing of the logical session cache during the fuzzer test suites  * SERVER-33569 The check for logical session existence must not allow partial results  * SERVER-33572 Provide a bounded, interruptible, thread safe, producer consumer queue  * SERVER-33619 Assign address_type for any EC2 instance in powertest.py\n\n  * SERVER-33640 Blacklist find_and_modify_concurrent_update.js from causal consistency passthrough  * SERVER-33670 Build mongo binaries with -ssl compile flag in system_perf.yml  * SERVER-33682 Python virtualenv not setup in \"Gather remote mongo coredumps\" post phase in evergreen.yml  * SERVER-33691 Remote EC2 instances should have a log disk  * SERVER-33692 Only backup data files from failed Powercycle tests  * SERVER-33696 Set timeout_secs for powercycle* task to restrict the duration of a single powercycle loop  * SERVER-33751 Auth when connecting to an existing cluster in ReplSetTest  * SERVER-33778 Remove change_stream_remove_shard.js  * SERVER-33843 PeriodicKillSecondaries hook can run validate while a secondary is still applying operations  * SERVER-33848 Update compile flags for sys-perf and performance projects  * SERVER-33935 utils_auth.js should support connecting to clusters running with SSL  * SERVER-33970 New baselines for mongo-perf  * SERVER-34062 powertest.py attempts to access an attribute named address_type rather than its value  * SERVER-34140 remove multiple distro specification for single tasks in evergreen configs  * SERVER-34178 Address lingering issues around TestData.excludedDBsFromDBHash  * SERVER-34213 testshard1.js should use _ids of [0, nItems -1] instead of [1, nItems]  * SERVER-34318 Set timeout for canary insert to 1 hour in powertest.py  * SERVER-34352 Run dbhash check before collection validation in jstestfuzz_sharded_continuous_stepdown.yml  * SERVER-34361 blacklist migration_critical_section_concurrency.js in v3.6 last stable  * WT-1228 Improve performance of WT_SESSION::open_cursor  * WT-3724 Log an error if flushing with F_FULLSYNC fails  * WT-3805 Avoid reading lookaside pages in truncate fast path  * WT-3815 Cursor caching: measure and tune performance  * WT-3829 WiredTiger metadata can be logically inconsistent.  * WT-3848 Enhance new prepare transaction API to enforce post conditions  * WT-3849 Add timestamp validation to WT_SESSION::prepare_transaction  * WT-3850 Implement WT_SESSSION::prepare_transaction  * WT-3867 Bi-weekly WT codebase lint  * WT-3868 Bi-weekly WT codebase lint  * WT-3869 Bi-weekly WT codebase lint  * WT-3870 Bi-weekly WT codebase lint  * WT-3901 Corruption of operation tracking log files  * WT-3904 Reconsider error path in log server thread  * WT-3905 Save the timestamp used for a checkpoint  * WT-3906 Respect stable_timestamp in WT_CONNECTION::close  * WT-3911 Ignore lookaside data on shutdown  * WT-3912 fast-delete pages should re-instantiate the delete transaction's timestamp.  * WT-3913 Enhance cursor operations to account for prepare state  * WT-3922 Allow truncate operations to be prepared  * WT-3923 __wt_txn_context_prepare_check() requires API initialization  * WT-3925 Fix test format operation selection code  * WT-3926 Allow read_timestamp to be set after begin_transaction  * WT-3927 Disable truncate testing with LSM  * WT-3930 Set the recovery timestamp even if recovery doesn't run  * WT-3931 cursor.prev split race  * WT-3932 WiredTiger memory allocation failure in js_test  * WT-3933 test/format failure illegal WT_REF.state rolling back deleted page  * WT-3934 LSM chunk checkpoints can race with system checkpoints  * WT-3935 Enable cursor caching by default in WiredTiger.  * WT-3936 Add multi-threaded tests for prepare_transaction()  * WT-3938 Reduce memory usage with many tables and sessions  * WT-3939 test_txn14.test_txn14.test_log_flush timeout  * WT-3940 s_export issue detected during WiredTiger release execution  * WT-3942 Update test_compact02 to handle being halted by eviction pressure.  * WT-3945 Support libwiredtiger.so checking in s_export  * WT-3946 Truncate segfault with a NULL start cursor  * WT-3947 Allow wiredtiger_open configuration to disable cursor caching\n\n  * WT-3949 Buffer overflow in WT_CURSOR::modify for string values  * WT-3950 Add some rollback_to_stable statistics  * WT-3952 page-delete update list traversed after it has been discarded.  * WT-3953 test/format can attempt to set an illegal prepare timestamp  * WT-3954 test/format: prepared operations evicted before commit  * WT-3958 Add query API to get most recent checkpoint's stable timestamp  * WT-3961 The all_committed timestamp should be less than any in-flight transaction  * WT-3964 Stop wrapping schema operations in a transaction  * WT-3967 Fix long test for cursor cache sweep  * WT-3969 enhance format tester to account for prepare state  * WT-3971 Make cursor duplication use cursor caching  * WT-3972 Allow more than 64K cursors to be open on a data source simultaneously  * WT-3973 Allow alter to modify app_metadata  * WT-3975 arg format mismatch after rwlock changes  * WT-3977 Print out actual checkpoint stable timestamp in timestamp_abort  * WT-3979 Fix warnings generated with newer Doxygen releases  * WT-3980 failure returning a modified update without a backing \"real\" update  * WT-3981 Make snapshot consistent with read_timestamp  * WT-3982 Fix transaction visibility bugs related to lookaside usage.  * WT-3984 Fix race conditions around prepare state transitions  * WT-3985 Pre-allocated log files accumulate on Windows  * WT-3987 Avoid reading lookaside pages in truncate fast path  * WT-3990 Fix Coverity warnings mostly in test programs  * WT-3996 Test truncate with timestamps and lookaside  * WT-3997 The cursor walk code can spin without sleeping on restart/split.  * WT-4002 Allow duplicates in api_data.py  * WT-4005 AddressSanitizer in __wt_timestamp_iszero().  * WT-4007 eviction instantiates pages from dead trees.  * WT-4008 Add ARM NEON support for row search operations  * WT-4011 Checkpoint should not read truncated pages  * WT-4022 Avoid WT_RESTART error return during eviction walk  * WT-4025 Allow debug dumping of internal pages \n3.6.3 CHANGELOG  SECURITY \n * SERVER-31893 Explicitly define timeout for synchronous LDAP calls  * SERVER-32086 Suppress LDAP user cache invalidation log message by default  * SERVER-32779 Upgrade third_party tomcrypt to 1.18.1 final  * SERVER-32933 Allow mongod to start when unable to reach LDAP server \nSHARDING \n * SERVER-27724 Explore whether we can further minimize chunk metadata reloads on shards  * SERVER-28923 Add diagnosability and supportability features for retryable writes  * SERVER-29423 Sharding balancer may schedule multiple migrations with the same source or destination  * SERVER-30152 add safe secondary reads targeted tests for mapReduce  * SERVER-30671 Don't expect operationTime to not change in causal_consistency_shell_support.js  * SERVER-31860 setFCV on a cluster can fail to call setFCV on shards if the config server primary's ShardRegistry is empty  * SERVER-31979 Chunk migration statistics are not recorded in the moveChunk.commit changelog entries  * SERVER-32235 Blacklist core/drop3.js test from causally consistent workloadss  * SERVER-32368 do not perform UUID check against a shard's config cache in the sharding_csrs_continuous_config_stepdown_WT suite  * SERVER-32554 Source shard stepdown while entering critical section can trigger cloner invariant  * SERVER-32568 The migration_sets_fromMigrate_flag.js test is not compatible with sharding continuous stepdown suite  * SERVER-32569 Introduce uniform way to allow config servers and shard replica sets to start in non-cluster mode  * SERVER-32592 Stepdown during migration cleanup can crash the source shard primary  * SERVER-32593 CSRS stepdown during migration commit can trigger fassert on source shard primary  * SERVER-32886 Unnecessary sleeps during chunk migration  * SERVER-32901 Pull the CollectionShardingState map out of ShardingState  * SERVER-32924 tag skip_sharding_configuration_checks test with requires_persistence tag  * SERVER-32970 Put back random chunk size initialization in mongos auto split tracking  * SERVER-33234 dropIndexes on mongos should ignore IndexNotFound from individual shards if some shard returned success\n\n \n * SERVER-21456 Improve closing connection behavior when stepping down from primary  * SERVER-28290 stepping down due to a higher term seen in a heartbeat should not discard term after stepdown  * SERVER-28895 Remove old form of replSetUpdatePosition command  * SERVER-31707 Test changeStreams on a sharded collection where the shard doesn't know the collection is sharded  * SERVER-32028 Make reconfig() in rslib.js resilient to NodeNotFound error  * SERVER-32209 Reset the in-memory optimes on PV downgrade and upgrade  * SERVER-32361 Rollback tests that expect a fatal assertion after node restart should not wait for connection  * SERVER-32402 dropDatabase after step down can cause fassert  * SERVER-32432 Race condition causes seg fault in ReplicationCoordinatorExternalStateImpl shutdown  * SERVER-32532 Two-phase dropDatabase may not replicate all collection drops before the database drop.  * SERVER-32556 Retryable write on local db hits an invariant  * SERVER-32564 Increase assert.soon timeout when waiting for election in rollback_crud_op_sequences.js  * SERVER-32624 dropDatabase() should wait for collection drops using ReplicationCoordinator::awaitReplication() instead of awaitReplicationOfLastOpForClient()  * SERVER-32783 CollectionCloner::shutdown() should not block on resetting _verifyCollectionDroppedScheduler  * SERVER-32794 Make timeouts unrelated to elections not depend on election timeout  * SERVER-32803 stepup.js should retry replSetStepUp command when it is overtaking another primary  * SERVER-32840 Remove pv0 jepsen tests  * SERVER-32919 initial_sync_many_dbs.js should use less data \nQUERY \n * SERVER-28260 Create a killAnyCursor privilege  * SERVER-31484 Operation deadline and awaitData timeout should be separate  * SERVER-31854 After the first drop, assertSchemaMatch() should assert that subsequent drops succeed  * SERVER-32441 3.6 mongod crash on find with index and nested $and/$or  * SERVER-32492 idhack_sharded.js should use _waitForDelete:true in moveChunk  * SERVER-32606 Tailing oplog on secondary fails with CappedPositionLost  * SERVER-33005 Contained $or access planning is incorrect for $elemMatch object, results in invariant failure  * SERVER-33089 Unable to start queryable mongod because it failed to regenerate index for admin.system.users  * SERVER-33092 elemMatchProjection.js incorrectly assumes that shell generates monotonically increasing _id values  * SERVER-33333 Prevent failed cross user getMores from having side effects \nAGGREGATION \n * SERVER-31760 Lookup sub-pipeline is not using index for equality match  * SERVER-32349 Resuming a sharded change stream when there are multiple changes with the same timestamp may be impossible  * SERVER-32690 Aggregation can trip invariant related to renamed fields optimization \nSTORAGE \n * SERVER-32259 Improve error handling for fetchTypeAndSourceURI()  * SERVER-32274 Do not timestamp applyOps writes on standalones  * SERVER-32533 In oplog truncate with WT, don't use a start key  * SERVER-32573 Skip wt_delayed_secondary_read_concern_majority.js when running with WiredTiger's LSM tree  * SERVER-32637 Ensure that upgrading to 3.6 when on pv0 doesn't break if you don't explicitly turn off readConcernMajority  * SERVER-32851 setFeatureCompatibilityVersion can race with createCollection such that FCV 3.6 is set and some collections do not have UUIDs \nOPERATIONS \n * SERVER-32072 DBRef with NumberInt ID changing to float in the shell  * SERVER-32473 Error loading history file on first shell usage  * SERVER-33140 mongodb+srv URI support broken on shell v3.6.2 for Windows \nBUILD AND PACKAGING \n * SERVER-32516 Fails to compile with Boost 1.66  * SERVER-32580 Microbenchmarks: update the enterprise branch for mongo v3.6  * SERVER-32649 got \"used vector type where scalar is required\" when using GCC 7  * SERVER-32932 When testing SLES 12 packages make sure required repos exist  * SERVER-33181 Upgrade ASIO to latest \nINTERNALS\n\n  * SERVER-28396 resmoke.py's logkeeper client should respect size limit for POST requests by splitting them up  * SERVER-28822 Improve DBConnectionPool's growth semantics  * SERVER-31636 Split up generational_jstestfuzz* Evergeen tasks into query_fuzzer and update_fuzzer tasks  * SERVER-31670 Change replica set fixture used by replica_sets_jscore_passthrough to make its secondary have zero votes  * SERVER-31768 Don't create the first collection chunk on a primary drained shard  * SERVER-31886 Powercycle tasks running on Windows  * SERVER-31935 Archive process of interest core dumps from remote EC2 instance  * SERVER-31942 Large changes in unreliable tests can prevent test lifecycle update  * SERVER-32060 Move MMAPv1 powercycle tasks to MMAPv1 specific variants  * SERVER-32071 Powercycle - use internal crash  * SERVER-32074 Powercycle - Add stack dump when SIGUSR1 or Windows event is received  * SERVER-32090 Powercycle - pymongo client args are overwritten  * SERVER-32107 Update DNS root nameservers for DNS Query test  * SERVER-32110 Powercycle remote host not accessible in Evergreen timeout phase  * SERVER-32161 MongoDB 3.2+ fails to compile on PPC64LE with glibc 2.26+  * SERVER-32169 A cursor created with a session cannot be killed outside that session  * SERVER-32197 Remote host is missing the debug symbols for hang_analyzer in evergreen.yml  * SERVER-32203 Log the remote EC2 instance status when a failure to access it is detected in evergreen.yml  * SERVER-32205 Dropping admin db sets FCV to 3.4 but leaves UUIDs  * SERVER-32222 change_streams_primary_shard_unaware.js needs to be marked as requiring persistence  * SERVER-32228 Powercycle - handle remote host restarts  * SERVER-32243 Add an option to have the validate hook skip some collections.  * SERVER-32260 Call workload_setup.py from mongo repo  * SERVER-32298 Add a background system task to periodically copy remote statistics in evergreen.yml  * SERVER-32299 Add disk statistics for remote EC2 instances  * SERVER-32376 Blacklist tests that manually call startSession in the retryable_writes_jscore_stepdown_passthrough suite  * SERVER-32403 Increase remote EC2 expire time to 3 hours  * SERVER-32410 Validate User::CredentialData before attempting to perform authentication  * SERVER-32414 Remove stale stdx:: requirements from lint.  * SERVER-32429 black list regex_targeting.js from step down suites  * SERVER-32468 Use a 1-node CSRS in non-stepdown sharding passthroughs tests  * SERVER-32475 Microbenchmarks on v3.6 use master version of enterprise module  * SERVER-32486 Windows builds of the shell should report the failing domain name in DNS lookup failures  * SERVER-32515 Powercycle - rsync file exclude list  * SERVER-32520 Add VPC support for launching AWS EC2 instances  * SERVER-32522 set_read_and_write_concerns.js treats mapReduce \"out\" field like aggregation's $out stage  * SERVER-32527 Update package tests to use new BUILD2 VPC  * SERVER-32541 Disable Powercycle MMAPv1 task  * SERVER-32551 Cluster with x.509 membership authentication serves client connection with cluster client certificate  * SERVER-32585 Remove unnecessary dependencies from requirements.txt for ese_WT tests  * SERVER-32609 update_test_lifecycle fails when a test group does not have historical data  * SERVER-32614 update_test_lifecycle may fail because datetime.strptime() is not thread safe  * SERVER-32631 specifying --bind_ip localhost results in error \"address already in use\"  * SERVER-32664 Sys-perf enable running baselines at lower priority  * SERVER-32680 Update perf projects to use newer baselines  * SERVER-32691 Create passthrough for w=\"majority\" with 2-node replica set to address lost test coverage  * SERVER-32704 sys-perf: Skip validating oplog as enabled by SERVER-32243  * SERVER-32740 Set bootstrap.ycsb_dir and bootstrap.workloads_dir in system_perf.yml\n\n  * SERVER-32772 dbtest task in Evergreen should write to task directory  * SERVER-32774 Ensure change_streams_secondary_reads suite has voting secondaries  * SERVER-32788 Update \"secondary performance\" sysperf test to use nonvoting secondaries  * SERVER-32806 retried findAndModify not using oplogHack when querying for pre/post image oplog entry  * SERVER-32891 Sys-perf change order of mongodb_setup and workload_setup calls  * SERVER-32896 Upload dsi-artifacts.yml in system_perf.yml  * SERVER-32925 sys-perf 3.6 should run the change stream tests  * SERVER-32987 Move the contents of the 'uuid' library to be under 'base'  * SERVER-33068 run_check_repl_dbhash.js hook exits without actually running dbhash on a replica set  * SERVER-33142 Experiment with disabling cluster reuse in system_perf.yml  * SERVER-33147 Limit the -j/num_jobs_available for high core ARM CI servers  * SERVER-33158 Logical Session refresh batches are too large  * SERVER-33219 Add a backpressure mechanism to the CRUD client in backup_restore*.js tests  * SERVER-33236 Update perf.yml to use ssh form of git clone  * WT-2705 High throughput cache bound many threaded workloads can experience long latencies  * WT-3074 Automate a test to stress eviction walk with many active trees  * WT-3133 Detect or track long latency operations  * WT-3295 Allow LSM to merge into custom data sources  * WT-3565 Test and understand mixed timestamp/no-timestamp usage to same data  * WT-3587 Remove HAVE_VERBOSE conditional compilation  * WT-3597 Add a diagnostic check for updates to the same key out of timestamp order  * WT-3632 Increase how granularly cache usage settings can be configured  * WT-3654 Fix warning in Windows build on evergreen  * WT-3695 format failed to report a stuck cache  * WT-3716 Restore the WT_VERB_TEMPORARY verbose flag.  * WT-3720 flags macros cast flags to unsigned values, hiding warnings.  * WT-3725 Add statistics to locks around timestamp structures  * WT-3732 Handle adding WT indices while cursors on the table are open  * WT-3734 Fix undefined behavior in verbose output  * WT-3738 Review internal session allocation accounting  * WT-3740 race in page dirty-byte decrement.  * WT-3750 Fast-path fs_directory_list of a single file.  * WT-3753 Building on Windows --enable-java  * WT-3766 Lookaside sweep for obsolete updates  * WT-3767 Avoid lookaside instantiation for faster reads  * WT-3768 Lookaside optimization: birthmarks  * WT-3769 Fix a bug in reverse cursor walks with particular delete patterns and prefix compression enabled  * WT-3772 Hot backup causes uncontrolled growth of WiredTigerPreplog files  * WT-3774 Enhance Python lookaside testing to cover cursor modify  * WT-3775 Improve commit timestamp is older than oldest timestamp error message  * WT-3779 Add support for string formats with WT_CURSOR::modify  * WT-3780 Improve error messages on invalid WT_CURSOR::modify usage  * WT-3783 Fix transaction isolation to use the correct enum  * WT-3787 test_compact02 failed as compaction halted due to eviction pressure  * WT-3790 Switch statistics to rdtsc from epoch calls  * WT-3792 LSM version 1 metadata incompatibility  * WT-3793 WiredTiger page debug dump functions should unpack integer keys  * WT-3794 Coverity 1383547 and lint  * WT-3795 lint cleanups for the op-tracking software, reduce record write size.  * WT-3796 Report a better error message if transaction commit fails  * WT-3799 Test/format with timestamps enabled pin cache full  * WT-3806 Make sure rdtsc values move forward in time  * WT-3807 clang static analysis updates  * WT-3809 Fix a bug in lookaside related to birthmarks  * WT-3810 wt_rdtsc calibration needs to be longer and verify validity  * WT-3811 Add basic ability to visualise operation tracking  * WT-3812 debugging page output should handle complex key/value items.  * WT-3816 Enable prefix compression on the lookaside table  * WT-3818 __rec_txn_read() code order cleanup\n\n  * WT-3820 Add a WT_SESSION.breakpoint method for Python debugging.  * WT-3822 Update WiredTiger copyrights to 2018  * WT-3824 tsc_nsec_ratio can compute to zero and cause divide by zero bug  * WT-3825 Fix calculation of CPU ticks per unit time  * WT-3826 random-abort test failure  * WT-3827 test_compact02 failure  * WT-3828 Link error on OS/X for __wt_process data reference  * WT-3831 uninitialized buffer value in statlog server path comparison  * WT-3832 Fixup shell script warning messages  * WT-3833 test/format cache_minimum value error  * WT-3835 cursor remove tries to return a key that doesn't exist  * WT-3840 Dump more information when data corruption is encountered  * WT-3841 Fix error message pattern in timestamp09  * WT-3842 full-build Friday & lint  * WT-3844 Checkpoints can hang on limbo pages  * WT-3845 Compiler warning in examples using GCC 5.4.0  * WT-3846 Refine operation tracking visualization tool  * WT-3847 Add a stub API for prepared transaction  * WT-3852 Update debugging when committing at an earlier timestamp  * WT-3853 LSM version 1 metadata incompatibility  * WT-3854 Remove write lock from commit path, leaving old entries in queue  * WT-3860 lint \n3.6.2 CHANGELOG  SECURITY \nSERVER-31625 The contents of {USER} needs to be escaped when querying for the groups using LDAP server \nSHARDING \n * SERVER-28992 Cleanup mongos write commands execution  * SERVER-31982 Shard does not call config commit chunk migration command with majority writeConcern nor checks for writeConcern errors.  * SERVER-32202 Do not clear the cached sharding filtering information on replication state changes  * SERVER-32255 UUIDs may be absent from shard secondary local collections  * SERVER-32480 Remove CatalogCache retrieval methods, which take StringData  * SERVER-32529 Requiring replSet for shards breaks Queryable Backup \nREPLICATION \n * SERVER-30626 Remove TopologyCoordinator interface  * SERVER-31267 CollectionCloner fails if collection is dropped between getMore calls  * SERVER-31684 QueryPlanKilled (operation exceeded time limit) in $changeStream with updateLookup  * SERVER-31749 Clarify the reason temporary unique collection names are made  * SERVER-32098 Disallow operations on drop-pending collections through applyOps  * SERVER-32136 initial_sync_drop_collection.js should wait for system to stablize after restarting node  * SERVER-32224 Disable chaining in initial_sync_drop_collection.js \nAGGREGATION \n * SERVER-32282 Aggregation text search returns text score even if it wasn't requested when targeting multiple shards in a sharded cluster  * SERVER-32430 DocumentSourceSort sorts array documents incorrectly if there is a non-simple collation \nSTORAGE \n * SERVER-29909 Libraries db/db_raii and db/s/sharding are directly cyclic  * SERVER-31750 Always log renameCollections in our tests  * SERVER-32242 Fix race in CompatibleFirstStress lock manager test  * SERVER-32315 Remove IndexObserver \nOPERATIONS \nSERVER-32396 mongo shell failed to connect with 3.6 connection string SRV \nBUILD AND PACKAGING \n * SERVER-31875 Fix long link times on OS X by passing -no_deduplicate to the linker  * SERVER-32415 MongoDB msi installer for Compass contains a typo \nINTERNALS \n * SERVER-30538 check that oplogTruncateAfterPoint is correct in PeriodicKillSecondaries hook  * SERVER-31972 Reduce the number of iterations for the toggle_feature_compatibility.js workload  * SERVER-31997 Add additional unittesting for CompatibleFirst policy  * SERVER-32091 Powercycle - remove mongod.lock file for MMAPV1 test  * SERVER-32145 Avoid dropping lock before cleaning up DocumentSourceCursor's PlanExecutor  * SERVER-32246 PID file permission on v3.6 make it not not monitorable using pid file  * SERVER-32497 implicitly_retry_on_database_drop_pending.js calls tojson() inside of a loop  * SERVER-32500 Disable the sharding Evergreen task on the Enterprise OS X 10.10 MMAPv1 builder \n3.6.1 CHANGELOG  SHARDING\n\n  * SERVER-30226 Force the recipient shard to refresh its metadata after migration commit  * SERVER-30768 Primary queries using maxTimeMS cause temporary shard write unavailability if ExceededTimeLimit  * SERVER-31056 Remove all usages of the default constructor of ScopedCollectionMetadata outside of MetadataManager  * SERVER-31627 ShardingTest.checkUUIDsConsistentAcrossCluster can fail to see collection in config.cache.collections  * SERVER-31865 setFCV on config server should only generate UUIDs for non-dropped sharded collections  * SERVER-31984 A migration will simply report \"Data transfer error\" when we have rich error details on the destination shard that are logged but not returned to the user  * SERVER-32043 Disallow users from creating new indexes in config.transactions  * SERVER-32055 Improve multi thread performance for retryable writes  * SERVER-32123 Enable sign range optimization for clusterTime  * SERVER-32149 ShardingCatalogManager::getDatabasesForShard should check if query was successful  * SERVER-32372 Mongos crashes on bulk inserts which size are slightly bigger than maxBsonObjectSize  * SERVER-32385 \"CommandNotFound: no such command\" in mongodb logs on 3.6 \nREPLICATION \n * SERVER-30457 Cancel catchup takeover if primary caught up according to heartbeats  * SERVER-31990 Rollback can abort on long collection names.  * SERVER-32085 $changeStream reports incorrect documentKey for unsharded collections that become sharded  * SERVER-32114 Delete unused rollback code  * SERVER-32131 ChangeStreams lookup_post_image.js test makes assumptions that don't hold up in secondary read passthrough  * SERVER-32159 fix typo in sync_tail.cpp fillWriterVectorsAndLastestSessionRecords()  * SERVER-32167 do a second majority write on oldPrimary before committed read in read_committed_after_rollback.js  * SERVER-32178 Do not use IDL on oldest oplog entry \nQUERY \n * SERVER-31978 Add an invariant that DocumentSourceCloseCursor does not execute on a mongod for a sharded $changeStream  * SERVER-32046 Arrays of certain NumberDecimals can trigger an invariant failure  * SERVER-32109 $rename does not update value if existing \"to\" field has the same numeric value but different type.  * SERVER-32173 Add deprecation warning to \"snapshot\" option on queries \nWRITE OPERATIONS \n * SERVER-8538 Deprecate $atomic/isolated update option  * SERVER-32048 Updates using a numeric path component may cause index entries not to be created \nAGGREGATION \n * SERVER-31731 Test that mongos accepts a --timeZoneInfo parameter, and can correctly execute expressions using time zones  * SERVER-31885 changeStream cursor is not returned on a mongos when the database does not exist. \nSTORAGE \n * SERVER-31304 remove SnapshotName class  * SERVER-31906 Test that applyOps can clone admin.system.version preserving its UUID  * SERVER-31952 return error if collMod provides a UUID that does not match the UUID of the collection specified  * SERVER-32022 allow enableMajorityReadConcern=false to work  * SERVER-32118 applyOps view creation should not assign UUID  * SERVER-32226 oldest_timestamp should track the last applied time, during initial sync \nOPERATIONS \nSERVER-29453 Disallow removing the featureCompatibilityVersion document \nBUILD AND PACKAGING \n * SERVER-32211 install_compass experience on OSX needs improvement  * SERVER-32286 Remove Type=forking from Debian SystemD service file \nTOOLS \nTOOLS-1895 qa-dump-restore-archiving oplog_rollover_test.js \nINTERNALS \n * SERVER-30770 system_perf.yml: Use new DSI interface and cleanup  * SERVER-31194 Add a version of retryable_writes_jscore_passthrough.yml with stepdowns  * SERVER-31225 The mongod process forks before listening for connections  * SERVER-31660 Bring BSONObj parsers back for IDL generated commands  * SERVER-31791 UUIDs should be added to local collections for replica sets only on clean startup  * SERVER-31808 HostAndPort for replSetInitiate() no longer finds hostname for localhost\n\n  * SERVER-31864 applyOps command with UUID containing op must require granular privileges  * SERVER-32053 explain3.js should assert that its writes succeed  * SERVER-32073 Improve signal given by list_local_sessions.js  * SERVER-32087 Run test stage of sys-perf and mongo-perf failures should lead to red evergreen boxes  * SERVER-32105 Require shard servers and config servers to be started with --replSet or 'replSetName'  * SERVER-32106 Migration of txn oplog entries can trigger fassert in secondary replication  * SERVER-32164 Shell SRV implementation does not allow authSource from TXT records to be used  * SERVER-32238 Revert erroneous high error codes  * TOOLS-1688 Evergreen tests broken in master  * TOOLS-1827 Implement Initial DNS Seedlist discovery spec  * TOOLS-1861 build.sh script doesn't abort on error  * TOOLS-1878 qa-tests-unstable force_table_scan.js  * TOOLS-1880 qa-tests-unstable no_primary_error_code.js  * TOOLS-1881 qa-tests-unstable no_sharded_secondary_reads.js  * WT-3079 Make sure eviction visits all trees  * WT-3776 Cursor remove operation unpins page too early  * WT-3786 Transactions with timestamps should read their writes ←  Release Notes for MongoDB 3.6Compatibility Changes in MongoDB 3.6 → On this page  * 3.6.23 Changelog\n * 3.6.22 Changelog\n * 3.6.21 Changelog\n * 3.6.20 Changelog\n * 3.6.19 Changelog\n * 3.6.18 Changelog\n * 3.6.17 Changelog\n * 3.6.16 Changelog\n * 3.6.15 Changelog\n * 3.6.14 Changelog\n * 3.6.13 Changelog\n * 3.6.12 Changelog\n * 3.6.11 Changelog\n * 3.6.10 Changelog\n * 3.6.9 Changelog\n * 3.6.8 Changelog\n * 3.6.7 Changelog\n * 3.6.6 Changelog\n * 3.6.5 Changelog\n * 3.6.4 Changelog\n * 3.6.3 Changelog\n * 3.6.2 Changelog\n * 3.6.1 Changelog Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/6.0-upgrade-replica-set/": " Docs Home → MongoDB Manual \nUPGRADE A REPLICA SET TO 6.0 \nOn this page    \n * Upgrade Recommendations and Checklists\n   \n * Prerequisites\n * Download 6.0 Binaries\n * Upgrade Procedure\n * Additional Upgrade Procedures Familiarize yourself with the content of this document, including thoroughly reviewing the prerequisites, prior to upgrading to MongoDB 6.0. The following steps outline the procedure to upgrade a mongod that is a replica set member from version 5.0 to 6.0. If you need guidance on upgrading to 6.0, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nUPGRADE RECOMMENDATIONS AND CHECKLISTS \nWhen upgrading, consider the following: \nUPGRADE VERSION PATH \nTo upgrade an existing MongoDB deployment to 6.0, you must be running a 5.0-series release. To upgrade from a version earlier than the 5.0-series, you must successively upgrade major releases until you have upgraded to 5.0-series. For example, if you are running a 4.4-series, you must upgrade first to 5.0 before you can upgrade to 6.0. \nCHECK DRIVER COMPATIBILITY \nBefore you upgrade MongoDB, check that you're using a MongoDB 6.0-compatible driver. Consult the driver documentation for your specific driver to verify compatibility with MongoDB 6.0. Upgraded deployments that run on incompatible drivers might encounter unexpected or undefined behavior. \nPREPAREDNESS \nBefore beginning your upgrade, see the Compatibility Changes in MongoDB 6.0 document to ensure that your applications and deployments are compatible with MongoDB 6.0. Resolve the incompatibilities in your deployment before starting the upgrade. Before upgrading MongoDB, always test your application in a staging environment before deploying the upgrade to your production environment. \nDOWNGRADE CONSIDERATION \nAfter upgrading to 6.0, if you need to downgrade, we recommend downgrading to the latest patch release of 5.0. \nPREREQUISITES  ALL MEMBERS VERSION \nAll replica set members must be running version 5.0. To upgrade a replica set from an 4.4-series and earlier, first upgrade all members of the replica set to the latest 5.0-series release, and then follow the procedure to upgrade from MongoDB 5.0 to 6.0. \nFEATURE COMPATIBILITY VERSION \nThe 5.0 replica set must have featureCompatibilityVersion set to \"5.0\". To ensure that all members of the replica set have featureCompatibilityVersion set to \"5.0\", connect to each replica set member and check the featureCompatibilityVersion: db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )  All members should return a result that includes \"featureCompatibilityVersion\" : { \"version\" : \"5.0\" }. To set or update featureCompatibilityVersion, run the following command on the primary. A majority of the data-bearing members must be available: db.adminCommand( { setFeatureCompatibilityVersion: \"5.0\" } )  For more information, see setFeatureCompatibilityVersion. \nREPLICA SET MEMBER STATE \nEnsure that no replica set member is in the ROLLBACK or RECOVERING state by issuing the replSetGetStatus command: db.adminCommand( { replSetGetStatus: 1 } )  \nDOWNLOAD 6.0 BINARIES  VIA PACKAGE MANAGER \nIf you installed MongoDB from the MongoDB apt, yum, dnf, or zypper repositories, you should upgrade to 6.0 using your package manager. Follow the appropriate 6.0 installation instructions for your Linux system. This will involve adding a repository for the new release, then performing the actual upgrade process. \nMANUALLY \nIf you have not installed MongoDB using a package manager, you can manually download the MongoDB binaries from the MongoDB Download Center. See 6.0 installation instructions for more information. \nUPGRADE PROCEDURE \nYou can upgrade from MongoDB 5.0 to 6.0 using a \"rolling\" upgrade to minimize downtime by upgrading the members individually while the other members are available. 1 \nSHUT DOWN THE REPLICA SET MEMBER. \nTo shut down the mongod process, use mongosh to connect to the replica set member and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 \nUPGRADE SECONDARY MEMBERS OF THE REPLICA SET. \nUpgrade the secondary members of the replica set one at a time:     \n\n  2. Restart the member. 3 \nSTEP DOWN THE REPLICA SET PRIMARY. \nConnect mongosh to the primary and use rs.stepDown() to step down the primary and force an election of a new primary. 4 \nUPGRADE THE PRIMARY. \nWhen rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state, upgrade the stepped-down primary:  1. Shut down the stepped-down primary and replace the mongod binary with the 6.0 binary.  2. Restart the member. 5 \nENABLE BACKWARDS-INCOMPATIBLE 6.0 FEATURES. \nAt this point, you can run the 6.0 binaries without the 6.0 features that are incompatible with 5.0. To enable these 6.0 features, set the feature compatibility version (FCV) to 6.0. \nTIP Enabling these backwards-incompatible features can complicate the downgrade process since you must remove any persisted backwards-incompatible features before you downgrade. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features. \nTIP Ensure that no initial sync is in progress. Running setFeatureCompatibilityVersion command while an initial sync is in progress will cause the initial sync to restart. On the primary, run the setFeatureCompatibilityVersion command in the admin database: db.adminCommand( { setFeatureCompatibilityVersion: \"6.0\" } )  Setting featureCompatibilityVersion (fCV) : \"6.0\" implicitly performs a replSetReconfig to add the term field to the configuration document and blocks until the new configuration propagates to a majority of replica set members. This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command on the primary as the operation is idempotent. \nADDITIONAL UPGRADE PROCEDURES \n * To upgrade a standalone, see Upgrade a Standalone to 6.0.  * To upgrade a sharded cluster, see Upgrade a Sharded Cluster to 6.0. ←  Upgrade a Standalone to 6.0Upgrade a Sharded Cluster to 6.0 → On this page  * Upgrade Recommendations and Checklists\n * Prerequisites\n * Download 6.0 Binaries\n * Upgrade Procedure\n * Additional Upgrade Procedures Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/2.4-upgrade/": " Docs Home → MongoDB Manual \nUPGRADE MONGODB TO 2.4 \nOn this page    \n * Upgrade Recommendations and Checklist\n   \n * Upgrade Standalone mongod Instance to MongoDB 2.4\n * Upgrade a Replica Set from MongoDB 2.2 to MongoDB 2.4\n * Upgrade a Sharded Cluster from MongoDB 2.2 to MongoDB 2.4\n * Rolling Upgrade Limitation for 2.2.0 Deployments Running with auth Enabled\n * Upgrade from 2.3 to 2.4\n * Downgrade MongoDB from 2.4 to Previous Versions In the general case, the upgrade from MongoDB 2.2 to 2.4 is a binary-compatible \"drop-in\" upgrade: shut down the mongod instances and replace them with mongod instances running 2.4. However, before you attempt any upgrade please familiarize yourself with the content of this document, particularly the procedure for upgrading sharded clusters and the considerations for reverting to 2.2 after running 2.4. \nUPGRADE RECOMMENDATIONS AND CHECKLIST \nWhen upgrading, consider the following:      * For all deployments using authentication, upgrade the drivers (i.e. client libraries), before upgrading the mongod instance or instances.  * To upgrade to 2.4 sharded clusters must upgrade following the meta-data upgrade procedure.  * If you're using 2.2.0 and running with authorization enabled, you will need to upgrade first to 2.2.1 and then upgrade to 2.4. See Rolling Upgrade Limitation for 2.2.0 Deployments Running with auth Enabled.  * If you have system.users documents (i.e. for authorization) that you created before 2.4 you must ensure that there are no duplicate values for the user field in the system.users collection in any database. If you do have documents with duplicate user fields, you must remove them before upgrading.\n   \n   See Security Enhancements for more information. \nUPGRADE STANDALONE MONGOD INSTANCE TO MONGODB 2.4 \n 1. Download binaries of the latest release in the 2.4 series from the MongoDB Download Page. See Install MongoDB for more information.  2. Shutdown your mongod instance. Replace the existing binary with the 2.4 mongod binary and restart mongod. \nUPGRADE A REPLICA SET FROM MONGODB 2.2 TO MONGODB 2.4 \nYou can upgrade to 2.4 by performing a \"rolling\" upgrade of the set by upgrading the members individually while the other members are available to minimize downtime. Use the following procedure:  1. Upgrade the secondary members of the set one at a time by shutting down the mongod and replacing the 2.2 binary with the 2.4 binary. After upgrading a mongod instance, wait for the member to recover to SECONDARY state before upgrading the next instance. To check the member's state, issue rs.status() in the mongo shell.  2. Use the mongo shell method rs.stepDown() to step down the primary to allow the normal failover procedure. rs.stepDown() expedites the failover procedure and is preferable to shutting down the primary directly.\n    \n    Once the primary has stepped down and another member has assumed PRIMARY state, as observed in the output of rs.status(), shut down the previous primary and replace mongod binary with the 2.4 binary and start the new process.\n    \n    \n    NOTE\n    \n    Replica set failover is not instant but will render the set unavailable to read or accept writes until the failover process completes. Typically this takes 10 seconds or more. You may wish to plan the upgrade during a predefined maintenance window. \nUPGRADE A SHARDED CLUSTER FROM MONGODB 2.2 TO MONGODB 2.4  IMPORTANT Only upgrade sharded clusters to 2.4 if all members of the cluster are currently running instances of 2.2. The only supported upgrade path for sharded clusters running 2.0 is via 2.2. \nOVERVIEW \nUpgrading a sharded cluster from MongoDB version 2.2 to 2.4 (or 2.3) requires that you run a 2.4 mongos with the --upgrade option, described in this procedure. The upgrade process does not require downtime.\n\n After completing the meta-data upgrade you can fully upgrade the components of the cluster. With the balancer disabled:  * Upgrade all mongos instances in the cluster.  * Upgrade all 3 mongod config server instances.  * Upgrade the mongod instances for each shard, one at a time. See Upgrade Sharded Cluster Components for more information. \nCLUSTER META-DATA UPGRADE \nCONSIDERATIONS \nBeware of the following properties of the cluster upgrade process:  * Before you start the upgrade, ensure that the amount of free space on the filesystem for the config database is at least 4 to 5 times the amount of space currently used by the config database data files.\n   \n   Additionally, ensure that all indexes in the config database are {v:1} indexes. If a critical index is a {v:0} index, chunk splits can fail due to known issues with the {v:0} format. {v:0} indexes are present on clusters created with MongoDB 2.0 or earlier.\n   \n   The duration of the metadata upgrade depends on the network latency between the node performing the upgrade and the three config servers. Ensure low latency between the upgrade process and the config servers.  * While the upgrade is in progress, you cannot make changes to the collection meta-data. For example, during the upgrade, do not perform:\n   \n   * sh.enableSharding(),\n   \n   * sh.shardCollection(),\n   \n   * sh.addShard(),\n   \n   * db.createCollection(),\n   \n   * db.collection.drop(),\n   \n   * db.dropDatabase(),\n   \n   * any operation that creates a database, or\n   \n   * any other operation that modifies the cluster meta-data in any way. See Sharding Reference for a complete list of sharding commands. Note, however, that not all commands on the Sharding Reference page modifies the cluster meta-data.  * Once you upgrade to 2.4 and complete the upgrade procedure do not use 2.0 mongod and mongos processes in your cluster. 2.0 process may re-introduce old meta-data formats into cluster meta-data. The upgraded config database will require more storage space than before, to make backup and working copies of the config.chunks and config.collections collections. As always, if storage requirements increase, the mongod might need to pre-allocate additional data files. See How can I get information on the storage use of a database? for more information. META-DATA UPGRADE PROCEDURE \nChanges to the meta-data format for sharded clusters, stored in the config database, require a special meta-data upgrade procedure when moving to 2.4. Do not perform operations that modify meta-data while performing this procedure. See Upgrade a Sharded Cluster from MongoDB 2.2 to MongoDB 2.4 for examples of prohibited operations.  1. Before you start the upgrade, ensure that the amount of free space on the filesystem for the config database is at least 4 to 5 times the amount of space currently used by the config database data files.\n    \n    Additionally, ensure that all indexes in the config database are {v:1} indexes. If a critical index is a {v:0} index, chunk splits can fail due to known issues with the {v:0} format. {v:0} indexes are present on clusters created with MongoDB 2.0 or earlier.\n    \n    The duration of the metadata upgrade depends on the network latency between the node performing the upgrade and the three config servers. Ensure low latency between the upgrade process and the config servers.\n    \n    To check the version of your indexes, use db.collection.getIndexes().\n    \n    If any index on the config database is {v:0}, you should rebuild those indexes by connecting to the mongos and either: rebuild all indexes using the db.collection.reIndex() method, or drop and rebuild specific indexes using db.collection.dropIndex() and then db.collection.ensureIndex(). If you need to upgrade the _id index to {v:1} use db.collection.reIndex().\n    \n    You may have {v:0} indexes on other databases in the cluster.\n\n  3. Ensure there are no version 2.0 mongod or mongos processes still active in the sharded cluster. The automated upgrade process checks for 2.0 processes, but network availability can prevent a definitive check. Wait 5 minutes after stopping or upgrading version 2.0 mongos processes to confirm that none are still active.  4. Start a single 2.4 mongos process with configDB pointing to the sharded cluster's config servers and with the --upgrade option. The upgrade process happens before the process becomes a daemon (i.e. before --fork.)\n    \n    You can upgrade an existing mongos instance to 2.4 or you can start a new mongos instance that can reach all config servers if you need to avoid reconfiguring a production mongos.\n    \n    Start the mongos with a command that resembles the following:\n    \n    mongos --configdb <config servers> --upgrade\n    \n    \n    \n    Without the --upgrade option 2.4 mongos processes will fail to start until the upgrade process is complete.\n    \n    The upgrade will prevent any chunk moves or splits from occurring during the upgrade process. If there are very many sharded collections or there are stale locks held by other failed processes, acquiring the locks for all collections can take seconds or minutes. See the log for progress updates.  5. When the mongos process starts successfully, the upgrade is complete. If the mongos process fails to start, check the log for more information.\n    \n    If the mongos terminates or loses its connection to the config servers during the upgrade, you may always safely retry the upgrade.\n    \n    However, if the upgrade failed during the short critical section, the mongos will exit and report that the upgrade will require manual intervention. To continue the upgrade process, you must follow the Resync after an Interruption of the Critical Section procedure.\n    \n    \n    NOTE\n    \n    \n    OPTIONAL\n    \n    If the mongos logs show the upgrade waiting for the upgrade lock, a previous upgrade process may still be active or may have ended abnormally. After 15 minutes of no remote activity mongos will force the upgrade lock. If you can verify that there are no running upgrade processes, you may connect to a 2.2 mongos process and force the lock manually:\n    \n    mongo <mongos.example.net>\n    \n    \n    \n    db.getMongo().getCollection(\"config.locks\").findOne({ _id : \"configUpgrade\" })\n    \n    \n    \n    If the process specified in the process field of this document is verifiably offline, run the following operation to force the lock.\n    \n    db.getMongo().getCollection(\"config.locks\").update({ _id : \"configUpgrade\" }, { $set : { state : 0 } })\n    \n    \n    \n    It is always more safe to wait for the mongos to verify that the lock is inactive, if you have any doubts about the activity of another upgrade operation. In addition to the configUpgrade, the mongos may need to wait for specific collection locks. Do not force the specific collection locks.  6. Upgrade and restart other mongos processes in the sharded cluster, without the --upgrade option.\n    \n    See Upgrade Sharded Cluster Components for more information.  7. Re-enable the balancer. You can now perform operations that modify cluster meta-data. Once you have upgraded, do not introduce version 2.0 MongoDB processes into the sharded cluster. This can reintroduce old meta-data formats into the config servers. The meta-data change made by this upgrade process will help prevent errors caused by cross-version incompatibilities in future versions of MongoDB. RESYNC AFTER AN INTERRUPTION OF THE CRITICAL SECTION\n\n \nNOTE Only perform the following procedure if something (e.g. network, power, etc.) interrupts the upgrade process during the short critical section of the upgrade. Remember, you may always safely attempt the meta data upgrade procedure. To resync the config servers:  1.  Turn off the balancer in the sharded cluster and stop all meta-data operations. If you are in the middle of an upgrade process (Upgrade a Sharded Cluster from MongoDB 2.2 to MongoDB 2.4), you have already disabled the balancer.  2.  Shut down two of the three config servers, preferably the last two listed in the configDB string. For example, if your configDB string is configA:27019,configB:27019,configC:27019, shut down configB and configC. Shutting down the last two config servers ensures that most mongos instances will have uninterrupted access to cluster meta-data.  3.  mongodump the data files of the active config server (configA).  4.  Move the data files of the deactivated config servers (configB and configC) to a backup location.  5.  Create new, empty data directories.  6.  Restart the disabled config servers with --dbpath pointing to the now-empty data directory and --port pointing to an alternate port (e.g. 27020).  7.  Use mongorestore to repopulate the data files on the disabled documents from the active config server (configA) to the restarted config servers on the new port (configB:27020,configC:27020). These config servers are now re-synced.  8.  Restart the restored config servers on the old port, resetting the port back to the old settings (configB:27019 and configC:27019).  9.  In some cases connection pooling may cause spurious failures, as the mongos disables old connections only after attempted use. 2.4 fixes this problem, but to avoid this issue in version 2.2, you can restart all mongos instances (one-by-one, to avoid downtime) and use the rs.stepDown() method before restarting each of the shard replica set primaries.  10. The sharded cluster is now fully resynced; however before you attempt the upgrade process again, you must manually reset the upgrade state using a version 2.2 mongos. Begin by connecting to the 2.2 mongos with the mongo shell:\n     \n     mongo <mongos.example.net>\n     \n     \n     \n     Then, use the following operation to reset the upgrade process:\n     \n     db.getMongo().getCollection(\"config.version\").update({ _id : 1 }, { $unset : { upgradeState : 1 } })\n     \n       11. Finally retry the upgrade process, as in Upgrade a Sharded Cluster from MongoDB 2.2 to MongoDB 2.4. \nUPGRADE SHARDED CLUSTER COMPONENTS \nAfter you have successfully completed the meta-data upgrade process described in Meta-data Upgrade Procedure, and the 2.4 mongos instance starts, you can upgrade the other processes in your MongoDB deployment. While the balancer is still disabled, upgrade the components of your sharded cluster in the following order:  * Upgrade all mongos instances in the cluster, in any order.  * Upgrade all 3 mongod config server instances, upgrading the first system in the mongos --configdb argument last.  * Upgrade each shard, one at a time, upgrading the mongod secondaries before running replSetStepDown and upgrading the primary of each shard. When this process is complete, you can now re-enable the balancer. \nROLLING UPGRADE LIMITATION FOR 2.2.0 DEPLOYMENTS RUNNING WITH AUTH ENABLED \nMongoDB cannot support deployments that mix 2.2.0 and 2.4.0, or greater, components. MongoDB version 2.2.1 and later processes can exist in mixed deployments with 2.4-series processes. Therefore you cannot perform a rolling upgrade from MongoDB 2.2.0 to MongoDB 2.4.0. To upgrade a cluster with 2.2.0 components, use one of the following procedures.\n\n  2. Stop all processes in the cluster. Upgrade all processes to a 2.4-series release of MongoDB, and start all processes at the same time. \nUPGRADE FROM 2.3 TO 2.4 \nIf you used a mongod from the 2.3 or 2.4-rc (release candidate) series, you can safely transition these databases to 2.4.0 or later; however, if you created 2dsphere or text indexes using a mongod before v2.4-rc2, you will need to rebuild these indexes. For example: db.records.dropIndex( { loc: \"2dsphere\" } )db.records.dropIndex( \"records_text\" )\ndb.records.ensureIndex( { loc: \"2dsphere\" } )db.records.ensureIndex( { records: \"text\" } )  \nDOWNGRADE MONGODB FROM 2.4 TO PREVIOUS VERSIONS \nFor some cases the on-disk format of data files used by 2.4 and 2.2 mongod is compatible, and you can upgrade and downgrade if needed. However, several new features in 2.4 are incompatible with previous versions:  * 2dsphere indexes are incompatible with 2.2 and earlier mongod instances.  * text indexes are incompatible with 2.2 and earlier mongod instances.  * using a hashed index as a shard key are incompatible with 2.2 and earlier mongos instances.  * hashed indexes are incompatible with 2.0 and earlier mongod instances. \nIMPORTANT Collections sharded using hashed shard keys, should not use 2.2 mongod instances, which cannot correctly support cluster operations for these collections. If you completed the meta-data upgrade for a sharded cluster, you can safely downgrade to 2.2 MongoDB processes. Do not use 2.0 processes after completing the upgrade procedure. \nNOTE In sharded clusters, once you have completed the meta-data upgrade procedure, you cannot use 2.0 mongod or mongos instances in the same cluster. If you complete the meta-data upgrade, you can safely downgrade components in any order. When upgrade again, always upgrade mongos instances before mongod instances. Do not create 2dsphere or text indexes in a cluster that has 2.2 components. \nCONSIDERATIONS AND COMPATIBILITY \nIf you upgrade to MongoDB 2.4, and then need to run MongoDB 2.2 with the same data files, consider the following limitations.  * If you use a hashed index as the shard key index, which is only possible under 2.4 you will not be able to query data in this sharded collection. Furthermore, a 2.2 mongos cannot properly route an insert operation for a collections sharded using a hashed index for the shard key index: any data that you insert using a 2.2 mongos, will not arrive on the correct shard and will not be reachable by future queries.  * If you never create an 2dsphere or text index, you can move between a 2.4 and 2.2 mongod for a given data set; however, after you create the first 2dsphere or text index with a 2.4 mongod you will need to run a 2.2 mongod with the --upgrade option and drop any 2dsphere or text index. \nUPGRADE AND DOWNGRADE PROCEDURES \nBASIC DOWNGRADE AND UPGRADE \nExcept as described below, moving between 2.2 and 2.4 is a drop-in replacement:  * stop the existing mongod, using the --shutdown option as follows:\n   \n   mongod --dbpath /var/mongod/data --shutdown\n   \n   \n   \n   Replace /var/mongod/data with your MongoDB dbPath.  * start the new mongod processes with the same dbPath setting, for example:\n   \n   mongod --dbpath /var/mongod/data\n   \n   \n   \n   Replace /var/mongod/data with your MongoDB dbPath. DOWNGRADE TO 2.2 AFTER CREATING A 2DSPHERE OR TEXT INDEX \nIf you have created 2dsphere or text indexes while running a 2.4 mongod instance, you can downgrade at any time, by starting the 2.2 mongod with the --upgrade option as follows:\n\n  Then, you will need to drop any existing 2dsphere or text indexes using db.collection.dropIndex(), for example: db.records.dropIndex( { loc: \"2dsphere\" } )db.records.dropIndex( \"records_text\" )  \nWARNING --upgrade will run repairDatabase on any database where you have created a 2dsphere or text index, which will rebuild all indexes. TROUBLESHOOTING UPGRADE/DOWNGRADE OPERATIONS \nIf you do not use --upgrade, when you attempt to start a 2.2 mongod and you have created a 2dsphere or text index, mongod will return the following message: 'need to upgrade database index_plugin_upgrade with pdfile version 4.6, new version: 4.5 Not upgrading, exiting'  While running 2.4, to check the data file version of a MongoDB database, use the following operation in the shell: db.getSiblingDB('<databasename>').stats().dataFileVersion  The major data file [1] version for both 2.2 and 2.4 is 4, the minor data file version for 2.2 is 5 and the minor data file version for 2.4 is 6 after you create a 2dsphere or text index. [1] The data file version (i.e. pdfile version) is independent and unrelated to the release version of MongoDB. ←  JavaScript Changes in MongoDB 2.4Compatibility and Index Type Changes in MongoDB 2.4 → On this page  * Upgrade Recommendations and Checklist\n * Upgrade Standalone mongod Instance to MongoDB 2.4\n * Upgrade a Replica Set from MongoDB 2.2 to MongoDB 2.4\n * Upgrade a Sharded Cluster from MongoDB 2.2 to MongoDB 2.4\n * Rolling Upgrade Limitation for 2.2.0 Deployments Running with auth Enabled\n * Upgrade from 2.3 to 2.4\n * Downgrade MongoDB from 2.4 to Previous Versions Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/2.4-index-types/": " Docs Home → MongoDB Manual \nCOMPATIBILITY AND INDEX TYPE CHANGES IN MONGODB 2.4 \nOn this page    \n * New Index Types\n   \n * Index Type Validation In 2.4 MongoDB includes two new features related to indexes that users upgrading to version 2.4 must consider, particularly with regard to possible downgrade paths. For more information on downgrades, see Downgrade MongoDB from 2.4 to Previous Versions. \nNEW INDEX TYPES \nIn 2.4 MongoDB adds two new index types: 2dsphere and text. These index types do not exist in 2.2, and for each database, creating a 2dsphere or text index, will upgrade the data-file version and make that database incompatible with 2.2. If you intend to downgrade, you should always drop all 2dsphere and text indexes before moving to 2.2. You can use the downgrade procedure to downgrade these databases and run 2.2 if needed, however this will run a full database repair (as with repairDatabase) for all affected databases. \nINDEX TYPE VALIDATION \nIn MongoDB 2.2 and earlier you could specify invalid index types that did not exist. In these situations, MongoDB would create an ascending (e.g. 1) index. Invalid indexes include index types specified by strings that do not refer to an existing index type, and all numbers other than 1 and -1. [1] In 2.4, creating any invalid index will result in an error. Furthermore, you cannot create a 2dsphere or text index on a collection if its containing database has any invalid index types. [1] \nEXAMPLE If you attempt to add an invalid index in MongoDB 2.4, as in the following: db.coll.ensureIndex( { field: \"1\" } )  MongoDB will return the following error document: {  \"err\" : \"Unknown index plugin '1' in index { field: \\\"1\\\" }\"  \"code\": 16734,  \"n\": <number>,  \"connectionId\": <number>,  \"ok\": 1}  [1](1, 2) In 2.4, indexes that specify a type of \"1\" or \"-1\" (the strings \"1\" and \"-1\") will continue to exist, despite a warning on start-up. However, a secondary in a replica set cannot complete an initial sync from a primary that has a \"1\" or \"-1\" index. Avoid all indexes with invalid types. ←  Upgrade MongoDB to 2.4Release Notes for MongoDB 2.2 → On this page  * New Index Types\n * Index Type Validation Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.4-upgrade-sharded-cluster/": " Docs Home → MongoDB Manual \nUPGRADE A SHARDED CLUSTER TO 3.4 \nOn this page    \n * Upgrade Recommendations and Checklists\n   \n * mongos and Earlier Versions of mongod Instances\n * Use Package Manager\n * Download 3.4 Binaries Manually \nNOTE      * Starting in version 3.4.21, MongoDB 3.4-series removes support for Ubuntu 16.04 POWER/PPC64LE.  * For earlier MongoDB Enterprise versions that support Ubuntu 16.04 POWER/PPC64LE:\n   \n   Due to a lock elision bug present in older versions of the glibc package on Ubuntu 16.04 for POWER, you must upgrade the glibc package to at least glibc 2.23-0ubuntu5 before running MongoDB. Systems with older versions of the glibc package will experience database server crashes and misbehavior due to random memory corruption, and are unsuitable for production deployments of MongoDB \nIMPORTANT Before you attempt any upgrade, please familiarize yourself with the content of this document. If you need guidance on upgrading to 3.4, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nUPGRADE RECOMMENDATIONS AND CHECKLISTS \nWhen upgrading, consider the following: \nUPGRADE VERSION PATH \nTo upgrade an existing MongoDB deployment to 3.4, you must be running a 3.2-series release. To upgrade from a version earlier than the 3.2-series, you must successively upgrade major releases until you have upgraded to 3.2-series. For example, if you are running a 3.0-series, you must 3.2 before you can upgrade to 3.4. \nCHECK DRIVER COMPATIBILITY \nBefore you upgrade MongoDB, check that you're using a MongoDB 3.4-compatible driver. Consult the driver documentation for your specific driver to verify compatibility with MongoDB 3.4. Upgraded deployments that run on incompatible drivers might encounter unexpected or undefined behavior. \nPREPAREDNESS \nBefore beginning your upgrade, see the Compatibility Changes in MongoDB 3.4 document to ensure that your applications and deployments are compatible with MongoDB 3.4. Resolve the incompatibilities in your deployment before starting the upgrade. Before upgrading MongoDB, always test your application in a staging environment before deploying the upgrade to your production environment. \nDOWNGRADE CONSIDERATION \nOnce upgraded to 3.4, you cannot downgrade to a 3.2.7 or earlier version. You can only downgrade to a 3.2.8 or later version. Avoid reconfiguring replica sets that contain members of different MongoDB versions as validation rules may differ across MongoDB versions. \nMONGOS AND EARLIER VERSIONS OF MONGOD INSTANCES \nVersion 3.4 mongos instances cannot connect to earlier versions of mongod instances. The 3.2 and earlier mongo shell is not compatible with 3.4 clusters. \nPREREQUISITES \n * Version 3.2 or GreaterTo upgrade a sharded cluster to 3.4, all members of the cluster must be at least version 3.2. The upgrade process checks all components of the cluster and will produce warnings if any component is running version earlier than 3.2.\n * Config Server as Replica Set (CSRS)\n   \n   Starting in 3.4, the use of the deprecated mirrored mongod instances as config servers (SCCC) is no longer supported.\n   \n   Before you can upgrade your sharded clusters to 3.4, you must convert your config servers from SCCC to a replica set (CSRS). To convert your config servers from SCCC to CSRS, see Upgrade Config Servers to Replica Set.  * Disable the balancer  * Back up the config DatabaseOptional but Recommended. As a precaution, take a backup of the config database before upgrading the sharded cluster. \nDOWNLOAD 3.4 BINARIES  USE PACKAGE MANAGER \nIf you installed MongoDB from the MongoDB apt, yum, dnf, or zypper repositories, you should upgrade to 3.4 using your package manager. Follow the appropriate 3.4 installation instructions for your Linux system. This will involve adding a repository for the new release, then performing the actual upgrade process. \nDOWNLOAD 3.4 BINARIES MANUALLY \nIf you have not installed MongoDB using a package manager, you can manually download the MongoDB binaries from the MongoDB Download Center. See 3.4 installation instructions for more information.\n\n \n1 DISABLE THE BALANCER. \nDisable the balancer as described in Disable the Balancer. 2 UPGRADE THE CONFIG SERVERS. \nIf the config servers are replica sets:  1. Upgrade the secondary members of the replica set one at a time:\n    \n    a. Shut down the secondary mongod instance and replace the 3.2 binary with the 3.4 binary.\n    \n    b. Start the 3.4 binary with both the --configsvr and --port options:\n       \n       mongod --configsvr --port <port> --dbpath <path>\n       \n       \n       \n       If using a configuration file, update the file to specify sharding.clusterRole: configsvr and net.port and start the 3.4 binary:\n       \n       sharding:   clusterRole: configsvrnet:   port: <port>storage:   dbpath: <path>\n       \n       \n       \n       Include any other configuration as appropriate for your deployment.\n    \n    c. Wait for the member to recover to SECONDARY state before upgrading the next secondary member. To check the member's state, issue rs.status() in the mongo shell.\n       \n       Repeat for each secondary member.  2. Step down the replica set primary.\n    \n    a. Connect a mongo shell to the primary and use rs.stepDown() to step down the primary and force an election of a new primary:\n       \n       rs.stepDown()\n       \n       \n    \n    b. When rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state, shut down the stepped-down primary and replace the mongod binary with the 3.4 binary.\n    \n    c. Start the 3.4 binary with both the --configsvr and --port options:\n       \n       mongod --configsvr --port <port> --dbpath <path>\n       \n       \n       \n       If using a configuration file, update the file to specify sharding.clusterRole: configsvr and net.port and start the 3.4 binary:\n       \n       sharding:   clusterRole: configsvrnet:   port: <port>storage:   dbpath: <path>\n       \n       \n       \n       Include any other configuration as appropriate for your deployment. 3 UPGRADE THE SHARDS. \nUpgrade the shards one at a time. If the shards are replica sets, for each shard:\n\n  2. Step down the replica set primary.\n    \n    Connect a mongo shell to the primary and use rs.stepDown() to step down the primary and force an election of a new primary:\n    \n    rs.stepDown()\n    \n      3. When rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state, upgrade the stepped-down primary:\n    \n    1. Shut down the stepped-down primary and replace the mongod binary with the 3.4 binary.\n    \n    2. Start the 3.4 binary with the --shardsvr and --port command line options.\n       \n       mongod --shardsvr --port <port> --dbpath <path>\n       \n       \n       \n       Of if using a configuration file, update the file to specify sharding.clusterRole: shardsvr and net.port and start the 3.4 binary:\n       \n       sharding:   clusterRole: shardsvrnet:   port: <port>storage:   dbpath: <path>\n       \n       \n       \n       Include any other configuration as appropriate for your deployment. 4 UPGRADE THE MONGOS INSTANCES. \nReplace each mongos instance with the 3.4 binary and restart. Include any other configuration as appropriate for your deployment. \nNOTE In 3.4, mongos no longer supports --chunkSize and --noAutoSplit runtime options (or the corresponding sharding.chunkSize and sharding.autoSplit settings). If your 3.2 mongos configuration includes these settings, remove the settings when running the 3.4 mongos. mongos --configdb csReplSet/<rsconfigsver1:port1>,<rsconfigsver2:port2>,<rsconfigsver3:port3> \n5 RE-ENABLE THE BALANCER. \nUsing a 3.4 mongo shell, re-enable the balancer as described in Enable the Balancer. The 3.2 and earlier mongo shell is not compatible with 3.4 clusters. 6 ENABLE BACKWARDS-INCOMPATIBLE 3.4 FEATURES. \nAt this point, you can run the 3.4 binaries without the 3.4 features that are incompatible with 3.2. To enable these 3.4 features, set the feature compatibility version to 3.4. \nWARNING Enabling these backwards-incompatible features can complicate the downgrade process. For details, see Remove 3.4 Incompatible Features. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features. On a mongos instance, run the setFeatureCompatibilityVersion command in the admin database: db.adminCommand( { setFeatureCompatibilityVersion: \"3.4\" } )  This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command on the mongos as the operation is idempotent. \nADDITIONAL UPGRADE PROCEDURES \n * To upgrade a standalone, see Upgrade a Standalone to 3.4.  * To upgrade a replica set, see Upgrade a Replica Set to 3.4. ←  Upgrade a Replica Set to 3.4Downgrade MongoDB 3.4 to 3.2 → On this page  * Upgrade Recommendations and Checklists\n * mongos and Earlier Versions of mongod Instances\n * Use Package Manager\n * Download 3.4 Binaries Manually Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.6/": " Docs Home → MongoDB Manual \nRELEASE NOTES FOR MONGODB 3.6 \nOn this page    \n * Patch Releases\n   \n * Security\n * Aggregation\n * Array Update Operator Enhancements\n * 3.6 Compatible Drivers\n * Change Streams\n * Client Sessions\n * Server Sessions\n * JSON Schema\n * Replica Sets\n * Sharded Clusters\n * General Enhancements\n * Changes Affecting Compatibility\n * Upgrade Procedures\n * Download\n * Known Issues in 3.6.3\n * Known Issues in 3.6.2\n * Known Issues in 3.6.1\n * Known Issues in 3.6.0\n * Report an Issue \nPATCH RELEASES  3.6.23 - MAR 19, 2021 \nIssues Fixed:      * SERVER-40361: Reduce memory footprint of plan cache entries  * SERVER-46686: Explain does not respect maxTimeMS  * SERVER-50267: Set output limit for 'rawMongoProgramOutput()'  * TOOLS-2803: [v3.6] Add --config option for password values  * All JIRA issues closed in 3.6.23  * 3.6.23 Changelog \n3.6.22 - FEB 8, 2021 \nIssues Fixed:  * SERVER-32437: Platform Support: add Amazon Linux 2  * SERVER-33747: Arbiter tries to start data replication if cannot find itself in config after restart  * SERVER-52654: new signing keys not generated by the monitoring-keys-for-HMAC thread  * SERVER-52680: Removed node on startup stuck in STARTUP2 after being re-added into the replica set  * SERVER-53026: Secondary cannot restart replication  * All JIRA issues closed in 3.6.22  * 3.6.22 Changelog \n3.6.21 - NOV 16, 2020 \nIssues Fixed:  * SERVER-26726: Check number of arguments for createIndex() and throw error if more than two arguments  * SERVER-34243: listCollections should not require a MODE_S database lock  * SERVER-49986: Convert isMaster command to hello and add aliases  * SERVER-50736: Make OpenSSL explicitly accept SNIs presented in ClientHello  * SERVER-51120: Find queries with MERGE_SORT incorrectly sort the results when the collation is specified  * All JIRA issues closed in 3.6.21  * 3.6.21 Changelog \n3.6.20 - SEP 14, 2020 \nIssues Fixed:  * SERVER-40317: $facet execution has no limit on how much memory it can consume  * SERVER-41600: IndexCatalog::refreshEntry should invalidate the index from the CollectionInfoCache  * SERVER-43233: Add ability to request only specific attribute(s) for the LDAP groups  * SERVER-48709: signing key generator thread on config server not waken up as expected  * SERVER-48993: explodeForSort can produce incorrect query plan  * All JIRA issues closed in 3.6.20  * 3.6.20 Changelog \n3.6.19 - JUL 23, 2020 \nIssues Fixed:  * SERVER-46487 The mongos routing for scatter/gather ops can have unbounded latency  * SERVER-45610 Some reads work while system is RECOVERING  * SERVER-47695 Write commands run by threads that can survive rollback can fail operationTime invariant in ServiceEntryPoint  * SERVER-47994 Fix for numerical overflow in GeoHash  * SERVER-47686 Upgrade static OpenSSL to 1.1.1g  * All JIRA issues closed in 3.6.19  * 3.6.19 Changelog \n3.6.18 - MAY 1, 2020 \nIssues fixed:  * SERVER-46466: Race with findAndModify retryable write and session migration  * SERVER-45935: [3.6] WT error handler incorrectly prints “An unsupported journal format detected”  * SERVER-45418: DocumentSourceCursor batching memory accounting does not account for empty documents, leads to unbounded memory use for count-like aggregates  * SERVER-45363: Issue with mongodb text indexes and weights when using wildcard specifier  * SERVER-35050: Don’t abort collection clone due to negative document count  * All JIRA issues closed in 3.6.18  * 3.6.18 Changelog \nNOTE Fixed issues include those that resolve the following Common Vulnerabilities and Exposure (CVE):\n\n \n3.6.17 - JAN 27, 2020 \nIssues fixed:  * SERVER-42565: Aggregations and find commands sort missing fields differently  * SERVER-44733: Change stream should throw ChangeStreamFatalError if a single shard cannot be targeted for updateLookup  * SERVER-44174: $push and $addToSet should restrict memory usage  * SERVER-37772: Platform Support: Add Community & Enterprise RHEL 8 x64  * SERVER-45396: fix the \"me\" field in isMaster responses when using splithorizon  * WT-5042: Reduce configuration parsing overhead from checkpoints  * All JIRA issues closed in 3.6.17  * 3.6.17 Changelog \n3.6.16 - DEC 6, 2019 \nIssues fixed:  * SERVER-38002: Upgrade Pcre to at least 8.42  * SERVER-35463: Mark listCommands as pre-auth  * SERVER-44584: Rewriting updates as modifications incorrectly considers logging state  * All JIRA issues closed in 3.6.16  * 3.6.16 Changelog \n3.6.15 - NOV 6, 2019 \nIssues fixed:  * SERVER-39004: Introduce a quota mechanism for the overflow file  * SERVER-38994: Step down on SIGTERM  * SERVER-40355: rs.config that contains an _id greater than the number of nodes will crash  * SERVER-43074: Do not use a global variable to encode 'multikeyPath' information when writing out catalog documents  * WT-4869: Stop adding cache pressure when eviction is falling behind  * All JIRA issues closed in 3.6.15  * 3.6.15 Changelog \n3.6.14 - AUG 26, 2019 \nIssues fixed:  * SERVER-36443: Long-running queries should not cause a build-up of unused ChunkManager objects  * SERVER-38945: SSL performance regression  * SERVER-40134: Distinct command against a view can return incorrect results when the distinct path is multikey  * SERVER-42055: Only acquire a collection IX lock to write the lastVote document  * SERVER-42603: Recent service file change may cause cyclic dependencies  * SERVER-42089: Platform Support: Remove Enterprise RHEL 6.7 zSeries  * WT-4803: Implement file_max configuration for Cache Overflow mechanism  * All JIRA issues closed in 3.6.14  * 3.6.14 Changelog \nNOTE Fixed issues include those that resolve the following Common Vulnerabilities and Exposures (CVEs):  * CVE-2019-2389 (See SERVER-40563)  * CVE-2019-2390 (See SERVER-42233) \n3.6.13 - JUN 10, 2019 \nIssues fixed:  * SERVER-16571: Use Actual Memory Constraint vs. Total System Memory When They Differ  * SERVER-38984: Attach IDs to users  * SERVER-41213: Unique background index builds may produce inconsistent keys  * SERVER-37765: Platform Support: Remove Ubuntu 14.04  * SERVER-37774: Platform Support: Remove Enterprise Ubuntu 16.04 PPCLE  * All JIRA issues closed in 3.6.13  * 3.6.13 Changelog \nNOTE Fixed issues include those that resolve the following Common Vulnerabilities and Exposures (CVEs):  * CVE-2019-2386 (See SERVER-38984) \n3.6.12 - APRIL 8, 2019 \nIssues fixed:  * SERVER-35219: Regain MongoDB balancer performance with sessions  * SERVER-39847: Migrating session info can trigger fassert when destination shard has transaction history truncated by oplog  * SERVER-37255: replSetReconfig with concurrent election can trigger invariant  * SERVER-39394: Views collation check should traverse through nested $lookup pipelines  * SERVER-39487: Aggregation operator $sqrt output for NumberDecimal is the same as $exp  * SERVER-37722: Quiet mode does not suppress connection end events  * All JIRA issues closed in 3.6.12  * 3.6.12 Changelog \n3.6.11 - MAR 1, 2019 \nIssues fixed:  * SERVER-32146: Log slow oplog entry application  * SERVER-9043: Link flushRouterConfig on the config server and shards  * SERVER-37624: Sessions never expire when you change value of localLogicalSessionTimeoutMinutes\n\n  * SERVER-39166: $graphLookup should force a pipeline to split in sharded cluster  * All JIRA issues closed in 3.6.11  * 3.6.11 Changelog \n3.6.10 - JAN 22, 2019 \nIssues fixed:  * SERVER-29825: Do not allow rename from unreplicated to replicated DB or vice-versa  * SERVER-35936: MongoDB Community installer fails to install Compass  * SERVER-36977: Initial mongod.log is created using umask vs mode 600  * SERVER-37080: Implement tuneable batch size for chunk migrations  * SERVER-37616: Implement tuneable batch size for the rangedeleter  * SERVER-37182: Different values when referencing whole object vs. a field of that object after $arrayToObject  * SERVER-37200: $match stage following $listSessions not working against mongos  * All JIRA issues closed in 3.6.10  * 3.6.10 Changelog \n3.6.9 - NOV 16, 2018 \nIssues fixed:  * SERVER-32943: Query during background index build can lead to suboptimal cached plan  * SERVER-35657: Do not delay journal flushes when operations are waiting for oplog visibility  * SERVER-37058: Update with numeric field names inside an array can cause validation to fail  * SERVER-37132: Negation of $in with regex can incorrectly plan from the cache, leading to missing query results  * All JIRA issues closed in 3.6.9  * 3.6.9 Changelog \n3.6.8 - SEP 19, 2018 \nIssues fixed:  * SERVER-34204: Tailable cursor fails on getMore against a sharded cluster.  * SERVER-34846: Covered index with collated field returns incorrect result when collation not involved in match or sort.  * SERVER-37002: Dropping a collection with long index names via rename fails under MMAPv1.  * SERVER-36149: Fix privileges on setFCV virtual namespace.  * All JIRA issues closed in 3.6.8  * 3.6.8 Changelog \n3.6.7 - AUG 25, 2018 \nIssues fixed:  * SERVER-27725: Use batch insert when migrating chunks  * SERVER-36070: Aggregation with $out results in error when Auditing is enabled  * TOOLS-2075: mongoreplay always replays to secondary  * All JIRA issues closed in 3.6.7  * 3.6.7 Changelog \n3.6.6 - JUL 10, 2018 \nIssues fixed:  * SERVER-33538: mapReduce \"replace\" on a sharded output collection can lead to UUIDCatalog inconsistencies.  * SERVER-32999: Platform Support: Remove Debian 7 builds.  * SERVER-5461 Add syncSourceHost field to replSetGetStatus output.  * All JIRA issues closed in 3.6.6  * 3.6.6 Changelog \n3.6.5 - MAY 29, 2018 \nIssues fixed:  * SERVER-34746: Segmentation fault when shard is started with --shardsvr before being added to a shard.  * SERVER-34423: collMod interruption may cause invariant failure.  * SERVER-34390: MongoDB binaries for macOS support TLS 1.2.  * SERVER-29463: Platform Support: add Debian 9 \"stretch\".  * SERVER-34399: $changeStream with invalid resume token crashes the server.  * SERVER-29301: Upgrade MozJS to ESR 45.9.0.  * All JIRA issues closed in 3.6.5  * 3.6.5 Changelog \n3.6.4 - APR 13, 2018 \nIssues fixed:  * SERVER-28670: Add sharding metadata refresh metrics section to serverStatus.  * SERVER-32677: Segmentation fault converting ReplicaSet to Replicated Shard Cluster.  * SERVER-33763: 3.6 drivers fail to communicate with 3.6 sharded clusters running at FCV 3.4.  * SERVER-32923: Remove SLES11 support.  * SERVER-31535: Remove Ubuntu 12.04 support.  * SERVER-32498: currentOp output no longer returns threadId field.  * All JIRA issues closed in 3.6.4  * 3.6.4 Changelog \n3.6.3 - FEBRUARY 23, 2018 \nIssues fixed:\n\n  * SERVER-32606: Tailing oplog on secondary fails with CappedPositionLost  * SERVER-32631: specifying --bind_ip localhost results in error \"address already in use\"  * All JIRA issues closed in 3.6.3  * 3.6.3 Changelog \n3.6.2 - JANUARY 10, 2018 \nIssues fixed:  * SERVER-31267: CollectionCloner fails if collection is dropped between getMore calls.  * SERVER-31625: The contents of Unknown macro: {USER} needs to be escaped when querying for the groups using LDAP server.  * SERVER-31684: QueryPlanKilled (operation exceeded time limit) in $changeStream with updateLookup.  * SERVER-31982: Shard does not call config commit chunk migration command with majority writeConcern nor checks for writeConcern errors..  * SERVER-32246: PID file permission on v3.6 make it not monitorable using pid file.  * SERVER-32255: UUIDs may be absent from shard secondary local collections.  * SERVER-32282: Aggregation text search returns text score even if it wasn't requested when targeting multiple shards in a sharded cluster.  * SERVER-32396 mongo shell failed to connect with 3.6 connection string SRV  * SERVER-32430: DocumentSourceSort sorts array documents incorrectly if there is a non-simple collation.  * SERVER-32529: Requiring replSet for shards breaks Queryable Backup.  * All JIRA issues closed in 3.6.2  * 3.6.2 Changelog \n3.6.1 - DEC 26, 2017 \nIssues fixed:  * SERVER-30768: Primary queries using maxTimeMS cause temporary shard write unavailability if ExceededTimeLimit.  * SERVER-31225: The mongod process forks before listening for connections.  * SERVER-31885: changeStream cursor is not returned on a mongos when the database does not exist.  * SERVER-32085: changeStream reports incorrect documentKey for unsharded collections that become sharded.  * SERVER-32046: Arrays of certain NumberDecimals can trigger an invariant failure.  * SERVER-32048: Updates using a numeric path component may cause index entries not to be created.  * All JIRA issues closed in 3.6.1  * 3.6.1 Changelog \nSECURITY  DEFAULT BIND TO LOCALHOST \nMongoDB binaries, mongod and mongos, bind to localhost by default. \nMONGODB-CR DEPRECATION \nAs of MongoDB 3.6, MONGODB-CR authentication mechanism is deprecated. If you have not upgraded your MONGODB-CR authentication schema to SCRAM, see Upgrade to SCRAM. \nAUTHENTICATION RESTRICTIONS \nTo restrict database user connections to specified IP addresses, added authenticationRestrictions parameter to the following: Commands\nMethods\ncreateUser\ndb.createUser()\nupdateUser\ndb.updateUser()\ncreateRole\ndb.createRole()\nupdateRole\ndb.updateRole() \nADDITIONAL SECURITY ENHANCEMENTS \n * Added the opensslCipherConfig parameter to control the OpenSSL ciphers when using TLS/SSL encryption.  * If authentication is turned on, you can only issue a getMore against cursors you created.  * Added the convertToCapped action to restore role.\n   \n   \n   TIP\n   \n   \n   SEE ALSO:\n   \n   Backwards Incompatible Features \nAGGREGATION \nThe following features are available starting in MongoDB 3.6. \nMORE EXPRESSIVE $LOOKUP \n$lookup adds support for specifying multiple join conditions as well as uncorrelated sub-queries by allowing variable specification and pipeline execution on the joined collection. For details, see the $lookup syntax for Join Conditions and Subqueries on a Joined Collection. \nNEW AGGREGATION STAGES \nMongoDB 3.6 adds the following new aggregation pipeline stages: Operator\nDescription\n$currentOp\nReturns a stream of documents containing information on active and/or dormant operations on a mongod instance. Uses the new aggregation helper db.aggregate().\n$listSessions\nLists server sessions in the system.sessions collection in the config database. Uses the new aggregation helper db.aggregate().\n$listLocalSessions\nLists server sessions cached in memory by the server. \nNEW AGGREGATION OPERATORS\n\n \nNEW AGGREGATION HELPER \nMongoDB 3.6 adds a helper, db.aggregate(), to perform aggregations that do not rely on an underlying collection, such as those that start with $currentOp or $listLocalSessions. \nNEW AGGREGATION VARIABLE \nREMOVE allows for the conditional exclusion of a field. \nNEW OPTIONS \naggregate command and the db.collection.aggregate() method support the following new options:  * hint option to specify the index to use.\n   \n   \n   NOTE\n   \n   The hint does not apply to $lookup and $graphLookup stages.  * comment option to help trace the operation through the database profiler, currentOp, and logs. \nSUPPORT FOR TIME ZONES \nMongoDB 3.6 adds support for time zones to aggregation date operators. \nTIP \nSEE ALSO: Aggregation Compatibility Changes \nARRAY UPDATE OPERATOR ENHANCEMENTS  ARRAYFILTERS \nThe following commands and methods can accept an arrayFilters parameter to specify which elements to modify in an array field: Commands\nMethods\nfindAndModify\ndb.collection.findOneAndUpdate()\ndb.collection.findAndModify()\nupdate\ndb.collection.updateOne()\ndb.collection.updateMany()\ndb.collection.update() db.collection.bulkWrite() for updateOne and updateMany operations\nBulk.find.arrayFilters() for Bulk() operation methods updateOne() and update(). \nMULTI-ELEMENT ARRAY UPDATES \nMongoDB 3.6 adds the following new positional operators for update operations on arrays and nested arrays:  * The all positional $[] operator updates all elements in an array.  * The filtered positional $[<identifier>] operator updates all elements in an array that match the arrayFilters criteria. \nNEGATIVE ARRAY INDEX POSITION FOR PUSH \nThe $position modifier for the $push update operator can accept a negative array index value to indicate a position starting from the end of the array. \n3.6 COMPATIBLE DRIVERS \nThe following drivers are feature compatible with MongoDB 3.6: Java 3.6+ Python 3.6+ C 1.9+ Go 1.8+ C# 2.5+ Node 3.0+ Ruby 2.5+ Rust 2.1+ Swift 1.2+ Perl 2.0+ PHPC 1.4+ Scala 2.2+ C++ 3.6.6+ \nDNS-CONSTRUCTED SEEDLIST MONGODB+SRV \nIn addition to the standard connection format, the 3.6 drivers support a DNS-constructed seedlist. For more information, see DNS Seed List Connection Format. \nCHANGE STREAMS \nMongoDB 3.6 supports opening change streams against replica sets and sharded clusters with replica set shards. Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog. Applications can use change streams to subscribe to all data changes on a collection and respond to those changes. You can open a change stream from any 3.6-series driver using the db.collection.watch() method. See the documentation for your preferred driver for complete instructions on usage. See Change Streams for more information. \nIMPORTANT To use change streams, featureCompatibilityVersion must be set to \"3.6\". For more information, see Get FeatureCompatibilityVersion and setFeatureCompatibilityVersion. \nCLIENT SESSIONS  CAUSAL CONSISTENCY \nTo provide causal consistency, MongoDB 3.6 enables causal consistency in client sessions. A causally consistent client session denotes that the associated sequence of read and acknowledged write operations have a causal relationship that is reflected by their ordering. Client applications must ensure that only one thread at a time executes these operations in a client session. Applications can start a client session and associate operations with a specific session. Applications must ensure that only one thread at a time executes these operations in a client session. \nIMPORTANT To use client sessions:  * Clients require MongoDB drivers updated for MongoDB 3.6.  * featureCompatibilityVersion must be \"3.6\". For more information, see Get FeatureCompatibilityVersion and setFeatureCompatibilityVersion. \nRETRYABLE WRITES  IMPORTANT For retryable writes:  * Clients require MongoDB drivers updated for MongoDB 3.6.  * featureCompatibilityVersion must be \"3.6\". For more information, see Get FeatureCompatibilityVersion and setFeatureCompatibilityVersion.\n\n With retryable writes, MongoDB drivers automatically retries these operations upon encountering network errors or encountering a replica set failover during which time the replica set has no primary. To enable retryable writes for the 3.6 drivers, see retryWrites. As the retry attempt is made only once, the retryable feature can help address transient network errors but not persistent network errors. For more information on retryable writes, see Retryable Writes. \nMONGO SHELL CHANGES \nMongoDB 3.6 adds the new command-line option --retryWrites to the mongo shell. The option enables Retryable Writes in the mongo shell. The following new methods have been added to the Mongo() connection object in the mongo shell:  * Mongo.isCausalConsistency()  * Mongo.startSession()  * Various Session() methods  * Various SessionOptions() methods \nSERVER SESSIONS \nMongoDB's server sessions, or logical sessions, are the underlying framework used by client sessions to support Causal Consistency and retryable writes. \nIMPORTANT Applications use client sessions to interface with server sessions. Server sessions are available for standalone mongod instances, replica sets, and sharded clusters. \nSERVER SESSION COMMANDS \nStarting in 3.6, MongoDB drivers associate all operations with a server session, with the exception of unacknowledged writes. If the deployment enforces authentication/authorization, server sessions are associated with the authenticated users. The following commands can be used to list, manage, and kill server sessions throughout MongoDB clusters: Commands\nDescriptions\nendSessions\nExpires specified server sessions.\nkillAllSessions\nKills all server sessions.\nkillAllSessionsByPattern\nKills all server sessions that match the specified pattern.\nkillSessions\nKills specified server sessions.\nrefreshSessions\nRefreshes idle server sessions.\nstartSession\nStarts a new server session. \nPARAMETERS \nThe following new parameters are available for server sessions:  * logicalSessionRefreshMinutes (replaced with logicalSessionRefreshMillis in 3.6.9)  * localLogicalSessionTimeoutMinutes  * maxAcceptableLogicalClockDriftSecs \nAGGREGATION STAGES \nTo support server sessions, MongoDB 3.6 adds the following new aggregation pipeline stages: Operator\nDescription\n$listSessions\nLists the server sessions in the system.sessions collection in the config database.\n$listLocalSessions\nLists the server sessions cached in memory by the server. Uses the new aggregation helper db.aggregate(). \nGENERAL \nserverStatus returns information on the number of logicalSessionRecordCache. \nCOMMAND OPTIONS \nStarting in 3.6, MongoDB drivers associate all operations with a server session, with the exception of unacknowledged writes. The following options are available for all commands to support association with a server session: \nIMPORTANT mongosh and the drivers assign these options to the commands in the session. Option\nType\nDescription\nlsid\nDocument\nThe document that specifies the unique ID of the session associated with the command. If the txnNumber is specified, the lsid is required.\ntxnNumber\n64-bit integer A strictly increasing non-negative number that uniquely identifies the command in the command's session. If specified, the command must also include the lsid option. For the delete, insert, and update commands that take an array of statements, the following option is also available: \nIMPORTANT Do not manually set stmtIds. MongoDB sets the stmtIds to be strictly increasing non-negative numbers. Option\nType\nDescription\nstmtIds\nArray of 32-bit integers\nArray of numbers that uniquely identify their respective write operations within the write command. \nJSON SCHEMA \nMongoDB 3.6 adds the $jsonSchema operator to support document validation using JSON Schema. For details, see $jsonSchema. To use $jsonSchema, featureCompatibilityVersion must be set to \"3.6\". \nTIP \nSEE ALSO: Backwards Incompatible Features \nREPLICA SETS \n * Deprecate replica set protocol version 0 (pv0). For more information on the replica set protocol versions, see Replica Set Protocol Version.  * Added the replSetResizeOplog command to dynamically resize a replica set member's oplog. Available for instances running the WiredTiger storage engine.  * Added the catchUpTakeoverDelayMillis configuration option, dictating the amount of time a node waits to run for election after determining that it is ahead of the current primary.\n\n  * Added the oplogInitialFindMaxSeconds parameter to adjust how long a member of a replica set should wait for its find command to finish during data synchronization.  * Added the waitForSecondaryBeforeNoopWriteMS parameter to specify how long a secondary must wait if the afterClusterTime is greater than the last applied time from the oplog.  * Added support for running the following during a replica set member's initial sync:\n   \n   * renameCollection\n   \n   * convertToCapped\n   \n   * $out stages in aggregation pipelines\n   \n   * Map-reduce jobs that output to a new collection \nSHARDED CLUSTERS \n * Starting in 3.6, shards must be replica sets. To upgrade your sharded cluster to version 3.6, the shard servers must be running as a replica set.\n   \n   To convert an existing shard standalone instance to a shard replica set, see Convert a Shard Standalone to a Shard Replica Set.  * Starting in 3.6, all members of a shard replica set, not just the primary, maintain the metadata regarding chunk metadata. This prevents reads from the secondaries from returning orphaned data when not using \"available\" read concern.  * Added ShardingTaskExecutorPoolMaxConnecting parameter for mongos to control the rate at which mongos adds connections to a mongod instance.  * Added orphanCleanupDelaySecs that determines the minimum delay before a migrated chunk is deleted from the source shard.  * The config.system.sessions collection in the config database may now be sharded. \nGENERAL ENHANCEMENTS  MONGODB COMPASS PACKAGING \nThe MongoDB Server download is packaged with a platform-specific installation script for MongoDB Compass Community Edition. This script installs MongoDB Compass as part of the MongoDB Server installation process. \nCOLLECTION IDENTIFIER \nCollections have an immutable unique identifier. The featureCompatibilityVersion must be set to \"3.6\". \nTIP \nSEE ALSO: Backwards Incompatible Features \nNEW QUERY OPERATORS \nMongoDB 3.6 adds the following new query operators:  * The new $jsonSchema operator matches documents that validate against the given JSON Schema. To use $jsonSchema, featureCompatibilityVersion must be set to \"3.6\".  * The $expr allows the use of aggregation expressions within the query language. \nTIP \nSEE ALSO: Backwards Incompatible Features \nREMOVED OPERATORS \nMongoDB 3.6 removes the deprecated $pushAll update operator. For more information, see Remove $pushAll Update Operator. \nINDEXES \n * An index can cover a query on fields within nested documents.  * Multikey indexes can cover queries over non-array keys if the index tracks which fields make it multikey.  * When creating an index, you cannot specify * as the name of the index. See Indexes Named *. \nCOMMANDS \n * Added the following options to the listDatabases command:\n   \n   * nameOnly to return only the database names (which does not require database locks) instead of returning both the database names and size information (which does require database locks).\n   \n   * filter to return databases that match the specified match criteria on the outputs.  * Modified behavior of the validate command and the db.collection.validate() method such that only for full validation does the WiredTiger storage engine force a checkpoint, flush all in-memory data to disk, then verify the on-disk data. See also validate Operation.  * The <database>.system.profile entry for update and delete contains the entire update/delete document applied to the named collection.  * dropDatabase waits until all collections drops in the database have propagated to a majority of the replica set members.  * For commands run on replica sets and sharded clusters, the response documents include the operationTime and $clusterTime. See db.runCommand() and db.adminCommand().  * Deprecated getPrevError command. \nWIRE PROTOCOL AND COMPRESSION \n * MongoDB 3.6 introduces a new wire protocol opcode called OP_MSG. This opcode's message format is extensible and designed to subsume the functionality of other opcodes.\n\n  * mongod and mongos enable network compression by default with snappy as the compressor. For more information on network compression, see net.compression.compressors. \nREAD CONCERN \n * New \"available\" read concern is available. For unsharded collections (including collections in a standalone deployment or a replica set deployment), \"local\" and \"available\" read concerns behave identically. For sharded clusters, \"available\" provides greater tolerance for partitions but may return orphan documents if the shard is undergoing chunk migrations.\n   \n   \n   TIP\n   \n   \n   SEE ALSO:\n   \n   orphanCleanupDelaySecs  * Starting in MongoDB 3.6, MongoDB enables support for \"majority\" read concern by default.\n   \n   For MongoDB 3.6.1 - 3.6.x, you can disable read concern \"majority\" to prevent the storage cache pressure from immobilizing a deployment with a primary-secondary-arbiter (PSA) architecture. Disabling \"majority\" read concern also disables support for change streams\n   \n   For more information, see Disable Read Concern Majority. \nFTDC \nMongoDB 3.6 adds support for Diagnostics Capture (also known as FTDC) in mongos. [1] In previous versions, the feature is available for mongod only. See Diagnostic Parameters. \nNOTE \nFTDC IS ENABLED BY DEFAULT.  [1] MongoDB 3.4.14+ also adds mongos support for FTDC. \nADDITIONAL ENHANCEMENTS \nMongoDB 3.6 includes the following enhancements:  * Added support for specifying full Unix domain socket paths with --bind_ip.  * mongod now offers a --timeZoneInfo option. Use this option to specify the path to your system time zone database. The default configuration file included with Linux and macOS packages sets this to /usr/share/zoneinfo.  * Date operations now accept consistent date ranges across all supported operating systems. You may safely work with years in the range 0 through 9999.  * The new honorSystemUmask startup option for mongod causes new files created by MongoDB to have the read/write permissions specified by the umask of the user who runs the mongod process. Only available on Linux and macOS systems.  * The maxWriteBatchSize limit of a database, which indicates the maximum number of write operations permitted in a write batch, raises from 1,000 to 100,000.  * The planCacheListPlans database command produces the same output as the PlanCache.getPlansByQuery() shell method. The output from both operations now includes a timestamp for when the plans were generated.  * The new KeysRotationIntervalSec server parameter specifies the number of seconds for which an HMAC signing key is valid before rotating to the next one.  * The find command's option oplogReplay now supports the $eq operator. \nCHANGES AFFECTING COMPATIBILITY \nSome changes can affect compatibility and may require user actions. For a detailed list of compatibility changes, see Compatibility Changes in MongoDB 3.6. \nUPGRADE PROCEDURES  IMPORTANT \nFEATURE COMPATIBILITY VERSION 3.4 To upgrade, the 3.4 instances must have featureCompatibilityVersion set to 3.4. To check the version: db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )  For specific details on verifying and setting the featureCompatibilityVersion as well as information on other prerequisites/considerations for upgrades, refer to the individual upgrade instructions:  * Upgrade a Standalone to 3.6  * Upgrade a Replica Set to 3.6  * Upgrade a Sharded Cluster to 3.6 If you need guidance on upgrading to 3.6, MongoDB offers major version upgrade services to help ensure a smooth transition without interruption to your MongoDB application. \nDOWNLOAD \nTo download the MongoDB 3.6, go to the MongoDB Download Center \nTIP \nSEE ALSO: All Third Party License Notices \nKNOWN ISSUES IN 3.6.3 \n * WT-3724:MongoDB 3.6 is not tested on APFS, the new filesystem in macOS 10.13 and may encounter errors.  * Operations that build in-memory structures are not causally consistent; however, only some operations return errors when associated with causally consistent sessions. See Causal Consistency Limitations. \nKNOWN ISSUES IN 3.6.2\n\n  * Operations that build in-memory structures are not causally consistent; however, only some operations return errors when associated with causally consistent sessions. See Causal Consistency Limitations. \nKNOWN ISSUES IN 3.6.1 \n * SERVER-31760:$expr does not use indexes for equality match against field, including when used as part of a $lookup aggregation stage with foreign pipeline.\n * WT-3724:MongoDB 3.6 is not tested on APFS, the new filesystem in macOS 10.13 and may encounter errors.  * Operations that build in-memory structures are not causally consistent; however, only some operations return errors when associated with causally consistent sessions. See Causal Consistency Limitations. \nKNOWN ISSUES IN 3.6.0 \n * SERVER-31760:$expr does not use indexes for equality match against field, including when used as part of a $lookup aggregation stage with foreign pipeline.\n * TOOLS-1827:When using a URI with SRV, fetched TXT records will be ignored if no query parameters are specified in the command line URI. To get around this, explicitly specify the TLS/SSL setting used to communicate with the database (either ssl=true or ssl=false) in the URI query string.\n * WT-3724:MongoDB 3.6 is not tested on APFS, the new filesystem in macOS 10.13 and may encounter errors.  * Operations that build in-memory structures are not causally consistent; however, only some operations return errors when associated with causally consistent sessions. See Causal Consistency Limitations. \nTIP \nSEE ALSO: All JIRA issues resolved in 3.6. \nREPORT AN ISSUE \nTo report an issue, see https://github.com/mongodb/mongo/wiki/Submit-Bug-Reports for instructions on how to file a JIRA ticket for the MongoDB server or one of the related projects. ←  4.0 Changelog3.6 Changelog → On this page  * Patch Releases\n * Security\n * Aggregation\n * Array Update Operator Enhancements\n * 3.6 Compatible Drivers\n * Change Streams\n * Client Sessions\n * Server Sessions\n * JSON Schema\n * Replica Sets\n * Sharded Clusters\n * General Enhancements\n * Changes Affecting Compatibility\n * Upgrade Procedures\n * Download\n * Known Issues in 3.6.3\n * Known Issues in 3.6.2\n * Known Issues in 3.6.1\n * Known Issues in 3.6.0\n * Report an Issue Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.6-upgrade-sharded-cluster/": " Docs Home → MongoDB Manual \nUPGRADE A SHARDED CLUSTER TO 3.6 \nOn this page    \n * Upgrade Recommendations and Checklists\n   \n * Default Bind to Localhost\n * Shard Replica Sets\n * Drivers\n * Read Concern Majority\n * Use Package Manager\n * Download 3.6 Binaries Manually \nNOTE      * MongoDB 3.6 is not tested on APFS, the new filesystem in macOS 10.13 and may encounter errors.  * Starting in MongoDB 3.6.13, MongoDB 3.6-series removes support for Ubuntu 16.04 PPCLE.\n   \n   For earlier MongoDB Enterprise versions that support Ubuntu 16.04 POWER/PPC64LE:\n   \n   Due to a lock elision bug present in older versions of the glibc package on Ubuntu 16.04 for POWER, you must upgrade the glibc package to at least glibc 2.23-0ubuntu5 before running MongoDB. Systems with older versions of the glibc package will experience database server crashes and misbehavior due to random memory corruption, and are unsuitable for production deployments of MongoDB \nIMPORTANT Before you attempt any upgrade, please familiarize yourself with the content of this document. If you need guidance on upgrading to 3.6, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nUPGRADE RECOMMENDATIONS AND CHECKLISTS \nWhen upgrading, consider the following: \nUPGRADE VERSION PATH \nTo upgrade an existing MongoDB deployment to 3.6, you must be running a 3.4-series release. To upgrade from a version earlier than the 3.4-series, you must successively upgrade major releases until you have upgraded to 3.4-series. For example, if you are running a 3.2-series, you must upgrade first to 3.4 before you can upgrade to 3.6. \nCHECK DRIVER COMPATIBILITY \nBefore you upgrade MongoDB, check that you're using a MongoDB 3.6-compatible driver. Consult the driver documentation for your specific driver to verify compatibility with MongoDB 3.6. Upgraded deployments that run on incompatible drivers might encounter unexpected or undefined behavior. \nPREPAREDNESS \nBefore beginning your upgrade, see the Compatibility Changes in MongoDB 3.6 document to ensure that your applications and deployments are compatible with MongoDB 3.6. Resolve the incompatibilities in your deployment before starting the upgrade. Before upgrading MongoDB, always test your application in a staging environment before deploying the upgrade to your production environment. \nDOWNGRADE CONSIDERATION \nOnce upgraded to 3.6, if you need to downgrade, we recommend downgrading to the latest patch release of 3.4. \nDEFAULT BIND TO LOCALHOST \nStarting in MongoDB 3.6, mongod and mongos instances bind to localhost by default. Remote clients, including other members of the replica set, cannot connect to an instance bound only to localhost. To override and bind to other ip addresses, use the net.bindIp configuration file setting or the --bind_ip command-line option to specify a list of ip addresses. The upgrade process will require that you specify the net.bindIp setting (or --bind_ip) if your sharded cluster members are run on different hosts or if you wish remote clients to connect to your sharded cluster. \nWARNING Before binding to a non-localhost (e.g. publicly accessible) IP address, ensure you have secured your cluster from unauthorized access. For a complete list of security recommendations, see Security Checklist. At minimum, consider enabling authentication and hardening network infrastructure. For more information, see Localhost Binding Compatibility Changes \nSHARD REPLICA SETS \nStarting in MongoDB 3.6, mongod instances with the shard server role must be replica set members. To upgrade your sharded cluster to version 3.6, the shard servers must be running as a replica set. To convert an existing shard standalone instance to a shard replica set, see Convert a Shard Standalone to a Shard Replica Set. \nDRIVERS \nFor MongoDB 3.6.0 - 3.6.3 binaries, you should upgrade your drivers to 3.6 feature compatible drivers only after you have upgraded the MongoDB binaries and updated the feature compatibility version of the sharded cluster to 3.6. For more information, see SERVER-33763. \nREAD CONCERN MAJORITY \nStarting in MongoDB 3.6, MongoDB enables support for \"majority\" read concern by default.\n\n For more information, see Disable Read Concern Majority. \nPREREQUISITES \n * Version 3.4 or GreaterTo upgrade a sharded cluster to 3.6, all members of the cluster must be at least version 3.4. The upgrade process checks all components of the cluster and will produce warnings if any component is running version earlier than 3.4.\n * Feature Compatibility Version\n   \n   The 3.4 sharded cluster must have featureCompatibilityVersion set to 3.4.\n   \n   To ensure that all members of the sharded cluster have featureCompatibilityVersion set to 3.4, connect to each shard replica set member and each config server replica set member and check the featureCompatibilityVersion:\n   \n   \n   TIP\n   \n   For a sharded cluster that has access control enabled, to run the following command against a shard replica set member, you must connect to the member as a shard local user.\n   \n   db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )\n   \n   \n   \n   All members should return a result that includes \"featureCompatibilityVersion\": \"3.4\".\n   \n   To set or update featureCompatibilityVersion, run the following command on the mongos:\n   \n   db.adminCommand( { setFeatureCompatibilityVersion: \"3.4\" } )\n   \n   \n   \n   For more information, see setFeatureCompatibilityVersion.  * Shard Aware\n   \n   The shards in the 3.4 sharded clusters must be shard aware (i.e. the shards must have received their shardIdentity document, located in the admin.system.version collection):\n   \n    * For sharded clusters that started as 3.4, the shards are shard aware.\n   \n    * For 3.4 sharded clusters that were upgraded from 3.2-series, when you update featureCompatibilityVersion from 3.2 to 3.4, the config server attempts to send the shards their respective shardIdentity document every 30 seconds until success. You must wait until all shards receive the documents.\n   \n    * To check whether a shard replica set member has received its shardIdentity document, issue the find command against the system.version collection in the admin database and check for a document where \"_id\" : \"shardIdentity\".\n      \n      For an example of a shardIdentity document:\n      \n      {  \"_id\" : \"shardIdentity\",  \"clusterId\" : ObjectId(\"2bba123c6eeedcd192b19024\"),  \"shardName\" : \"shard2\",  \"configsvrConnectionString\" : \"configDbRepl/alpha.example.net:28100,beta.example.net:28100,charlie.example.net:28100\" }\n      \n        * Disable the balancer  * Back up the config DatabaseOptional but Recommended. As a precaution, take a backup of the config database before upgrading the sharded cluster. \nDOWNLOAD 3.6 BINARIES  USE PACKAGE MANAGER \nIf you installed MongoDB from the MongoDB apt, yum, dnf, or zypper repositories, you should upgrade to 3.6 using your package manager. Follow the appropriate 3.6 installation instructions for your Linux system. This will involve adding a repository for the new release, then performing the actual upgrade process. \nDOWNLOAD 3.6 BINARIES MANUALLY \nIf you have not installed MongoDB using a package manager, you can manually download the MongoDB binaries from the MongoDB Download Center. See 3.6 installation instructions for more information. \nUPGRADE PROCESS \n1 DISABLE THE BALANCER. \nConnect a mongo shell to a mongos instance in the sharded cluster, and run sh.stopBalancer() to disable the balancer: sh.stopBalancer()  \nNOTE If a migration is in progress, the system will complete the in-progress migration before stopping the balancer. You can run sh.isBalancerRunning() to check the balancer's current state. To verify that the balancer is disabled, run sh.getBalancerState(), which returns false if the balancer is disabled:\n\n  For more information on disabling the balancer, see Disable the Balancer. 2 UPGRADE THE CONFIG SERVERS. \n 1. Upgrade the secondary members of the replica set one at a time:\n    \n    a. Shut down the secondary mongod instance and replace the 3.4 binary with the 3.6 binary.\n    \n    b. Start the 3.6 binary with the --configsvr, --replSet, and --port. Include any other options as used by the deployment.\n       \n       \n       NOTE\n       \n       The --bind_ip option must be specified when the sharded cluster members are run on different hosts or if remote clients connect to the sharded cluster. For more information, see Localhost Binding Compatibility Changes.\n       \n       mongod --configsvr --replSet <replSetName> --port <port> --dbpath <path> --bind_ip localhost,<hostname(s)|ip address(es)>\n       \n       \n       \n       If using a configuration file, update the file to specify sharding.clusterRole: configsvr, replication.replSetName, net.port, and net.bindIp, then start the 3.6 binary:\n       \n       sharding:   clusterRole: configsvrreplication:   replSetName: <string>net:   port: <port>   bindIp: localhost,<hostname(s)|ip address(es)>storage:   dbpath: <path>\n       \n       \n       \n       Include any other settings as appropriate for your deployment.\n    \n    c. Wait for the member to recover to SECONDARY state before upgrading the next secondary member. To check the member's state, issue rs.status() in the mongo shell.\n       \n       Repeat for each secondary member.\n\n 3 UPGRADE THE SHARDS. \nUpgrade the shards one at a time. For each shard replica set:  1. Upgrade the secondary members of the replica set one at a time:\n    \n    a. Shut down the mongod instance and replace the 3.4 binary with the 3.6 binary.\n    \n    b. Start the 3.6 binary with the --shardsvr, --replSet, --port, and --bind_ip options. Include any optional command line options used by the previous deployment:\n       \n       mongod --shardsvr --replSet <replSetName> --port <port> --dbpath <path> --bind_ip localhost,<hostname(s)|ip address(es)>\n       \n       \n       \n       \n       NOTE\n       \n       The --bind_ip option must be specified when the sharded cluster members are run on different hosts or if remote clients connect to the sharded cluster. For more information, see Localhost Binding Compatibility Changes.\n       \n       If using a configuration file, update the file to include sharding.clusterRole: shardsvr, replication.replSetName, net.port, and net.bindIp, then start the 3.6 binary:\n       \n       sharding:   clusterRole: shardsvrreplication:   replSetName: <string>net:   port: <port>   bindIp: localhost,<hostname(s)|ip address(es)>storage:   dbpath: <path>\n       \n       \n       \n       Include any other configuration as appropriate for your deployment.\n    \n    c. Wait for the member to recover to SECONDARY state before upgrading the next secondary member. To check the member's state, you can issue rs.status() in the mongo shell.\n       \n       Repeat for each secondary member.  2. Step down the replica set primary.\n    \n    Connect a mongo shell to the primary and use rs.stepDown() to step down the primary and force an election of a new primary:\n    \n    rs.stepDown()\n    \n    \n\n 4 UPGRADE THE MONGOS INSTANCES. \nReplace each mongos instance with the 3.6 binary and restart. Include any other configuration as appropriate for your deployment. \nNOTE The --bind_ip option must be specified when the sharded cluster members are run on different hosts or if remote clients connect to the sharded cluster. For more information, see Localhost Binding Compatibility Changes. mongos --configdb csReplSet/<rsconfigsver1:port1>,<rsconfigsver2:port2>,<rsconfigsver3:port3> --bind_ip localhost,<hostname(s)|ip address(es)> \n5 RE-ENABLE THE BALANCER. \nUsing a 3.6 mongo shell, connect to a mongos in the cluster and run sh.setBalancerState() to re-enable the balancer: sh.setBalancerState(true)  The 3.4 and earlier mongo shell is not compatible with 3.6 clusters. For more information about re-enabling the balancer, see Enable the Balancer. 6 ENABLE BACKWARDS-INCOMPATIBLE 3.6 FEATURES. \nAt this point, you can run the 3.6 binaries without the 3.6 features that are incompatible with 3.4. That is, you can run the 3.6 sharded cluster with feature compatibility version set to 3.4 \nIMPORTANT For MongoDB 3.6.0-3.6.3, you should upgrade your drivers to 3.6 feature compatible drivers only after you have updated the feature compatibility version of the sharded cluster to 3.6. For more information, see SERVER-33763. To enable these 3.6 features, set the feature compatibility version (FCV) to 3.6. \nNOTE Enabling these backwards-incompatible features can complicate the downgrade process since you must remove any persisted backwards-incompatible features before you downgrade. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features. On a mongos instance, run the setFeatureCompatibilityVersion command in the admin database: db.adminCommand( { setFeatureCompatibilityVersion: \"3.6\" } )  This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command on the mongos as the operation is idempotent. 7 RESTART MONGOS INSTANCES. \nAfter changing the featureCompatibilityVersion, all mongos instances need to be restarted to pick up the changes in the causal consistency behavior. \nADDITIONAL UPGRADE PROCEDURES \n * To upgrade a standalone, see Upgrade a Standalone to 3.6.  * To upgrade a replica set, see Upgrade a Replica Set to 3.6. ←  Upgrade a Replica Set to 3.6Downgrade 3.6 Standalone to 3.4 → On this page  * Upgrade Recommendations and Checklists\n * Default Bind to Localhost\n * Shard Replica Sets\n * Drivers\n * Read Concern Majority\n * Use Package Manager\n * Download 3.6 Binaries Manually Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/6.0-upgrade-sharded-cluster/": " Docs Home → MongoDB Manual \nUPGRADE A SHARDED CLUSTER TO 6.0 \nOn this page    \n * Upgrade Recommendations and Checklists\n   \n * Prerequisites\n * Download 6.0 Binaries\n * Upgrade Procedure\n * Additional Upgrade Procedures \nWARNING Due to balancing policy updates in MongoDB 6.0.3, the balancer may start immediately after upgrade, even if the number of chunks stays the same. Familiarize yourself with the content of this document, including thoroughly reviewing the prerequisites, prior to upgrading to MongoDB 6.0. The following steps outline the procedure to upgrade a mongod that is a shard member from version 5.0 to 6.0. If you need guidance on upgrading to 6.0, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nUPGRADE RECOMMENDATIONS AND CHECKLISTS \nWhen upgrading, consider the following: \nUPGRADE VERSION PATH \nTo upgrade an existing MongoDB deployment to 6.0, you must be running a 5.0-series release. To upgrade from a version earlier than the 5.0-series, you must successively upgrade major releases until you have upgraded to 5.0-series. For example, if you are running a 4.4-series, you must upgrade first to 5.0 before you can upgrade to 6.0. \nCHECK DRIVER COMPATIBILITY \nBefore you upgrade MongoDB, check that you're using a MongoDB 6.0-compatible driver. Consult the driver documentation for your specific driver to verify compatibility with MongoDB 6.0. Upgraded deployments that run on incompatible drivers might encounter unexpected or undefined behavior. \nPREPAREDNESS \nBefore beginning your upgrade, see the Compatibility Changes in MongoDB 6.0 document to ensure that your applications and deployments are compatible with MongoDB 6.0. Resolve the incompatibilities in your deployment before starting the upgrade. Before upgrading MongoDB, always test your application in a staging environment before deploying the upgrade to your production environment. \nDOWNGRADE CONSIDERATION \nAfter upgrading to 6.0, if you need to downgrade, we recommend downgrading to the latest patch release of 5.0. \nPREREQUISITES  ALL MEMBERS VERSION \nTo upgrade a sharded cluster to 6.0, all members of the cluster must be at least version 5.0. The upgrade process checks all components of the cluster and will produce warnings if any component is running version earlier than 5.0. \nFEATURE COMPATIBILITY VERSION \nThe 5.0 sharded cluster must have featureCompatibilityVersion set to \"5.0\". To ensure that all members of the sharded cluster have featureCompatibilityVersion set to \"5.0\", connect to each shard replica set member and each config server replica set member and check the featureCompatibilityVersion: \nTIP For a sharded cluster that has access control enabled, to run the following command against a shard replica set member, you must connect to the member as a shard local user. db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )  All members should return a result that includes \"featureCompatibilityVersion\" : { \"version\" : \"5.0\" }. To set or update featureCompatibilityVersion, run the following command on the mongos: db.adminCommand( { setFeatureCompatibilityVersion: \"5.0\" } )  For more information, see setFeatureCompatibilityVersion. \nREPLICA SET MEMBER STATE \nFor shards and config servers, ensure that no replica set member is in the ROLLBACK or RECOVERING state. \nBACK UP THE CONFIG DATABASE \nOptional but Recommended. As a precaution, take a backup of the config database before upgrading the sharded cluster. \nDOWNLOAD 6.0 BINARIES  USE PACKAGE MANAGER \nIf you installed MongoDB from the MongoDB apt, yum, dnf, or zypper repositories, you should upgrade to 6.0 using your package manager. Follow the appropriate 6.0 installation instructions for your Linux system. This will involve adding a repository for the new release, then performing the actual upgrade process. \nDOWNLOAD 6.0 BINARIES MANUALLY \nIf you have not installed MongoDB using a package manager, you can manually download the MongoDB binaries from the MongoDB Download Center. See 6.0 installation instructions for more information. \nUPGRADE PROCEDURE \n1 \nDISABLE THE BALANCER.\n\n sh.stopBalancer()  \nNOTE If a migration is in progress, the system will complete the in-progress migration before stopping the balancer. You can run sh.isBalancerRunning() to check the balancer's current state. To verify that the balancer is disabled, run sh.getBalancerState(), which returns false if the balancer is disabled: sh.getBalancerState()  For more information on disabling the balancer, see Disable the Balancer. 2 \nUPGRADE THE CONFIG SERVERS. \n1 UPGRADE THE SECONDARY MEMBERS OF THE REPLICA SET, ONE AT A TIME. \n1 SHUT DOWN THE SECONDARY INSTANCE. \nTo shut down the mongod process, use mongosh to connect to the cluster member and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 REPLACE THE 5.0 BINARY WITH THE 6.0 BINARY. \n3 START THE 6.0 BINARY. \nStart the 6.0 binary with the --configsvr, --replSet, and --port. Include any other options as used by the deployment. mongod --configsvr --replSet <replSetName> --port <port> --dbpath <path> --bind_ip localhost,<ip address>  If using a configuration file, update the file to specify sharding.clusterRole: configsvr, replication.replSetName, net.port, and net.bindIp, then start the 6.0 binary: sharding:   clusterRole: configsvrreplication:   replSetName: <string>net:   port: <port>   bindIp: localhost,<ip address>storage:   dbpath: <path>  Include any other settings as appropriate for your deployment. 4 WAIT FOR THE MEMBER TO RECOVER TO THE SECONDARY STATE BEFORE UPGRADING THE NEXT SECONDARY MEMBER. \nTo check the member's state, issue rs.status() in mongosh. 5 REPEAT FOR EACH SECONDARY MEMBER. \n2 STEP DOWN THE REPLICA SET PRIMARY. \n1 Connect mongosh to the primary and use rs.stepDown() to step down the primary and force an election of a new primary: rs.stepDown() \n2 SHUT DOWN THE STEPPED-DOWN PRIMARY. \nWhen rs.status() shows that the primary has stepped down and another member has assumed the PRIMARY state, shut down the stepped-down primary. To shut down the stepped-down primary, use mongosh to connect to the primary and run the following command: db.adminCommand( { shutdown: 1 } ) \n3 REPLACE THE MONGOD BINARY WITH THE 6.0 BINARY. \n4 START THE 6.0 BINARY. \nStart the 6.0 with the --configsvr, --replSet, --port, and --bind_ip options. Include any optional command line options used by the previous deployment: mongod --configsvr --replSet <replSetName> --port <port> --dbpath <path> --bind_ip localhost,<ip address>  If using a configuration file, update the file to specify sharding.clusterRole: configsvr, replication.replSetName, net.port, and net.bindIp, then start the 6.0 binary: sharding:  clusterRole: configsvrreplication:  replSetName: <string>net:  port: <port>  bindIp: localhost,<ip address>storage:  dbpath: <path>  Include any other configuration as appropriate for your deployment. 3 \nUPGRADE THE SHARDS. \nUpgrade the shards one at a time. For each shard replica set: 1 UPGRADE THE SECONDARY MEMBERS OF THE REPLICA SET, ONE AT A TIME. \n1 SHUT DOWN THE SECONDARY INSTANCE. \nTo shut down the mongod process, use mongosh to connect to the cluster member and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 REPLACE THE 5.0 BINARY WITH THE 6.0 BINARY. \nStart the 6.0 binary with the --shardsvr, --replSet, --port, and --bind_ip options. Include any additional command line options as appropriate for your deployment: mongod --shardsvr --replSet <replSetName> --port <port> --dbpath <path> --bind_ip localhost,<ip address> \n\n sharding:   clusterRole: shardsvrreplication:   replSetName: <string>net:   port: <port>   bindIp: localhost,<ip address>storage:   dbpath: <path>  Include any other configuration as appropriate for your deployment. 3 WAIT FOR THE MEMBER TO RECOVER TO THE SECONDARY STATE BEFORE UPGRADING THE NEXT SECONDARY MEMBER. \nTo check the member's state, you can issue rs.status() in mongosh. 4 REPEAT FOR EACH SECONDARY MEMBER. \n2 STEP DOWN THE REPLICA SET PRIMARY. \nConnect mongosh to the primary and use rs.stepDown() to step down the primary and force an election of a new primary: rs.stepDown() \n3 UPGRADE THE STEPPED-DOWN PRIMARY. \nWhen rs.status() shows that the primary has stepped down and another member has assumed the PRIMARY state, upgrade the stepped-down primary: 1 SHUT DOWN THE STEPPED-DOWN PRIMARY. \nTo shut down the stepped-down primary, use mongosh to connect to the replica set member and run the following command: db.adminCommand( { shutdown: 1 } ) \n2 REPLACE THE MONGOD BINARY. WITH THE 6.0 BINARY. \n3 START THE 6.0 BINARY. \nStart the 6.0 binary with the --shardsvr, --replSet, --port, and --bind_ip options. Include any additional command line options as appropriate for your deployment: mongod --shardsvr --replSet <replSetName> --port <port> --dbpath <path> --bind_ip localhost,<ip address>  If using a configuration file, update the file to specify sharding.clusterRole: shardsvr, replication.replSetName, net.port, and net.bindIp, then start the 6.0 binary: sharding:    clusterRole: shardsvrreplication:   replSetName: <string>net:   port: <port>   bindIp: localhost,<ip address>storage:   dbpath: <path>  Include any other configuration as appropriate for your deployment. 4 \nUPGRADE THE MONGOS INSTANCES. \nReplace each mongos instance with the 6.0 binary and restart. Include any other configuration as appropriate for your deployment. \nNOTE The --bind_ip option must be specified when the sharded cluster members are run on different hosts or if remote clients connect to the sharded cluster. For more information, see Localhost Binding Compatibility Changes. mongos --configdb csReplSet/<rsconfigsver1:port1>,<rsconfigsver2:port2>,<rsconfigsver3:port3> --bind_ip localhost,<ip address> \n5 \nRE-ENABLE THE BALANCER. \nUsing mongosh, connect to a mongos in the cluster and run sh.startBalancer() to re-enable the balancer: sh.startBalancer()  Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0, sh.startBalancer() also enables auto-splitting for the sharded cluster. If you do not wish to enable auto-splitting while the balancer is enabled, you must also run sh.disableAutoSplit(). Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. For more information about re-enabling the balancer, see Enable the Balancer. 6 \nENABLE BACKWARDS-INCOMPATIBLE 6.0 FEATURES. \nAt this point, you can run the 6.0 binaries without the 6.0 features that are incompatible with 5.0. To enable these 6.0 features, set the feature compatibility version (FCV) to 6.0. \nTIP Enabling these backwards-incompatible features can complicate the downgrade process since you must remove any persisted backwards-incompatible features before you downgrade. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features.\n\n db.adminCommand( { setFeatureCompatibilityVersion: \"6.0\" } )  Setting featureCompatibilityVersion (fCV) : \"6.0\" implicitly performs a replSetReconfig on each shard to add the term field to the shard replica configuration document and blocks until the new configuration propagates to a majority of replica set members. This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command on the mongos as the operation is idempotent. \nNOTE While setFeatureCompatibilityVersion is running on the sharded cluster, chunk migrations, splits, and merges can fail with ConflictingOperationInProgress. Any orphaned documents that exist on your shards will be cleaned up when you set the setFeatureCompatibilityVersion to 6.0. The cleanup process:      * Does not block the upgrade from completing, and  * Is rate limited. To mitigate the potential effect on performance during orphaned document cleanup, see Range Deletion Performance Tuning. \nNOTE \nADDITIONAL CONSIDERATION The mongos binary will crash when attempting to connect to mongod instances whose feature compatibility version (fCV) is greater than that of the mongos. For example, you cannot connect a MongoDB 5.0 version mongos to a 6.0 sharded cluster with fCV set to 6.0. You can, however, connect a MongoDB 5.0 version mongos to a 6.0 sharded cluster with fCV set to 5.0. \nADDITIONAL UPGRADE PROCEDURES \n * To upgrade a standalone, see Upgrade a Standalone to 6.0.  * To upgrade a replica set, see Upgrade a Replica Set to 6.0. ←  Upgrade a Replica Set to 6.06.0 Changelog → On this page  * Upgrade Recommendations and Checklists\n * Prerequisites\n * Download 6.0 Binaries\n * Upgrade Procedure\n * Additional Upgrade Procedures Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/3.2-downgrade/": " Docs Home → MongoDB Manual \nDOWNGRADE MONGODB FROM 3.2 \nOn this page    \n * Downgrade Recommendations and Checklist\n   \n * Prerequisites\n * Downgrade a Standalone mongod Instance\n * Downgrade a 3.2 Replica Set\n * Downgrade a 3.2 Sharded Cluster Before you attempt any downgrade, familiarize yourself with the content of this document, particularly the Downgrade Recommendations and Checklist and the procedure for downgrading sharded clusters. \nDOWNGRADE RECOMMENDATIONS AND CHECKLIST \nWhen downgrading, consider the following: \nDOWNGRADE PATH \nTo downgrade, use the latest version in the 3.0-series. \nPREPAREDNESS \n     * Remove or downgrade version 3 text indexes before downgrading MongoDB 3.2 to 3.0.  * Remove or downgrade version 3 2dsphere indexes before downgrading MongoDB 3.2 to 3.0. \nPROCEDURES \nFollow the downgrade procedures:  * To downgrade sharded clusters, see Downgrade a 3.2 Sharded Cluster.  * To downgrade replica sets, see Downgrade a 3.2 Replica Set.  * To downgrade a standalone MongoDB instance, see Downgrade a Standalone mongod Instance. \nPREREQUISITES  TEXT INDEX VERSION CHECK \nIf you have version 3 text indexes (i.e. the default version for text indexes in MongoDB 3.2), drop the version 3 text indexes before downgrading MongoDB. After the downgrade, recreate the dropped text indexes. To determine the version of your text indexes, run db.collection.getIndexes() to view index specifications. For text indexes, the method returns the version information in the field textIndexVersion. For example, the following shows that the text index on the quotes collection is version 3. {   \"v\" : 1,   \"key\" : {      \"_fts\" : \"text\",      \"_ftsx\" : 1   },   \"name\" : \"quote_text_translation.quote_text\",   \"ns\" : \"test.quotes\",   \"weights\" : {      \"quote\" : 1,      \"translation.quote\" : 1   },   \"default_language\" : \"english\",   \"language_override\" : \"language\",   \"textIndexVersion\" : 3}  \n2DSPHERE INDEX VERSION CHECK \nIf you have version 3 2dsphere indexes (i.e. the default version for 2dsphere indexes in MongoDB 3.2), drop the version 3 2dsphere indexes before downgrading MongoDB. After the downgrade, recreate the 2dsphere indexes. To determine the version of your 2dsphere indexes, run db.collection.getIndexes() to view index specifications. For 2dsphere indexes, the method returns the version information in the field 2dsphereIndexVersion. For example, the following shows that the 2dsphere index on the locations collection is version 3. {   \"v\" : 1,   \"key\" : {      \"geo\" : \"2dsphere\"   },   \"name\" : \"geo_2dsphere\",   \"ns\" : \"test.locations\",   \"sparse\" : true,   \"2dsphereIndexVersion\" : 3}  \nPARTIAL INDEXES CHECK \nBefore downgrading MongoDB, drop any partial indexes. \nDOWNGRADE A STANDALONE MONGOD INSTANCE \nThe following steps outline the procedure to downgrade a standalone mongod from version 3.2 to 3.0. 1 \nDOWNLOAD THE LATEST 3.0 BINARIES. \nFor the downgrade, use the latest release in the 3.0 series. 2 \nRESTART WITH THE LATEST 3.0 MONGOD INSTANCE.  IMPORTANT If your mongod instance is using the WiredTiger storage engine, you must include the --storageEngine option (or storage.engine if using the configuration file) with the 3.0 binary. Shut down your mongod instance. Replace the existing binary with the downloaded mongod binary and restart. \nDOWNGRADE A 3.2 REPLICA SET \nThe following steps outline a \"rolling\" downgrade process for the replica set. The \"rolling\" downgrade process minimizes downtime by downgrading the members individually while the other members are available: 1\n\n \nConnect a mongo shell to the current primary and downgrade the replication protocol: cfg = rs.conf();cfg.protocolVersion=0;rs.reconfig(cfg); \n2 \nDOWNGRADE SECONDARY MEMBERS OF THE REPLICA SET. \nDowngrade each secondary member of the replica set, one at a time:  a. Shut down the mongod. See Stop mongod Processes for instructions on safely terminating mongod processes.  b. Replace the 3.2 binary with the 3.0 binary and restart.\n    \n    \n    IMPORTANT\n    \n    If your mongod instance is using the WiredTiger storage engine, you must include the --storageEngine option (or storage.engine if using the configuration file) with the 3.0 binary.  c. Wait for the member to recover to SECONDARY state before downgrading the next secondary. To check the member's state, use the rs.status() method in the mongo shell. 3 \nSTEP DOWN THE PRIMARY. \nUse rs.stepDown() in the mongo shell to step down the primary and force the normal failover procedure. rs.stepDown()  rs.stepDown() expedites the failover procedure and is preferable to shutting down the primary directly. 4 \nREPLACE AND RESTART FORMER PRIMARY MONGOD. \nWhen rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state, shut down the previous primary and replace the mongod binary with the 3.0 binary and start the new instance. \nIMPORTANT If your mongod instance is using the WiredTiger storage engine, you must include the --storageEngine option (or storage.engine if using the configuration file) with the 3.0 binary. Replica set failover is not instant but will render the set unavailable to writes and interrupt reads until the failover process completes. Typically this takes 10 seconds or more. You may wish to plan the downgrade during a predetermined maintenance window. \nDOWNGRADE A 3.2 SHARDED CLUSTER  REQUIREMENTS \nWhile the downgrade is in progress, you cannot make changes to the collection metadata. For example, during the downgrade, do not do any of the following:  * sh.enableSharding()  * sh.shardCollection()  * sh.addShard()  * db.createCollection()  * db.collection.drop()  * db.dropDatabase()  * any operation that creates a database  * any other operation that modifies the cluster meta-data in any way. See Sharding Reference for a complete list of sharding commands. Note, however, that not all commands on the Sharding Reference page modifies the cluster meta-data. \nDOWNGRADE A SHARDED CLUSTER WITH SCCC CONFIG SERVERS \n1 DISABLE THE BALANCER. \nTurn off the balancer in the sharded cluster, as described in Disable the Balancer. 2 DOWNGRADE EACH SHARD, ONE AT A TIME. \nFor each replica set shard:  a. Downgrade the protocolVersion.  b. Downgrade the mongod secondaries before downgrading the primary.  c. To downgrade the primary, run replSetStepDown and then downgrade. For details on downgrading a replica set, see Downgrade a 3.2 Replica Set. 3 DOWNGRADE THE SCCC CONFIG SERVERS. \nIf the sharded cluster uses 3 mirrored mongod instances for the config servers, downgrade all three instances in reverse order of their listing in the --configdb option for mongos. For example, if mongos has the following --configdb listing: --configdb confserver1,confserver2,confserver3  Downgrade first confserver3, then confserver2, and lastly, confserver1. If your mongod instance is using the WiredTiger storage engine, you must include the --storageEngine option (or storage.engine if using the configuration file) with the 3.0 binary. mongod --configsvr --dbpath <path> --port <port> --storageEngine <storageEngine> \n4 DOWNGRADE THE MONGOS INSTANCES. \nDowngrade the binaries and restart. 5 RE-ENABLE THE BALANCER. \nOnce the downgrade of sharded cluster components is complete, re-enable the balancer. \nDOWNGRADE A SHARDED CLUSTER WITH CSRS CONFIG SERVERS \n1 DISABLE THE BALANCER. \nTurn off the balancer in the sharded cluster, as described in Disable the Balancer. 2 CHECK THE MINOPTIMEUPDATERS VALUE.\n\n To check the value, for each shard, connect to the primary member (or if a shard is a standalone, connect to the standalone) and query the system.version collection in the admin database for the minOpTimeRecovery document: use admindb.system.version.findOne( { _id: \"minOpTimeRecovery\" }, { minOpTimeUpdaters: 1 } )  If minOpTimeUpdaters is non-zero, clear the value by stepping down the current primary. The value is cleared when a new primary gets elected. rs.stepDown()  If the shard is a standalone, restart the shard to clear the value. 3 PREPARE CSRS CONFIG SERVERS FOR DOWNGRADE. \nIf the sharded cluster uses CSRS:  a. Remove secondary members from the replica set to have only a primary and two secondaries and only the primary can vote and be eligible to be primary; i.e. the other two members have 0 for votes and priority.\n    \n    Connect a mongo shell to the primary and run:\n    \n    rs.reconfig(   {      \"_id\" : <name>,      \"configsvr\" : true,      \"protocolVersion\" : NumberLong(1),      \"members\" : [         {            \"_id\" : 0,            \"host\" : \"<host1>:<port1>\",            \"priority\" : 1,            \"votes\" : 1         },         {            \"_id\" : 1,            \"host\" : \"<host2>:<port2>\",            \"priority\" : 0,            \"votes\" : 0         },         {            \"_id\" : 2,            \"host\" : \"<host3>:<port3>\",            \"priority\" : 0,            \"votes\" : 0         }      ]   })\n    \n      b. Step down the primary using replSetStepDown against the admin database. Ensure enough time for the secondaries to catch up.\n    \n    Connect a mongo shell to the primary and run:\n    \n    db.adminCommand( { replSetStepDown: 360, secondaryCatchUpPeriodSecs: 300 })\n    \n      c. Shut down all members of the config server replica set, the mongos instances, and the shards.\n\n  e. Restart each config server as standalone 3.2 mongod; i.e. without the --replSet or, if using a configuration file, replication.replSetName.\n    \n    mongod --configsvr --dbpath <path> --port <port> --storageEngine <storageEngine>\n    \n     4 UPDATE THE PROTOCOLVERSION FOR EACH SHARD. \nRestart each replica set shard and update the protocolVersion. Connect a mongo shell to the current primary and downgrade the replication protocol: cfg = rs.conf();cfg.protocolVersion=0;rs.reconfig(cfg); \n5 DOWNGRADE THE MONGOS INSTANCES.  IMPORTANT As the config servers changed from a replica set to three mirrored mongod instances, update the --configdb setting. All mongos must use the same --configdb string. Downgrade the binaries and restart. 6 DOWNGRADE CONFIG SERVERS. \nDowngrade the binaries and restart. Downgrade in reverse order of their listing in the --configdb option for mongos. If your mongod instance is using the WiredTiger storage engine, you must include the --storageEngine option (or storage.engine if using the configuration file) with the 3.0 binary. mongod --configsvr --dbpath <path> --port <port> --storageEngine <storageEngine> \n7 DOWNGRADE EACH SHARD, ONE AT A TIME. \nFor each shard, remove the minOpTimeRecovery document from the admin.system.version collection using the following remove operation. If the shard is a replica set, issue the remove operation on the primary of the replica set for each shard: use admindb.system.version.remove(   { _id: \"minOpTimeRecovery\" },   { writeConcern: { w: \"majority\", wtimeout: 30000 } })  \nNOTE If the cluster is running with authentication enabled, you must have a user with the proper privileges to remove the minOpTimeRecovery document from the admin.system.version collection. The following operation creates a downgrade user on the admin database with the proper privileges: use admin;\ndb.createRole({  role: \"downgrade_csrs\",  privileges: [     { resource: { db: \"admin\", collection: \"system.version\"}, actions: [ \"remove\" ] },  ],  roles: [  ]});\ndb.createUser({  user: \"downgrade\",  roles: [    { role: \"downgrade_csrs\", db: \"admin\" }  ]});  For each replica set shard, downgrade the mongod binaries and restart. If your mongod instance is using the WiredTiger storage engine, you must include the --storageEngine option (or storage.engine if using the configuration file) with the 3.0 binary.  1. Downgrade the mongod secondaries before downgrading the primary.  2. To downgrade the primary, run replSetStepDown and then downgrade. For details on downgrading a replica set, see Downgrade a 3.2 Replica Set. Optionally, drop the local database from the SCCC members if it exists. 8 RE-ENABLE THE BALANCER. \nOnce the downgrade of sharded cluster components is complete, re-enable the balancer. ←  Upgrade MongoDB to 3.2Release Notes for MongoDB 3.0 → On this page  * Downgrade Recommendations and Checklist\n * Prerequisites\n * Downgrade a Standalone mongod Instance\n * Downgrade a 3.2 Replica Set\n * Downgrade a 3.2 Sharded Cluster Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/4.2-upgrade-replica-set/": " Docs Home → MongoDB Manual \nUPGRADE A REPLICA SET TO 4.2 \nOn this page    \n * Upgrade Recommendations and Checklists\n   \n * Read Concern Majority (3-Member Primary-Secondary-Arbiter Architecture)\n * Change Stream Resume Tokens\n * All Members Version\n * MMAPv1 to WiredTiger Storage Engine\n * Increase Open Files ulimit Setting\n * Review Current Configuration\n * Feature Compatibility Version\n * Replica Set Member State\n * Via Package Manager\n * Manually \nIMPORTANT Before you attempt any upgrade, please familiarize yourself with the content of this document. If you need guidance on upgrading to 4.2, MongoDB professional services offer major version upgrade support to help ensure a smooth transition without interruption to your MongoDB application. \nUPGRADE RECOMMENDATIONS AND CHECKLISTS \nWhen upgrading, consider the following: \nUPGRADE VERSION PATH \nTo upgrade an existing MongoDB deployment to 4.2, you must be running a 4.0-series release. To upgrade from a version earlier than the 4.0-series, you must successively upgrade major releases until you have upgraded to 4.0-series. For example, if you are running a 3.6-series, you must upgrade first to 4.0 before you can upgrade to 4.2. \nCHECK DRIVER COMPATIBILITY \nBefore you upgrade MongoDB, check that you're using a MongoDB 4.2-compatible driver. Consult the driver documentation for your specific driver to verify compatibility with MongoDB 4.2. Upgraded deployments that run on incompatible drivers might encounter unexpected or undefined behavior. \nPREPAREDNESS \nBefore beginning your upgrade, see the Compatibility Changes in MongoDB 4.2 document to ensure that your applications and deployments are compatible with MongoDB 4.2. Resolve the incompatibilities in your deployment before starting the upgrade. Before upgrading MongoDB, always test your application in a staging environment before deploying the upgrade to your production environment. \nDOWNGRADE CONSIDERATION \nOnce upgraded to 4.2, if you need to downgrade, we recommend downgrading to the latest patch release of 4.0. \nREAD CONCERN MAJORITY (3-MEMBER PRIMARY-SECONDARY-ARBITER ARCHITECTURE) \nStarting in MongoDB 3.6, MongoDB enables support for \"majority\" read concern by default. You can disable read concern \"majority\" to prevent the storage cache pressure from immobilizing a three-member replica set with a primary-secondary-arbiter (PSA) architecture or a sharded cluster with a three-member PSA shards. \nNOTE Disabling \"majority\" read concern affects support for transactions on sharded clusters. Specifically:      * A transaction cannot use read concern \"snapshot\" if the transaction involves a shard that has disabled read concern \"majority\".  * A transaction that writes to multiple shards errors if any of the transaction's read or write operations involves a shard that has disabled read concern \"majority\". However, it does not affect transactions on replica sets. For transactions on replica sets, you can specify read concern \"majority\" (or \"snapshot\" or \"local\" ) for multi-document transactions even if read concern \"majority\" is disabled. Disabling \"majority\" read concern prevents collMod commands which modify an index from rolling back. If such an operation needs to be rolled back, you must resync the affected nodes with the primary node. Disabling \"majority\" read concern has no effect on change streams availability. When upgraded to 4.2 with read concern \"majority\" disabled, you can use change streams for your deployment. For more information, see Primary-Secondary-Arbiter Replica Sets. \nCHANGE STREAM RESUME TOKENS \nMongoDB 4.2 uses the version 1 (i.e. v1) change streams resume tokens, introduced in version 4.0.7. The resume token _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (fcv) at the time of the change stream's opening/resumption (i.e. a change in fcv value does not affect the resume tokens for already opened change streams):\n\n \nIMPORTANT \nWHEN UPGRADING FROM MONGODB 4.0.6 OR EARLIER TO MONGODB 4.2 During the upgrade process, a client may try to resume change streams using the new v1 resume token when connected to a member that has not been updated (i.e. only accepts v0 token) and fail. In such cases, the client must wait for the 4.2 upgrade to complete before resuming change streams. \nPREREQUISITES  ALL MEMBERS VERSION \nAll replica set members must be running version 4.0. To upgrade a replica set from an 3.6-series and earlier, first upgrade all members of the replica set to the latest 4.0-series release, and then follow the procedure to upgrade from MongoDB 4.0 to 4.2. \nMMAPV1 TO WIREDTIGER STORAGE ENGINE \nMongoDB 4.2 removes support for the deprecated MMAPv1 storage engine. If your 4.0 deployment uses MMAPv1, you must change the 4.0 deployment to WiredTiger Storage Engine before upgrading to MongoDB 4.2. For details, see Change Replica Set to WiredTiger. \nINCREASE OPEN FILES ULIMIT SETTING \nStarting in MongoDB 4.2, incoming connections to a mongod or mongos instance require two file descriptors. In previous versions of MongoDB, incoming connections required one file descriptor. Prior to upgrading from MongoDB 4.0 to 4.2, you may need to increase the value of your open files ulimit setting (-n). \nREVIEW CURRENT CONFIGURATION \nWith MongoDB 4.2, the mongod and mongos processes will not start with MMAPv1 Specific Configuration Options. Previous versions of MongoDB running WiredTiger ignored MMAPv1 configurations options if they were specified. With MongoDB 4.2, you must remove these from your configuration. \nFEATURE COMPATIBILITY VERSION \nThe 4.0 replica set must have featureCompatibilityVersion set to 4.0. To ensure that all members of the replica set have featureCompatibilityVersion set to 4.0, connect to each replica set member and check the featureCompatibilityVersion: db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )  All members should return a result that includes \"featureCompatibilityVersion\" : { \"version\" : \"4.0\" }. To set or update featureCompatibilityVersion, run the following command on the primary. A majority of the data-bearing members must be available: db.adminCommand( { setFeatureCompatibilityVersion: \"4.0\" } )  For more information, see setFeatureCompatibilityVersion. \nREPLICA SET MEMBER STATE \nEnsure that no replica set member is in ROLLBACK or RECOVERING state. \nDOWNLOAD 4.2 BINARIES  VIA PACKAGE MANAGER \nIf you installed MongoDB from the MongoDB apt, yum, dnf, or zypper repositories, you should upgrade to 4.2 using your package manager. Follow the appropriate 4.2 installation instructions for your Linux system. This will involve adding a repository for the new release, then performing the actual upgrade process. \nMANUALLY \nIf you have not installed MongoDB using a package manager, you can manually download the MongoDB binaries from the MongoDB Download Center. See 4.2 installation instructions for more information. \nUPGRADE PROCESS \nYou can upgrade from MongoDB 4.0 to 4.2 using a \"rolling\" upgrade to minimize downtime by upgrading the members individually while the other members are available. 1 UPGRADE SECONDARY MEMBERS OF THE REPLICA SET. \nUpgrade the secondary members of the replica set one at a time:  1. Shut down the mongod instance and replace the 4.0 binary with the 4.2 binary.  2. Restart the member. 2 STEP DOWN THE REPLICA SET PRIMARY. \nConnect a mongo shell to the primary and use rs.stepDown() to step down the primary and force an election of a new primary. 3 UPGRADE THE PRIMARY. \nWhen rs.status() shows that the primary has stepped down and another member has assumed PRIMARY state, upgrade the stepped-down primary:  1. Shut down the stepped-down primary and replace the mongod binary with the 4.2 binary.  2. Restart the member. 4 ENABLE BACKWARDS-INCOMPATIBLE 4.2 FEATURES. \nAt this point, you can run the 4.2 binaries without the 4.2 features that are incompatible with 4.0.\n\n \nTIP Enabling these backwards-incompatible features can complicate the downgrade process since you must remove any persisted backwards-incompatible features before you downgrade. It is recommended that after upgrading, you allow your deployment to run without enabling these features for a burn-in period to ensure the likelihood of downgrade is minimal. When you are confident that the likelihood of downgrade is minimal, enable these features. \nTIP Ensure that no initial sync is in progress. Running setFeatureCompatibilityVersion command while an initial sync is in progress will cause the initial sync to restart. On the primary, run the setFeatureCompatibilityVersion command in the admin database: db.adminCommand( { setFeatureCompatibilityVersion: \"4.2\" } )  This command must perform writes to an internal system collection. If for any reason the command does not complete successfully, you can safely retry the command on the primary as the operation is idempotent. \nPOST UPGRADE \nTLS Options Replace Deprecated SSL OptionsStarting in MongoDB 4.2, MongoDB deprecates the SSL options for the mongod, the mongos, and the mongo shell as well as the corresponding net.ssl Options configuration file options.To avoid deprecation messages, use the new TLS options for the mongod, the mongos, and the mongo.  * For the command-line TLS options, refer to the mongod, mongos, and mongo shell pages.  * For the corresponding mongod and mongos configuration file options, refer to the configuration file page.  * For the connection string tls options, refer to the connection string page. 4.2+ compatible Drivers Retry Writes by DefaultDrivers compatible with MongoDB 4.2 and higher enable Retryable Writes by default. Earlier drivers require the retryWrites=true option. The retryWrites=true option can be omitted in applications that use drivers compatible with MongoDB 4.2 and higher. \nTo disable retryable writes, applications that use drivers compatible with MongoDB 4.2 and higher must include retryWrites=false in the connection string. \nADDITIONAL UPGRADE PROCEDURES \n * To upgrade a standalone, see Upgrade a Standalone to 4.2.  * To upgrade a sharded cluster, see Upgrade a Sharded Cluster to 4.2. ←  Upgrade a Standalone to 4.2Upgrade a Sharded Cluster to 4.2 → On this page  * Upgrade Recommendations and Checklists\n * Read Concern Majority (3-Member Primary-Secondary-Arbiter Architecture)\n * Change Stream Resume Tokens\n * All Members Version\n * MMAPv1 to WiredTiger Storage Engine\n * Increase Open Files ulimit Setting\n * Review Current Configuration\n * Feature Compatibility Version\n * Replica Set Member State\n * Via Package Manager\n * Manually Share Feedback\n",
  "https://www.mongodb.com/docs/manual/release-notes/4.2-compatibility/": " Docs Home → MongoDB Manual \nCOMPATIBILITY CHANGES IN MONGODB 4.2 \nOn this page    \n * Removal of MMAPv1 Storage Engine\n   \n * Removed or Deprecated Commands and Methods\n * Aggregation\n * Transactions\n * Change Streams\n * Increased File Descriptors Required for Incoming Connections\n * MongoDB Tools\n * Replica Set State Changes\n * 4.2 Drivers Enable Retryable Writes by Default\n * General Changes\n * 4.2 Feature Compatibility \nREMOVAL OF MMAPV1 STORAGE ENGINE \nMongoDB 4.2 removes support for the deprecated MMAPv1 storage engine. If your 4.0 deployment uses MMAPv1, you must change the deployment to WiredTiger Storage Engine before upgrading to MongoDB 4.2. For details, see:      * Change Standalone to WiredTiger  * Change Replica Set to WiredTiger  * Change Sharded Cluster to WiredTiger \nMMAPV1 SPECIFIC CONFIGURATION OPTIONS \nMongoDB removes the following MMAPv1 specific configuration options: Removed Configuration File Setting\nRemoved Command-line Option\nstorage.mmapv1.journal.commitIntervalMs storage.mmapv1.journal.debugFlags\nmongod --journalOptions\nstorage.mmapv1.nsSize\nmongod --nssize\nstorage.mmapv1.preallocDataFiles\nmongod --noprealloc\nstorage.mmapv1.quota.enforced\nmongod --quota\nstorage.mmapv1.quota.maxFilesPerDB\nmongod --quotaFiles\nstorage.mmapv1.smallFiles\nmongod --smallfiles\nstorage.repairPath\nmongod --repairpath\nreplication.secondaryIndexPrefetch\nmongod --replIndexPrefetch \nNOTE Starting in version 4.2, MongoDB processes will not start with these options. Remove any MMAPv1 specific configuration options if using a WiredTiger deployment. \nMMAPV1 SPECIFIC PARAMETERS \nMongoDB removes the following MMAPv1 parameters:  * newCollectionsUsePowerOf2Sizes  * replIndexPrefetch \nMMAPV1 SPECIFIC COMMAND \nMongoDB removes the MMAPv1 specific touch command. \nMMAPV1 SPECIFIC OPTIONS FOR COMMANDS AND METHODS \nMongoDB removes the MMAPv1 specific options:  * noPadding and usePowerOf2Sizes for collMod  * verbose for collStats  * flags for create  * paddingFactor, paddingBytes, preservePadding for db.createCollection(). MongoDB ignores the MMAPv1 specific option async for fsync. \nREMOVED OR DEPRECATED COMMANDS AND METHODS  REMOVE SUPPORT FOR THE GROUP COMMAND \nStarting in version 4.2, MongoDB removes the group command (deprecated since version 3.4) and its mongo shell helper db.collection.group(). Use db.collection.aggregate() with the $group stage instead. \nREMOVE SUPPORT FOR THE EVAL COMMAND \nStarting in version 4.2, MongoDB removes the eval command. The eval command has been deprecated since version 3.0. The associated MongoDB 4.2 mongo shell methods db.eval() and db.collection.copyTo() can only be run when connected to MongoDB 4.0 or earlier. \nREMOVE SUPPORT FOR THE COPYDB AND CLONE COMMANDS \nStarting in version 4.2, MongoDB removes the deprecated copydb command and clone command. The corresponding mongo shell helpers db.copyDatabase() and db.cloneDatabase() can only be run when connected to MongoDB 4.0 or earlier. As alternatives, users can use mongodump and mongorestore (with the mongorestore options --nsFrom and --nsTo) or write a script using the drivers. For example, to copy the test database from a local instance running on the default port 27017 to the examples database on the same instance, you can:  1. Use mongodump to dump the test database to an archive mongodump-test-db:\n    \n    mongodump --archive=\"mongodump-test-db\" --db=test\n    \n    \n\n \nTIP Include additional options as necessary, such as to specify the uri or host, username, password and authentication database. Alternatively, instead of using an archive file, you can mongodump the test database to the standard output stream and pipe into mongorestore: mongodump --archive --db=test | mongorestore --archive  --nsFrom='test.*' --nsTo='examples.*'  \nREMOVE SUPPORT FOR THE PARALLELCOLLECTIONSCAN COMMAND \nStarting in version 4.2, MongoDB removes the parallelCollectionScan command. \nREMOVE MAXSCAN \nMongoDB removes the deprecated option maxScan for the find command and the mongo shell helper cursor.maxScan(). Use either the maxTimeMS option for the find command or the helper cursor.maxTimeMS() instead. \nREMOVE SUPPORT FOR THE GEONEAR COMMAND \nStarting in version 4.2, MongoDB removes the geoNear command. Use the $geoNear aggregation stage instead. The options for $geoNear are similar to the removed geoNear command with the following exceptions:  * The removed geoNear command includes in its output a field named dis that included the distance information.\n   \n   For the $geoNear stage, specify the distance field name in distanceField.  * The removed geoNear command accepts a boolean value for the includeLocs option to include the loc field.\n   \n   For the $geoNear stage, specify the location field name in includeLocs.  * The removed geoNear command includes the avgDistance and maxDistance of the returned results.\n   \n   You can use the aggregation pipeline to return the avgDistance and maxDistance as well. Specifically, after the $geoNear stage, include a $group stage to calculate the avgDistance and maxDistance:\n   \n   db.places.aggregate([   { $geoNear: { near: <...>, distanceField: \"dis\", includeLocs: \"loc\", spherical: true, ... } },   { $group: { _id: null, objectsLoaded: { $sum: 1 }, maxDistance:         { $max: \"$dis\" }, avgDistance: { $avg: \"$dis\" } } }])\n   \n   \n   \n   \n   TIP\n   \n   \n   SEE ALSO:\n   \n   Remove limit and num Options from $geoNear \nREMOVE SUPPORT FOR THE REPAIRDATABASE COMMAND \nStarting in version 4.2, MongoDB removes the repairDatabase command and its mongo shell helper db.repairDatabase() as well as the repairDatabase privilege. As alternatives:  * To compact data for a mongod, use the compact command. For details about the operation, see compact command.  * To rebuild indexes on a standalone, use the reIndex command or its helper db.collection.reIndex(). For details about the operation, see reIndex command and db.collection.reIndex() reference pages.  * To recover data for a standalone, use mongod --repair. See Recover a Standalone after an Unexpected Shutdown for details. \nREMOVE SUPPORT FOR GETPREVERROR COMMAND \nStarting in version 4.2, MongoDB removes the deprecated getPrevError command and its mongo shell helper db.getPrevError(). \nDEPRECATE SUPPORT FOR CLONECOLLECTION \nMongoDB deprecates the cloneCollection command and its mongo shell helper db.cloneCollection() As alternatives,  * Users can use mongoexport and mongoimport.  * Users can use the aggregation pipeline $out or $merge stages.  * Write a script using the drivers. \nDEPRECATED PLAN CACHE COMMANDS/METHODS \nMongoDB deprecates the following:  * PlanCache.getPlansByQuery() method/planCacheListPlans command.\n   \n   To get the cached query plans for a shape, use the $planCacheStats aggregation stage instead. See Find Cache Entry Details for a Query Shape.  * PlanCache.listQueryShapes() method/planCacheListQueryShapes command.\n   \n   To list the cached query shapes, use the $planCacheStats aggregation stage instead. See List Query Shapes. \nAGGREGATION  $OUT STAGE RESTRICTIONS \n$OUT AND VIEWS \nThe view definition pipeline cannot include the $out stage. If you already have an existing view that includes the $out stage, you can no longer create new views from this existing view.\n\n $OUT AND $LOOKUP \nThe $lookup stage cannot include the $out stage in its nested pipeline field for the joined collection. $OUT AND LINEARIZABLE READ CONCERN LEVEL \nThe $out stage cannot be used in conjunction with read concern \"linearizable\". $OUT AND EXPLAIN \nYou cannot run the db.collection.explain() method (or the explain command) in executionStats mode or allPlansExecution mode if the aggregation pipeline contains the $out stage. If the aggregation pipeline contains the $out stage, to view executionStats or allPlansExecution information, run explain without the $out stage in order to return explain results for the preceding stages. Alternatively, you can run explain in queryPlanner mode for an aggregation pipeline that contains the $out stage. $OUT AND MAJORITY READ CONCERN LEVEL \nStarting in MongoDB 4.2, you can specify read concern level \"majority\" for an aggregation that includes an $out stage. \nREMOVE LIMIT AND NUM OPTIONS FROM $GEONEAR \nStarting in version 4.2, MongoDB removes the limit and num options for the $geoNear stage as well as the default limit of 100 documents. To limit the results of $geoNear, use the $geoNear stage with the $limit stage. For example, the following aggregation where the $geoNear stage contains the num option is no longer valid in 4.2. db.places.aggregate([   {     $geoNear: {        near: { type: \"Point\", coordinates: [ -73.99279 , 40.719296 ] },        distanceField: \"distance\",        num: 5,                 // Not supported in 4.2        spherical: true     }   }]) Instead, you can rewrite the aggregation to the following pipeline: db.places.aggregate([   {     $geoNear: {        near: { type: \"Point\", coordinates: [ -73.99279 , 40.719296 ] },        distanceField: \"distance\",        spherical: true     }   },   { $limit: 5 }])  \nTIP \nSEE ALSO: Aggregation Improvements \nTRANSACTIONS \n * Starting in MongoDB 4.2, you cannot specify killCursors as the first operation in a transaction.  * Starting in MongoDB 4.2, you cannot write to capped collections in transactions. Reads from capped collections are still supported in transactions.  * Starting in MongoDB 4.2, MongoDB removes the 16MB total size limit for a transaction. In version 4.2, MongoDB creates as many oplog entries as necessary to encapsulate all write operations in a transaction. In previous versions, MongoDB creates a single entry for all write operations in a transaction, thereby imposing a 16MB total size limit for a transaction. \nCHANGE STREAMS  AVAILABILITY \nStarting in MongoDB 4.2, change streams are available regardless of the \"majority\" read concern support; that is, read concern majority support can be either enabled (default) or disabled to use change streams. In MongoDB 4.0 and earlier, change streams are available only if \"majority\" read concern support is enabled (default). \nTIP \nSEE ALSO: 4.2 Changes to Change Streams \nDEFAULT COLLATION \nStarting in MongoDB 4.2, change streams use simple binary comparisons unless an explicit collation is provided. In earlier versions, change streams opened on a single collection (db.collection.watch()) would inherit that collection's default collation. \nRESUME TOKEN MODIFICATION \nStarting in MongoDB 4.2, change streams will throw an exception if the change stream aggregation pipeline modifies an event's _id field. \nINCREASED FILE DESCRIPTORS REQUIRED FOR INCOMING CONNECTIONS \nStarting in MongoDB 4.2, incoming connections to a mongod or mongos instance require two file descriptors. In previous versions of MongoDB, incoming connections required one file descriptor. Prior to upgrading from MongoDB 4.0 to 4.2, you may need to increase the value of your open files ulimit setting (-n). \nMONGODB TOOLS  FIPS MODE \nStarting in version 4.2, MongoDB removes the --sslFIPSMode option for the following programs:  * mongodump  * mongoexport  * mongofiles\n\n  * mongorestore  * mongostat  * mongotop The programs will use FIPS compliant connections to mongod / mongos if the mongod / mongos instances are configured to use FIPS mode. \nEXTENDED JSON V2 \nStarting in version 4.2: Binary\nChanges\nbsondump\nUses Extended JSON v2.0 (Canonical mode) format.\nmongodump Use Extended JSON v2.0 (Canonical mode) format for the metadata. Requires mongorestore version 4.2 or later that supports Extended JSON v2.0 (Canonical mode or Relaxed) format. \nTIP In general, use corresponding versions of mongodump and mongorestore. That is, to restore data files created with a specific version of mongodump, use the corresponding version of mongorestore. mongoexport\nCreates output data in Extended JSON v2.0 (Relaxed mode) by default.\nCreates output data in Extended JSON v2.0 (Canonical mode) if used with --jsonFormat.\nmongoimport\nExpects import data to be in Extended JSON v2.0 (either Relaxed or Canonical mode) by default.\nCan recognize data that is in Extended JSON v1.0 format if the option --legacy is specified. \nTIP In general, the versions of mongoexport and mongoimport should match. That is, to import data created from mongoexport, you should use the corresponding version of mongoimport. For details on MongoDB extended JSON v2, see MongoDB Extended JSON (v2). --QUERY OPTIONS \nStarting in version 4.2, the query option for mongodump --query and mongoexport --query must be in Extended JSON v2 format (relaxed or canonical/strict mode), including enclosing the field names and the operators in quotes, as in the following: mongoexport -d=test -c=records -q='{ \"a\": { \"$gte\": 3 }, \"date\": { \"$lt\": { \"$date\": \"2016-01-01T00:00:00.000Z\" } } }' --out=exportdir/myRecords.json  In earlier versions, the query options uses the Extended JSON v1 format and the field names and the operators do not need to be in quotes: mongoexport -d=test -c=records -q='{ a: { $gte: 3 }, date: { $lt: { \"$date\": \"2016-01-01T00:00:00.000Z\" } } }' --out=exportdir/myRecords.json  \nREPLICA SET STATE CHANGES  PRIMARY STEP DOWN \nStarting in MongoDB 4.2, replSetStepDown (and replSetReconfig that results in a step down) no longer closes all client connections. However, writes that were in progress are killed. In MongoDB 4.0 and earlier, replSetStepDown closes all client connections during the step down. \nROLLBACK STATE \nStarting in version 4.2, MongoDB kills all in-progress user operations when a member enters the ROLLBACK state. \n4.2 DRIVERS ENABLE RETRYABLE WRITES BY DEFAULT \nDrivers compatible with MongoDB 4.2 and higher enable Retryable Writes by default. Earlier drivers require the retryWrites=true option. The retryWrites=true option can be omitted in applications that use drivers compatible with MongoDB 4.2 and higher.  \nTo disable retryable writes, applications that use drivers compatible with MongoDB 4.2 and higher must include retryWrites=false in the connection string. \nIMPORTANT The local database does not support retryable writes. Applications which write to the local database will encounter write errors upon upgrading to a 4.2-series driver unless retryable writes are explicitly disabled. \nGENERAL CHANGES  INDEXES \nSTRONGER RESTRICTIONS ON REINDEX \nMongoDB implements a stronger restriction on running reIndex command and db.collection.reIndex() shell helper against a collection in a sharded cluster by disallowing reIndex and db.collection.reIndex() on a mongos. RESTRICTION ON DB.COLLECTION.DROPINDEX() METHOD \nYou cannot specify db.collection.dropIndex(\"*\") to drop all non-_id indexes. Use db.collection.dropIndexes() instead. DUPLICATE INDEX CREATION ATTEMPT ERROR MESSAGE\n\n Starting in version 4.2, the createIndexes command and the mongo shell helpers db.collection.createIndex() and db.collection.createIndexes() report an error if you create an index with one name, and then try to create the same index again but with another name. {   \"ok\" : 0,   \"errmsg\" : \"Index with name: x_1 already exists with a different name\",   \"code\" : 85,   \"codeName\" : \"IndexOptionsConflict\"}  In previous versions, MongoDB did not create the index again, but would return a response object with ok value of 1 and a note that implied that the index was not recreated. For example: {   \"numIndexesBefore\" : 2,   \"numIndexesAfter\" : 2,   \"note\" : \"all indexes already exist\",   \"ok\" : 1}  HASHED INDEX ON POWERPC \nFor hashed indexes, MongoDB 4.2 ensures that the hashed value for the floating point value 2 63 on PowerPC is consistent with other platforms. In previous versions, the hashed value for the floating point value 2 63 on PowerPC is inconsistent with other platforms. Although hashed indexes on a field that may contain floating point values greater than 2 53 is an unsupported configuration, clients may still insert documents where the indexed field has the value 2 63. To list all hashed indexes for your deployment, see PowerPC and 2 63. If the current MongoDB 4.0 sharded cluster on PowerPC contains hashed values for 2 63 as part of the shard key, additional considerations must be taken before upgrading the sharded cluster to 4.2. See Upgrade a Sharded Cluster to 4.2. \nMIN()/MAX() \nStarting in MongoDB 4.2, when specifying min()/max() for a db.collection.find() operation, you must explicitly specify the index for min()/max() with the cursor.hint() method unless the find() query is an equality condition on the _id field { _id: <value> }. Similarly, when specifying min/max in the find command, you must also explicitly specify the hint for the min/max index. In previous versions, you could run min()/max() (or the corresponding min/max fields in the command) with or without explicitly hinting the index regardless of the query condition. If run without the hint in 4.0 and earlier, MongoDB selects the index using the fields in the indexBounds; however, if multiple indexes exist on same fields with different sort orders, the selection of the index may be ambiguous. \nCURRENTOP \n * When reporting on \"getmore\" operations, the $currentOp aggregation stage, along with currentOp command and the db.currentOp() helper, now returns the originatingCommand field as a nested field in the new cursor field. In previous versions, the originatingCommand was a top-level field for the associated \"getmore\" document. See also 4.2 currentOp Changes. \nSERVER STATUS \n * The serverStatus and db.serverStatus() method return opcounters and opcountersRepl metrics as 64-bit integers (i.e. NumberLong) instead of 32-bit integers (i.e. NumberInt). \nLOGGING \n * When logging to syslog, the format of the message text includes the component. For example:\n   \n   ...  ACCESS   [repl writer worker 5] Unsupported modification to roles collection ...\n   \n   \n   \n   Previously, the syslog message text did not include the component. For example:\n   \n   ... [repl writer worker 1] Unsupported modification to roles collection ...\n   \n     * Starting in MongoDB 4.2, the getLog command truncates any event that contains more than 1024 characters. In earlier versions, getLog truncates after 512 characters.  * Starting in version 4.2, MongoDB logs the debug verbosity level. For example, if verbosity level is 2, MongoDB logs D2.\n   \n   In previous versions, MongoDB log messages only specified D for Debug level. \nWIRE PROTOCOL \n * MongoDB no longer supports the deprecated internal OP_COMMAND and the corresponding OP_COMMANDREPLY wire protocol. \nKILLCURSORS CHANGES \nTRANSACTIONS \nStarting in MongoDB 4.2, you cannot specify killCursors as the first operation in a transaction. PRIVILEGES\n\n In MongoDB 3.6.3 through MongoDB 4.0.x, users required the killCursors privilege in order to kill their own cursors when access control is enabled. \nREMOVES ASYNCREQUESTSSENDERUSEBATON PARAMETER \nIn MongoDB 4.2+ deployment, MongoDB removes the AsyncRequestsSenderUseBaton parameter and always enables the performance enhancement controlled by the parameter. \nSTRICTER VALIDATION OF COUNT SYNTAX \nStarting in version 4.2, MongoDB implements a stricter validation of the option names for the count command. The command now errors if you specify an unknown option name. In previous versions, MongoDB ignores invalid option names. \nCAUSAL CONSISTENCY SESSIONS \nStarting in MongoDB 4.2, the following commands no longer support afterClusterTime:  * dbHash command  * mapReduce command  * validate command As such, these operations cannot be associated with causally consistent sessions. \nREMOVES FASTMODINSERT METRIC \nMongoDB 4.2 removes the deprecated fastmodinsert metric from various outputs, including the explain executionStats, the profiler output, etc. \nMAP-REDUCE \nStarting in version 4.2, MongoDB deprecates:  * The map-reduce option to create a new sharded collection as well as the use of the sharded option for map-reduce. To output to a sharded collection, create the sharded collection first. MongoDB 4.2 also deprecates the replacement of an existing sharded collection.  * The explicit specification of nonAtomic: false option. \nBALANCER STATE AND AUTOSPLIT \nStarting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0:  * The balancerStart command and the mongo shell helper methods sh.startBalancer() and sh.setBalancerState(true) also enable auto-splitting for the sharded cluster.\n   \n   To disable auto-splitting when the balancer is enabled, you can use sh.disableAutoSplit().  * The balancerStop command and the mongo shell helper methods sh.stopBalancer() and sh.setBalancerState(false) also disable auto-splitting for the sharded cluster.\n   \n   To enable auto-splitting when the balancer is disabled, you can use sh.enableAutoSplit(). The mongo methods sh.enableBalancing(namespace) and sh.disableBalancing(namespace) have no affect on the auto-splitting. \nLOCK DIAGNOSTICS REPORTING \nStarting in version 4.2, MongoDB reports on ReplicationStateTransition lock information. In addition, MongoDB 4.2 separates ParallelBatchWriterMode lock information from Global lock information. Earlier MongoDB versions report ParallelBatchWriterMode lock information as part of Global locks. For operations that report on lock information, see:  * serverStatus command and db.serverStatus() method.  * $currentOp aggregation pipeline stage, currentOp command, and db.currentOp() method. \nFINDANDMODIFY QUERY/SORT/PROJECTION ARGUMENT VALIDATION \nStarting in MongoDB 4.2 (and 4.0.12+ and 3.6.14+), the findAndModify command and its associated mongo shell methods error if the specified query, sort, or projection argument is not a document. In earlier versions, the operation treated non-document query or sort argument as an empty document {}. See:  * findAndModify  * db.collection.findOneAndDelete()  * db.collection.findOneAndReplace()  * db.collection.findOneAndUpdate()  * db.collection.findAndModify() \nDROPDATABASE AND MOVEPRIMARY \nStarting in MongoDB 4.2,  * If you drop a database and create a new database with the same name, either:\n   \n   * Restart all mongos instances and mongod shard members; or\n   \n   * Use the flushRouterConfig command on all mongos instances and mongod shard members before reading or writing to that database.\n\n This ensures that mongos and shard instances refresh their metadata cache. Otherwise, the you may miss data on reads, and may not write data to the correct shard. To recover, you must manually intervene. In earlier versions, you only need to restart or run flushRouterConfig on the mongos instances. For more information, see dropDatabase and movePrimary. \nLIBLDAP AND LIBLDAP_R \nFor MongoDB 4.2 Enterprise binaries linked against libldap (such as when running on RHEL), access to the libldap is synchronized, incurring some performance/latency costs. For MongoDB 4.2 Enterprise binaries linked against libldap_r, there is no change in behavior from earlier MongoDB versions. \nCONNECTION POOLING AND LDAP SERVER \nStarting in version 4.2, MongoDB changes the ldapUseConnectionPool default values to:  * true on Windows.  * true on Linux where MongoDB Enterprise binaries are linked against libldap_r. That is, on those systems, MongoDB, by default, uses connection pooling to connect to the LDAP server for authentication/authorization. In earlier versions (versions 4.0.9+), MongoDB uses false as the default value for ldapUseConnectionPool. That is, MongoDB, by default, does not use connection pooling to connect to the LDAP server for authentication/authorization. See ldapUseConnectionPool for details. \nREMOVAL OF SYSTEM.INDEXES AND SYSTEM.NAMESPACES COLLECTIONS \nStarting in version 4.2, MongoDB removes the system.indexes and system.namespaces collections (deprecated since v3.0). With the removal of these collections, built-in roles clusterManager, clusterMonitor, dbAdmin, read, restore, and other roles that inherit from these roles no longer provide privileges to directly access system.indexes and system.namespaces collections. \nARBITER DOWNGRADE REQUIRES CLEARING DATA DIRECTORY \nMongoDB 4.2 arbiter data files are incompatible with MongoDB 4.0. Downgrading from MongoDB 4.2 to 4.0 requires deleting arbiter data files as an intermediary step. Running a MongoDB 4.0 arbiter against MongoDB 4.2 data files may result in unexpected behavior. The downgrade instructions for replica sets and sharded clusters include specific steps for downgrading arbiters from 4.2 to 4.0:  * Downgrade 4.2 Replica Set to 4.0.  * Downgrade 4.2 Sharded Cluster to 4.0. \nSHARDED COLLECTIONS AND REPLACE DOCUMENTS \nStarting in MongoDB 4.2,  * Operations which replace documents, such as replaceOne() or update() (when used with a replacement document), will first attempt to target a single shard by using the query filter. If the operation cannot target a single shard by the query filter, it then attempts to target by the replacement document. In earlier versions, these operations only attempt to target using the replacement document.  * The save() method is deprecated: use the insertOne() or replaceOne() method instead. The save() method cannot be used with sharded collections that are not sharded by _id, and attempting to do so will result in an error.  * For a replace document operation that includes upsert: true and is on a sharded collection, the filter must include an equality match on the full shard key. \n4.2 FEATURE COMPATIBILITY \nSome features in 4.2 require not just the 4.2 binaries but the featureCompatibilityVersion (fCV) set to 4.2. These features include:  * Distributed transactions.  * Removal of Index Key Limit for MongoDB versions with fCV set to 4.2+. In concert with the removal of this limit, the failIndexKeyTooLong parameter has no effect for MongoDB versions with fCV set to 4.2+ and only applies for MongoDB 2.6 through MongoDB versions with fCV set to \"4.0\" or earlier.  * Removal of Index Name Length for MongoDB versions with fCV set to 4.2+.  * New internal format for unique indexes. The new format applies to both existing unique indexes as well as newly created/rebuilt unique indexes.  * Starting in MongoDB 4.2, users can no longer use the query filter $type: 0 as a synonym for $exists:false. To query for null or missing fields, see Query for Null or Missing Fields.  * MongoDB 4.2 adds wildcard indexes to support workloads where users query against custom fields or a large variety of fields in a collection.\n\n On this page  * Removal of MMAPv1 Storage Engine\n * Removed or Deprecated Commands and Methods\n * Aggregation\n * Transactions\n * Change Streams\n * Increased File Descriptors Required for Incoming Connections\n * MongoDB Tools\n * Replica Set State Changes\n * 4.2 Drivers Enable Retryable Writes by Default\n * General Changes\n * 4.2 Feature Compatibility Share Feedback\n",
  "https://www.mongodb.com/docs/manual/administration/production-checklist-development/": " Docs Home → MongoDB Manual \nDEVELOPMENT CHECKLIST \nOn this page    \n * Data Durability\n   \n * Schema Design\n * Replication\n * Sharding\n * Drivers The following checklist, along with the Operations Checklist, provides recommendations to help you avoid issues in your production MongoDB deployment. \nDATA DURABILITY \n     * Ensure that your replica set includes at least three data-bearing voting members and that your write operations use w: majority write concern. Three data-bearing voting members are required for replica-set wide data durability.  * Ensure that all instances use journaling. \nSCHEMA DESIGN \nData in MongoDB has a dynamic schema. Collections do not enforce document structure. This facilitates iterative development and polymorphism. Nevertheless, collections often hold documents with highly homogeneous structures. See Data Modeling Concepts for more information.  * Determine the set of collections that you will need and the indexes required to support your queries. With the exception of the _id index, you must create all indexes explicitly: MongoDB does not automatically create any indexes other than _id.  * Ensure that your schema design supports your deployment type: if you are planning to use sharded clusters for horizontal scaling, design your schema to include a strong shard key. While you can change your shard key later, it is important to carefully consider your shard key choice to avoid scalability and perfomance issues.  * Ensure that your schema design does not rely on indexed arrays that grow in length without bound. Typically, best performance can be achieved when such indexed arrays have fewer than 1000 elements.  * Consider the document size limits when designing your schema. The BSON Document Size limit is 16MB per document. If you require larger documents, use GridFS. \nREPLICATION \n * Use an odd number of voting members to ensure that elections proceed successfully. You can have up to 7 voting members. If you have an even number of voting members, and constraints, such as cost, prohibit adding another secondary to be a voting member, you can add an arbiter to ensure an odd number of votes. For additional considerations when using an arbiter for a 3-member replica set (P-S-A), see Replica Set Arbiter.\n   \n   \n   NOTE\n   \n   For the following MongoDB versions, pv1 increases the likelihood of w:1 rollbacks compared to pv0 (no longer supported in MongoDB 4.0+) for replica sets with arbiters:\n   \n    * MongoDB 3.4.1\n   \n    * MongoDB 3.4.0\n   \n    * MongoDB 3.2.11 or earlier\n   \n   See Replica Set Protocol Version.  * Ensure that your secondaries remain up-to-date by using monitoring tools and by specifying appropriate write concern.  * Do not use secondary reads to scale overall read throughput. See: Can I use more replica nodes to scale for an overview of read scaling. For information about secondary reads, see: Read Preference. \nSHARDING \n * Ensure that your shard key distributes the load evenly on your shards. See: Shard Keys for more information.  * Use targeted operations for workloads that need to scale with the number of shards.  * For MongoDB 3.4 and earlier, read from the primary nodes for non-targeted or broadcast queries as these queries may be sensitive to stale or orphaned data.  * For MongoDB 3.6 and later, secondaries no longer return orphaned data unless using read concern \"available\" (which is the default read concern for reads against secondaries when not associated with causally consistent sessions).\n   Starting in MongoDB 3.6, all members of the shard replica set maintain chunk metadata, allowing them to filter out orphans when not using \"available\". As such, non-targeted or broadcast queries that are not using \"available\" can be safely run on any member and will not return orphaned data.\n   The \"available\" read concern can return orphaned documents from secondary members since it does not check for updated chunk metadata. However, if the return of orphaned documents is immaterial to an application, the \"available\" read concern provides the lowest latency reads possible among the various read concerns.  * Pre-split and manually balance chunks when inserting large data sets into a new non-hashed sharded collection. Pre-splitting and manually balancing enables the insert load to be distributed among the shards, increasing performance for the initial load. \nDRIVERS\n\n  * Ensure that your applications handle transient write and read errors during replica set elections.  * Ensure that your applications handle failed requests and retry them if applicable. Drivers do not automatically retry failed requests.  * Use exponential backoff logic for database request retries.  * Use cursor.maxTimeMS() for reads and wtimeout for writes if you need to cap execution time for database operations. ←  Operations ChecklistMongoDB Performance → On this page  * Data Durability\n * Schema Design\n * Replication\n * Sharding\n * Drivers Share Feedback\n",
  "https://www.mongodb.com/docs/manual/administration/install-enterprise-linux/": " Docs Home → MongoDB Manual \nINSTALL MONGODB ENTERPRISE ON LINUX \nThese documents provide instructions to install MongoDB Enterprise Edition for supported Linux systems. Install on Red HatInstall MongoDB Enterprise and required dependencies on Red Hat Enterprise or CentOS Systems using packages.Install on UbuntuInstall MongoDB Enterprise and required dependencies on Ubuntu Linux Systems using packages.Install on DebianInstall MongoDB Enterprise and required dependencies on Debian Linux Systems using packages.Install on SUSEInstall MongoDB Enterprise and required dependencies on SUSE Enterprise Linux.Install on AmazonInstall MongoDB Enterprise and required dependencies on Amazon Linux AMI.\n←  Install MongoDB EnterpriseInstall MongoDB Enterprise Edition on Red Hat or CentOS → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/administration/sharded-cluster-config-servers/": " Docs Home → MongoDB Manual \nSHARDED CLUSTER CONFIG SERVER ADMINISTRATION \nReplace a Config ServerReplace a config server in a config server replica set. \nTIP \nSEE ALSO: Backup and Restore Sharded Clusters ←  Sharded Cluster AdministrationReplace a Config Server → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/administration/configuration/": " Docs Home → MongoDB Manual \nRUN-TIME DATABASE CONFIGURATION \nOn this page    \n * Configure the Database\n   \n * Security Considerations\n * Replication and Sharding Configuration\n * Run Multiple Database Instances on the Same System\n * Diagnostic Configurations The command line and configuration file interfaces provide MongoDB administrators with a large number of options and settings for controlling the operation of the database system. This document provides an overview of common configurations and examples of best-practice configurations for common use cases. While both interfaces provide access to the same collection of options and settings, this document primarily uses the configuration file interface.      * If you installed MongoDB with a package manager such as yum or apt on Linux or brew on macOS, or with the MSI installer on Windows, a default configuration file has been provided as part of your installation:\n   \n   Platform\n   Method\n   Configuration File\n   Linux\n   apt, yum, or zypper Package Manager\n   /etc/mongod.conf\n   macOS\n   brew Package Manager\n   \n   /usr/local/etc/mongod.conf (on Intel processors), or\n   \n   /opt/homebrew/etc/mongod.conf (on Apple M1 processors)\n   \n   Windows\n   MSI Installer\n   <install directory>\\bin\\mongod.cfg  * If you installed MongoDB via a downloaded TGZ or ZIP file, you will need to create your own configuration file. The basic example configuration is a good place to start. For package installations of MongoDB on Linux or macOS, an initialization script which uses this default configuration file is also provided. This initialization script can be used to start the mongod on these platforms in the following manner:  * On Linux systems that use the systemd init system (the systemctl command):\n   \n   sudo systemctl start mongod\n   \n     * On Linux systems that use the SystemV init init system (the service command):\n   \n   sudo service mongod start\n   \n     * On macOS, using the brew package manger:\n   \n   brew services start mongodb-community@6.0\n   \n    If you installed MongoDB using a TGZ or ZIP file, you will need to create your own configuration file. A basic example configuration can be found later in this document. Once you have created a configuration file, you can start a MongoDB instance with this configuration file by using either the --config or -f options to mongod. For example, on Linux: mongod --config /etc/mongod.confmongod -f /etc/mongod.conf  Modify the values in the mongod.conf file on your system to control the configuration of your database instance. \nCONFIGURE THE DATABASE \nConsider the following basic configuration: processManagement:   fork: truenet:   bindIp: localhost   port: 27017storage:   dbPath: /var/lib/mongosystemLog:   destination: file   path: \"/var/log/mongodb/mongod.log\"   logAppend: truestorage:   journal:      enabled: true  For most standalone servers, this is a sufficient base configuration. It makes several assumptions, but consider the following explanation:  * fork is true, which enables a daemon mode for mongod, which detaches (i.e. \"forks\") the MongoDB from the current session and allows you to run the database as a conventional server.  * bindIp is localhost, which forces the server to only listen for requests on the localhost IP. Only bind to secure interfaces that the application-level systems can access with access control provided by system network filtering (i.e. \"firewall\").  * port is 27017, which is the default MongoDB port for database instances. MongoDB can bind to any port. You can also filter access based on port using network filtering tools.\n   \n   \n   NOTE\n   \n   UNIX-like systems require superuser privileges to attach processes to ports lower than 1024.  * quiet is true. This disables all but the most critical entries in output/log file, and is not recommended for production systems. If you do set this option, you can use setParameter to modify this setting during run time.\n\n  * systemLog.path is /var/log/mongodb/mongod.log which is where mongod will write its output. If you do not set this value, mongod writes all output to standard output (e.g. stdout.)  * logAppend is true, which ensures that mongod does not overwrite an existing log file following the server start operation.  * storage.journal.enabled is true, which enables journaling. Journaling ensures single instance write-durability. 64-bit builds of mongod enable journaling by default. Thus, this setting may be redundant. Given the default configuration, some of these values may be redundant. However, in many situations explicitly stating the configuration increases overall system intelligibility. \nSECURITY CONSIDERATIONS \nThe following configuration options are useful for limiting access to a mongod instance: net:   bindIp: localhost,10.8.0.10,192.168.4.24,/tmp/mongod.socksecurity:   authorization: enabled \nnet.bindIp This example provides four values to the bindIp option:  * localhost, the localhost interface;  * 10.8.0.10, a private IP address typically used for local networks and VPN interfaces;  * 192.168.4.24, a private network interface typically used for local networks; and  * /tmp/mongod.sock, a Unix domain socket path. Because production MongoDB instances need to be accessible from multiple database servers, it is important to bind MongoDB to multiple interfaces that are accessible from your application servers. At the same time it's important to limit these interfaces to interfaces controlled and protected at the network layer. security.authorizationSetting this option to true enables the authorization system within MongoDB. If enabled you will need to log in by connecting over the localhost interface for the first time to create user credentials. \nTIP \nSEE ALSO: Security \nREPLICATION AND SHARDING CONFIGURATION  REPLICATION CONFIGURATION \nreplica set configuration is straightforward, and only requires that the replSetName have a value that is consistent among all members of the set. Consider the following: replication:   replSetName: set0  Use descriptive names for sets. Once configured, use mongosh to add hosts to the replica set. \nTIP \nSEE ALSO: Replica set reconfiguration To enable authentication for the replica set using keyfiles , add the following keyFile option [1]: security:   keyFile: /srv/mongodb/keyfile  Setting keyFile enables authentication and specifies a keyfile for the replica set member to use when authenticating to each other. \nTIP \nSEE ALSO: The Replica Set Security section for information on configuring authentication with replica sets. The Replication document for more information on replication in MongoDB and replica set configuration in general. [1] Sharded clusters and replica sets can use x.509 for membership verification instead of keyfiles. For details, see x.509. \nSHARDING CONFIGURATION \nSharding requires mongod instances with different mongod configurations for the config servers and the shards. The config servers store the cluster's metadata, while the shards store the data. To configure the config server mongod instances, in the configuration file, specify configsvr for the sharding.clusterRole setting. Changed in version 3.4: Starting in version 3.4, MongoDB removes support for mirrored config servers and config servers must be deployed as a replica set.  sharding:    clusterRole: configsvr net:    bindIp: 10.8.0.12    port: 27001replication:    replSetName: csRS  To deploy config servers as a replica set, the config servers must run the WiredTiger Storage Engine. Initiate the replica set and add members. To configure the shard mongod instances, specify shardsvr for the sharding.clusterRole setting, and if running as a replica set, the replica set name: sharding:   clusterRole: shardsvrreplication:   replSetName: shardA  If running as a replica set, initiate the shard replica set and add members. For the router (i.e. mongos), configure at least one mongos process with the following setting: sharding:   configDB: csRS/10.8.0.12:27001  You can specify additional members of the config server replica set by specifying hostnames and ports in the form of a comma separated list after the replica set name. \nTIP \nSEE ALSO: The Sharding section of the manual for more information on sharding and cluster configuration.\n\n \nIn many cases running multiple instances of mongod on a single system is not recommended. On some types of deployments [2] and for testing purposes you may need to run more than one mongod on a single system. In these cases, use a base configuration for each instance, but consider the following configuration values: storage:   dbPath: /var/lib/mongo/db0/processManagement:   pidFilePath: /var/lib/mongo/db0.pid  The dbPath value controls the location of the mongod instance's data directory. Ensure that each database has a distinct and well labeled data directory. The pidFilePath controls where mongod process places it's process ID (PID) file. As this tracks the specific mongod file, it is crucial that file be unique and well labeled to make it easy to start and stop these processes. Create additional init scripts and/or adjust your existing MongoDB configuration and init script as needed to control these processes. [2] Single-tenant systems with SSD or other high performance disks may provide acceptable performance levels for multiple mongod instances. Additionally, you may find that multiple databases with small working sets may function acceptably on a single system. \nDIAGNOSTIC CONFIGURATIONS \nThe following configuration options control various mongod behaviors for diagnostic purposes:  * operationProfiling.mode sets the database profiler level. The profiler is not active by default because of the possible impact on the profiler itself on performance. Unless this setting is on, queries are not profiled.  * operationProfiling.slowOpThresholdMs configures the threshold which determines whether a query is \"slow\" for the purpose of the logging system and the profiler. The default value is 100 milliseconds. Set to a lower value if the logging system and the database profiler do not return useful results or set to a higher value to only log the longest running queries.\n   \n   \n   \n   Starting in version 4.2, secondary members of a replica set now log oplog entries that take longer than the slow operation threshold to apply. These slow oplog messages:\n   \n   * Are logged for the secondaries in the diagnostic log.\n   \n   * Are logged under the REPL component with the text applied op: <oplog entry> took <num>ms.\n   \n   * Do not depend on the log levels (either at the system or component level)\n   \n   * Do not depend on the profiling level.\n   \n   * May be affected by slowOpSampleRate, depending on your MongoDB version:\n     \n     * In MongoDB 4.2, these slow oplog entries are not affected by the slowOpSampleRate. MongoDB logs all slow oplog entries regardless of the sample rate.\n     \n     * In MongoDB 4.4 and later, these slow oplog entries are affected by the slowOpSampleRate.\n   \n   The profiler does not capture slow oplog entries.  * systemLog.verbosity controls the amount of logging output that mongod write to the log. Only use this option if you are experiencing an issue that is not reflected in the normal logging level.\n   \n   Starting in MongoDB 3.0, you can also specify verbosity level for specific components using the systemLog.component.<name>.verbosity setting. For the available components, see component verbosity settings. For more information, see also Database Profiling and MongoDB Performance. ←  Configuration and MaintenanceUpgrade to the Latest Revision of MongoDB → On this page  * Configure the Database\n * Security Considerations\n * Replication and Sharding Configuration\n * Run Multiple Database Instances on the Same System\n * Diagnostic Configurations Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/2dsphere/": " Docs Home → MongoDB Manual \n2DSPHERE INDEXES \nOn this page    \n * Overview\n   \n * Versions\n * Considerations\n * Create a 2dsphere Index \nOVERVIEW \nA 2dsphere index supports queries that calculate geometries on an earth-like sphere. 2dsphere index supports all MongoDB geospatial queries: queries for inclusion, intersection and proximity. For more information on geospatial queries, see Geospatial Queries. The 2dsphere index supports data stored as GeoJSON objects and legacy coordinate pairs (See also 2dsphere Indexed Field Restrictions). For legacy coordinate pairs, the index converts the data to GeoJSON Point. \nVERSIONS \n2dsphere Index Version\nDescription\nVersion 3\nMongoDB 3.2 introduces a version 3 of 2dsphere indexes. Version 3 is the default version of 2dsphere indexes created in MongoDB 3.2 and later.\nVersion 2\nMongoDB 2.6 introduces a version 2 of 2dsphere indexes. Version 2 is the default version of 2dsphere indexes created in MongoDB 2.6 and 3.0 series. To override the default version and specify a different version, include the option { \"2dsphereIndexVersion\": <version> } when creating the index. \nSPARSE PROPERTY \nVersion 2 and later 2dsphere indexes are always sparse and ignore the sparse option. If a document lacks a 2dsphere index field (or the field is null or an empty array), MongoDB does not add an entry for the document to the index. For inserts, MongoDB inserts the document but does not add to the 2dsphere index. For a compound index that includes a 2dsphere index key along with keys of other types, only the 2dsphere index field determines whether the index references a document. Earlier versions of MongoDB only support 2dsphere (Version 1) indexes. 2dsphere (Version 1) indexes are not sparse by default and will reject documents with null location fields. \nADDITIONAL GEOJSON OBJECTS \nVersion 2 and later 2dsphere indexes includes support for additional GeoJSON object: MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection. For details on all supported GeoJSON objects, see GeoJSON Objects. \nCONSIDERATIONS  GEONEAR AND $GEONEAR RESTRICTIONS \nStarting in MongoDB 4.0, you can specify a key option to the $geoNear pipeline stage to indicate the indexed field path to use. This allows the $geoNear stage to be used on a collection that has multiple 2dsphere index and/or multiple 2d index:      * If your collection has multiple 2dsphere index and/or multiple 2d index, you must use the key option to specify the indexed field path to use.  * If you do not specify the key, you cannot have multiple 2dsphere index and/or multiple 2d index since without the key, index selection among multiple 2d indexes or 2dsphere indexes is ambiguous. \nNOTE If you do not specify the key, and you have at most only one 2dsphere index and/or only one 2d index, MongoDB looks first for a 2d index to use. If a 2d index does not exists, then MongoDB looks for a 2dsphere index to use. \nSHARD KEY RESTRICTIONS \nYou cannot use a 2dsphere index as a shard key when sharding a collection. However, you can create a geospatial index on a sharded collection by using a different field as the shard key. \n2DSPHERE INDEXED FIELD RESTRICTIONS \nFields with 2dsphere indexes must hold geometry data in the form of coordinate pairs or GeoJSON data. If you attempt to insert a document with non-geometry data in a 2dsphere indexed field, or build a 2dsphere index on a collection where the indexed field has non-geometry data, the operation will fail. \nLIMITED NUMBER OF INDEX KEYS \nTo generate keys for a 2dsphere index, mongod maps GeoJSON shapes to an internal representation. The resulting internal representation may be a large array of values. When mongod generates index keys on a field that holds an array, mongod generates an index key for each array element. For compound indexes, mongod calculates the cartesian product of the sets of keys that are generated for each field. If both sets are large, then calculating the cartesian product could cause the operation to exceed memory limits.\n\n \nCREATE A 2DSPHERE INDEX \nTo create a 2dsphere index, use the db.collection.createIndex() method and specify the string literal \"2dsphere\" as the index type: db.collection.createIndex( { <location field> : \"2dsphere\" } )  where the <location field> is a field whose value is either a GeoJSON object or a legacy coordinates pair. Unlike a compound 2d index which can reference one location field and one other field, a compound 2dsphere index can reference multiple location and non-location fields. For the following examples, consider a collection places with documents that store location data as GeoJSON Point in a field named loc: db.places.insertMany( [   {      loc : { type: \"Point\", coordinates: [ -73.97, 40.77 ] },      name: \"Central Park\",      category : \"Parks\"   },   {      loc : { type: \"Point\", coordinates: [ -73.88, 40.78 ] },      name: \"La Guardia Airport\",      category : \"Airport\"   }] )  \nCREATE A 2DSPHERE INDEX \nThe following operation creates a 2dsphere index on the location field loc: db.places.createIndex( { loc : \"2dsphere\" } )  \nCREATE A COMPOUND INDEX WITH 2DSPHERE INDEX KEY \nA compound index can include a 2dsphere index key in combination with non-geospatial index keys. For example, the following operation creates a compound index where the first key loc is a 2dsphere index key, and the remaining keys category and names are non-geospatial index keys, specifically descending (-1) and ascending (1) keys respectively. db.places.createIndex( { loc : \"2dsphere\" , category : -1, name: 1 } )  Unlike the 2d index, a compound 2dsphere index does not require the location field to be the first field indexed. For example: db.places.createIndex( { category : 1 , loc : \"2dsphere\" } ) \n←  Wildcard Index RestrictionsQuery a 2dsphere Index → On this page  * Overview\n * Versions\n * Considerations\n * Create a 2dsphere Index Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/text-search-operators/": " Docs Home → MongoDB Manual \nTEXT SEARCH OPERATORS (SELF-MANAGED DEPLOYMENTS) \nOn this page    \n * Query Framework\n   \n * Aggregation Pipeline \nNOTE This page describes text search capabilities for self-managed (non-Atlas) deployments. For data hosted on MongoDB Atlas, MongoDB offers an improved full-text search solution, Atlas Search. \nQUERY FRAMEWORK \nUse the $text query operator to perform text searches on a collection with a text index. $text will tokenize the search string using whitespace and most punctuation as delimiters, and perform a logical OR of all such tokens in the search string. For example, you could use the following query to find all stores containing any terms from the list \"coffee\", \"shop\", and \"java\" in the stores collection: db.stores.find( { $text: { $search: \"java coffee shop\" } } )  Use the $meta query operator to obtain and sort by the relevance score of each matching document. For example, to order a list of coffee shops in order of relevance, run the following: db.stores.find(   { $text: { $search: \"coffee shop cake\" } },   { score: { $meta: \"textScore\" } }).sort( { score: { $meta: \"textScore\" } } )  For more information on the $text and $meta operators, including restrictions and behavior, see:      * $text Reference Page  * $text Query Examples  * $meta projection operator \nAGGREGATION PIPELINE \nWhen working with Aggregation Operations pipelines, use $match with a $text expression to execute a text search query. To sort the results in order of relevance score, use the $meta aggregation operator in the $sort stage [1]. For more information and examples of text search in Aggregation Operations pipelines, see Text Search in the Aggregation Pipeline. For data hosted on MongoDB Atlas, Atlas Search provides the $search aggregation stage to perform full-text search on your collections. [1] The behavior and requirements of the $meta projection operator differ from that of the $meta aggregation operator. For details on the $meta aggregation operator, see the $meta aggregation operator reference page. ←  Perform a Text Search (Self-Managed Deployments)Text Search in the Aggregation Pipeline → On this page  * Query Framework\n * Aggregation Pipeline Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/materialized-views/": " Docs Home → MongoDB Manual \nON-DEMAND MATERIALIZED VIEWS \nOn this page    \n * Comparison with Standard Views\n   \n * Indexes\n * Performance\n * Example\n * 1. Define the On-Demand Materialized View\n * 2. Perform Initial Run\n * 3. Refresh Materialized View\n * Additional Information \nNOTE \nDISAMBIGUATION This page discusses on-demand materialized views. For discussion of standard views, see Views. To understand the differences between the view types, see Comparison with Standard Views. An on-demand materialized view is a pre-computed aggregation pipeline result that is stored on and read from disk. On-demand materialized views are typically the results of a $merge or $out stage. \nCOMPARISON WITH STANDARD VIEWS \nMongoDB provides two different view types: standard views and on-demand materialized views. Both view types return the results from an aggregation pipeline.      * Standard views are computed when you read the view, and are not stored to disk.  * On-demand materialized views are stored on and read from disk. They use a $merge or $out stage to update the saved data. \nINDEXES \nStandard views use the indexes of the underlying collection. As a result, you cannot create, drop or re-build indexes on a standard view directly, nor get a list of indexes on the view. You can create indexes directly on on-demand materialized views because they are stored on disk. \nPERFORMANCE \nOn-demand materialized views provide better read performance than standard views because they are read from disk instead of computed as part of the query. This performance benefit increases based on the complexity of the pipeline and size of the data being aggregated. \nEXAMPLE \nAssume near the end of January 2019, the collection bakesales contains the sales information by items:\n\n  \n1. DEFINE THE ON-DEMAND MATERIALIZED VIEW \nThe following updateMonthlySales function defines a monthlybakesales materialized view that contains the cumulative monthly sales information. In the example, the function takes a date parameter to only update monthly sales information starting from a particular date. updateMonthlySales = function(startDate) {   db.bakesales.aggregate( [      { $match: { date: { $gte: startDate } } },      { $group: { _id: { $dateToString: { format: \"%Y-%m\", date: \"$date\" } }, sales_quantity: { $sum: \"$quantity\"}, sales_amount: { $sum: \"$amount\" } } },      { $merge: { into: \"monthlybakesales\", whenMatched: \"replace\" } }   ] );};   * The $match stage filters the data to process only those sales greater than or equal to the startDate.  * The $group stage groups the sales information by the year-month. The documents output by this stage have the form:\n   \n   { \"_id\" : \"<YYYY-mm>\", \"sales_quantity\" : <num>, \"sales_amount\" : <NumberDecimal> }  * The $merge stage writes the output to the monthlybakesales collection.\n   \n   Based on the _id field (the default for unsharded output collections), the stage checks if the document in the aggregation results matches an existing document in the collection:\n   \n   * When there is a match (i.e. a document with the same year-month already exists in the collection), the stage replaces the existing document with the document from the aggregation results.\n   \n   * When there is not a match, the stage inserts the document from the aggregation results into the collection (the default behavior when not matched). \n2. PERFORM INITIAL RUN \nFor the initial run, you can pass in a date of new ISODate(\"1970-01-01\"): updateMonthlySales(new ISODate(\"1970-01-01\"));  After the initial run, the monthlybakesales contains the following documents; i.e. db.monthlybakesales.find().sort( { _id: 1 } ) returns the following: { \"_id\" : \"2018-12\", \"sales_quantity\" : 41, \"sales_amount\" : NumberDecimal(\"506\") }{ \"_id\" : \"2019-01\", \"sales_quantity\" : 86, \"sales_amount\" : NumberDecimal(\"896\") }  \n3. REFRESH MATERIALIZED VIEW \nAssume that by the first week in February 2019, the bakesales collection is updated with newer sales information; specifically, additional January and February sales.\n\n  To refresh the monthlybakesales data for January and February, run the function again to rerun the aggregation pipeline, starting with new ISODate(\"2019-01-01\"). updateMonthlySales(new ISODate(\"2019-01-01\"));  The content of monthlybakesales has been updated to reflect the most recent data in the bakesales collection; i.e. db.monthlybakesales.find().sort( { _id: 1 } ) returns the following: { \"_id\" : \"2018-12\", \"sales_quantity\" : 41, \"sales_amount\" : NumberDecimal(\"506\") }{ \"_id\" : \"2019-01\", \"sales_quantity\" : 102, \"sales_amount\" : NumberDecimal(\"1142\") }{ \"_id\" : \"2019-02\", \"sales_quantity\" : 15, \"sales_amount\" : NumberDecimal(\"284\") }  \nADDITIONAL INFORMATION \nThe $merge stage:  * Can output to a collection in the same or different database.  * Creates a new collection if the output collection does not already exist.  * Can incorporate results (insert new documents, merge documents, replace documents, keep existing documents, fail the operation, process documents with a custom update pipeline) into an existing collection.  * Can output to a sharded collection. Input collection can also be sharded. See $merge for:  * More information on $merge and available options  * Example: On-Demand Materialized View: Initial Creation  * Example: On-Demand Materialized View: Update/Replace Data  * Example: Only Insert New Data ←  Supported Operations for ViewsCapped Collections → On this page  * Comparison with Standard Views\n * Indexes\n * Performance\n * Example\n * 1. Define the On-Demand Materialized View\n * 2. Perform Initial Run\n * 3. Refresh Materialized View\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/sharding-refine-a-shard-key/": " Docs Home → MongoDB Manual \nREFINE A SHARD KEY \nNew in version 4.4. Refining a collection's shard key allows for a more fine-grained data distribution and can address situations where the existing key has led to jumbo chunks due to insufficient cardinality. \nWARNING Do not modify the range or hashed type for any of the current shard key fields. It causes data inconsistencies. For example, do not modify a shard key from { customer_id: 1 } to { customer_id: \"hashed\", order_id: 1 }. \nNOTE Starting in MongoDB 5.0, you can also reshard your collection by providing a new shard key for the collection. To refine a collection's shard key, use the refineCollectionShardKey command. The refineCollectionShardKey adds a suffix field or fields to the existing key to create the new shard key. For example, you may have an existing orders collection in a test database with the shard key { customer_id: 1 }. You can use the refineCollectionShardKey command to change the shard key to the new shard key { customer_id: 1, order_id: 1 }: db.adminCommand( {   refineCollectionShardKey: \"test.orders\",   key: { customer_id: 1, order_id: 1 }} ) \n←  Change a Shard KeyReshard a Collection → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/workload-isolation/": " Docs Home → MongoDB Manual \nWORKLOAD ISOLATION IN MONGODB DEPLOYMENTS \nOn this page    \n * Operational Overview \nOPERATIONAL OVERVIEW \nMongoDB includes a number of features that allow database administrators and developers to isolate workload by functional or geographical groupings. This capability provides \"data center awareness,\" which allows applications to target MongoDB deployments with consideration of the physical location of the mongod instances. MongoDB supports segmentation of operations across different dimensions, which may include multiple data centers and geographical regions in multi-data center deployments, racks, networks, or power circuits in single data center deployments. MongoDB also supports workload isolation based on functional or operational parameters, to ensure that certain mongod instances are only used for reporting workloads or that certain high-frequency portions of a sharded collection only exist on specific shards. Specifically, with MongoDB, you can:      * ensure write operations propagate to specific members of a replica set, or to specific members of replica sets.  * ensure that specific members of a replica set respond to queries.  * ensure that specific ranges of your shard key balance onto and reside on specific shards.  * combine the above features in a single distributed deployment, on a per-operation (for read and write operations) and collection (for chunk distribution in sharded clusters distribution) basis. For full documentation of these features, see the following documentation in the MongoDB Manual:  * Read Preferences, which controls how drivers help applications target read operations to members of a replica set.  * Write Concerns, which controls how MongoDB ensures that write operations propagate to members of a replica set.  * Replica Set Tags, which control how applications create and interact with custom groupings of replica set members to create custom application-specific read preferences and write concerns.  * Zones in sharded clusters, which allows MongoDB administrators to create zones that represent a group of shards and associate one or more ranges of shard key values to these zones. You can associate each zone with one or more shards in the cluster. A shard can associate with any number of zones. In a balanced cluster, MongoDB directs reads and writes covered by a zone only to the shards inside the zone. \nTIP \nSEE ALSO: Before adding workload isolation features to your application and MongoDB deployment, become familiar with all documentation of replication, and sharding. ←  Data Center AwarenessZones → On this page  * Operational Overview Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/replica-set-priority-0-member/": " Docs Home → MongoDB Manual \nPRIORITY 0 REPLICA SET MEMBERS \nOn this page    \n * Priority 0 Members as Standbys\n   \n * Failover Considerations\n * Example A priority 0 member is a member that cannot become primary and cannot trigger elections. Priority 0 members can acknowledge write operations issued with write concern of w : <number>. For \"majority\" write concern, the priority 0 member must also be a voting member (i.e. members[n].votes is greater than 0) to acknowledge the write. Non-voting replica set members (i.e. members[n].votes is 0) cannot contribute to acknowledging write operations with \"majority\" write concern. Other than the aforementioned restrictions, secondaries that have priority 0 function as normal secondaries: they maintain a copy of the data set, accept read operations, and vote in elections. Configuring a replica set member with priority 0 might be desired if the particular member is deployed in a data center that is distant from the main deployment and therefore has higher latency. It may serve local read requests well, but might not be an ideal candidate to perform the duties of a primary due to its latency. For this situation, the following diagram shows a data center on the left which hosts the primary and a secondary, and a data center on the right which hosts a secondary that has been configured to have priority 0 to prevent it from becoming primary. Because of this setting, only the members in the left data center are eligible to become primary in an election.  Compare this to the default priority for replica set members, priority 1, where either of the secondaries in this scenario would be eligible to serve as primary. See Replica Sets Distributed Across Two or More Data Centers for more information. \nPRIORITY 0 MEMBERS AS STANDBYS \nA secondary with priority 0 can function as a standby. In some replica sets, it might not be possible to add a new member in a reasonable amount of time. A standby member keeps a current copy of the data to be able to replace an unavailable member. In many cases, you need not set standby to priority 0. However, in replica sets with varied hardware or geographic distribution, a priority 0 standby ensures that only certain members become primary. A priority 0 standby may also be valuable for some members of a set with different hardware or workload profiles. In these cases, deploy a member with priority 0 so it can't become primary. Also consider using an hidden member for this purpose. If your set already has seven voting members, also configure the member as non-voting. \nFAILOVER CONSIDERATIONS \nWhen configuring a secondary to have priority 0, consider potential failover patterns, including all possible network partitions. Always ensure that your main data center contains both a quorum of voting members and members that are eligible to be primary. \nEXAMPLE \nTo configure a secondary to have priority 0, see Prevent Secondary from Becoming Primary. ←  Replica Set Secondary MembersHidden Replica Set Members → On this page  * Priority 0 Members as Standbys\n * Failover Considerations\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/sharding-data-partitioning/": " Docs Home → MongoDB Manual \nDATA PARTITIONING WITH CHUNKS \nOn this page    \n * Initial Chunks\n   \n * Range Size\n * Range Migration\n * Indivisible/Jumbo Chunks\n * moveChunk directory MongoDB uses the shard key associated to the collection to partition the data into chunks owned by a specific shard. A chunk consists of a range of sharded data. A range can be a portion of the chunk or the whole chunk. The balancer migrates data between shards. Each chunk has inclusive lower and exclusive upper limits based on the shard key.  The smallest unit of data a chunk can represent is a single unique shard key value. \nINITIAL CHUNKS  POPULATED COLLECTION \n     * The sharding operation creates one large initial chunk to cover all of the shard key values.  * After the initial chunk creation, the balancer moves ranges off of the initial chunk when it needs to start balancing data. \nEMPTY COLLECTION \n * If you have zones and zone ranges defined for an empty or non-existing collection.\n   \n   * The sharding operation creates empty chunks for the defined zone ranges as well as any additional chunks to cover the entire range of the shard key values and performs an initial chunk distribution based on the zone ranges. This initial creation and distribution of chunks allows for faster setup of zoned sharding.\n   \n   * After the initial distribution, the balancer manages the chunk distribution going forward.  * If you do not have zones and zone ranges defined for an empty or non-existing collection:\n   \n   * For hashed sharding:\n     \n     * The sharding operation creates empty chunks to cover the entire range of the shard key values and performs an initial chunk distribution. By default, the operation creates 2 chunks per shard and migrates across the cluster. You can use numInitialChunks option to specify a different number of initial chunks. This initial creation and distribution of chunks allows for faster setup of sharding.\n     \n     * After the initial distribution, the balancer manages the chunk distribution going forward.\n   \n   * For ranged sharding:\n     \n     * The sharding operation creates a single empty chunk to cover the entire range of the shard key values.\n     \n     * After the initial chunk creation, the balancer migrates the initial chunk across the shards as appropriate as well as manages the chunk distribution going forward. \nTIP \nSEE ALSO: sh.balancerCollectionStatus() \nRANGE SIZE \nThe default range size in MongoDB is 128 megabytes. You can increase or reduce the chunk size. Consider the implications of changing the default chunk size:  1. Small ranges lead to a more even distribution of data at the expense of more frequent migrations. This creates expense at the query routing (mongos) layer.  2. Large ranges lead to fewer migrations. This is more efficient both from the networking perspective and in terms of internal overhead at the query routing layer. But, these efficiencies come at the expense of a potentially uneven distribution of data.  3. Range size affects the Maximum Number of Documents Per Range to Migrate.  4. Range size affects the maximum collection size when sharding an existing collection. Post-sharding, data range size does not constrain collection size. For many deployments, it makes sense to avoid frequent and potentially spurious migrations at the expense of a slightly less evenly distributed data set. \nRANGE MIGRATION \nMongoDB migrates data ranges in a sharded cluster to distribute the data of a sharded collection evenly among shards. Migrations may be either:  * Manual. Only use manual migration in limited cases, such as to distribute data during bulk inserts. See Migrating Chunks Manually for more details.  * Automatic. The balancer process automatically migrates data when there is an uneven distribution of a sharded collection's data across the shards. See Migration Thresholds for more details. For more information on the sharded cluster balancer, see Sharded Cluster Balancer. \nTIP \nSEE ALSO: shardingStatistics.countDonorMoveChunkLockTimeout \nBALANCING \nThe balancer is a background process that manages data migrations. If the difference in amount of data between the largest and smallest shard exceed the migration thresholds, the balancer begins migrating data across the cluster to ensure an even distribution.  You can manage certain aspects of the balancer. The balancer also respects any zones created as a part of configuring zones in a sharded cluster.\n\n \nINDIVISIBLE/JUMBO CHUNKS \nIn some cases, chunks can grow beyond the specified chunk size but cannot undergo a split. The most common scenario is when a chunk represents a single shard key value. Since the chunk cannot split, it continues to grow beyond the chunk size, becoming a jumbo chunk. These jumbo chunks can become a performance bottleneck as they continue to grow, especially if the shard key value occurs with high frequency. Starting in MongoDB 5.0, you can reshard a collection by changing a document's shard key. Starting in MongoDB 4.4, MongoDB provides the refineCollectionShardKey command. Refining a collection's shard key allows for a more fine-grained data distribution and can address situations where the existing key insufficient cardinality leads to jumbo chunks. For more information, see:  * Change a Shard Key  * Clear jumbo Flag  * Maximum Number of Documents Per Range to Migrate \nMOVECHUNK DIRECTORY \nBy default, sharding.archiveMovedChunks is disabled. ←  Distribute Collections Using ZonesCreate Ranges in a Sharded Cluster → On this page  * Initial Chunks\n * Range Size\n * Range Migration\n * Indivisible/Jumbo Chunks\n * moveChunk directory Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/document/": " Docs Home → MongoDB Manual \nDOCUMENTS \nOn this page    \n * Document Structure\n   \n * Dot Notation\n * Document Limitations\n * Other Uses of the Document Structure\n * Further Reading MongoDB stores data records as BSON documents. BSON is a binary representation of JSON documents, though it contains more data types than JSON. For the BSON spec, see bsonspec.org. See also BSON Types.  \nDOCUMENT STRUCTURE \nMongoDB documents are composed of field-and-value pairs and have the following structure: {   field1: value1,   field2: value2,   field3: value3,   ...   fieldN: valueN}  The value of a field can be any of the BSON data types, including other documents, arrays, and arrays of documents. For example, the following document contains values of varying types: var mydoc = {               _id: ObjectId(\"5099803df3f4948bd2f98391\"),               name: { first: \"Alan\", last: \"Turing\" },               birth: new Date('Jun 23, 1912'),               death: new Date('Jun 07, 1954'),               contribs: [ \"Turing machine\", \"Turing test\", \"Turingery\" ],               views : NumberLong(1250000)            }  The above fields have the following data types:      * _id holds an ObjectId.  * name holds an embedded document that contains the fields first and last.  * birth and death hold values of the Date type.  * contribs holds an array of strings.  * views holds a value of the NumberLong type. \nFIELD NAMES \nField names are strings. Documents have the following restrictions on field names:  * The field name _id is reserved for use as a primary key; its value must be unique in the collection, is immutable, and may be of any type other than an array. If the _id contains subfields, the subfield names cannot begin with a ($) symbol.  * Field names cannot contain the null character.  * The server permits storage of field names that contain dots (.) and dollar signs ($).  * MongodB 5.0 adds improved support for the use of ($) and (.) in field names. There are some restrictions. See Field Name Considerations for more details. BSON documents may have more than one field with the same name. Most MongoDB interfaces, however, represent MongoDB with a structure (e.g. a hash table) that does not support duplicate field names. If you need to manipulate documents that have more than one field with the same name, see the driver documentation for your driver. Some documents created by internal MongoDB processes may have duplicate fields, but no MongoDB process will ever add duplicate fields to an existing user document. \nFIELD VALUE LIMIT \nMongoDB 2.6 through MongoDB versions with featureCompatibilityVersion (fCV) set to \"4.0\" or earlierFor indexed collections, the values for the indexed fields have a Maximum Index Key Length. See Maximum Index Key Length for details. \nDOT NOTATION \nMongoDB uses the dot notation to access the elements of an array and to access the fields of an embedded document. \nARRAYS \nTo specify or access an element of an array by the zero-based index position, concatenate the array name with the dot (.) and zero-based index position, and enclose in quotes: \"<array>.<index>\"  For example, given the following field in a document: {   ...   contribs: [ \"Turing machine\", \"Turing test\", \"Turingery\" ],   ...}  To specify the third element in the contribs array, use the dot notation \"contribs.2\". For examples querying arrays, see:  * Query an Array  * Query an Array of Embedded Documents \nTIP \nSEE ALSO:  * $[] all positional operator for update operations,  * $[<identifier>] filtered positional operator for update operations,  * $ positional operator for update operations,  * $ projection operator when array index position is unknown  * Query an Array for dot notation examples with arrays. \nEMBEDDED DOCUMENTS\n\n \"<embedded document>.<field>\"  For example, given the following field in a document: {   ...   name: { first: \"Alan\", last: \"Turing\" },   contact: { phone: { type: \"cell\", number: \"111-222-3333\" } },   ...}   * To specify the field named last in the name field, use the dot notation \"name.last\".  * To specify the number in the phone document in the contact field, use the dot notation \"contact.phone.number\". For examples querying embedded documents, see:  * Query on Embedded/Nested Documents  * Query an Array of Embedded Documents \nDOCUMENT LIMITATIONS \nDocuments have the following attributes: \nDOCUMENT SIZE LIMIT \nThe maximum BSON document size is 16 megabytes. The maximum document size helps ensure that a single document cannot use excessive amount of RAM or, during transmission, excessive amount of bandwidth. To store documents larger than the maximum size, MongoDB provides the GridFS API. See mongofiles and the documentation for your driver for more information about GridFS. \nDOCUMENT FIELD ORDER \nUnlike JavaScript objects, the fields in a BSON document are ordered. FIELD ORDER IN QUERIES \nFor queries, the field order behavior is as follows:  * When comparing documents, field ordering is significant. For example, when comparing documents with fields a and b in a query:\n   \n   * {a: 1, b: 1} is equal to {a: 1, b: 1}\n   \n   * {a: 1, b: 1} is not equal to {b: 1, a: 1}  * For efficient query execution, the query engine may reorder fields during query processing. Among other cases, reordering fields may occur when processing these projection operators: $project, $addFields, $set, and $unset.\n   \n   * Field reordering may occur in intermediate results as well as the final results returned by a query.\n   \n   * Because some operations may reorder fields, you should not rely on specific field ordering in the results returned by a query that uses the projection operators listed earlier. FIELD ORDER IN WRITE OPERATIONS \nFor write operations, MongoDB preserves the order of the document fields except for the following cases:  * The _id field is always the first field in the document.  * Updates that include renaming of field names may result in the reordering of fields in the document. \nTHE _ID FIELD \nIn MongoDB, each document stored in a collection requires a unique _id field that acts as a primary key. If an inserted document omits the _id field, the MongoDB driver automatically generates an ObjectId for the _id field. This also applies to documents inserted through update operations with upsert: true. The _id field has the following behavior and constraints:  * By default, MongoDB creates a unique index on the _id field during the creation of a collection.  * The _id field is always the first field in the documents. If the server receives a document that does not have the _id field first, then the server will move the field to the beginning. _ If the _id contains subfields, the subfield names cannot beginwith a ($) symbol.  * The _id field may contain values of any BSON data type, other than an array, regex, or undefined.\n   \n   \n   WARNING\n   \n   To ensure functioning replication, do not store values that are of the BSON regular expression type in the _id field. The following are common options for storing values for _id:  * Use an ObjectId.  * Use a natural unique identifier, if available. This saves space and avoids an additional index.  * Generate an auto-incrementing number.  * Generate a UUID in your application code. For a more efficient storage of the UUID values in the collection and in the _id index, store the UUID as a value of the BSON BinData type.\n   \n   Index keys that are of the BinData type are more efficiently stored in the index if:\n   \n   * the binary subtype value is in the range of 0-7 or 128-135, and\n   \n   * the length of the byte array is: 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 20, 24, or 32.\n\n \nNOTE Most MongoDB driver clients will include the _id field and generate an ObjectId before sending the insert operation to MongoDB; however, if the client sends a document without an _id field, the mongod will add the _id field and generate the ObjectId. \nOTHER USES OF THE DOCUMENT STRUCTURE \nIn addition to defining data records, MongoDB uses the document structure throughout, including but not limited to: query filters, update specifications documents, and index specification documents \nQUERY FILTER DOCUMENTS \nQuery filter documents specify the conditions that determine which records to select for read, update, and delete operations. You can use <field>:<value> expressions to specify the equality condition and query operator expressions. {  <field1>: <value1>,  <field2>: { <operator>: <value> },  ...}  For examples, see:  * Query Documents  * Query on Embedded/Nested Documents  * Query an Array  * Query an Array of Embedded Documents \nUPDATE SPECIFICATION DOCUMENTS \nUpdate specification documents use update operators to specify the data modifications to perform on specific fields during an update operation. {  <operator1>: { <field1>: <value1>, ... },  <operator2>: { <field2>: <value2>, ... },  ...}  For examples, see Update specifications. \nINDEX SPECIFICATION DOCUMENTS \nIndex specification documents define the field to index and the index type: { <field1>: <type1>, <field2>: <type2>, ...  }  \nFURTHER READING \nFor more information on the MongoDB document model, download the MongoDB Application Modernization Guide. The download includes the following resources:  * Presentation on the methodology of data modeling with MongoDB  * White paper covering best practices and considerations for migrating to MongoDB from an RDBMS data model  * Reference MongoDB schema with its RDBMS equivalent  * Application Modernization scorecard ←  Clustered CollectionsMongoDB Query API → On this page  * Document Structure\n * Dot Notation\n * Document Limitations\n * Other Uses of the Document Structure\n * Further Reading Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/replica-set-primary/": " Docs Home → MongoDB Manual \nREPLICA SET PRIMARY \nThe primary is the only member in the replica set that receives write operations. MongoDB applies write operations on the primary and then records the operations on the primary's oplog. Secondary members replicate this log and apply the operations to their data sets. In the following three-member replica set, the primary accepts all write operations. Then the secondaries replicate the oplog to apply to their data sets.  All members of the replica set can accept read operations. However, by default, an application directs its read operations to the primary member. See Read Preference for details on changing the default read behavior. The replica set can have at most one primary. [1] If the current primary becomes unavailable, an election determines the new primary. See Replica Set Elections for more details. In the following 3-member replica set, the primary becomes unavailable. This triggers an election which selects one of the remaining secondaries as the new primary.  [1] In some circumstances, two nodes in a replica set may transiently believe that they are the primary, but at most, one of them will be able to complete writes with { w: \"majority\" } write concern. The node that can complete { w: \"majority\" } writes is the current primary, and the other node is a former primary that has not yet recognized its demotion, typically due to a network partition. When this occurs, clients that connect to the former primary may observe stale data despite having requested read preference primary, and new writes to the former primary will eventually roll back. ←  Replica Set MembersReplica Set Secondary Members → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/journaling/": " Docs Home → MongoDB Manual \nJOURNALING \nOn this page    \n * Journaling and the WiredTiger Storage Engine\n   \n * Journaling and the In-Memory Storage Engine To provide durability in the event of a failure, MongoDB uses write ahead logging to on-disk journal files. \nJOURNALING AND THE WIREDTIGER STORAGE ENGINE  IMPORTANT The log mentioned in this section refers to the WiredTiger write-ahead log (i.e. the journal) and not the MongoDB log file. WiredTiger uses checkpoints to provide a consistent view of data on disk and allow MongoDB to recover from the last checkpoint. However, if MongoDB exits unexpectedly in between checkpoints, journaling is required to recover information that occurred after the last checkpoint. \nNOTE Starting in MongoDB 4.0, you cannot specify --nojournal option or storage.journal.enabled: false for replica set members that use the WiredTiger storage engine. With journaling, the recovery process:       1. Looks in the data files to find the identifier of the last checkpoint.  2. Searches in the journal files for the record that matches the identifier of the last checkpoint.  3. Apply the operations in the journal files since the last checkpoint. \nJOURNALING PROCESS \nChanged in version 3.2. With journaling, WiredTiger creates one journal record for each client initiated write operation. The journal record includes any internal write operations caused by the initial write. For example, an update to a document in a collection may result in modifications to the indexes; WiredTiger creates a single journal record that includes both the update operation and its associated index modifications. MongoDB configures WiredTiger to use in-memory buffering for storing the journal records. Threads coordinate to allocate and copy into their portion of the buffer. All journal records up to 128 kB are buffered. WiredTiger syncs the buffered journal records to disk upon any of the following conditions:  * For replica set members (primary and secondary members),\n   \n   * If there are operations waiting for oplog entries. Operations that can wait for oplog entries include:\n     \n     * forward scanning queries against the oplog\n     \n     * read operations performed as part of causally consistent sessions\n   \n   * Additionally for secondary members, after every batch application of the oplog entries.  * If a write operation includes or implies a write concern of j: true.\n   \n   \n   NOTE\n   \n   Write concern \"majority\" implies j: true if the writeConcernMajorityJournalDefault is true.  * At every 100 milliseconds (See storage.journal.commitIntervalMs).  * When WiredTiger creates a new journal file. Because MongoDB uses a journal file size limit of 100 MB, WiredTiger creates a new journal file approximately every 100 MB of data. \nIMPORTANT In between write operations, while the journal records remain in the WiredTiger buffers, updates can be lost following a hard shutdown of mongod. \nTIP \nSEE ALSO: The serverStatus command returns information on the WiredTiger journal statistics in the wiredTiger.log field. \nJOURNAL FILES \nFor the journal files, MongoDB creates a subdirectory named journal under the dbPath directory. WiredTiger journal files have names with the following format WiredTigerLog.<sequence> where <sequence> is a zero-padded number starting from 0000000001. JOURNAL RECORDS \nJournal files contain a record per each client initiated write operation  * The journal record includes any internal write operations caused by the initial write. For example, an update to a document in a collection may result in modifications to the indexes; WiredTiger creates a single journal record that includes both the update operation and its associated index modifications.  * Each record has a unique identifier.  * The minimum journal record size for WiredTiger is 128 bytes. COMPRESSION \nBy default, MongoDB configures WiredTiger to use snappy compression for its journaling data. To specify a different compression algorithm or no compression, use the storage.wiredTiger.engineConfig.journalCompressor setting. For details, see Change WiredTiger Journal Compressor.s \nNOTE If a log record less than or equal to 128 bytes (the mininum log record size for WiredTiger), WiredTiger does not compress that record. JOURNAL FILE SIZE LIMIT \nWiredTiger journal files for MongoDB have a maximum size limit of approximately 100 MB.\n\n  * WiredTiger automatically removes old journal files to maintain only the files needed to recover from last checkpoint. PRE-ALLOCATION \nWiredTiger pre-allocates journal files. \nJOURNALING AND THE IN-MEMORY STORAGE ENGINE \nStarting in MongoDB Enterprise version 3.2.6, the In-Memory Storage Engine is part of general availability (GA). Because its data is kept in memory, there is no separate journal. Write operations with a write concern of j: true are immediately acknowledged. If any voting member of a replica set uses the in-memory storage engine, you must set writeConcernMajorityJournalDefault to false. \nNOTE Starting in version 4.2 (and 4.0.13 and 3.6.14 ), if a replica set member uses the in-memory storage engine (voting or non-voting) but the replica set has writeConcernMajorityJournalDefault set to true, the replica set member logs a startup warning. With writeConcernMajorityJournalDefault set to false, MongoDB does not wait for w: \"majority\" writes to be written to the on-disk journal before acknowledging the writes. As such, \"majority\" write operations could possibly roll back in the event of a transient loss (e.g. crash and restart) of a majority of nodes in a given replica set. \nTIP \nSEE ALSO: In-Memory Storage Engine: Durability ←  In-Memory Storage EngineManage Journaling → On this page  * Journaling and the WiredTiger Storage Engine\n * Journaling and the In-Memory Storage Engine Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/replica-set-architecture-geographically-distributed/": " Docs Home → MongoDB Manual \nREPLICA SETS DISTRIBUTED ACROSS TWO OR MORE DATA CENTERS \nOn this page    \n * Overview\n   \n * Distribution of the Members\n * Electability of Members\n * Connectivity \nOVERVIEW \nWhile replica sets provide basic protection against single-instance failure, replica sets whose members are all located in a single data center are susceptible to data center failures. Power outages, network interruptions, and natural disasters are all issues that can affect replica sets whose members are located in a single facility. Distributing replica set members across geographically distinct data centers adds redundancy and provides fault tolerance if one of the data centers is unavailable. \nDISTRIBUTION OF THE MEMBERS \nTo protect your data in case of a data center failure, keep at least one member in an alternate data center. If possible, use an odd number of data centers, and choose a distribution of members that maximizes the likelihood that even with a loss of a data center, the remaining replica set members can form a majority or at minimum, provide a copy of your data. \nEXAMPLES \nTHREE-MEMBER REPLICA SET \nFor example, for a three-member replica set, some possible distributions of members include:      * Two data centers: two members to Data Center 1 and one member to Data Center 2. If one of the members of the replica set is an arbiter, distribute the arbiter to Data Center 1 with a data-bearing member.\n   \n   * If Data Center 1 goes down, the replica set becomes read-only.\n   \n   * If Data Center 2 goes down, the replica set remains writable as the members in Data Center 1 can hold an election.  * Three data centers: one member to Data Center 1, one member to Data Center 2, and one member to Data Center 3.\n   \n   * If any Data Center goes down, the replica set remains writable as the remaining members can hold an election. \nNOTE Distributing replica set members across two data centers provides benefit over a single data center. In a two data center distribution,  * If one of the data centers goes down, the data is still available for reads unlike a single data center distribution.  * If the data center with a minority of the members goes down, the replica set can still serve write operations as well as read operations.  * However, if the data center with the majority of the members goes down, the replica set becomes read-only. If possible, distribute members across at least three data centers. For config server replica sets (CSRS), the best practice is to distribute across three (or more depending on the number of members) centers. If the cost of the third data center is prohibitive, one distribution possibility is to evenly distribute the data bearing members across the two data centers and store the remaining member in the cloud if your company policy allows. FIVE-MEMBER REPLICA SET \nFor a replica set with 5 members, some possible distributions of members include:  * Two data centers: three members to Data Center 1 and two members to Data Center 2.\n   \n   * If Data Center 1 goes down, the replica set becomes read-only.\n   \n   * If Data Center 2 goes down, the replica set remains writeable as the members in Data Center 1 can create a majority.  * Three data centers: two member to Data Center 1, two members to Data Center 2, and one member to site Data Center 3.\n   \n   * If any Data Center goes down, the replica set remains writeable as the remaining members can hold an election. \nNOTE Distributing replica set members across two data centers provides benefit over a single data center. In a two data center distribution,  * If one of the data centers goes down, the data is still available for reads unlike a single data center distribution.  * If the data center with a minority of the members goes down, the replica set can still serve write operations as well as read operations.  * However, if the data center with the majority of the members goes down, the replica set becomes read-only. If possible, distribute members across at least three data centers. For config server replica sets (CSRS), the best practice is to distribute across three (or more depending on the number of members) centers. If the cost of the third data center is prohibitive, one distribution possibility is to evenly distribute the data bearing members across the two data centers and store the remaining member in the cloud if your company policy allows. For example, the following 5 member replica set distributes its members across three data centers.  \nELECTABILITY OF MEMBERS\n\n In some cases, you may prefer that the members in one data center be elected primary before the members in the other data centers. You can modify the priority of the members such that the members in the one data center has higher priority than the members in the other data centers. In the following example, the replica set members in Data Center 1 have a higher priority than the members in Data Center 2 and 3; the members in Data Center 2 have a higher priority than the member in Data Center 3:  \nCONNECTIVITY \nVerify that your network configuration allows communication among all members; i.e. each member must be able to connect to every other member. \nTIP \nSEE ALSO:  * Deploy a Geographically Redundant Replica Set  * Deploy a Replica Set  * Add an Arbiter to Replica Set  * Add Members to a Replica Set ←  Three Member Replica SetsReplica Set High Availability → On this page  * Overview\n * Distribution of the Members\n * Electability of Members\n * Connectivity Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/hashed-sharding/": " Docs Home → MongoDB Manual \nHASHED SHARDING \nHashed sharding uses either a single field hashed index or a compound hashed index (New in 4.4) as the shard key to partition data across your sharded cluster. Sharding on a Single Field Hashed Index Hashed sharding provides a more even data distribution across the sharded cluster at the cost of reducing Targeted Operations vs. Broadcast Operations. Post-hash, documents with \"close\" shard key values are unlikely to be on the same chunk or shard - the mongos is more likely to perform Broadcast Operations to fulfill a given ranged query. mongos can target queries with equality matches to a single shard.  Hashed indexes compute the hash value of a single field as the index value; this value is used as your shard key. [1] Sharding on a Compound Hashed Index MongoDB 4.4 adds support for creating compound indexes with a single hashed field. To create a compound hashed index, specify hashed as the value of any single index key when creating the index. Compound hashed index compute the hash value of a single field in the compound index; this value is used along with the other fields in the index as your shard key. Compound hashed sharding supports features like zone sharding, where the prefix (i.e. first) non-hashed field or fields support zone ranges while the hashed field supports more even distribution of the sharded data. Compound hashed sharding also supports shard keys with a hashed prefix for resolving data distribution issues related to monotonically increasing fields. \nTIP MongoDB automatically computes the hashes when resolving queries using hashed indexes. Applications do not need to compute hashes. \nWARNING MongoDB hashed indexes truncate floating point numbers to 64-bit integers before hashing. For example, a hashed index would store the same value for a field that held a value of 2.3, 2.2, and 2.9. To prevent collisions, do not use a hashed index for floating point numbers that cannot be reliably converted to 64-bit integers (and then back to floating point). MongoDB hashed indexes do not support floating point values larger than 2 53. To see what the hashed value would be for a key, see convertShardKeyToHashed(). [1] Starting in version 4.0, mongosh provides the method convertShardKeyToHashed(). This method uses the same hashing function as the hashed index and can be used to see what the hashed value would be for a key. \nHASHED SHARDING SHARD KEY \nThe field you choose as your hashed shard key should have a good cardinality, or large number of different values. Hashed keys are ideal for shard keys with fields that change monotonically like ObjectId values or timestamps. A good example of this is the default _id field, assuming it only contains ObjectId values. To shard a collection using a hashed shard key, see Shard a Collection. \nHASHED VS RANGED SHARDING \nGiven a collection using a monotonically increasing value X as the shard key, using ranged sharding results in a distribution of incoming inserts similar to the following:  Since the value of X is always increasing, the chunk with an upper bound of maxKey receives the majority incoming writes. This restricts insert operations to the single shard containing this chunk, which reduces or removes the advantage of distributed writes in a sharded cluster. By using a hashed index on X, the distribution of inserts is similar to the following:  Since the data is now distributed more evenly, inserts are efficiently distributed throughout the cluster. \nSHARD THE COLLECTION \nUse the sh.shardCollection() method, specifying the full namespace of the collection and the target hashed index to use as the shard key. sh.shardCollection( \"database.collection\", { <field> : \"hashed\" } )  To shard a collection on a compound hashed index, specify the full namespace of the collection and the target compound hashed index to use as the shard key: sh.shardCollection(  \"database.collection\",  { \"fieldA\" : 1, \"fieldB\" : 1, \"fieldC\" : \"hashed\" })  \nIMPORTANT      * Starting in MongoDB 5.0, you can reshard a collection by changing a collection's shard key.  * Starting in MongoDB 4.4, you can refine a shard key by adding a suffix field or fields to the existing shard key.  * In MongoDB 4.2 and earlier, the choice of shard key cannot be changed after sharding.\n\n \nIf you shard a populated collection using a hashed shard key:  * The sharding operation creates an initial chunk to cover all of the shard key values.  * After the initial chunk creation, the balancer moves ranges of the initial chunk when it needs to balance data. \nSHARD AN EMPTY COLLECTION \nStarting in MongoDB 4.0.3, the shard collection operation can perform an initial chunk creation and distribution for empty or non-existing collections if zones and zone ranges have been defined for the collection. Initial creation and distribution of chunk allows for faster setup of zoned sharding. After the initial distribution, the balancer manages the chunk distribution going forward per usual. Sharding Empty Collection on Single Field Hashed Shard Key  * With no zones and zone ranges specified for the empty or non-existing collection:\n   \n   * The sharding operation creates empty chunks to cover the entire range of the shard key values and performs an initial chunk distribution. By default, the operation creates 2 chunks per shard and migrates across the cluster. You can use numInitialChunks option to specify a different number of initial chunks. This initial creation and distribution of chunks allows for faster setup of sharding.\n   \n   * After the initial distribution, the balancer manages the chunk distribution going forward.  * With zones and zone ranges specified for the empty or a non-existing collection (Available starting in MongoDB 4.0.3):\n   \n   * The sharding operation creates empty chunks for the defined zone ranges as well as any additional chunks to cover the entire range of the shard key values and performs an initial chunk distribution based on the zone ranges. This initial creation and distribution of chunks allows for faster setup of zoned sharding.\n   \n   * After the initial distribution, the balancer manages the chunk distribution going forward. Sharding Empty Collection on Compound Hashed Shard Key with Hashed Field Prefix If the compound hashed shard key has the hashed field as the prefix (i.e. the hashed field is the first field in the shard key):  * With no zones and zone ranges specified for the empty or non-existing collection:\n   \n   * The sharding operation creates empty chunks to cover the entire range of the shard key values and performs an initial chunk distribution. The value of all non-hashed fields is MinKey at each split point. By default, the operation creates 2 chunks per shard and migrates across the cluster. You can use numInitialChunks option to specify a different number of initial chunks. This initial creation and distribution of chunks allows for faster setup of sharding.\n   \n   * After the initial distribution, the balancer manages the chunk distribution going forward.  * With a single zone with a range from MinKey to MaxKey specified for the empty or a non-existing collection and the presplitHashedZones option specified to sh.shardCollection():\n   \n   * The sharding operation creates empty chunks for the defined zone range as well as any additional chunks to cover the entire range of the shard key values and performs an initial chunk distribution based on the zone ranges. This initial creation and distribution of chunks allows for faster setup of zoned sharding.\n   \n   * After the initial distribution, the balancer manages the chunk distribution going forward. Sharding Empty Collection on Compound Hashed Shard Key with Non-Hashed Prefix If the compound hashed shard key has one or more non-hashed fields as the prefix (i.e. the hashed field is not the first field in the shard key):  * With no zones and zone ranges specified for the empty or non-existing collection and preSplitHashedZones is false or omitted, MongoDB does not perform any initial chunk creation or distribution when sharding the collection.  * With no zones and zone ranges specified for the empty or non-existing collection and preSplitHashedZones, sh.shardCollection() / shardCollection returns an error.\n\n \nTIP \nSEE ALSO: To learn how to deploy a sharded cluster and implement hashed sharding, see Deploy a Sharded Cluster. ←  Troubleshoot Shard KeysRanged Sharding → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/queryable-encryption/reference/libmongocrypt/": " Docs Home → MongoDB Manual \nINSTALL LIBMONGOCRYPT \nOn this page    \n * Overview\n   \n * macOS Installion\n * Windows Installation\n * Linux Installation\n * Debian\n * Ubuntu\n * RedHat\n * Amazon Linux 2\n * Amazon Linux\n * Suse Queryable Encryption is in Public Preview and available for evaluation purposes. Public Preview is not recommended for production deployments as breaking changes may be introduced. To learn more about the Preview please see the Queryable Encryption Preview blog post. New in version 4.2. \nOVERVIEW \nLearn how to install libmongocrypt, a core component of Queryable Encryption. \nWARNING Do not build libmongocrypt from source. Use one of the installation methods listed on this page. \nMACOS INSTALLION \nbrew install mongodb/brew/libmongocrypt  \nWINDOWS INSTALLATION \nClick here to begin a download with the latest release binary and includes directory. \nLINUX INSTALLATION  DEBIAN \n1 Import the public key used to sign the package repositories: sudo sh -c 'curl -s --location https://www.mongodb.org/static/pgp/libmongocrypt.asc | gpg --dearmor >/etc/apt/trusted.gpg.d/libmongocrypt.gpg' \n2 Add the MongoDB repository to your package sources: \nIMPORTANT Change <release> in the following shell command to your platform release (e.g. \"xenial\" or \"buster\"). echo \"deb https://libmongocrypt.s3.amazonaws.com/apt/debian <release>/libmongocrypt/1.7 main\" | sudo tee /etc/apt/sources.list.d/libmongocrypt.list \n3 Update the package cache: sudo apt-get update \n4 Install libmongocrypt: sudo apt-get install -y libmongocrypt  \nUBUNTU \n1 Import the public key used to sign the package repositories: sudo sh -c 'curl -s --location https://www.mongodb.org/static/pgp/libmongocrypt.asc | gpg --dearmor >/etc/apt/trusted.gpg.d/libmongocrypt.gpg' \n2 Add the MongoDB repository to your package sources: \nIMPORTANT Change <release> in the following shell command to your platform release (e.g. \"xenial\" or \"buster\"). echo \"deb https://libmongocrypt.s3.amazonaws.com/apt/ubuntu <release>/libmongocrypt/1.7 universe\" | sudo tee /etc/apt/sources.list.d/libmongocrypt.list \n3 Update the package cache: sudo apt-get update \n4 Install libmongocrypt: sudo apt-get install -y libmongocrypt  \nREDHAT \n1 Create a repository file for the libmongocrypt package: [libmongocrypt]name=libmongocrypt repositorybaseurl=https://libmongocrypt.s3.amazonaws.com/yum/redhat/$releasever/libmongocrypt/1.7/x86_64gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/libmongocrypt.asc \n2 Install the libmongocrypt package: sudo yum install -y libmongocrypt  \nAMAZON LINUX 2 \n1 Create a repository file for the libmongocrypt package: [libmongocrypt]name=libmongocrypt repositorybaseurl=https://libmongocrypt.s3.amazonaws.com/yum/amazon/2/libmongocrypt/1.7/x86_64gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/libmongocrypt.asc \n2 Install the libmongocrypt package: sudo yum install -y libmongocrypt  \nAMAZON LINUX \n1 Create a repository file for the libmongocrypt package: [libmongocrypt]name=libmongocrypt repositorybaseurl=https://libmongocrypt.s3.amazonaws.com/yum/amazon/2013.03/libmongocrypt/1.7/x86_64gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/libmongocrypt.asc \n2 Install the libmongocrypt package:\n\n  \nSUSE \n1 Import the public key used to sign the package repositories: sudo rpm --import https://www.mongodb.org/static/pgp/libmongocrypt.asc \n2 Add the repository to your package sources: \nIMPORTANT Change <release> in the following shell command to your platform release (e.g. \"12\" or \"15\"). sudo zypper addrepo --gpgcheck \"https://libmongocrypt.s3.amazonaws.com/zypper/suse/<release>/libmongocrypt/1.7/x86_64\" libmongocrypt \n3 Install the libmongocrypt package: sudo zypper -n install libmongocrypt \n←  Automatic Encryption Shared LibraryInstall and Configure mongocryptd → On this page  * Overview\n * macOS Installion\n * Windows Installation\n * Linux Installation\n * Debian\n * Ubuntu\n * RedHat\n * Amazon Linux 2\n * Amazon Linux\n * Suse Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/queryable-encryption/fundamentals/manage-collections/": " Docs Home → MongoDB Manual \nENCRYPTED COLLECTION MANAGEMENT \nOn this page    \n * Overview\n   \n * Metadata Collections\n * Storage Costs\n * Write Costs\n * Insert Operations\n * Update Operations\n * Delete Operations\n * Index Compaction Queryable Encryption is in Public Preview and available for evaluation purposes. Public Preview is not recommended for production deployments as breaking changes may be introduced. To learn more about the Preview please see the Queryable Encryption Preview blog post. In this guide, you can learn how to manage your encrypted collections, and the storage and write costs of Queryable Encryption. \nOVERVIEW \nQueryable Encryption introduces the ability to encrypt sensitive fields in your documents using randomized encryption, while still being able to query the encrypted fields. With Queryable Encryption, a given plaintext value always encrypts to a different ciphertext, while still remaining queryable. To enable this functionality, Queryable Encryption uses four data structures:      * Three metadata collections  * A field in every document in the encrypted collection called __safeContent__ \nIMPORTANT It is critical that these data structures are not modified or deleted, or query results will be incorrect. \nMETADATA COLLECTIONS \nWhen you create an encrypted collection using Queryable Encryption, MongoDB creates three metadata collections:  * enxcol_.<collectionName>.esc, referred to as ESC  * enxcol_.<collectionName>.ecc, referred to as ECC  * enxcol_.<collectionName>.ecoc, referred to as ECOC \nEXAMPLE If you create a collection called \"patients\", MongoDB creates the following metadata collections:  * enxcol_.patients.esc  * enxcol_.patients.ecc  * enxcol_.patients.ecoc When you insert documents with encrypted fields that you wish to query on, MongoDB updates the metadata collections to maintain an index that enables you to query. MongoDB refers to this as \"indexed field\". This comes at a cost in storage and write speed. \nSTORAGE COSTS \nStorage and write costs increase based on the number of indexed fields per document. \nIMPORTANT \nTECHNICAL PREVIEW MongoDB's guidance during the technical preview is to expect two to three times the storage requirement for a Queryable Encryption collection and associated metadata collections. For example, a 1 GB collection may have a storage requirement of 2-3 GB for associated metadata collections. This guidance will be tuned in a future release. \nWRITE COSTS  INSERT OPERATIONS \nWhen inserting a document, each indexed field requires two writes to metadata collections.  * One write to ESC  * One write to ECOC \nEXAMPLE Inserting a document with two indexed fields requires:  * One write to the encrypted collection.  * Four writes to the metadata collections. \nUPDATE OPERATIONS \nWhen updating a document, each indexed field requires four writes to metadata collections.  * One write to ESC  * One write to ECC  * Two writes to ECOC \nEXAMPLE Updating a document with two indexed fields requires:  * One write to the encrypted collection.  * Eight writes to the metadata collections. \nDELETE OPERATIONS \nWhen deleting a document, each indexed field requires two writes to the metadata collections.  * One write to ECC  * One write to ECOC \nEXAMPLE Deleting a document with two indexed fields requires:  * One write into the encrypted collection.  * Four writes to the metadata collections. \nINDEX COMPACTION  IMPORTANT \nTECHNICAL PREVIEW You are required to run index compaction during the technical preview. MongoDB plans to automatically run index compaction in a future release. As you insert, update, and delete documents, the metadata collections change and grow. Index compaction is a process that prunes the metadata collections and reduces their size. You should run index compaction when the size of ECOC exceeds 1 GB. You can check the size of your collections using mongosh and issuing the db.collection.totalSize() command. \nEXAMPLE In this example, the encrypted collection is named \"patients\". db.enxcol_.patients.ecoc.totalSize() HIDE OUTPUT 1407960328 \nIMPORTANT You must configure your client for Queryable Encryption to run index compaction. To run index compaction, use mongosh and run the db.collection.compactStructuredEncryptionData() command to reduce the size of the metadata collections. \nEXAMPLE\n\n HIDE OUTPUT {  \"stats\": {     ...  },  \"ok\": 1,  ...} ←  Field Encryption and QueryabilityExplicit Encryption → On this page  * Overview\n * Metadata Collections\n * Storage Costs\n * Write Costs\n * Insert Operations\n * Update Operations\n * Delete Operations\n * Index Compaction Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/queryable-encryption/fundamentals/manage-keys/": " Docs Home → MongoDB Manual \nENCRYPTION KEY MANAGEMENT \nOn this page    \n * Encryption Components\n   \n * Supported Key Management Services\n * Reasons to Use a Remote KMS\n * Learn More Queryable Encryption is in Public Preview and available for evaluation purposes. Public Preview is not recommended for production deployments as breaking changes may be introduced. To learn more about the Preview please see the Queryable Encryption Preview blog post. In this guide, you can learn how to manage your encryption keys with a Key Management System (KMS) in your Queryable Encryption enabled application. \nENCRYPTION COMPONENTS \nMongoDB uses the following components to perform Queryable Encryption:      * Data Encryption Keys (DEKs)  * Key Vault collections  * Customer Master Keys (CMKs)  * Key Management System (KMS) Your Data Encryption Key is the key you use to encrypt the fields in your MongoDB documents. Your DEK is stored in a document in a MongoDB collection called the Key Vault collection. Your Customer Master Key is the key you use to encrypt your Data Encryption Keys. MongoDB automatically encrypts Data Encryption Keys using the specified CMK during Data Encryption Key creation. The CMK is the most sensitive key in Queryable Encryption. If your CMK is compromised, all of your encrypted data can be decrypted. Use a Key Management System to store your Customer Master Key. To learn more about the relationship between keys, see Keys and Key Vaults. \nIMPORTANT \nUSE A REMOTE KEY MANAGEMENT SERVICE PROVIDER Ensure you store your Customer Master Key (CMK) on a remote KMS. To learn more about why you should use a remote KMS, see Reasons to Use a Remote KMS. To view a list of all supported KMS providers, see the KMS Providers page. \nSUPPORTED KEY MANAGEMENT SERVICES \nQueryable Encryption supports the following Key Management System (KMS) providers:  * Amazon Web Services KMS  * Azure Key Vault  * Google Cloud Platform KMS  * Any KMIP Compliant Key Management System  * Local Key Provider To learn more about these providers, including diagrams that show how your application uses them to perform Queryable Encryption, see KMS Providers. \nREASONS TO USE A REMOTE KMS \nUsing a remote KMS to manage your Customer Master Key (CMK) has the following advantages over using your local filesystem to host the CMK:  * Secure storage of the key with access auditing  * Reduced risk of access permission issues  * Availability and distribution of the key to remote clients  * Automated key backup and recovery  * Centralized encryption key lifecycle management Additionally, for the following KMS providers, your KMS remotely encrypts and decrypts your Data Encryption Key, ensuring your Customer Master Key is never exposed to your Queryable Encryption enabled application:  * Amazon Web Services KMS  * Azure Key Vault  * Google Cloud Platform KMS \nLEARN MORE \nFor tutorials detailing how to set up a Queryable Encryption enabled application with each of the supported KMS providers, see the following pages:  * Use Automatic Queryable Encryption with AWS  * Use Automatic Queryable Encryption with Azure  * Use Automatic Queryable Encryption with GCP ←  Keys and Key VaultsKMS Providers → On this page  * Encryption Components\n * Supported Key Management Services\n * Reasons to Use a Remote KMS\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/csfle/quick-start/": " Docs Home → MongoDB Manual \nQUICK START \nOn this page    \n * Overview\n   \n * Before You Get Started\n * Procedure\n * Create a Customer Master Key\n * Create a Unique Index on your Key Vault collection\n * Create a Data Encryption Key\n * Configure the MongoClient\n * Insert a Document with Encrypted Fields\n * Retrieve Your Encrypted Document\n * Learn More The next-generation Queryable Encryption feature is now in Public Preview. To learn more about Queryable Encryption, see Queryable Encryption. \nOVERVIEW \nThis guide shows you how to encrypt a document with automatic Client-Side Field Level Encryption (CSFLE) and a MongoDB driver. After completing this guide, you should have the following knowledge and software:      * Knowledge of the steps to configure a driver to encrypt fields in a document.  * A working, but not production-ready, client application that utilizes automatic Client-Side Field Level Encryption. \nIMPORTANT \nDO NOT USE THIS APPLICATION IN PRODUCTION Since this example application stores an encryption key on your application's filesystem, you risk unauthorized access to the key or loss of the key to decrypt your data. To view a tutorial that demonstrates how to create a production-ready CSFLE-enabled application, see Tutorials. \nBEFORE YOU GET STARTED \nTo complete and run the code in this guide, you need to set up your development environment as shown in the Installation Requirements page. Select the programming language for which you want to see code examples for from the Select your language dropdown menu on the right side of the page. \nTIP \nSEE: FULL APPLICATION To view the complete runnable application code for this tutorial, go to the following link:   PROCEDURE  \n1 \nCREATE A CUSTOMER MASTER KEY \nYou must create a Customer Master Key (CMK) to perform CSFLE. Create a 96-byte Customer Master Key and save it in your Local Key Provider, which is your filesystem, as the file master-key.txt: openssl rand 96 > master-key.txt  \nNOTE \nUSE A PROGRAMMING LANGUAGE TO CREATE A CUSTOMER MASTER KEY If you would rather use your preferred programming language to generate your CMK, you can view code snippets demonstrating how to generate a Customer Master Key in each of the supported languages of this guide on GitHub. \nWARNING \nDO NOT USE THE LOCAL KEY PROVIDER IN PRODUCTION The Local Key Provider is an insecure method of storage and is not recommended for production. Instead, you should store your Customer Master Keys in a remote Key Management System (KMS). To learn how to use a remote KMS in your CSFLE implementation, see the Tutorials guide. 2 \nCREATE A UNIQUE INDEX ON YOUR KEY VAULT COLLECTION \nCreate a unique index on the keyAltNames field in your encryption.__keyVault collection. Select the tab corresponding to your preferred MongoDB driver:  3 \nCREATE A DATA ENCRYPTION KEY \n1 READ THE CUSTOMER MASTER KEY AND SPECIFY KMS PROVIDER SETTINGS \nRetrieve the contents of the Customer Master Key file that you generated in the Create a Customer Master Key step of this guide. Pass the CMK value to your KMS provider settings. The client uses these settings to discover the CMK. As you are using the Local Key Provider, set the provider name to local.  2 CREATE A DATA ENCRYPTION KEY \nConstruct a client with your MongoDB connection string and Key Vault collection namespace, and create a Data Encryption Key: \nNOTE \nKEY VAULT COLLECTION NAMESPACE PERMISSIONS The Key Vault collection in this guide is the __keyVault collection in the encryption database. Ensure that the database user your application uses to connect to MongoDB has ReadWrite permissions on the encryption.__keyVault namespace.  \nThe output from the code above should resemble the following: DataKeyId [base64]: 3k13WkSZSLy7kwAAP4HDyQ==  \nTIP \nSEE: COMPLETE CODE \n4 \nCONFIGURE THE MONGOCLIENT \n1 SPECIFY THE KEY VAULT COLLECTION NAMESPACE \nSpecify encryption.__keyVault as the Key Vault collection namespace.  2 SPECIFY THE LOCAL CUSTOMER MASTER KEY \nSpecify the KMS provider and specify your key inline:  3 CREATE AN ENCRYPTION SCHEMA FOR YOUR COLLECTION  TIP \nADD YOUR DATA ENCRYPTION KEY BASE64 ID\n\n  4 SPECIFY THE LOCATION OF THE ENCRYPTION BINARY \nConfigure the client to spawn the mongocryptd process by specifying the path to the binary using the following configuration options:  5 CREATE THE MONGOCLIENT \nInstantiate a MongoDB client object with the following automatic encryption settings:  5 \nINSERT A DOCUMENT WITH ENCRYPTED FIELDS \nUse your CSFLE-enabled MongoClient instance to insert an encrypted document into the medicalRecords.patients namespace using the following code snippet:  \nWhen you insert a document, your CSFLE-enabled client encrypts the fields of your document such that it resembles the following: {  \"_id\": { \"$oid\": \"<_id of your document>\" },  \"name\": \"Jon Doe\",  \"ssn\": {    \"$binary\": \"<cipher-text>\",    \"$type\": \"6\"  },  \"bloodType\": {    \"$binary\": \"<cipher-text>\",    \"$type\": \"6\"  },  \"medicalRecords\": {    \"$binary\": \"<cipher-text>\",    \"$type\": \"6\"  },  \"insurance\": {    \"provider\": \"MaestCare\",    \"policyNumber\": {      \"$binary\": \"<cipher-text>\",      \"$type\": \"6\"    }  }}  \nTIP \nSEE: COMPLETE CODE \n6 \nRETRIEVE YOUR ENCRYPTED DOCUMENT \nRetrieve the encrypted document you inserted in the Insert a Document with Encrypted Fields step of this guide. To show the functionality of CSFLE, the following code snippet queries for your document with a client configured for automatic CSFLE as well as a client that is not configured for automatic CSFLE.  \nThe output of the preceding code snippet should look like this:\n\n  \nTIP \nSEE: COMPLETE CODE  \nLEARN MORE \nTo view a tutorial on production-ready CSFLE with a remote KMS, see Tutorials. To learn how CSFLE works, see Fundamentals. To learn more about the topics mentioned in this guide, see the following links:  * Customer Master Keys  * Key Management System providers  * Data Encryption Keys  * Key Vault collections  * Encryption Schemas  * mongocryptd  * CSFLE-specific MongoClient settings  * Automatic CSFLE Writes ←  Installation RequirementsFundamentals → On this page  * Overview\n * Before You Get Started\n * Procedure\n * Create a Customer Master Key\n * Create a Unique Index on your Key Vault collection\n * Create a Data Encryption Key\n * Configure the MongoClient\n * Insert a Document with Encrypted Fields\n * Retrieve Your Encrypted Document\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/csfle/reference/decryption/": " Docs Home → MongoDB Manual \nHOW CSFLE DECRYPTS DOCUMENTS \nOn this page    \n * Metadata Used for Decryption\n   \n * Automatic Decryption Process\n * Automatically Encrypted Read Behavior\n * Learn More The next-generation Queryable Encryption feature is now in Public Preview. To learn more about Queryable Encryption, see Queryable Encryption. This page describes how CSFLE uses metadata from your Data Encryption Key and Customer Master Key to decrypt data. \nMETADATA USED FOR DECRYPTION \nWhen you encrypt data using CSFLE, the data you encrypt is stored as a BinData subtype 6 object that includes the following metadata:      * The _id of the Data Encryption Key used to encrypt the data  * The encryption algorithm used to encrypt the data Data Encryption Keys contain metadata that describes what Customer Master Key was used to encrypt them. Drivers and mongosh use this metadata to attempt to automatically decrypt your data. \nAUTOMATIC DECRYPTION PROCESS \nTo automatically decrypt your data, your CSFLE-enabled client performs the following procedure:  1. Check the BinData blob metadata of the field you intend to decrypt for the Data Encryption Key and encryption algorithm used to encrypt the value.  2. Check the Key Vault collection configured in the current database connection for the specified Data Encryption Key. If the Key Vault collection does not contain the specified key, automatic decryption fails and the driver returns the encrypted BinData blob.  3. Check the Data Encryption Key metadata for the Customer Master Key (CMK) used to encrypt the key material.  4. Decrypt the Data Encryption Key. This process varies by KMS provider:\n    \n    \n      5. Decrypt the BinData value using the decrypted Data Encryption Key and appropriate algorithm. Applications with access to the MongoDB server that do not also have access to the required CMK and Data Encryption Keys cannot decrypt the BinData values. \nAUTOMATICALLY ENCRYPTED READ BEHAVIOR \nFor read operations, the driver encrypts field values in the query document using your encryption schema prior to issuing the read operation. Your client application then uses the BinData metadata to automatically decrypt the document you receive from MongoDB. To learn more about encryption schemas, see Encryption Schemas. \nLEARN MORE \nTo learn how to configure the database connection for Client-Side Field Level Encryption, see CSFLE-Specific MongoClient Options. To learn more about the relationship between Data Encryption Keys and Customer Master Keys, see Keys and Key Vaults. ←  Encryption ComponentsCryptographic Primitives → On this page  * Metadata Used for Decryption\n * Automatic Decryption Process\n * Automatically Encrypted Read Behavior\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/csfle/tutorials/gcp/gcp-automatic/": " Docs Home → MongoDB Manual \nUSE AUTOMATIC CLIENT-SIDE FIELD LEVEL ENCRYPTION WITH GCP \nOn this page    \n * Overview\n   \n * Before You Get Started\n * Set Up the KMS\n * Register a GCP Service Account\n * Create a GCP Customer Master Key\n * Create the Application\n * Create a Unique Index on your Key Vault collection\n * Create a Data Encryption Key\n * Configure the MongoClient\n * Insert a Document with Encrypted Fields\n * Retrieve Your Encrypted Document\n * Learn More \nOVERVIEW \nThis guide shows you how to build a Client-Side Field Level Encryption (CSFLE)-enabled application using Google Cloud Key Management Service. After you complete the steps in this guide, you should have:      * A CMK hosted on Google Cloud Key Management Service.  * A client application that inserts encrypted documents using your CMK. \nBEFORE YOU GET STARTED \nTo complete and run the code in this guide, you need to set up your development environment as shown in the Installation Requirements page. Throughout this guide, code examples use placeholder text. Before you run the examples, substitute your own values for these placeholders. For example: dek_id := \"<Your Base64 DEK ID>\" You would replace everything between quotes with your DEK ID. dek_id := \"abc123\" Select the programming language for which you want to see code examples for from the Select your language dropdown menu on the right side of the page. \nTIP \nSEE: FULL APPLICATION To view the complete runnable application code for this tutorial, go to the following link:   SET UP THE KMS  \n1 \nREGISTER A GCP SERVICE ACCOUNT \n1 REGISTER OR LOG INTO YOUR EXISTING ACCOUNT ON GOOGLE CLOUD. \n2 CREATE A SERVICE ACCOUNT FOR YOUR PROJECT \nTo create a service account on Google Cloud, follow the Creating a service account guide in Google's official documentation. 3 ADD A SERVICE ACCOUNT KEY \nTo add a service account key on Google Cloud, follow the Managing service account keys guide in Google's official documentation. \nIMPORTANT When creating your service account key, you receive a one-time download of the private key information. Make sure to download this file in either the PKCS12 or JSON format for use later in this tutorial. 2 \nCREATE A GCP CUSTOMER MASTER KEY \n1 CREATE A NEW CUSTOMER MASTER KEY \nCreate a key ring and a symmetric key by following the Create a key guide from Google's official documentation. This key is your Customer Master Key (CMK). Record the following details of your CMK for use in a future step of this tutorial. Field\nRequired\nDescription\nkey_name\nYes\nIdentifier for the CMK.\nkey_ring\nYes\nIdentifier for the group of keys your key belongs to.\nkey_version\nNo\nThe version of the named key.\nlocation\nYes\nRegion specified for your key.\nendpoint\nNo\nThe host and optional port of the Google Cloud KMS. The default value is cloudkms.googleapis.com. \nCREATE THE APPLICATION \nSelect the tab that corresponds to the MongoDB driver you are using in your application to see relevant code samples. 1 \nCREATE A UNIQUE INDEX ON YOUR KEY VAULT COLLECTION \nCreate a unique index on the keyAltNames field in your encryption.__keyVault collection. Select the tab corresponding to your preferred MongoDB driver:  2 \nCREATE A DATA ENCRYPTION KEY \n1 ADD YOUR GCP KMS CREDENTIALS \nAdd the service account credentials to your CSFLE-enabled client code. \nTIP You saved a file containing your service account key credentials in the Create a GCP Service Account step of this guide. If you downloaded your credentials in JSON format, you can use the following command to extract the value of your private key, substituting <credentials-filename> with the name of your credentials file: cat <credentials-filename> | jq -r .private_key | openssl pkcs8 -topk8 -nocrypt -inform PEM -outform DER | base64  If you downloaded your credentials in PKCS12 format, you need to specify your GCP service account import password and to add a PEM pass phrase to access the key when accessing it using the following command, substituting <credentials-filename> with the name of your credentials file: openssl pkcs12 -info -in <credentials-filename>   \nTIP \nLEARN MORE\n\n 2 ADD YOUR KEY INFORMATION \nUpdate the following code to specify your Customer Master Key: \nTIP You recorded your Customer Master Key details in the in the Create a Customer Master Key step of this guide.  3 GENERATE YOUR DATA ENCRYPTION KEY   \nTIP \nLEARN MORE To view a diagram showing how your client application creates your Data Encryption Key when using an Google Cloud Key Management Service, see Architecture. To learn more about the options for creating a Data Encryption Key encrypted with a Customer Master Key hosted in Azure Key Vault, see kmsProviders Object and dataKeyOpts Object. \nTIP \nSEE: COMPLETE CODE \n3 \nCONFIGURE THE MONGOCLIENT \n1 SPECIFY THE KEY VAULT COLLECTION NAMESPACE \nSpecify encryption.__keyVault as the Key Vault collection namespace.  2 SPECIFY YOUR CLOUD ACCOUNT CREDENTIALS \nSpecify the cloud provider and your service account credentials:  3 CREATE AN ENCRYPTION SCHEMA FOR YOUR COLLECTION  TIP \nADD YOUR DATA ENCRYPTION KEY BASE64 ID Make sure to update the following code to include your Base64 DEK ID. You received this value in the Generate your Data Encryption Key step of this guide.  4 SPECIFY THE LOCATION OF THE ENCRYPTION BINARY \nConfigure the client to spawn the mongocryptd process by specifying the path to the binary using the following configuration options:  5 CREATE THE MONGOCLIENT \nInstantiate a MongoDB client object with the following automatic encryption settings:  4 \nINSERT A DOCUMENT WITH ENCRYPTED FIELDS \nUse your CSFLE-enabled MongoClient instance to insert an encrypted document into the medicalRecords.patients namespace using the following code snippet:  \nWhen you insert a document, your CSFLE-enabled client encrypts the fields of your document such that it resembles the following: {  \"_id\": { \"$oid\": \"<_id of your document>\" },  \"name\": \"Jon Doe\",  \"ssn\": {    \"$binary\": \"<cipher-text>\",    \"$type\": \"6\"  },  \"bloodType\": {    \"$binary\": \"<cipher-text>\",    \"$type\": \"6\"  },  \"medicalRecords\": {    \"$binary\": \"<cipher-text>\",    \"$type\": \"6\"  },  \"insurance\": {    \"provider\": \"MaestCare\",    \"policyNumber\": {      \"$binary\": \"<cipher-text>\",      \"$type\": \"6\"    }  }}  \nTIP \nSEE: COMPLETE CODE \n5 \nRETRIEVE YOUR ENCRYPTED DOCUMENT \nRetrieve the encrypted document you inserted in the Insert a Document with Encrypted Fields step of this guide. To show the functionality of CSFLE, the following code snippet queries for your document with a client configured for automatic CSFLE as well as a client that is not configured for automatic CSFLE.  \nThe output of the preceding code snippet should look like this:\n\n  \nTIP \nSEE: COMPLETE CODE  \nLEARN MORE \nTo learn more about the topics mentioned in this guide, see the following links:  * Learn more about CSFLE components on the Reference page.  * Learn how Customer Master Keys and Data Encryption Keys work on the Keys and Key Vaults page  * See how KMS Providers manage your CSFLE keys on the KMS Providers page. ←  Use Automatic Client-Side Field Level Encryption with AzureUse Automatic Client-Side Field Level Encryption with KMIP → On this page  * Overview\n * Before You Get Started\n * Set Up the KMS\n * Register a GCP Service Account\n * Create a GCP Customer Master Key\n * Create the Application\n * Create a Unique Index on your Key Vault collection\n * Create a Data Encryption Key\n * Configure the MongoClient\n * Insert a Document with Encrypted Fields\n * Retrieve Your Encrypted Document\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/schema-validation/specify-validation-level/": " Docs Home → MongoDB Manual \nSPECIFY VALIDATION LEVEL FOR EXISTING DOCUMENTS \nOn this page    \n * Context\n   \n * Prerequisite\n * Steps: Use strict Validation\n * Steps: Use moderate Validation\n * Learn More For documents that already exist in your collection prior to adding validation, you can specify how MongoDB applies validation rules to these documents. \nCONTEXT \nYour schema's validationLevel determines the documents for which MongoDB applies validation rules: Validation Level\nBehavior\nstrict\n(Default) MongoDB applies validation rules to all inserts and updates.\nmoderate\nMongoDB only applies validation rules to existing valid documents. Updates to invalid documents which exist prior to the validation being added are not checked for validity. \nPREREQUISITE \nThe examples on this page use a contacts collection with these documents: db.contacts.insertMany([   { \"_id\": 1, \"name\": \"Anne\", \"phone\": \"+1 555 123 456\", \"city\": \"London\", \"status\": \"Complete\" },   { \"_id\": 2, \"name\": \"Ivan\", \"city\": \"Vancouver\" }])  \nSTEPS: USE STRICT VALIDATION \nThe following example adds a strict validation to the contacts collection and shows the results when attempting to update invalid documents. 1 \nSPECIFY VALIDATION RULES WITH STRICT VALIDATION LEVEL. \nAdd a validator to the contacts collection with strict validationLevel: db.runCommand( {   collMod: \"contacts\",   validator: { $jsonSchema: {      bsonType: \"object\",      required: [ \"phone\", \"name\" ],      properties: {         phone: {            bsonType: \"string\",            description: \"phone must be a string and is required\"         },         name: {            bsonType: \"string\",            description: \"name must be a string and is required\"         }      }   } },   validationLevel: \"strict\"} )  Because the validationLevel is strict, when any document is updated, MongoDB checks that document for validation. 2 \nTEST THE VALIDATION. \nThe following update commands modify both documents in the contacts collection such that neither of the documents are consistent with the validation rule which requires name to be a string: db.contacts.updateOne(   { _id: 1 },   { $set: { name: 10 } })\ndb.contacts.updateOne(   { _id: 2 },   { $set: { name: 20 } }) \n3 \nOBSERVE RESULTS. \nBoth update operations fail. MongoDB returns the following output for each operation:\n\n \nSTEPS: USE MODERATE VALIDATION \nThe following example adds a moderate validation to the contacts collection and shows the results when attempting to update invalid documents. 1 \nSPECIFY VALIDATION RULES WITH MODERATE VALIDATION LEVEL. \nAdd a validator to the contacts collection with moderate validationLevel: db.runCommand( {   collMod: \"contacts\",   validator: { $jsonSchema: {      bsonType: \"object\",      required: [ \"phone\", \"name\" ],      properties: {         phone: {            bsonType: \"string\",            description: \"phone must be a string and is required\"         },         name: {            bsonType: \"string\",            description: \"name must be a string and is required\"         }      }   } },   validationLevel: \"moderate\"} )  Because the validationLevel is moderate:      * If you update the document with _id: 1, MongoDB applies the new validation rules because the existing document meets the validation requirements.  * If you update the document with _id: 2, MongoDB does not apply the new validation rules because the existing document does not meet the validation requirements. 2 \nTEST THE VALIDATION. \nThe following update commands modify both documents in the contacts collection such that neither of the documents are consistent with the validation rule which requires name to be a string: db.contacts.updateOne(   { _id: 1 },   { $set: { name: 10 } })\ndb.contacts.updateOne(   { _id: 2 },   { $set: { name: 20 } }) \n3 \nOBSERVE RESULTS. \nMongoDB returns the following output for each operation: // _id: 1\nMongoServerError: Document failed validationAdditional information: {  failingDocumentId: 1,  details: {    operatorName: '$jsonSchema',    schemaRulesNotSatisfied: [      {        operatorName: 'properties',        propertiesNotSatisfied: [          {            propertyName: 'name',            description: 'name must be a string and is required',            details: [              {                operatorName: 'bsonType',                specifiedAs: { bsonType: 'string' },                reason: 'type did not match',                consideredValue: 10,                consideredType: 'int'              }            ]          }        ]      }    ]  }}\n// _id: 2\n{   acknowledged: true,   insertedId: null,   matchedCount: 1,   modifiedCount: 0,   upsertedCount: 0} The output shows that:  * The update fails for the document with _id: 1. This document met the initial validation requirements, and MongoDB applies validation rules to this document.  * The update succeeds for the document with _id: 2. This document did not meet the initial validation requirements, and MongoDB does not apply validation rules to this document. \nIMPORTANT The error output is intended for human consumption. It may change in the future and should not be relied upon in scripts. \nLEARN MORE \n * Choose How to Handle Invalid Documents  * Modify Schema Validation ←  Modify Schema ValidationChoose How to Handle Invalid Documents → On this page\n\n Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/schema-validation/specify-json-schema/": " Docs Home → MongoDB Manual \nSPECIFY JSON SCHEMA VALIDATION \nOn this page    \n * Context\n   \n * Restrictions\n * Steps\n * Additional Information\n * Learn More JSON Schema is a vocabulary that allows you to annotate and validate JSON documents. You can use JSON schema to specify validation rules for your fields in a human-readable format. \nCONTEXT \nMongoDB supports draft 4 of JSON Schema, including core specification and validation specification, with some differences. For details, see Extensions and Omissions. For more information about JSON Schema, see the official website. \nRESTRICTIONS \nYou can't specify schema validation for:      * Collections in the admin, local, and config databases  * System collections \nSTEPS \nIn this example, you create a students collection with validation rules and observe the results after you attempt to insert an invalid document. 1 \nCREATE A COLLECTION WITH VALIDATION. \nCreate a students collection and use the $jsonSchema operator to set schema validation rules. For example: db.createCollection(\"students\", {   validator: {      $jsonSchema: {         bsonType: \"object\",         title: \"Student Object Validation\",         required: [ \"address\", \"major\", \"name\", \"year\" ],         properties: {            name: {               bsonType: \"string\",               description: \"'name' must be a string and is required\"            },            year: {               bsonType: \"int\",               minimum: 2017,               maximum: 3017,               description: \"'year' must be an integer in [ 2017, 3017 ] and is required\"            },            gpa: {               bsonType: [ \"double\" ],               description: \"'gpa' must be a double if the field exists\"            }         }      }   }} )  \nTIP \nCLARIFY RULES WITH TITLE AND DESCRIPTION FIELDS You can use title and description fields to provide an explanation of validation rules when the rules are not immediately clear. When a document fails validation, MongoDB includes these fields in the error output. 2 \nCONFIRM THAT THE VALIDATION PREVENTS INVALID DOCUMENTS. \nThe following insert operation fails because gpa is an integer when the validator requires a double. db.students.insertOne( {   name: \"Alice\",   year: Int32( 2019 ),   major: \"History\",   gpa: Int32(3),   address: {      city: \"NYC\",      street: \"33rd Street\"   }} )  The operation returns this error:\n\n 3 \nINSERT A VALID DOCUMENT. \nThe insert succeeds after you change the gpa field to a double: db.students.insertOne( {   name: \"Alice\",   year: NumberInt(2019),   major: \"History\",   gpa: Double(3.0),   address: {      city: \"NYC\",      street: \"33rd Street\"   }} ) \n4 \nQUERY FOR THE VALID DOCUMENT. \nTo confirm that the document was successfully inserted, query the students collection: db.students.find()  MongoDB returns the inserted document: [  {    _id: ObjectId(\"62bb413014b92d148400f7a5\"),    name: 'Alice',    year: 2019,    major: 'History',    gpa: 3,    address: { city: 'NYC', street: '33rd Street' }  }] \nADDITIONAL INFORMATION \nYou can combine JSON Schema validation with query operator validation. For example, consider a sales collection with this schema validation: db.createCollection{ \"sales\", {  validator: {    \"$and\": [      // Validation with query operators      {        \"$expr\": {          \"$lt\": [\"$lineItems.discountedPrice\", \"$lineItems.price\"]        }      },      // Validation with JSON Schema      {        \"$jsonSchema\": {          \"properties\": {            \"items\": { \"bsonType\": \"array\" }          }        }      }    ]  }}  The preceding validation enforces these rules for documents in the sales collection:  * lineItems.discountedPrice must be less than lineItems.price. This rule is specified using the $lt operator.  * The items field must be an array. This rule is specified using $jsonSchema. \nLEARN MORE \n * To see the complete list of allowed keywords in a JSON schema, see Available Keywords.  * To restrict what values a certain field can contain, see Specify Allowed Field Values.  * To avoid issues with JSON schema validation, see Tips for JSON Schema Validation. ←  Schema ValidationSpecify Allowed Field Values → On this page  * Context\n * Restrictions\n * Steps\n * Additional Information\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/schema-validation/use-json-schema-query-conditions/": " Docs Home → MongoDB Manual \nQUERY FOR AND MODIFY VALID OR INVALID DOCUMENTS \nOn this page    \n * Examples\n   \n * Define a Schema Object\n * Find Documents that Match the Schema\n * Find Documents that Don't Match the Schema\n * Update Documents that Don't Match the Schema\n * Delete Documents that Don't Match the Schema\n * Learn More If you add validation to your collection after you create it, or modify an existing validation schema, you may have invalid documents in your collection. Similarly, if your schema's validationAction is warn, your collection is allowed to contain invalid documents. You can query for invalid documents to potentially update or delete them from your collection. To find documents that either match or don't match a specified schema, use $jsonSchema with query operators. Similarly, you can update or delete documents based on a schema by using $jsonSchema in query conditions for write operations. \nEXAMPLES \nCreate a sample collection inventory with the following documents: db.inventory.insertMany( [   { item: \"journal\", qty: NumberInt(25), size: { h: 14, w: 21, uom: \"cm\" }, instock: true },   { item: \"notebook\", qty: NumberInt(50), size: { h: 8.5, w: 11, uom: \"in\" }, instock: true },   { item: \"paper\", qty: NumberInt(100), size: { h: 8.5, w: 11, uom: \"in\" }, instock: 1 },   { item: \"planner\", qty: NumberInt(75), size: { h: 22.85, w: 30, uom: \"cm\" }, instock: 1 },   { item: \"postcard\", qty: NumberInt(45), size: { h: 10, w: 15.25, uom: \"cm\" }, instock: true },   { item: \"apple\", qty: NumberInt(45), status: \"A\", instock: true },   { item: \"pears\", qty: NumberInt(50), status: \"A\", instock: true }] )  \nDEFINE A SCHEMA OBJECT \nDefine a sample schema object and store it in a variable called myschema: let myschema ={   $jsonSchema: {      required: [ \"item\", \"qty\", \"instock\" ],      properties: {         item: { bsonType: \"string\" },         qty: { bsonType: \"int\" },         size: {            bsonType: \"object\",            required: [ \"uom\" ],            properties: {               uom: { bsonType: \"string\" },               h: { bsonType: \"double\" },               w: { bsonType: \"double\" }            }          },          instock: { bsonType: \"bool\" }      }   }}  \nFIND DOCUMENTS THAT MATCH THE SCHEMA \nThese commands return all documents that match the schema: db.inventory.find(myschema)db.inventory.aggregate( [ { $match: myschema } ] )  Both commands return the same result: [  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a3\"),    item: 'apple',    qty: 45,    status: 'A',    instock: true  },  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a4\"),    item: 'pears',    qty: 50,    status: 'A',    instock: true  }] \nFIND DOCUMENTS THAT DON'T MATCH THE SCHEMA\n\n db.inventory.find( { $nor: [ myschema ] } )  Output: [  {    _id: ObjectId(\"62b5cd5a14b92d148400f79e\"),    item: 'journal',    qty: 25,    size: { h: 14, w: 21, uom: 'cm' },    instock: true  },  {    _id: ObjectId(\"62b5cd5a14b92d148400f79f\"),    item: 'notebook',    qty: 50,    size: { h: 8.5, w: 11, uom: 'in' },    instock: true  },  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a0\"),    item: 'paper',    qty: 100,    size: { h: 8.5, w: 11, uom: 'in' },    instock: 1  },  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a1\"),    item: 'planner',    qty: 75,    size: { h: 22.85, w: 30, uom: 'cm' },    instock: 1  },  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a2\"),    item: 'postcard',    qty: 45,    size: { h: 10, w: 15.25, uom: 'cm' },    instock: true  }] \nUPDATE DOCUMENTS THAT DON'T MATCH THE SCHEMA \nThis command updates all documents that don't match the schema and sets the documents' isValid field to false: db.inventory.updateMany(   {      $nor: [ myschema ]   },   {      $set: { isValid: false }   })  To verify the update, query the collection: db.inventory.find()  Output:\n\n \nDELETE DOCUMENTS THAT DON'T MATCH THE SCHEMA \nThis command deletes all documents that don't match the schema: db.inventory.deleteMany( { $nor: [ myschema ] } )  To verify the update, query the collection: db.inventory.find()  Output: [  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a3\"),    item: 'apple',    qty: 45,    status: 'A',    instock: true  },  {    _id: ObjectId(\"62b5cd5a14b92d148400f7a4\"),    item: 'pears',    qty: 50,    status: 'A',    instock: true  }] \nLEARN MORE \n     * Query and Projection Operators  * Specify Validation Level for Existing Documents ←  Choose How to Handle Invalid DocumentsBypass Schema Validation → On this page  * Examples\n * Define a Schema Object\n * Find Documents that Match the Schema\n * Find Documents that Don't Match the Schema\n * Update Documents that Don't Match the Schema\n * Delete Documents that Don't Match the Schema\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/core/timeseries/timeseries-limitations/": " Docs Home → MongoDB Manual \nTIME SERIES COLLECTION LIMITATIONS \nOn this page    \n * Unsupported Features\n   \n * Aggregation $out and $merge\n * Updates and Deletes\n * Time Series Secondary Indexes\n * Time Series Secondary Indexes with MongoDB 6.0 and Later\n * Time Series Secondary Indexes with MongoDB 5.0 and Earlier\n * Capped Collections\n * Modification of Collection Type\n * Modification of timeField and metaField\n * Modification of granularity\n * Sharding\n * Sharding Administration Commands\n * Shard Key Fields\n * Resharding\n * Transactions\n * View Limitations This page describes limitations on using time series collections. \nUNSUPPORTED FEATURES \nThe following features are not supported for time series collections:      * Atlas Search  * Change streams  * Client-Side Field Level Encryption  * Database Triggers  * GraphQL API  * Schema validation rules  * reIndex  * renameCollection Atlas Device Sync is only supported if the time series collections are asymmetrically synchronized. For details, see Enable Atlas Device Sync. \nAGGREGATION $OUT AND $MERGE \nYou cannot use the $out or $merge aggregation pipeline stages to add data from another collection to a time series collection. \nUPDATES AND DELETES \nStarting in MongoDB 5.1, you can perform some delete and update operations. Delete commands must meet the following requirements:  * You can only match on metaField field values.  * Your delete command must not limit the number of documents to be deleted. Set justOne: false or use the deleteMany() method. Update commands must meet the following requirements:  * You can only match on the metaField field value.  * You can only modify the metaField field value.  * Your update document can only contain update operator expressions.  * Your update command must not limit the number of documents to be updated. Set multi: true or use the updateMany() method.  * Your update command must not set upsert: true. In MongoDB 5.0, time series collections only support insert operations and read queries. Updates and manual delete operations result in an error. To automatically delete old data, set up automatic removal (TTL). To remove all documents from a collection, use the drop() method to drop the collection. \nTIME SERIES SECONDARY INDEXES \nThere is improved support for secondary indexes in MongoDB 6.0. \nTIME SERIES SECONDARY INDEXES WITH MONGODB 6.0 AND LATER \nStarting in MongoDB 6.0, you can add a secondary index to any field. These index types are not supported:  * Text indexes  * 2d indexes  * Unique indexes The TTL index property is not supported. For TTL deletion, use expireAfterSeconds. You can only use the multikey index type on the metaField. These index properties are partially supported. You can create:  * partial indexes on every field except metaField and timeField.  * sparse indexes on the metaField. For improvements to time series secondary indexes available starting in MongoDB 6.0, see Time Series Secondary Indexes in MongoDB 6.0. If there are secondary indexes on time series collections and you need to downgrade the Feature Compatibility Version (FCV), you must first drop any secondary indexes that are incompatible with the downgraded FCV. See setFeatureCompatibilityVersion. \nTIME SERIES SECONDARY INDEXES WITH MONGODB 5.0 AND EARLIER \nIn MongoDB 5.0 and earlier:  * The metaField can have secondary indexes.  * The timeField can have secondary indexes.  * If the metaField is a document, you can add secondary indexes on fields inside the document. \nTIP \nSEE ALSO: Indexes \nCAPPED COLLECTIONS \nYou cannot create a time series collection as a capped collection. \nMODIFICATION OF COLLECTION TYPE \nYou can only set the collection type when you create a collection:  * An existing collection cannot be converted into a time series collection.  * A time series collection cannot be converted into a different collection type. To move data from an existing collection to a time series collection, migrate data into a time series collection. \nMODIFICATION OF TIMEFIELD AND METAFIELD \nYou can only set a collection's timeField and metaField parameters when you create the collection. You cannot modify these parameters later. \nMODIFICATION OF GRANULARITY \nAfter you set the granularity, you can only increase it one level at a time. The granularity can change from \"seconds\" to \"minutes\" or from \"minutes\" to \"hours\". Other changes are not allowed.\n\n \nSHARDING \nStarting in MongoDB 5.1 (and 5.0.6), you can create sharded time series collections. In versions earlier than MongoDB 5.0.6, you cannot shard time series collections. \nSHARDING ADMINISTRATION COMMANDS \nStarting in MongoDB 5.2 (and 5.1.2, 5.0.6), you can run sharding administration commands (such as moveChunk) on the system.buckets collection. In versions earlier than MongoDB 5.0.6, you cannot run sharding administration commands for sharded time series collections. \nSHARD KEY FIELDS \nWhen sharding time series collections, you can only specify the following fields in the shard key:  * The metaField  * Sub-fields of metaField  * The timeField You may specify combinations of these fields in the shard key. No other fields, including _id, are allowed in the shard key pattern. When you specify the shard key:  * metaField can be either a:\n   \n   * Hashed shard key\n   \n   * Ranged shard key  * timeField must be:\n   \n   * A ranged shard key\n   \n   * At the end of the shard key pattern \nTIP Avoid specifying only the timeField as the shard key. Since the timeField increases monotonically, it may result in all writes appearing on a single chunk within the cluster. Ideally, data is evenly distributed across chunks. To learn how to best choose a shard key, see:  * Choose a Shard Key  * MongoDB Blog: On Selecting a Shard Key for MongoDB. \nRESHARDING \nYou cannot reshard sharded time series collections. \nTRANSACTIONS \nYou cannot write to time series collections in transactions. \nNOTE Reads from time series collections are supported in transactions. \nVIEW LIMITATIONS \nTime series collections are writable non-materialized views. Limitations for views apply to time series collections.  * You cannot create a view from a time series bucket collection namespace (namely, a collection prefixed with system.buckets). ←  Time Series CompressionTransactions → On this page  * Unsupported Features\n * Aggregation $out and $merge\n * Updates and Deletes\n * Time Series Secondary Indexes\n * Time Series Secondary Indexes with MongoDB 6.0 and Later\n * Time Series Secondary Indexes with MongoDB 5.0 and Earlier\n * Capped Collections\n * Modification of Collection Type\n * Modification of timeField and metaField\n * Modification of granularity\n * Sharding\n * Sharding Administration Commands\n * Shard Key Fields\n * Resharding\n * Transactions\n * View Limitations Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/mongo/": " Docs Home → MongoDB Manual \nLEGACY MONGO SHELL  WARNING The mongo shell has been deprecated in MongoDB v5.0. The replacement is mongosh. Older mongo shell documentation is included with the corresponding documentation for that MongoDB release. Quick Links to prior versions      * mongo shell v4.4  * mongo shell v4.2  * mongo shell v4.0 ←  System CollectionsRelease Notes → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/read-concern-snapshot/": " Docs Home → MongoDB Manual \"snapshot\"  READ CONCERN \"SNAPSHOT\" \nOn this page    \n * Operations\n   \n * Read Concern and Transactions\n * Read Concern and atClusterTime\n * Read Concern on Capped Collections New in version 4.0. Changed in version 5.0. A query with read concern \"snapshot\" returns majority-committed data as it appears across shards from a specific single point in time in the recent past. Read concern \"snapshot\" provides its guarantees only if the transaction commits with write concern \"majority\". Read concern \"snapshot\" is available for multi-document transactions, and starting in MongoDB 5.0, certain read operations outside of multi-document transactions.      * If the transaction is not part of a causally consistent session, upon transaction commit with write concern \"majority\", the transaction operations are guaranteed to have read from a snapshot of majority-committed data.  * If the transaction is part of a causally consistent session, upon transaction commit with write concern \"majority\", the transaction operations are guaranteed to have read from a snapshot of majority-committed data that provides causal consistency with the operation immediately preceding the transaction start. Outside of multi-document transactions, read concern \"snapshot\" is available on primaries and secondaries for the following read operations:  * find  * aggregate  * distinct (on unsharded collections) All other read commands prohibit \"snapshot\". \nOPERATIONS \nFor a list of all operations that accept read concerns, see Operations That Support Read Concern. \nREAD CONCERN AND TRANSACTIONS \nMulti-document transactions support read concern \"snapshot\" as well as \"local\", and \"majority\". \nNOTE You set the read concern at the transaction level, not at the individual operation level. To set the read concern for transactions, see Transactions and Read Concern. \nREAD CONCERN AND ATCLUSTERTIME \nNew in version 5.0. Outside of multi-document transactions, reads with read concern \"snapshot\" support the optional parameter atClusterTime. The parameter atClusterTime allows you to specify the timestamp for the read. To satisfy a read request with a specified atClusterTime of T, the mongod performs the request based on the data available at time T. You can obtain the operationTime or clusterTime of an operation from the response of db.runCommand() or from the Session() object. The following command performs a find operation with read concern \"snapshot\" and specifies that the operation should read data from the snapshot at cluster time Timestamp(1613577600, 1). db.runCommand( {    find: \"restaurants\",    filter: { _id: 5 },    readConcern: {        level: \"snapshot\",        atClusterTime: Timestamp(1613577600, 1)    },} )  If the parameter atClusterTime is not supplied, the mongos, or in single member replica sets the mongod, selects the timestamp of the latest majority-committed snapshot as the atClusterTime and returns it to the client. Outside of transactions, \"snapshot\" reads are guaranteed to read from majority-committed data. \nATCLUSTERTIME CONSIDERATIONS AND BEHAVIOR \n * The allowed values for atClusterTime depend on the minSnapshotHistoryWindowInSeconds parameter. minSnapshotHistoryWindowInSeconds is the minimum time window in seconds for which the storage engine keeps the snapshot history. If you specify an atClusterTime value older than the oldest snapshot retained according to minSnapshotHistoryWindowInSeconds, mongod returns an error.  * If you perform a read operation with \"snapshot\" against a delayed replica set member, the returned majority-committed data could be stale.  * It is not possible to specify atClusterTime for \"snapshot\" inside of causally consistent sessions. \nREAD CONCERN ON CAPPED COLLECTIONS \nStarting in version 5.0, you cannot use read concern \"snapshot\" when reading from a capped collection. ←  Read Concern \"linearizable\"Write Concern → On this page  * Operations\n * Read Concern and Transactions\n * Read Concern and atClusterTime\n * Read Concern on Capped Collections Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/geojson/": " Docs Home → MongoDB Manual \nGEOJSON OBJECTS \nOn this page    \n * Overview\n   \n * Point\n * LineString\n * Polygon\n * MultiPoint\n * MultiLineString\n * MultiPolygon\n * GeometryCollection \nOVERVIEW \nMongoDB supports the GeoJSON object types listed on this page. To specify GeoJSON data, use an embedded document with:      * a field named type that specifies the GeoJSON object type and  * a field named coordinates that specifies the object's coordinates.\n   \n   \n   IMPORTANT\n   \n   If specifying latitude and longitude coordinates, list the longitude first, and then latitude.\n   \n    * Valid longitude values are between -180 and 180, both inclusive.\n   \n    * Valid latitude values are between -90 and 90, both inclusive. <field>: { type: <GeoJSON type> , coordinates: <coordinates> }  MongoDB geospatial queries on GeoJSON objects calculate on a sphere; MongoDB uses the WGS84 reference system for geospatial queries on GeoJSON objects. \nPOINT \nThe following example specifies a GeoJSON Point: { type: \"Point\", coordinates: [ 40, 5 ] }  \nLINESTRING \nThe following example specifies a GeoJSON LineString: { type: \"LineString\", coordinates: [ [ 40, 5 ], [ 41, 6 ] ] }  \nPOLYGON \nPolygons consist of an array of GeoJSON LinearRing coordinate arrays. These LinearRings are closed LineStrings. Closed LineStrings have at least four coordinate pairs and specify the same position as the first and last coordinates. The line that joins two points on a curved surface may or may not contain the same set of co-ordinates that joins those two points on a flat surface. The line that joins two points on a curved surface will be a geodesic. Carefully check points to avoid errors with shared edges, as well as overlaps and other types of intersections. \nPOLYGONS WITH A SINGLE RING \nThe following example specifies a GeoJSON Polygon with an exterior ring and no interior rings (or holes). The first and last coordinates must match in order to close the polygon: {  type: \"Polygon\",  coordinates: [ [ [ 0 , 0 ] , [ 3 , 6 ] , [ 6 , 1 ] , [ 0 , 0  ] ] ]}  For Polygons with a single ring, the ring cannot self-intersect. \nPOLYGONS WITH MULTIPLE RINGS \nFor Polygons with multiple rings:  * The first described ring must be the exterior ring.  * The exterior ring cannot self-intersect.  * Any interior ring must be entirely contained by the outer ring.  * Interior rings cannot intersect or overlap each other. Interior rings cannot share an edge. The following example represents a GeoJSON polygon with an interior ring: {  type : \"Polygon\",  coordinates : [     [ [ 0 , 0 ] , [ 3 , 6 ] , [ 6 , 1 ] , [ 0 , 0 ] ],     [ [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 2 ] , [ 2 , 2 ] ]  ]}   MULTIPOINT \nRequires Versions GeoJSON MultiPoint embedded documents encode a list of points. {  type: \"MultiPoint\",  coordinates: [     [ -73.9580, 40.8003 ],     [ -73.9498, 40.7968 ],     [ -73.9737, 40.7648 ],     [ -73.9814, 40.7681 ]  ]}  \nMULTILINESTRING \nRequires Versions The following example specifies a GeoJSON MultiLineString: {  type: \"MultiLineString\",  coordinates: [     [ [ -73.96943, 40.78519 ], [ -73.96082, 40.78095 ] ],     [ [ -73.96415, 40.79229 ], [ -73.95544, 40.78854 ] ],     [ [ -73.97162, 40.78205 ], [ -73.96374, 40.77715 ] ],     [ [ -73.97880, 40.77247 ], [ -73.97036, 40.76811 ] ]  ]}  \nMULTIPOLYGON \nRequires Versions The following example specifies a GeoJSON MultiPolygon:\n\n  \nGEOMETRYCOLLECTION \nRequires Versions The following example stores coordinates of GeoJSON type GeometryCollection: {  type: \"GeometryCollection\",  geometries: [     {       type: \"MultiPoint\",       coordinates: [          [ -73.9580, 40.8003 ],          [ -73.9498, 40.7968 ],          [ -73.9737, 40.7648 ],          [ -73.9814, 40.7681 ]       ]     },     {       type: \"MultiLineString\",       coordinates: [          [ [ -73.96943, 40.78519 ], [ -73.96082, 40.78095 ] ],          [ [ -73.96415, 40.79229 ], [ -73.95544, 40.78854 ] ],          [ [ -73.97162, 40.78205 ], [ -73.96374, 40.77715 ] ],          [ [ -73.97880, 40.77247 ], [ -73.97036, 40.76811 ] ]       ]     }  ]} \n←  Find Restaurants with Geospatial QueriesRead Concern → On this page  * Overview\n * Point\n * LineString\n * Polygon\n * MultiPoint\n * MultiLineString\n * MultiPolygon\n * GeometryCollection Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/aggregation-variables/": " Docs Home → MongoDB Manual \nVARIABLES IN AGGREGATION EXPRESSIONS \nOn this page    \n * User Variables\n   \n * System Variables Aggregation expressions can use both user-defined and system variables. Variables can hold any BSON type data. To access the value of the variable, prefix the variable name with double dollar signs ($$); i.e. \"$$<variable>\". If the variable references an object, to access a specific field in the object, use the dot notation; i.e. \"$$<variable>.<field>\". \nUSER VARIABLES \nUser variable names can contain the ascii characters [_a-zA-Z0-9] and any non-ascii character. User variable names must begin with a lowercase ascii letter [a-z] or a non-ascii character. \nSYSTEM VARIABLES \nMongoDB offers the following system variables: Variable\nDescription\nNOW \nA variable that returns the current datetime value. NOW returns the same value for all members of the deployment and remains the same throughout all stages of the aggregation pipeline. New in version 4.2. CLUSTER_TIME \nA variable that returns the current timestamp value. CLUSTER_TIME is only available on replica sets and sharded clusters. CLUSTER_TIME returns the same value for all members of the deployment and remains the same throughout all stages of the pipeline. New in version 4.2. ROOT References the root document, i.e. the top-level document, currently being processed in the aggregation pipeline stage.\nCURRENT \nReferences the start of the field path being processed in the aggregation pipeline stage. Unless documented otherwise, all stages start with CURRENT the same as ROOT. CURRENT is modifiable. However, since $<field> is equivalent to $$CURRENT.<field>, rebinding CURRENT changes the meaning of $ accesses. REMOVE \nA variable which evaluates to the missing value. Allows for the conditional exclusion of fields. In a $project, a field set to the variable REMOVE is excluded from the output. For an example of its usage, see Conditionally Exclude Fields. DESCEND One of the allowed results of a $redact expression.\nPRUNE One of the allowed results of a $redact expression.\nKEEP One of the allowed results of a $redact expression.\nSEARCH_META \nA variable that stores the metadata results of an Atlas Search query. In all supported aggregation pipeline stages, a field set to the variable $$SEARCH_META returns the metadata results for the query. For an example of its usage, see Atlas Search facet and count. \nTIP \nSEE ALSO:      * $let  * $redact  * $map ←  Aggregation Commands ComparisonSQL to Aggregation Mapping Chart → On this page  * User Variables\n * System Variables Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/system-roles-collection/": " Docs Home → MongoDB Manual \nSYSTEM.ROLES COLLECTION \nOn this page    \n * system.roles Schema\n   \n * Examples The system.roles collection in the admin database stores the user-defined roles. To create and manage these user-defined roles, MongoDB provides role management commands. \nSYSTEM.ROLES SCHEMA \nThe documents in the system.roles collection have the following schema: {  _id: <system-defined id>,  role: \"<role name>\",  db: \"<database>\",  privileges:      [          {              resource: { <resource> },              actions: [ \"<action>\", ... ]          },          ...      ],  roles:      [          { role: \"<role name>\", db: \"<database>\" },          ...      ]}  A system.roles document has the following fields: admin.system.roles.role \nThe role field is a string that specifies the name of the role. admin.system.roles.db \nThe db field is a string that specifies the database to which the role belongs. MongoDB uniquely identifies each role by the pairing of its name (i.e. role) and its database. admin.system.roles.privileges \nThe privileges array contains the privilege documents that define the privileges for the role. A privilege document has the following syntax: {  resource: { <resource> },  actions: [ \"<action>\", ... ]}  Each privilege document has the following fields: admin.system.roles.privileges[n].resource \nA document that specifies the resources upon which the privilege actions apply. The document has one of the following form: { db: <database>, collection: <collection> }  or { cluster : true }  See Resource Document for more details. admin.system.roles.privileges[n].actions \nAn array of actions permitted on the resource. For a list of actions, see Privilege Actions. admin.system.roles.roles \nThe roles array contains role documents that specify the roles from which this role inherits privileges. A role document has the following syntax: { role: \"<role name>\", db: \"<database>\" }  A role document has the following fields: admin.system.roles.roles[n].role \nThe name of the role. A role can be a built-in role provided by MongoDB or a user-defined role. admin.system.roles.roles[n].db \nThe name of the database where the role is defined. \nEXAMPLES \nConsider the following sample documents found in system.roles collection of the admin database. \nA USER-DEFINED ROLE SPECIFIES PRIVILEGES \nThe following is a sample document for a user-defined role appUser defined for the myApp database: {  _id: \"myApp.appUser\",  role: \"appUser\",  db: \"myApp\",  privileges: [       { resource: { db: \"myApp\" , collection: \"\" },         actions: [ \"find\", \"createCollection\", \"dbStats\", \"collStats\" ] },       { resource: { db: \"myApp\", collection: \"logs\" },         actions: [ \"insert\" ] },       { resource: { db: \"myApp\", collection: \"data\" },         actions: [ \"insert\", \"update\", \"remove\", \"compact\" ] },       { resource: { db: \"myApp\", collection: \"system.js\" },         actions: [ \"find\" ] },  ],  roles: []}  The privileges array lists the five privileges that the appUser role specifies:      * The first privilege permits its actions ( \"find\", \"createCollection\", \"dbStats\", \"collStats\") on all the collections in the myApp database excluding its system collections. See Specify a Database as Resource.  * The next two privileges permits additional actions on specific collections, logs and data, in the myApp database. See Specify a Collection of a Database as Resource.\n\n As indicated by the empty roles array, appUser inherits no additional privileges from other roles. \nUSER-DEFINED ROLE INHERITS FROM OTHER ROLES \nThe following is a sample document for a user-defined role appAdmin defined for the myApp database: The document shows that the appAdmin role specifies privileges as well as inherits privileges from other roles: {  _id: \"myApp.appAdmin\",  role: \"appAdmin\",  db: \"myApp\",  privileges: [      {         resource: { db: \"myApp\", collection: \"\" },         actions: [ \"insert\", \"dbStats\", \"collStats\", \"compact\" ]      }  ],  roles: [      { role: \"appUser\", db: \"myApp\" }  ]}  The privileges array lists the privileges that the appAdmin role specifies. This role has a single privilege that permits its actions ( \"insert\", \"dbStats\", \"collStats\", \"compact\") on all the collections in the myApp database excluding its system collections. See Specify a Database as Resource. The roles array lists the roles, identified by the role names and databases, from which the role appAdmin inherits privileges. ←  Security Referencesystem.users Collection → On this page  * system.roles Schema\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/replica-configuration/": " Docs Home → MongoDB Manual \nREPLICA SET CONFIGURATION \nOn this page    \n * Replica Set Configuration Document Example\n   \n * Replica Set Configuration Fields You can access the configuration of a replica set using the rs.conf() method or the replSetGetConfig command. To modify the configuration for a replica set, use the rs.reconfig() method, passing a configuration document to the method. See rs.reconfig() for more information. \nWARNING Avoid reconfiguring replica sets that contain members of different MongoDB versions as validation rules may differ across MongoDB versions. \nREPLICA SET CONFIGURATION DOCUMENT EXAMPLE \nThe following document provides a representation of a replica set configuration document. The configuration of your replica set may include only a subset of these settings: {  _id: <string>,  version: <int>,  term: <int>,  protocolVersion: <number>,  writeConcernMajorityJournalDefault: <boolean>,  configsvr: <boolean>,  members: [    {      _id: <int>,      host: <string>,      arbiterOnly: <boolean>,      buildIndexes: <boolean>,      hidden: <boolean>,      priority: <number>,      tags: <document>,      secondaryDelaySecs: <int>,      votes: <number>    },    ...  ],  settings: {    chainingAllowed : <boolean>,    heartbeatIntervalMillis : <int>,    heartbeatTimeoutSecs: <int>,    electionTimeoutMillis : <int>,    catchUpTimeoutMillis : <int>,    getLastErrorModes : <document>,    getLastErrorDefaults : <document>,    replicaSetId: <ObjectId>  }}  \nREPLICA SET CONFIGURATION FIELDS \n_id \nType: string The name of the replica set. _id must be identical to the replication.replSetName or the value of --replSet specified to mongod on the command line. \nTIP \nSEE: \nversion \nType: int An incrementing number used to distinguish revisions of the replica set configuration document from previous iterations of the configuration. Changed in version 4.4: Replica set members use term and version to achieve consensus on the \"newest\" replica configuration. When members compare replica configuration documents, the configuration document with a larger term is considered the \"newest\". If term is the same or absent, the configuration document with the larger version is considered \"newest\". term \nType: int New in version 4.4. Only available with featureCompatibilityVersion (fCV) \"4.4\" or later. An incrementing number used to distinguish revisions of the replica set configuration document from previous iterations of the configuration. The term of a configuration document matches the term of the replica set primary which performed the reconfiguration. The primary increments its term each time it steps up after winning an election. The primary ignores the term field if set explicitly in the replSetReconfig operation. Issuing a force reconfiguration removes the term field. When the primary next issues replSetReconfig without force, it sets the term to its own term. Replica set members use term and version to achieve consensus on the \"newest\" replica configuration. When members compare replica configuration documents, the configuration document with a larger term is considered the \"newest\". If term is the same or absent, the configuration document with the larger version is considered \"newest\". configsvr \nType: boolean Default: false Indicates whether the replica set is used for a sharded cluster's config servers. Set to true if the replica set is for a sharded cluster's config servers. \nTIP \nSEE ALSO: Sharded Cluster Enhancements protocolVersion \nType: number Default: 1 Starting in 4.0, MongoDB only supports protocolVersion: 1 and no longer supports protocolVersion: 0. \nTIP \nSEE ALSO: Replica Set Protocol Version writeConcernMajorityJournalDefault \nType: boolean Default: true Determines the behavior of { w: \"majority\" } write concern if the write concern does not explicitly specify the journal option j. The following table lists the writeConcernMajorityJournalDefault values and the associated { w: \"majority\" } behavior: Value\n{ w: \"majority\" } Behavior\ntrue MongoDB acknowledges the write operation after a majority of the voting members have written to the on-disk journal.\n\n All voting members of the replica set must run with journaling when writeConcernMajorityJournalDefault is true. If any voting member of a replica set uses the in-memory storage engine, you must set writeConcernMajorityJournalDefault to false. If any voting member of a replica set uses the in-memory storage engine and writeConcernMajorityJournalDefault is true, \"majority\" write operations may fail. These include operations that inherently use \"majority\" write concern, such as the replSetStepDown command, or various mongosh methods that by default use \"majority\" write concern, such as user management methods and role management methods. Starting in version 4.2 (and 4.0.13 and 3.6.14 ), if a replica set member uses the in-memory storage engine (voting or non-voting) but the replica set has writeConcernMajorityJournalDefault set to true, the replica set member logs a startup warning. false MongoDB acknowledges the write operation after a majority of the voting members have applied the operation in memory. \nWARNING If any voting member of a replica set uses the in-memory storage engine, you must set writeConcernMajorityJournalDefault to false. Starting in version 4.2 (and 4.0.13 and 3.6.14 ), if a replica set member uses the in-memory storage engine (voting or non-voting) but the replica set has writeConcernMajorityJournalDefault set to true, the replica set member logs a startup warning. You cannot run transactions on a sharded cluster that has a shard with writeConcernMajorityJournalDefault set to false (such as a shard with a voting member that uses the in-memory storage engine). \nTIP \nSEE ALSO:      * Acknowledgment Behavior  * Replica Set Protocol Version \nMEMBERS \nmembers \nType: array An array of member configuration documents, one for each member of the replica set. The members array is a zero-indexed array. Each member-specific configuration document can contain the following fields: members[n]._id \nType: integer An integer identifier for the member in the replica set, unique among all members. Starting in MongoDB 5.0, values may be any integer value greater than or equal to 0. Previously, this value was limited to an integer between 0 and 255 inclusive. Each replica set member must have a unique _id. Avoid re-using _id values even if no members[n] entry is using that _id in the current configuration. Once set, you cannot change the _id of a member. \nNOTE When updating the replica configuration object, access the replica set members in the members array with the array index. The array index begins with 0. Do not confuse this index value with the value of the members[n]._id field in each document in the members array. members[n].host \nType: string The hostname and, if specified, the port number, of the set member. The hostname name must be resolvable for every host in the replica set. \nWARNING members[n].host cannot hold a value that resolves to localhost or the local interface unless all members of the set are on hosts that resolve to localhost. members[n].arbiterOnly \nOptional. Type: boolean Default: false A boolean that identifies an arbiter. A value of true indicates that the member is an arbiter. When using the rs.addArb() method to add an arbiter, the method automatically sets members[n].arbiterOnly to true for the added member. For the following MongoDB versions, pv1 increases the likelihood of w:1 rollbacks compared to pv0 (no longer supported in MongoDB 4.0+) for replica sets with arbiters:  * MongoDB 3.4.1  * MongoDB 3.4.0  * MongoDB 3.2.11 or earlier See Replica Set Protocol Version. members[n].buildIndexes \nOptional. Type: boolean Default: true A boolean that indicates whether the mongod builds indexes on this member. You can only set this value when adding a member to a replica set. You cannot change members[n].buildIndexes field after the member has been added to the set. To add a member, see rs.add() and rs.reconfig(). Do not set to false for mongod instances that receive queries from clients. Setting buildIndexes to false may be useful if all the following conditions are true:  * you are only using this instance to perform backups using mongodump, and  * this member will receive no queries, and  * index creation and maintenance overburdens the host system.\n\n \nWARNING If you set members[n].buildIndexes to false, you must also set members[n].priority to 0. If members[n].priority is not 0, MongoDB will return an error when attempting to add a member with members[n].buildIndexes equal to false. To ensure the member receives no queries, you should make all instances that do not build indexes hidden. Other secondaries cannot replicate from a member where members[n].buildIndexes is false. members[n].hidden \nOptional. Type: boolean Default: false When this value is true, the replica set hides this instance and does not include the member in the output of db.hello() or hello. This prevents read operations (i.e. queries) from ever reaching this host by way of secondary read preference. Hidden members can acknowledge write operations issued with Write Concern. For write operations issued with \"majority\" write concern, the member must also be a voting member (i.e. votes is greater than 0). \nTIP \nSEE ALSO: Hidden Replica Set Members members[n].priority \nChanged in version 3.6: Starting in MongoDB 3.6, arbiters have the priority 0. If an arbiter has a priority of 1, MongoDB 3.6 reconfigures the arbiter to have a priority of 0. Optional. Type: Number between 0 and 1000 for primary/secondary; 0 or 1 for arbiters. Default: 1.0 for primary/secondary; 0 for arbiters. A number that indicates the relative eligibility of a member to become a primary. Specify higher values to make a member more eligible to become primary, and lower values to make the member less eligible. A member with a members[n].priority of 0 is ineligible to become primary. Members with priority greater than 0 cannot have 0 votes. Changing the balance of priority in a replica set will trigger one or more elections. If a lower priority secondary is elected over a higher priority secondary, replica set members will continue to call elections until the highest priority available member becomes primary. Members with priority of 0 can acknowledge write operations issued with Write Concern. For write operations issued with \"majority\" write concern, the member must also be a voting member (i.e. votes is greater than 0). \nTIP \nSEE ALSO: Replica Set Elections. members[n].tags \nOptional. Type: document Default: none A tags document contains user-defined tag field and value pairs for the replica set member. { \"<tag1>\": \"<string1>\", \"<tag2>\": \"<string2>\",... }  * For read operations, you can specify a tag set in the read preference to direct the operations to replica set member(s) with the specified tag(s).  * For write operations, you can create a customize write concern using settings.getLastErrorModes and settings.getLastErrorDefaults. For more information, see Configure Replica Set Tag Sets. members[n].secondaryDelaySecs \nOptional. Type: integer Default: 0 The number of seconds \"behind\" the primary that this replica set member should \"lag\". Use this option to create delayed members. Delayed members maintain a copy of the data that reflects the state of the data at some time in the past. Delayed members can contribute to acknowledging write operations issued with Write Concern. However, they return write acknowledgment no earlier than the configured delay value. For write operations issued with \"majority\" write concern, the member must also be a voting member (i.e. votes is greater than 0). \nTIP \nSEE ALSO: Delayed Replica Set Members members[n].votes \nOptional. Type: integer Default: 1 The number of votes a server will cast in a replica set election. The number of votes each member has is either 1 or 0, and arbiters always have exactly 1 vote. Members with priority greater than 0 cannot have 0 votes. A replica set can have up to 50 members but only 7 voting members. If you need more than 7 members in one replica set, set members[n].votes to 0 for the additional non-voting members. Non-voting (i.e. votes is 0) members must have priority of 0. Starting in MongoDB 5.0, a newly added secondary does not count as a voting member and cannot be elected until it has reached SECONDARY state. Non-voting members cannot acknowledge write operations issued with a \"majority\" write concern. \nTIP \nSEE ALSO:  * replSetGetStatus.votingMembersCount  * replSetGetStatus.writableVotingMembersCount \nSETTINGS \nsettings \nOptional. Type: document A document that contains configuration options that apply to the whole replica set. The settings document contain the following fields: settings.chainingAllowed \nOptional. Type: boolean\n\n In MongoDB 5.0.1, 4.2.15, 4.4.7, and earlier, if settings.chainingAllowed is:  * true, replica set secondary members can replicate data from other secondary members.  * false, secondary members can replicate data only from the primary. Starting in MongoDB 5.0.2, 4.2.16, and 4.4.8:  * Replica set secondary members can replicate data from other secondary members even if settings.chainingAllowed is false.  * To override settings.chainingAllowed, set the enableOverrideClusterChainingSetting server parameter to true.  * The default for enableOverrideClusterChainingSetting is false. \nTIP \nSEE ALSO: Manage Chained Replication settings.getLastErrorDefaults \nOptional. Type: document Unavailable starting in MongoDB 5.0. \nIMPORTANT Starting in MongoDB 5.0, you cannot specify a default write concern with settings.getLastErrorDefaults other than the default of { w: 1, wtimeout: 0 } . Instead, use the setDefaultRWConcern command to set the default read or write concern configuration for a replica set or sharded cluster. settings.getLastErrorModes \nOptional. Type: document A document used to define a custom write concern through the use of members[n].tags. The custom write concern can provide data-center awareness. { getLastErrorModes: {   <name of write concern> : { <tag1>: <number>, .... },   ...} }  The <number> refers to the number of different tag values required to satisfy the write concern. For example, the following settings.getLastErrorModes defines a write concern named datacenter that requires the write to propagate to two members whose dc tag values differ. { getLastErrorModes: { datacenter: { \"dc\": 2 } } }  To use the custom write concern, pass in the write concern name to the w Option, e.g. { w: \"datacenter\" }  See Configure Replica Set Tag Sets for more information and example. settings.heartbeatTimeoutSecs \nOptional. Type: int Default: 10 Number of seconds that the replica set members wait for a successful heartbeat from each other. If a member does not respond in time, other members mark the delinquent member as inaccessible. \nNOTE For pv1, settings.electionTimeoutMillis has a greater influence on whether the secondary members call for an election than the settings.heartbeatTimeoutSecs. settings.electionTimeoutMillis \nOptional. Type: int Default: 10000 (10 seconds) The time limit in milliseconds for detecting when a replica set's primary is unreachable:  * Higher values result in slower failovers but decreased sensitivity to primary node or network slowness or spottiness.  * Lower values result in faster failover, but increased sensitivity to primary node or network slowness or spottiness. The setting only applies when using protocolVersion: 1. \nNOTE Changed in version 4.0.2: If the parameter enableElectionHandoff is true (default), when a primary steps down from rs.stepDown() (or the replSetStepDown command without the force: true), the stepped-down primary nominates an eligible secondary to call an election immediately. Otherwise, secondaries can wait up to settings.electionTimeoutMillis before calling an election. The stepped down primary does not wait for the effects of the handoff. For more information, see enableElectionHandoff. settings.catchUpTimeoutMillis \nOptional. Type: int Changed in version 3.6: Default: -1, infinite catchup time. Time limit in milliseconds for a newly elected primary to sync (catch up) with the other replica set members that may have more recent writes. Infinite or high time limits may reduce the amount of data that the other members would need to roll back after an election but may increase the failover time. The newly elected primary ends the catchup period early once it is fully caught up with other members of the set. During the catchup period, the newly elected primary is unavailable for writes from clients. Use replSetAbortPrimaryCatchUp to abort the catchup then complete the transition to primary. The setting only applies when using protocolVersion: 1. \nNOTE To downgrade a replica set initiated in version 3.6 to 3.4, change catchUpTimeoutMillis from -1 to a positive number. Failure to change this value to a positive number causes nodes running version 3.4 to neither restart nor join the replica set. settings.catchUpTakeoverDelayMillis \nOptional. Type: int Default: 30000 (30 seconds)\n\n After the node initiating the takeover determines that it is ahead of the current primary, it waits the specified number of milliseconds and then verifies the following:  1. It is still ahead of the current primary,  2. It is the most up-to-date node among all available nodes,  3. The current primary is currently catching up to it. Once determining that all of these conditions are met, the node initiating the takeover immediately runs for election. For more information on Replica Set Elections, see Replica Set Elections. \nNOTE Setting catchUpTakeoverDelayMillis to -1 disables catchup takeover. Setting catchUpTimeoutMillis to 0 disables primary catchup and consequently also catchup takeover. settings.heartbeatIntervalMillis \nInternal use only. The frequency in milliseconds of the heartbeats. settings.replicaSetId \nType: ObjectId The ObjectId associated with the replica set and automatically created during rs.initiate() or replSetInitiate. You cannot change the replicaSetId. ←  Replication ReferenceReplica Set Protocol Version → On this page  * Replica Set Configuration Document Example\n * Replica Set Configuration Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/config-database/": " Docs Home → MongoDB Manual \nCONFIG DATABASE \nOn this page    \n * Restrictions\n   \n * Collections to Support Sharded Cluster Operations\n * Collections to Support Sessions The collections in the config database support:      * Sharded cluster operations  * Causally consistent sessions for standalones, replica sets, and sharded clusters and retryable writes for replica sets and sharded clusters. \nRESTRICTIONS \nStarting in MongoDB 5.0, non-transaction reads are not allowed on the config.transactions collection with the following read concerns and options:  * \"snapshot\"  * \"majority\" and the afterClusterTime option is set  * When using a MongoDB Driver and \"majority\" within a causally consistent session \nIMPORTANT The schema of the config database is internal and may change between releases of MongoDB. The config database is not a dependable API, and users should not write data to the config database in the course of normal operation or maintenance. \nNOTE You cannot perform read/write operations to the collections in the config database inside a multi-document transaction. \nCOLLECTIONS TO SUPPORT SHARDED CLUSTER OPERATIONS \nTo access the config database and view the list of collections that support sharding operations, connect mongosh to a mongos instance in a sharded cluster and issue the following: use config\nshow collections  \nNOTE If running with access control, ensure you have privileges that grant listCollections action on the database. The config database is mainly for internal use, and during normal operations you should never manually insert or store data in it. However, if you need to verify the write availability of the config server for a sharded cluster, you can insert a document into a test collection (after making sure that no collection of that name already exists): \nWARNING Modification of the config database on a functioning system may lead to instability or inconsistent data sets. If you must modify the config database, use mongodump to create a full backup of the config database. db.testConfigServerWriteAvail.insertOne( { a : 1 } )  If the operation succeeds, the config server is available to process writes. Future releases of the server may include different collections in the config database, so be careful when selecting a name for your test collection. MongoDB uses the following collections in the config database to support sharding: config.changelog  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The changelog collection stores a document for each change to the metadata of a sharded collection. \nEXAMPLE The following example displays a single record of a chunk split from a changelog collection:\n\n  Each document in the changelog collection contains the following fields: config.changelog._id \nThe value of changelog._id is: <hostname>-<timestamp>-<increment>. config.changelog.server \nThe hostname of the server that holds this data. config.changelog.clientAddr \nA string that holds the address of the client, a mongos instance that initiates this change. config.changelog.time \nA ISODate timestamp that reflects when the change occurred. config.changelog.what \nReflects the type of change recorded. Possible values include:  * dropCollection  * dropCollection.start  * dropDatabase  * dropDatabase.start  * moveChunk.start  * moveChunk.commit  * split  * multi-split config.changelog.ns \nNamespace where the change occurred. config.changelog.details \nA document that contains additional details regarding the change. The structure of the details document depends on the type of change. config.chunks  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The chunks collection stores a document for each chunk in the cluster. Consider the following example of a document for a chunk named mydb.foo-a_\\\"cat\\\": {   \"_id\" : \"mydb.foo-a_\\\"cat\\\"\",   \"lastmod\" : Timestamp(2, 1),   \"uuid\": \"c025d039-e626-435e-b2d2-c1d436038041\",   \"min\" : {         \"animal\" : \"cat\"   },   \"max\" : {         \"animal\" : \"dog\"   },   \"shard\" : \"shard0004\",   \"history\" : [ { \"validAfter\" : Timestamp(1569368571, 27), \"shard\" : \"shard0004\" } ]}  These documents store the range of values for the shard key that describe the chunk in the min and max fields. Additionally the shard field identifies the shard in the cluster that \"owns\" the chunk. config.collections  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The collections collection stores a document for each sharded collection in the cluster. Given a collection named pets in the records database, a document in the collections collection would resemble the following: {   \"_id\" : \"records.pets\",   \"lastmod\" : ISODate(\"2021-07-21T15:48:15.193Z\"),   \"timestamp\": Timestamp(1626882495, 1),   \"key\" : {         \"a\" : 1   },   \"unique\" : false,   \"lastmodEpoch\" : ObjectId(\"5078407bd58b175c5c225fdc\"),   \"uuid\" :  UUID(\"f8669e52-5c1b-4ea2-bbdc-a00189b341da\")} \nconfig.databases  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The databases collection stores a document for each database in the cluster. For each database, the corresponding document displays the name, the database's primary shard , the database's sharding enabled status, and a version.\n\n  The method sh.status() returns this information in the Databases section. config.lockpings  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The lockpings collection keeps track of the active components in the sharded cluster. Given a cluster with a mongos running on example.com:30000, the document in the lockpings collection would resemble: { \"_id\" : \"example.com:30000:1350047994:16807\", \"ping\" : ISODate(\"2012-10-12T18:32:54.892Z\") } \nconfig.locks  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The locks collection stores the distributed locks. The primary of the config server replica set takes a lock by inserting a document into the locks collection. {   \"_id\" : \"test.myShardedCollection\",   \"state\" : 2,   \"process\" : \"ConfigServer\",   \"ts\" : ObjectId(\"5be0b9ede46e4f441a60d891\"),   \"when\" : ISODate(\"2018-11-05T21:52:00.846Z\"),   \"who\" : \"ConfigServer:Balancer\",   \"why\" : \"Migrating chunk(s) in collection test.myShardedCollection\"}  As of version 3.4, the state field will always have a value 2 to prevent any legacy mongos instances from performing the balancing operation. The when field specifies the time when the config server member became the primary. In version 3.4, when the balancer is active, the balancer takes a lock, as in the following 3.4 example: {   \"_id\" : \"balancer\",   \"state\" : 2,   \"ts\" : ObjectId(\"5be0bc6cb20effa83b15baa8\"),   \"who\" : \"ConfigServer:Balancer\",   \"process\" : \"ConfigServer\",   \"when\" : ISODate(\"2018-11-05T21:56:13.096Z\"),   \"why\" : \"CSRS Balancer\"}  Starting in version 3.6, the balancer no longer takes a \"lock\". If you have upgraded from 3.4 to 3.6, you may choose to delete any residual \"_id\" : \"balancer\" documents. config.migrationCoordinators \nNew in version 4.4. The migrationCoordinators collection exists on each shard and stores a document for each in-progress chunk migration from this shard to another shard. The chunk migration fails if the document cannot be written to a majority of the members of the shard's replica set. When a migration is complete, the document describing the migration is deleted from the collection. config.mongos  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The mongos collection stores a document for each mongos instance affiliated with the cluster. The cluster maintains this collection for reporting purposes. Each document in the mongos collection contains these fields: Field\nData Type\nDescription\n_id\nString\nThe hostname and port where the mongos is running. The _id is formatted as <hostname>:<port>.\nadvisoryHostFQDNs\nArray of strings\nArray of the mongos's fully qualified domain names (FQDNs).\ncreated\nDate When the mongos was started. New in version 5.2. mongoVersion\nString\nVersion of MongoDB that the mongos is running.\nping\nDate\nmongos instances send pings to the config server every 30 seconds. This field indicates the last ping time.\nup\nNumberLong\nNumber of seconds the mongos has been up as of the last ping.\nwaiting\nBoolean\nAs of MongoDB 3.4, this field is always true and is only present for backward compatibility. The following document shows the status of the mongos running on example.com:27017.\n\n \nconfig.rangeDeletions \nThe rangeDeletions collection exists on each shard and stores a document for each chunk range that contains orphaned documents. The chunk migration fails if the document cannot be written to a majority of the members of the shard's replica set. When the orphaned chunk range is cleaned up, the document describing the range is deleted from the collection. config.settings  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The settings collection holds the following sharding configuration settings:  * Range size. To change range size, see Modify Range Size in a Sharded Cluster. The specified chunksize value is in megabytes.  * Balancer settings. To change the balancer settings, including balancer status, see Manage Sharded Cluster Balancer.  * Autosplit:\n   \n   Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes.\n   \n   In MongoDB versions earlier than 6.0:\n   \n   * balancerStart also enables auto-splitting for the sharded cluster.\n   \n   * balancerStop also disables auto-splitting for the sharded cluster.\n   \n   * To enable or disable the autosplit flag, use the corresponding sh.enableAutoSplit() method or sh.disableAutoSplit() method. Example documents in the settings collection: { \"_id\" : \"chunksize\", \"value\" : 64 }{ \"_id\" : \"balancer\", \"mode\" : \"full\", \"stopped\" : false } \nconfig.shards  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The shards collection represents each shard in the cluster in a separate document, as in the following: { \"_id\" : \"shard0000\", \"host\" : \"localhost:30000\", \"state\" : 1 }  If the shard is a replica set, the host field displays the name of the replica set, then a slash, then a comma-separated list of the hostnames of each member of the replica set, as in the following example: { \"_id\" : \"shard0001\", \"host\" : \"shard0001/localhost:27018,localhost:27019,localhost:27020\", \"state\" : 1 }  If the shard has zones assigned, this document has a tags field, that holds an array of the zones to which it is assigned, as in the following example: { \"_id\" : \"shard0002\", \"host\" : \"localhost:30002\", \"state\" : 1, \"tags\": [ \"NYC\" ] } \nconfig.tags  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The tags collection holds documents for each zone range in the cluster. The documents in the tags collection resemble the following: {    \"_id\" : { \"ns\" : \"records.users\", \"min\" : { \"zipcode\" : \"10001\" } },    \"ns\" : \"records.users\",    \"min\" : { \"zipcode\" : \"10001\" },    \"max\" : { \"zipcode\" : \"10281\" },    \"tag\" : \"NYC\"} \nconfig.version  TIP \nINTERNAL MONGODB METADATA The config database is internal: applications and administrators should not modify or depend upon its content in the course of normal operation. The version collection holds the current metadata version number. This collection contains only one document. For example: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5d8bc01a690d8abbd2014ddd\") }  To access the version collection, you must use the db.getCollection() method. For example, to retrieve the collection's document: db.getCollection(\"version\").find()  \nCOLLECTIONS TO SUPPORT SESSIONS\n\n \nWARNING Do not manually modify or drop these collections. To access these collections for a mongod or mongos instance, connect mongosh to the instance. config.system.sessions \nThe system.sessions collection stores session records that are available to all members of the deployment. When a user creates a session on a mongod or mongos instance, the record of the session initially exists only in-memory on the instance. Periodically, the instance will sync its cached sessions to the system.sessions collection; at which time, they are visible to all members of the deployment. To view records in the system.sessions collection, use $listSessions. \nWARNING Do not manually modify or drop the system.sessions collection. In a sharded cluster, the system.sessions collection is sharded.  * When adding a shard to the sharded cluster, if the shard to add already contains its own system.sessions collection, MongoDB drops the new shard's system.sessions collection during the add process.  * Starting in version 4.4 (and 4.2.7), MongoDB automatically splits the system.sessions collection into at least 1024 chunks and distributes the chunks uniformly across shards in the cluster. config.transactions \nThe transactions collection stores records used to support retryable writes and transactions for replica sets and sharded clusters. \nWARNING Do not manually modify or drop the transactions collection. ←  Troubleshoot Sharded ClustersChange Streams → On this page  * Restrictions\n * Collections to Support Sharded Cluster Operations\n * Collections to Support Sessions Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.initializeUnorderedBulkOp/": " Docs Home → MongoDB Manual \nDB.COLLECTION.INITIALIZEUNORDEREDBULKOP() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDEFINITION \ndb.collection.initializeUnorderedBulkOp()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Initializes and returns a new Bulk() operations builder for a collection. The builder constructs an unordered list of write operations that MongoDB executes in bulk. \nBEHAVIOR  ORDER OF OPERATION \nWith an unordered operations list, MongoDB can execute in parallel the write operations in the list and in any order. If the order of operations matter, use db.collection.initializeOrderedBulkOp() instead. \nEXECUTION OF OPERATIONS \nWhen executing an unordered list of operations, MongoDB groups the operations. With an unordered bulk operation, the operations in the list may be reordered to increase performance. As such, applications should not depend on the ordering when performing unordered bulk operations. Bulk() operations in mongosh and comparable methods in the drivers do not have a limit for the number of operations in a group. To see how the operations are grouped for bulk operation execution, call Bulk.getOperations() after the execution. \nTIP \nSEE ALSO:      * Write Command Batch Limit Size \nERROR HANDLING \nIf an error occurs during the processing of one of the write operations, MongoDB will continue to process remaining write operations in the list. \nEXAMPLE \nThe following initializes a Bulk() operations builder and adds a series of insert operations to add multiple documents: var bulk = db.users.initializeUnorderedBulkOp();bulk.insert( { user: \"abc123\", status: \"A\", points: 0 } );bulk.insert( { user: \"ijk123\", status: \"A\", points: 0 } );bulk.insert( { user: \"mop123\", status: \"P\", points: 0 } );bulk.execute();  \nTIP \nSEE ALSO:  * db.collection.initializeOrderedBulkOp()  * Bulk()  * Bulk.insert()  * Bulk.execute() ←  db.collection.initializeOrderedBulkOp()Bulk() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/UUID/": " Docs Home → MongoDB Manual \nUUID() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \nGenerates a BSON UUID object. UUID() has the following syntax: UUID(<string>) Parameter\nType\nDescription\nhex\nstring\nOptional. Specify a 36 character string to convert to a UUID BSON object. If not provided, MongoDB generates a random UUID in RFC 4122 v4 format. Returns:A BSON UUID object. \nEXAMPLE  CONVERT CHARACTER STRING TO UUID \nCreate a 36 character string you wish to convert to a UUID: var myuuid = '3b241101-e2bb-4255-8caf-4136c566a962'  The following command outputs the myuuid variable as a BSON UUID object: UUID(myuuid)  This command generates the following output: UUID(\"3b241101-e2bb-4255-8caf-4136c566a962\")  \nGENERATE RANDOM UUID \nYou can run the UUID() method without specifying an argument to generate a random UUID: UUID()  This command outputs a random UUID in the following form: UUID(\"dee11d4e-63c6-4d90-983c-5c9f1e79e96c\") \n←  ObjectId.valueOf()WriteResult() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.hasNext/": " Docs Home → MongoDB Manual \nCURSOR.HASNEXT() \ncursor.hasNext()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Returns:Boolean. cursor.hasNext() returns true if the cursor returned by the db.collection.find() query can iterate further to return more documents. ←  cursor.forEach()cursor.hint() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.comment/": " Docs Home → MongoDB Manual \nCURSOR.COMMENT() \nOn this page    \n * Definition\n   \n * Behavior\n * Example\n * Output Examples \nDEFINITION \ncursor.comment()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Adds a comment field to the query. cursor.comment() has the following syntax: cursor.comment( <string> )  comment() has the following parameter: Parameter\nType\nDescription\ncomment\nstring\nThe comment to apply to the query. \nBEHAVIOR \ncomment() associates a comment string with the find operation. This can make it easier to track a particular query in the following diagnostic outputs:      * The system.profile  * The QUERY log component  * db.currentOp() See configure log verbosity for the mongod log, the Database Profiler tutorial, or the db.currentOp() command. \nEXAMPLE \nThe following operation attaches a comment to a query on the restaurants collection: db.restaurants.find(   { \"borough\" : \"Manhattan\" }).comment( \"Find all Manhattan restaurants\" )  \nOUTPUT EXAMPLES  SYSTEM.PROFILE \nThe following is an excerpt from the system.profile: {   \"op\" : \"query\",   \"ns\" : \"guidebook.restaurant\",   \"query\" : {      \"find\" : \"restaurant\",      \"filter\" : {         \"borough\" : \"Manhattan\"      },      \"comment\" : \"Find all Manhattan restaurants\"   },   ...}  \nMONGOD LOG \nThe following is an excerpt from the mongod log. It has been formatted for readability. \nIMPORTANT The verbosity level for QUERY must be greater than 0. See Configure Log Verbosity Levels 2015-11-23T13:09:16.202-05:00 I COMMAND  [conn1]   command guidebook.restaurant command: find {      find: \"restaurant\",      filter: { \"borough\" : \"Manhattan\" },      comment: \"Find all Manhattan restaurants\"   }   ...  \nDB.CURRENTOP() \nSuppose the following operation is currently running on a mongod instance: db.restaurants.find(   { \"borough\" : \"Manhattan\" }).comment(\"Find all Manhattan restaurants\")  Running the db.currentOp() command returns the following: {   \"inprog\" : [      {         \"host\" : \"198.51.100.1:27017\",         \"desc\" : \"conn3\",         \"connectionId\" : 3,         ...\n         \"op\" : \"query\",         \"ns\" : \"test.$cmd\",         \"command\" : {            \"find\" : \"restaurants\",            \"filter\" : {               \"borough\" : \"Manhattan\"            },            \"comment\" : \"Find all Manhattan restaurants\",            \"$db\" : \"test\"         },         \"numYields\" : 0,         ...      }   ],   \"ok\" : 1} \n←  cursor.collation()cursor.count() → On this page  * Definition\n * Behavior\n * Example\n * Output Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.isClosed/": " Docs Home → MongoDB Manual \nCURSOR.ISCLOSED() \ncursor.isClosed()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Returns:Boolean. cursor.isClosed() returns true if the server has closed the cursor. A closed cursor may still have documents remaining in the last received batch. Use cursor.isExhausted() or cursor.hasNext() to check if the cursor is fully exhausted. ←  cursor.close()cursor.collation() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.grantRolesToRole/": " Docs Home → MongoDB Manual \nDB.GRANTROLESTOROLE() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.grantRolesToRole(rolename, roles, writeConcern) \nGrants roles to a user-defined role. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the grantRolesToRole command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.grantRolesToRole() method uses the following syntax: db.grantRolesToRole( \"<rolename>\", [ <roles> ], { <writeConcern> } )  The db.grantRolesToRole() method takes the following arguments: Parameter\nType\nDescription\nrolename\nstring\nThe name of the role to which to grant sub roles.\nroles\narray\nAn array of roles from which to inherit.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. In the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where db.grantRolesToRole() runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.grantRolesToRole() is executed using \"majority\" write concern by default. \nSCOPE \nA role can inherit privileges from other roles in its database. A role created on the admin database can inherit privileges from roles in any database. \nREQUIRED ACCESS \nYou must have the grantRole action on a database to grant a role on that database. \nEXAMPLE \nThe following db.grantRolesToRole() operation updates the productsReaderWriter role in the products database to inherit the privileges of productsReader role: use productsdb.grantRolesToRole(    \"productsReaderWriter\",    [ \"productsReader\" ],    { w: \"majority\" , wtimeout: 5000 }) \n←  db.revokePrivilegesFromRole()db.revokeRolesFromRole() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.execute/": " Docs Home → MongoDB Manual \nBULK.EXECUTE() \nOn this page    \n * Description\n   \n * Behavior\n * Examples \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.execute() \nExecutes the list of operations built by the Bulk() operations builder. Bulk.execute() accepts the following parameter: Parameter\nType\nDescription\nwriteConcern\ndocument Optional. Write concern document for the bulk operation as a whole. Omit to use default. For a standalone mongod server, the write concern defaults to { w: majority }. With a replica set, the default write concern is { w: majority } unless modified as part of the replica set configuration, or potentially if the replica set contains multiple arbiters. See Override Default Write Concern for an example. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. Returns:A BulkWriteResult() object that contains the status of the operation. After execution, you cannot re-execute the Bulk() object without reinitializing. See db.collection.initializeUnorderedBulkOp() and db.collection.initializeOrderedBulkOp(). \nBEHAVIOR  ORDERED OPERATIONS \nWhen executing an ordered list of operations, MongoDB groups the operations by the operation type and contiguity; i.e. contiguous operations of the same type are grouped together. For example, if an ordered list has two insert operations followed by an update operation followed by another insert operation, MongoDB groups the operations into three separate groups: first group contains the two insert operations, second group contains the update operation, and the third group contains the last insert operation. This behavior is subject to change in future versions. Bulk() operations in mongosh and comparable methods in the drivers do not have a limit for the number of operations in a group. To see how the operations are grouped for bulk operation execution, call Bulk.getOperations() after the execution. \nTIP \nSEE ALSO:      * Write Command Batch Limit Size Executing an ordered list of operations on a sharded collection will generally be slower than executing an unordered list since with an ordered list, each operation must wait for the previous operation to finish. \nUNORDERED OPERATIONS \nWhen executing an unordered list of operations, MongoDB groups the operations. With an unordered bulk operation, the operations in the list may be reordered to increase performance. As such, applications should not depend on the ordering when performing unordered bulk operations. Bulk() operations in mongosh and comparable methods in the drivers do not have a limit for the number of operations in a group. To see how the operations are grouped for bulk operation execution, call Bulk.getOperations() after the execution. \nTIP \nSEE ALSO:  * Write Command Batch Limit Size \nTRANSACTIONS \nBulk() can be used inside multi-document transactions. For Bulk.insert() operations, the collection must already exist. For Bulk.find.upsert(), if the operation results in an upsert, the collection must already exist. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. \nEXAMPLES  EXECUTE BULK OPERATIONS \nThe following initializes a Bulk() operations builder on the items collection, adds a series of insert operations, and executes the operations: var bulk = db.items.initializeUnorderedBulkOp();bulk.insert( { item: \"abc123\", status: \"A\", defaultQty: 500, points: 5 } );bulk.insert( { item: \"ijk123\", status: \"A\", defaultQty: 100, points: 10 } );bulk.execute( );  The operation returns the following BulkWriteResult() object:\n\n  For details on the return object, see BulkWriteResult(). For details on the batches executed, see Bulk.getOperations(). \nOVERRIDE DEFAULT WRITE CONCERN \nThe following operation to a replica set specifies a write concern of \"w: 1\" with a wtimeout of 5000 milliseconds such that the method returns after the writes propagate to a majority of the voting replica set members or the method times out after 5 seconds. var bulk = db.items.initializeUnorderedBulkOp();bulk.insert( { item: \"efg123\", status: \"A\", defaultQty: 100, points: 0 } );bulk.insert( { item: \"xyz123\", status: \"A\", defaultQty: 100, points: 0 } );bulk.execute( { w: 1, wtimeout: 5000 } );  The operation returns the following BulkWriteResult() object: BulkWriteResult({   \"writeErrors\" : [ ],   \"writeConcernErrors\" : [ ],   \"nInserted\" : 2,   \"nUpserted\" : 0,   \"nMatched\" : 0,   \"nModified\" : 0,   \"nRemoved\" : 0,   \"upserted\" : [ ]})  \nTIP \nSEE: Bulk() for a listing of methods available for bulk operations. ←  Bulk()Bulk.find() → On this page  * Description\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.batchSize/": " Docs Home → MongoDB Manual \nCURSOR.BATCHSIZE() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \ncursor.batchSize(size)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Specifies the number of documents to return in each batch of the response from the MongoDB instance. In most cases, modifying the batch size will not affect the user or the application, as mongosh and most drivers return results as if MongoDB returned a single batch. The batchSize() method takes the following parameter: Parameter\nType\nDescription\nsize\ninteger\nThe number of documents to return per batch. Do not use a batch size of 1. \nNOTE Specifying 1 or a negative number is analogous to using the limit() method. \nEXAMPLE \nThe following example sets the batch size for the results of a query (i.e. find()) to 10. The batchSize() method does not change the output in mongosh, which, by default, iterates over the first 20 documents. db.inventory.find().batchSize(10) \n←  cursor.allowPartialResults()cursor.close() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Mongo/": " Docs Home → MongoDB Manual \nMONGO() \nOn this page    \n * Description\n   \n * AutoEncryptionOpts\n * api\n * Examples\n * Connect to a MongoDB Cluster\n * Connect to a Cluster with Client-Side Encryption Enabled\n * Connect to a Cluster with Automatic Client-Side Encryption Enabled\n * Connect to a Cluster with the Stable API Enabled \nDESCRIPTION \nChanged in version 4.2. Mongo(host, autoEncryptionOpts, api) \nJavaScript constructor to instantiate a database connection from mongosh or from a JavaScript file. The Mongo() method has the following parameters: Parameter\nType\nDescription\nhost\nstring Optional. The host, either in the form of <host> or <host><:port>. If omitted, Mongo() instantiates a connection to the localhost interface on the default port 27017. autoEncryptionOpts\ndocument New in version 4.2. Optional. Configuration parameters for enabling Client-Side Field Level Encryption. autoEncryptionOpts overrides the existing client-side field level encryption configuration of the database connection. If omitted, Mongo() inherits the client-side field level encryption configuration of the current database connection. See AutoEncryptionOpts for usage and syntax details. api\ndocument Optional. Configuration options for enabling the Stable API. See api for usage and syntax details. \nTIP \nSEE ALSO: Mongo.getDB() and db.getMongo() \nAUTOENCRYPTIONOPTS \nNew in version 4.2. The autoEncryptionOpts document specifies configuration options for Client-Side Field Level Encryption. If the database connection has an existing client-side field level encryption configuration, specifying autoEncryptionOpts overrides that configuration. For example, starting mongosh with client-side field level encryption command-line options enables client-side encryption for that connection. New database connections created using Mongo() inherit the encryption settings unless Mongo() includes autoEncryptionOpts. The autoEncryptionOpts document has the following syntax: {  \"keyVaultClient\" : <object>,  \"keyVaultNamespace\" : \"<string>\",  \"kmsProviders\" : <object>,  \"schemaMap\" : <object>,  \"bypassAutoEncryption\" : <boolean>}  The autoEncryptionOpts document takes the following parameters: Parameter\nType\nDescription\nkeyVaultClient\nMongo() connection object. (Optional) The MongoDB cluster hosting the key vault collection. Specify a Mongo() connection object pointing to the cluster: var keyVaultClient = Mongo(<MongoDB URI>);\nvar autoEncryptionOptions = {  \"keyVaultClient\" : keyVaultClient,  \"keyVaultNamespace\" : \"<database>.<collection>\",  \"kmsProviders\" : { ... }}  If keyVaultClient is omitted, the host specified to the Mongo() object containing the autoEncryptionOpts document is used as the key vault host. keyVaultNamespace\nstring\n(Required) The full namespace of the key vault collection.\nkmsProviders\ndocument (Required) The Key Management Service (KMS) used by client-side field level encryption for managing a Customer Master Key (CMK). Client-side field level encryption uses the CMK for encrypting and decrypting data encryption keys. Client-Side Field Level Encryption supports the following KMS providers:      * Amazon Web Services KMS  * Azure Key Vault  * Google Cloud Platform KMS  * Locally Managed Key If possible, consider defining the credentials provided in kmsProviders as environment variables, and then passing them to mongosh using the --eval option. This minimizes the chances of credentials leaking into logs. See Create a Data Key for examples of this approach for each supported KMS. Amazon Web Services KMS \nIMPORTANT For AWS KMS support, use mongosh, or the MongoDB 4.2.2 or later legacy mongo shell. The 4.2.0 and 4.2.1 legacy mongo shell do not support the AWS KMS service due to an unexpected change in the KMS response object. See SERVER-44721 for more information. Specify the aws document to kmsProviders with the following fields: \"kmsProviders\" : {   \"aws\" : {     \"accessKeyId\" : \"AWSAccessKeyId\",     \"secretAccessKey\" : \"AWSSecretAccessKey\"   } }  The specified accessKeyId must correspond to an IAM user with all List and Read permissions for the KMS service. Azure Key Vault\n\n \"kmsProviders\" : {  \"azure\" : {    \"tenantId\" : \"AzureTenantId\",    \"clientId\" : \"AzureClientId\",    \"clientSecret\" : \"AzureClientSecret\"  }}  New in version 5.0. Google Cloud KMS Specify the gcp document to kmsProviders with the following fields: \"kmsProviders\" : {  \"gcp\" : {    \"email\" : \"GCPEmail\",    \"privateKey\" : \"GCPPrivateKey\"  }}  New in version 5.0. Locally Managed Key Specify the local document to kmsProviders with the following field: \"kmsProviders\" : {  \"local\" : {     \"key\" : BinData(0, \"<96 byte base-64 encoded key>\")  }}  The specified key must be a base64-encoded 96-byte string with no newline characters. schemaMap\ndocument (Optional) The automatic client-side field level encryption rules specified using the JSON schema Draft 4 standard syntax and encryption-specific keywords. For complete documentation, see Encryption Schemas. bypassAutoEncryption\nboolean\n(Optional) Specify true to bypass automatic client-side field level encryption rules and perform explicit (manual) per-field encryption.\nbypassQueryAnalysis\nboolean\n(Optional) Specify true to use explicit encryption on indexed fields without the crypt_shared library. For details, see MongoClient Options for Queryable Encryption. \nAPI \nThe api parameter specifies configuration options for the Stable API. You can enable or disable optional behavior using the following options: Option\nType\nDescription\nversion\nstring\nSpecifies the API Version. \"1\" is currently the only supported version.\nstrict\nboolean If true, using a command that is not part of the declared API version returns an APIStrictError error. If you specify strict, you must also specify version. If not specified, defaults to false. deprecationErrors\nboolean If true, using a command or behavior that is deprecated in the specified API version returns an APIDeprecationError. If you specify deprecationErrors, you must also specify version. If not specified, defaults to false. The api parameter has the following syntax: { api: { version: <string>, strict: <boolean>, deprecationErrors: <boolean> } }  \nEXAMPLES  CONNECT TO A MONGODB CLUSTER \nThe following operation creates a new connection object from within a mongosh session: cluster = Mongo(\"mongodb://mymongo.example.net:27017/?replicaSet=myMongoCluster\")  Issue operations against the cluster object to interact with the mymongo.example.net:27017 cluster: myDB = cluster.getDB(\"myDB\"); //returns the database objectmyColl = myDB.getCollection(\"myColl\"); // returns the collection object  \nCONNECT TO A CLUSTER WITH CLIENT-SIDE ENCRYPTION ENABLED \nkey:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key export TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  The following operation creates a new connection object from within a mongosh session. The AutoEncryptionOpts option specifies the required options for enabling client-side field level encryption using a locally managed key: var autoEncryptionOpts = {  \"keyVaultNamespace\" : \"encryption.__dataKeys\",  \"kmsProviders\" : {    \"local\" : {      \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])    }  }}\ncluster = Mongo(  \"mongodb://mymongo.example.net:27017/?replicaSet=myMongoCluster\",  autoEncryptionOpts)  Issue operations against the cluster object to interact with the mymongo.example.net:27017 cluster and perform explicit encryption: // returns the database objectmyDB = cluster.getDB(\"myDB\");\n// returns the collection objectmyColl = myDB.getCollection(\"myColl\");\n// returns object for managing data encryption keyskeyVault = cluster.getKeyVault();\n// returns object for explicit encryption/decryptionclientEncryption = cluster.getClientEncryption(); \n\n \nCONNECT TO A CLUSTER WITH AUTOMATIC CLIENT-SIDE ENCRYPTION ENABLED \nConfiguring client-side field level encryption for a locally managed key requires specifying a base64-encoded 96-byte string with no line breaks. The following operation generates a key that meets the stated requirements and loads it into mongosh: TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb --shell --eval \"var TEST_LOCAL_KEY='$TEST_LOCAL_KEY'\"  The following operation creates a new connection object from within a mongosh session. The AutoEncryptionOpts option specifies the required options for enabling automatic client-side encryption on the hr.employees collection: var autoEncryptionOpts = {  \"keyVaultNamespace\" : \"encryption.dataKeys\",  \"kmsProviders\" : {    \"local\" : {      \"key\" : BinData(0,\"BASE64-ENCODED-96-BYTE-LOCAL-KEY\")    }  },  schemaMap : {    \"hr.employees\" : {      \"bsonType\": \"object\",      \"properties\" : {        \"taxid\" : {          \"encrypt\" : {            \"keyId\" : [UUID(\"bffb361b-30d3-42c0-b7a4-d24a272b72e3\")],            \"bsonType\" : \"string\",            \"algorithm\" : \"AEAD_AES_256_CBC_HMAC_SHA_512-Random\"          }        },        \"taxid-short\": {          \"encrypt\": {            \"keyId\": [UUID(\"33408ee9-e499-43f9-89fe-5f8533870617\")],            \"algorithm\": \"AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic\",            \"bsonType\": \"string\"          }        }      }    }  }}\ncluster = Mongo(  \"mongodb://mymongo.example.net:27017/?replicaSet=myMongoCluster\",  autoEncryptionOpts)  Issue operations against the cluster object to interact with the mymongo.example.net:27017 cluster and utilize automatic encryption: // returns the database objectmyDB = cluster.getDB(\"myDB\");\n// returns the collection objectmyColl = myDB.getCollection(\"myColl\");\nmyColl.insertOne(  {    \"name\" : \"J Doe\",    \"taxid\" : \"123-45-6789\",    \"taxid-short\" : \"6789\"  })  The specified automatic encryption rules encrypt the taxid and taxid-short fields using the specified data encryption key and algorithm. Only clients configured for the correct KMS and access to the specified data encryption key can decrypt the field. See Client-Side Field Level Encryption Methods for a complete list of client-side field level encryption methods. \nCONNECT TO A CLUSTER WITH THE STABLE API ENABLED \nThe following operation creates a new connection object from within a mongosh session. The api option enables Stable API V1 and specifies that you cannot run deprecated command or commands outside of the Stable API. cluster = Mongo(  \"mongodb://mymongo.example.net:27017/?replicaSet=myMongoCluster\",   null,   { api: { version: \"1\", strict: true, deprecationErrors: true } })  To interact with the mymongo.example.net:27017 cluster, issue operations against the cluster object. For a full list of Stable API commands, see Stable API Commands. ←  connect()Mongo.getDB() →\n\n  * Description\n * AutoEncryptionOpts\n * api\n * Examples\n * Connect to a MongoDB Cluster\n * Connect to a Cluster with Client-Side Encryption Enabled\n * Connect to a Cluster with Automatic Client-Side Encryption Enabled\n * Connect to a Cluster with the Stable API Enabled Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/ObjectId.toString/": " Docs Home → MongoDB Manual \nOBJECTID.TOSTRING() \nOn this page    \n * Example ObjectId.toString() \nReturns the string representation of the ObjectId(). \nEXAMPLE \nThe following example:      * Generates a new ObjectId() and stores it in the variable myObjectId.  * Creates a string representation of myObjectId using the toString() method.  * Stores the string representation in the variable myObjectIdString. Run the following commands in mongosh: myObjectId = ObjectId(\"507c7f79bcf86cd7994f6c0e\")myObjectIdString = myObjectId.toString()  The operation returns the following string: 507c7f79bcf86cd7994f6c0e To confirm the type of myObjectIdString, use the typeof JavaScript operator: typeof myObjectIdString  The operation returns the following: string ←  ObjectId.getTimestamp()ObjectId.valueOf() → On this page  * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.removeUser/": " Docs Home → MongoDB Manual \nDB.REMOVEUSER() \nOn this page    \n * Definition Deprecated since version 2.6: Use db.dropUser() instead of db.removeUser() \nDEFINITION \ndb.removeUser(username) \nRemoves the specified username from the database. The db.removeUser() method has the following parameter: Parameter\nType\nDescription\nusername\nstring\nThe database username. ←  db.grantRolesToUser()db.revokeRolesFromUser() → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.printSlaveReplicationInfo/": " Docs Home → MongoDB Manual \nDB.PRINTSLAVEREPLICATIONINFO() \nOn this page    \n * Definition\n   \n * Output \nDEFINITION \ndb.printSlaveReplicationInfo() \nDeprecated since version 4.4.1: Use db.printSecondaryReplicationInfo() instead. Returns a formatted report of the status of a replica set from the perspective of the secondary member of the set. The output is identical to that of rs.printSlaveReplicationInfo(). \nOUTPUT \nThe following is example output from the db.printSlaveReplicationInfo() method issued on a replica set with two secondary members: source: m1.example.net:27017    syncedTo: Thu Apr 10 2014 10:27:47 GMT-0400 (EDT)    0 secs (0 hrs) behind the primarysource: m2.example.net:27017    syncedTo: Thu Apr 10 2014 10:27:47 GMT-0400 (EDT)    0 secs (0 hrs) behind the primary  \nNOTE The db.printSlaveReplicationInfo() method run in mongosh does not return JSON. Use db.printSlaveReplicationInfo() for manual inspection, and rs.status() in scripts. A delayed member may show as 0 seconds behind the primary when the inactivity period on the primary is greater than the members[n].secondaryDelaySecs value. ←  db.printShardingStatus()db.resetError() → On this page  * Definition\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Mongo.getReadPrefTagSet/": " Docs Home → MongoDB Manual \nMONGO.GETREADPREFTAGSET() \nMongo.getReadPrefTagSet() \nReturns:The current read preference tag set for the Mongo() connection object. See Read Preference for an introduction to read preferences and tag sets in MongoDB. Use getReadPrefTagSet() to return the current read preference tag set, as in the following example: db.getMongo().getReadPrefTagSet()  Use the following operation to return and print the current read preference tag set: printjson(db.getMongo().getReadPrefTagSet());  \nTIP \nSEE ALSO:      * Read Preference  * readPref()  * setReadPref()  * getReadPrefTagSet() ←  Mongo.getReadPrefMode()Mongo.getWriteConcern() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.skip/": " Docs Home → MongoDB Manual \nCURSOR.SKIP() \nOn this page    \n * Definition\n   \n * Behavior\n * Pagination Example \nDEFINITION \ncursor.skip(<offset>)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Call the skip() method on a cursor to control where MongoDB begins returning results. This approach may be useful in implementing paginated results. \nNOTE You must apply skip() to the cursor before retrieving any documents from the database. The skip() method has the following parameter: Parameter\nType\nDescription\noffset\nnumber\nThe number of documents to skip in the results set. \nBEHAVIOR  USING SKIP() WITH SORT() \nIf using skip() with sort(), be sure to include at least one field in your sort that contains unique values, before passing results to skip(). Sorting on fields that contain duplicate values may return an inconsistent sort order for those duplicate fields over multiple executions, especially when the collection is actively receiving writes. The easiest way to guarantee sort consistency is to include the _id field in your sort query. See Consistent sorting with the sort() method for more information. \nPAGINATION EXAMPLE  USING SKIP() \nThe following JavaScript function uses skip() to paginate a collection by its _id field: function printStudents(pageNumber, nPerPage) {  print( \"Page: \" + pageNumber );  db.students.find()             .sort( { _id: 1 } )             .skip( pageNumber > 0 ? ( ( pageNumber - 1 ) * nPerPage ) : 0 )             .limit( nPerPage )             .forEach( student => {               print( student.name );             } );}  The skip() method requires the server to scan from the beginning of the input results set before beginning to return results. As the offset increases, skip() will become slower. \nUSING RANGE QUERIES \nRange queries can use indexes to avoid scanning unwanted documents, typically yielding better performance as the offset grows compared to using skip() for pagination. DESCENDING ORDER \nUse this procedure to implement pagination with range queries:      * Choose a field such as _id which generally changes in a consistent direction over time and has a unique index to prevent duplicate values,  * Query for documents whose field is less than the start value using the $lt and sort() operators, and  * Store the last-seen field value for the next query. For example, the following function uses the above procedure to print pages of student names from a collection, sorted approximately in order of newest documents first using the _id field (that is, in descending order): function printStudents(startValue, nPerPage) {  let endValue = null;  db.students.find( { _id: { $lt: startValue } } )             .sort( { _id: -1 } )             .limit( nPerPage )             .forEach( student => {               print( student.name );               endValue = student._id;             } );\n  return endValue;}  You may then use the following code to print all student names using this pagination function, using MaxKey to start from the largest possible key: let currentKey = MaxKey;while (currentKey !== null) {  currentKey = printStudents(currentKey, 10);}  \nNOTE While ObjectId values should increase over time, they are not necessarily monotonic. This is because they:  * Only contain one second of temporal resolution, so ObjectId values created within the same second do not have a guaranteed ordering, and  * Are generated by clients, which may have differing system clocks. ASCENDING ORDER \nReturning paginated results in ascending order is similar to the previous, but uses $gt with an ascending sort order:\n\n  Using this function is likewise similar, but with MinKey as the starting key: let currentKey = MinKey;while (currentKey !== null) {  currentKey = printStudents(currentKey, 10);} \n←  cursor.size()cursor.sort() → On this page  * Definition\n * Behavior\n * Pagination Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Session.commitTransaction/": " Docs Home → MongoDB Manual \nSESSION.COMMITTRANSACTION() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \nSession.commitTransaction() \nSaves the changes made by the operations in the multi-document transaction and ends the transaction. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the commitTransaction command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Changed in version 4.2: Starting in MongoDB 4.2, multi-document transactions are available for both sharded clusters and replica sets. \nBEHAVIOR  WRITE CONCERN \nWhen commiting the transaction, the session uses the write concern specified at the transaction start. See Session.startTransaction(). If you commit using \"w: 1\" write concern, your transaction can be rolled back during the failover process. \nATOMICITY \nWhen a transaction commits, all data changes made in the transaction are saved and visible outside the transaction. That is, a transaction will not commit some of its changes while rolling back others. Until a transaction commits, the data changes made in the transaction are not visible outside the transaction. However, when a transaction writes to multiple shards, not all outside read operations need to wait for the result of the committed transaction to be visible across the shards. For example, if a transaction is committed and write 1 is visible on shard A but write 2 is not yet visible on shard B, an outside read at read concern \"local\" can read the results of write 1 without seeing write 2. \nRETRYABLE \nIf the commit operation encounters an error, MongoDB drivers retry the commit operation a single time regardless of whether retryWrites is set to false. For more information, see Transaction Error Handling. \nEXAMPLE \nConsider a scenario where as changes are made to an employee's record in the hr database, you want to ensure that the events collection in the reporting database are in sync with the hr changes. That is, you want to ensure that these writes are done as a single transaction, such that either both operations succeed or fail. The employees collection in the hr database has the following documents: { \"_id\" : ObjectId(\"5af0776263426f87dd69319a\"), \"employee\" : 3, \"name\" : { \"title\" : \"Mr.\", \"name\" : \"Iba Ochs\" }, \"status\" : \"Active\", \"department\" : \"ABC\" }{ \"_id\" : ObjectId(\"5af0776263426f87dd693198\"), \"employee\" : 1, \"name\" : { \"title\" : \"Miss\", \"name\" : \"Ann Thrope\" }, \"status\" : \"Active\", \"department\" : \"ABC\" }{ \"_id\" : ObjectId(\"5af0776263426f87dd693199\"), \"employee\" : 2, \"name\" : { \"title\" : \"Mrs.\", \"name\" : \"Eppie Delta\" }, \"status\" : \"Active\", \"department\" : \"XYZ\" }  The events collection in the reporting database has the following documents: { \"_id\" : ObjectId(\"5af07daa051d92f02462644a\"), \"employee\" : 1, \"status\" : { \"new\" : \"Active\", \"old\" : null }, \"department\" : { \"new\" : \"ABC\", \"old\" : null } }{ \"_id\" : ObjectId(\"5af07daa051d92f02462644b\"), \"employee\" : 2, \"status\" : { \"new\" : \"Active\", \"old\" : null }, \"department\" : { \"new\" : \"XYZ\", \"old\" : null } }{ \"_id\" : ObjectId(\"5af07daa051d92f02462644c\"), \"employee\" : 3, \"status\" : { \"new\" : \"Active\", \"old\" : null }, \"department\" : { \"new\" : \"ABC\", \"old\" : null } }  The following example opens a transaction, updates an employee's status to Inactive in the employees status and inserts a corresponding document to the events collection, and commits the two operations as a single transaction.\n\n  \nTIP \nSEE ALSO:  * Session.abortTransaction()  * Session.commitTransaction() ←  Session.abortTransaction()Session.startTransaction() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.aggregate/": " Docs Home → MongoDB Manual \nDB.AGGREGATE() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \ndb.aggregate() \nRuns a specified admin/diagnostic pipeline which does not require an underlying collection. For aggregations on collection data, see db.collection.aggregate(). \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the aggregate command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.aggregate() method has the following syntax: db.aggregate( [ <pipeline> ], { <options> } )  The pipeline parameter is an array of stages to execute. It must start with a compatible stage that does not require an underlying collection, such as $currentOp or $listLocalSessions. The options document can contain the following fields and values: Field\nType\nDescription\nexplain\nboolean Optional. Specifies that the method should return information on the processing of the pipeline. See Return Information on Aggregation Pipeline Operation for an example. Not available in multi-document transactions. allowDiskUse\nboolean Optional. Enables writing to temporary files. When set to true, aggregation operations can write data to the _tmp subdirectory in the dbPath directory. See Interaction with allowDiskUseByDefault for an example. Starting in MongoDB 4.2, the profiler log messages and diagnostic log messages includes a usedDisk indicator if any aggregation stage wrote data to temporary files due to memory restrictions. cursor\ndocument\nOptional. Specifies the initial batch size for the cursor. The value of the cursor field is a document with the field batchSize. See Specify an Initial Batch Size for syntax and example.\nmaxTimeMS\nnon-negative integer Optional. Specifies a time limit in milliseconds for processing operations on a cursor. If you do not specify a value for maxTimeMS, operations will not time out. A value of 0 explicitly specifies the default unbounded behavior. MongoDB terminates operations that exceed their allotted time limit using the same mechanism as db.killOp(). MongoDB only terminates an operation at one of its designated interrupt points. bypassDocumentValidation\nboolean Optional. Applicable only if you specify the $out or $merge aggregation stages. Enables db.collection.aggregate() to bypass document validation during the operation. This lets you insert documents that do not meet the validation requirements. readConcern\ndocument Optional. Specifies the read concern. Starting in MongoDB 3.6, the readConcern option has the following syntax: readConcern: { level: <value> } Possible read concern levels are:  * \"local\". This is the default read concern level for read operations against the primary and secondaries.  * \"available\". Available for read operations against the primary and secondaries. \"available\" behaves the same as \"local\" against the primary and non-sharded secondaries. The query returns the instance's most recent data.  * \"majority\". Available for replica sets that use WiredTiger storage engine.  * \"linearizable\". Available for read operations on the primary only. For more formation on the read concern levels, see Read Concern Levels. Starting in MongoDB 4.2, the $out stage cannot be used in conjunction with read concern \"linearizable\". That is, if you specify \"linearizable\" read concern for db.collection.aggregate(), you cannot include the $out stage in the pipeline. The $merge stage cannot be used in conjunction with read concern \"linearizable\". That is, if you specify \"linearizable\" read concern for db.collection.aggregate(), you cannot include the $merge stage in the pipeline. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document.\n\n If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. hint\nstring or document Optional. The index to use for the aggregation. The index is on the initial collection/view against which the aggregation is run. Specify the index either by the index name or by the index specification document. \nNOTE The hint does not apply to $lookup and $graphLookup stages. comment\nstring\nOptional. Users can specify an arbitrary string to help trace the operation through the database profiler, currentOp, and logs.\nwriteConcern\ndocument Optional. A document that expresses the write concern to use with the $out or $merge stage. Omit to use the default write concern with the $out or $merge stage. \nEXAMPLE  PIPELINE WITH $CURRENTOP \nThe following example runs a pipeline with two stages. The first stage runs the $currentOp operation and the second stage filters the results of that operation. use admindb.aggregate( [ {   $currentOp : { allUsers: true, idleConnections: true } }, {   $match : { shard: \"shard01\" }   }] ) \n←  db.adminCommand()db.commandHelp() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.dropIndexes/": " Docs Home → MongoDB Manual \nDB.COLLECTION.DROPINDEXES() \nOn this page    \n * Definition\n   \n * Behavior \nDEFINITION \ndb.collection.dropIndexes()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the dropIndexes command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Drops the specified index or indexes (except the index on the _id field and the last remaining shard key index) from a collection. You can use the method to:  * Drop all but the _id index from a collection.\n   \n   db.collection.dropIndexes()\n   \n     * Drop a specified index from a collection. To specify the index, you can pass the method either:\n   \n   * The index specification document (unless the index is a text index in which case, use the index name to drop):\n     \n     db.collection.dropIndexes( { a: 1, b: 1 } )\n     \n     \n   \n   * The index name:\n     \n     db.collection.dropIndexes( \"a_1_b_1\" )\n     \n     \n     \n     \n     TIP\n     \n     To get the names of the indexes, use the db.collection.getIndexes() method.  * Drop specified indexes from a collection. (Available starting in MongoDB 4.2). To specify multiple indexes to drop, pass the method an array of index names:\n   \n   db.collection.dropIndexes( [ \"a_1_b_1\", \"a_1\", \"a_1__id_-1\" ] )\n   \n   \n   \n   If the array of index names includes a non-existent index, the method errors without dropping any of the specified indexes.\n   \n   \n   TIP\n   \n   To get the names of the indexes, use the db.collection.getIndexes() method. The db.collection.dropIndexes() method takes the following optional parameter: Parameter\nType\nDescription\nindexes\nstring or document or array of strings Optional. Specifies the index or indexes to drop. To drop all but the _id index from the collection, omit the parameter. To drop a single index, specify either the index name, the index specification document (unless the index is a text index), or an array of the index name. To drop a text index, specify the index name or an array of the index name instead of the index specification document. To drop multiple indexes (Available starting in MongoDB 4.2), specify an array of the index names. \nBEHAVIOR \nStarting in MongoDB 6.0, db.collection.dropIndexes() raises an error if you attempt to use it to remove the last remaining shard key compatible index. Passing \"*\" to db.collection.dropIndexes() drops all indexes except the _id index and the last remaining shard key compatible index, if one exists. Starting in MongoDB 5.2, you can use db.collection.dropIndexes() to drop existing indexes on the same collection even if there is a build in progress on another index. In earlier versions, attempting to drop a different index during an in-progress index build results in a BackgroundOperationInProgressForNamespace error. \nKILL RELATED QUERIES ONLY \nStarting in MongoDB 4.2, the dropIndexes() operation only kills queries that are using the index being dropped. This may include queries considering the index as part of query planning. Prior to MongoDB 4.2, dropping an index on a collection would kill all open queries on the collection. \nRESOURCE LOCKING \nChanged in version 4.2. db.collection.dropIndexes() obtains an exclusive lock on the specified collection for the duration of the operation. All subsequent operations on the collection must wait until db.collection.dropIndexes() releases the lock.\n\n \nINDEX NAMES \nIf the method is passed an array of index names that includes a non-existent index, the method errors without dropping any of the specified indexes. \n_ID INDEX \nYou cannot drop the default index on the _id field. \nTEXT INDEXES \nTo drop a text index, specify the index name instead of the index specification document. \nSTOP IN-PROGRESS INDEX BUILDS \nStarting in MongoDB 4.4, if an index specified to db.collection.dropIndexes() is still building, db.collection.dropIndexes() attempts to stop the in-progress build. Stopping an index build has the same effect as dropping the built index. In versions earlier than MongoDB 4.4, db.collection.dropIndexes() returns an error if there are any index builds in progress on the collection. For replica sets, run db.collection.dropIndexes() on the primary. The primary stops the index build and creates an associated \"abortIndexBuild\" oplog entry. Secondaries which replicate the \"abortIndexBuild\" oplog entry stop the in-progress index build and discard the build job. See Index Build Process for detailed documentation on the index build process. Use currentOp to identify the index builds associated with a createIndexes or db.collection.createIndexes() operation. See Active Indexing Operations for an example. \nHIDDEN INDEXES \nStarting in version 4.4, MongoDB adds the ability to hide or unhide indexes from the query planner. By hiding an index from the planner, users can evaluate the potential impact of dropping an index without actually dropping the index. If after the evaluation, the user decides to drop the index, the user can drop the hidden index; i.e. you do not need to unhide it first to drop it. If, however, the impact is negative, the user can unhide the index instead of having to recreate a dropped index. And because indexes are fully maintained while hidden, the indexes are immediately available for use once unhidden. For more information on hidden indexes, see Hidden Indexes. ←  db.collection.dropIndex()db.collection.ensureIndex() → On this page  * Definition\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Mongo.getWriteConcern/": " Docs Home → MongoDB Manual \nMONGO.GETWRITECONCERN() \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nMongo.getWriteConcern() \nReturns:The current write concern for the Mongo() connection object. See the Write Concern for an introduction to write concerns in MongoDB. \nSYNTAX \nThe command takes the following form: db.getMongo().getWriteConcern()  This operation returns a document with the following values: { w: <value>, wtimeout: <number>, j: <boolean> } The fields are: Field\nDescription\nw The number of mongod or mongod instances that must acknowledge a write. Possible values are:      * \"majority\". A majority of the target instances must acknowledge the write.  * <number>. The specified number of target instances must acknowledge the write.  * <custom write concern name>. A user defined write concern, the tagged instances must acknowledge the write. See write concern specification for details. j\nA boolean value. j: true requests acknowledgment that the write operation has been written to the on-disk journal.\nwtimeout\nThe number of milliseconds to wait for acknowledgement of the write concern. wtimeout is only applicable when w has a value greater than 1. \nEXAMPLE \nTo return the current write concern, enter the following: db.getMongo().getWriteConcern()  When a write concern is specified using Mongo.setWriteConcern(), the output of Mongo.getWriteConcern() is similar to: WriteConcern { w: 2, wtimeout: 1000, j: true }  The Mongo.getWriteConcern() command returns an empty line if no write concern has been specified. \nTIP \nSEE ALSO:  * setWriteConcern() ←  Mongo.getReadPrefTagSet()Mongo.setCausalConsistency() → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.dropRole/": " Docs Home → MongoDB Manual \nDB.DROPROLE() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.dropRole( rolename, writeConcern ) \nDeletes a user-defined role from the database on which you run the method. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the dropRole command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.dropRole() method takes the following arguments: Parameter\nType\nDescription\nrolename\nstring\nThe name of the user-defined role to remove from the database.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.dropRole() is executed using \"majority\" write concern by default. \nREQUIRED ACCESS \nYou must have the dropRole action on a database to drop a role from that database. \nEXAMPLE \nThe following operations remove the readPrices role from the products database: use productsdb.dropRole( \"readPrices\", { w: \"majority\" } ) \n←  db.createRole()db.dropAllRoles() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.ensureIndex/": " Docs Home → MongoDB Manual \nDB.COLLECTION.ENSUREINDEX() \ndb.collection.ensureIndex(keys, options)  NOTE \nREMOVED IN 5.0 db.collection.ensureIndex() has been replaced by db.collection.createIndex(). \nTIP \nSEE ALSO: db.collection.createIndex() ←  db.collection.dropIndexes()db.collection.estimatedDocumentCount() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.fsyncUnlock/": " Docs Home → MongoDB Manual \nDB.FSYNCUNLOCK() \nOn this page    \n * Definition\n   \n * Compatibility with WiredTiger\n * Example \nDEFINITION \ndb.fsyncUnlock() \nReduces the lock taken by db.fsyncLock() on a mongod instance by 1. \nIMPORTANT The db.fsyncLock() and db.fsyncUnlock() operations maintain a lock count. db.fsyncLock() increments the lock count, and db.fsyncUnlock() decrements the lock count. To unlock a mongod instance for writes, the lock count must be zero. That is, for a given number of db.fsyncLock() operations, you must issue a corresponding number of db.fsyncUnlock() operations to unlock the instance for writes. db.fsyncUnlock() is an administrative operation. Typically you will use db.fsyncUnlock() following a database backup operation. db.fsyncUnlock() has the syntax: db.fsyncUnlock()  The operation returns a document with the following fields: info\nInformation on the status of the operation.\nlockCount (New in version 3.4)\nThe number of locks remaining on the instance after the operation.\nok\nThe status code. The db.fsyncUnlock() method wraps the fsyncUnlock command. \nCOMPATIBILITY WITH WIREDTIGER \ndb.fsyncLock() ensures that the data files are safe to copy using low-level backup utilities such as cp, scp, or tar. A mongod started using the copied files contains user-written data that is indistinguishable from the user-written data on the locked mongod. The data files of a locked mongod may change due to operations such as journaling syncs or WiredTiger snapshots. While this has no affect on the logical data (e.g. data accessed by clients), some backup utilities may detect these changes and emit warnings or fail with errors. For more information on MongoDB- recommended backup utilities and procedures, see MongoDB Backup Methods. \nEXAMPLE \nConsider a situation where db.fsyncLock() has been issued two times. The following db.fsyncUnlock() operation reduces the locks taken by db.fsyncLock() by 1: db.fsyncUnlock()  The operation returns the following document: { \"info\" : \"fsyncUnlock completed\", \"lockCount\" : NumberLong(1), \"ok\" : 1 }  As the lockCount is greater than 0, the mongod instance is locked against writes. To unlock the instance for writes, run db.fsyncLock() again: db.fsyncUnlock()  The operation returns the following document: { \"info\" : \"fsyncUnlock completed\", \"lockCount\" : NumberLong(0), \"ok\" : 1 }  The mongod instance is unlocked for writes. ←  db.fsyncLock()db.getCollection() → On this page  * Definition\n * Compatibility with WiredTiger\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.status/": " Docs Home → MongoDB Manual \nRS.STATUS() \nOn this page    \n * Definition\n   \n * Output \nDEFINITION \nrs.status() \nReturns the replica set status from the point of view of the member where the method is run. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the replSetGetStatus command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 This output reflects the current status of the replica set, using data derived from the heartbeat packets sent by the other members of the replica set. \nOUTPUT \nFor an example and details on the output, see replSetGetStatus.  * Starting in MongoDB 4.2.1\n   \n   If you run the mongosh helper method rs.status() (or the replSetGetStatus command) on a member during its initial sync (i.e. STARTUP2 state), the command returns replSetGetStatus.initialSyncStatus metrics.\n   \n   Once the member completes its initial sync, the replSetGetStatus.initialSyncStatus metrics becomes unavailable.  * In earlier versions (3.4.x-4.2.0)\n   \n   To return initial sync status information, you must the replSetGetStatus command with the initialSync: 1 option on a secondary member or a member in STARTUP2 state:\n   \n   db.adminCommand( { replSetGetStatus: 1, initialSync: 1 } )\n   \n   \n   \n   The replSetGetStatus.initialSyncStatus metrics remains available after the member completes its initial sync. That is, you can run the replSetGetStatus command with the initialSync: 1 on the secondary member to return its initial sync information.\n   \n   You cannot specify initialSync: 1 to the mongosh helper method rs.status(). ←  rs.remove()rs.stepDown() → On this page  * Definition\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.serverStatus/": " Docs Home → MongoDB Manual \nDB.SERVERSTATUS() \nOn this page    \n * Behavior\n   \n * Output db.serverStatus() \nReturns a document that provides an overview of the database process's state. This command provides a wrapper around the database command serverStatus. \nBEHAVIOR \nBy default, db.serverStatus() excludes in its output:      * some content in the repl document.  * mirroredReads document. (Available starting in version 4.4) To include fields that are excluded by default, specify the top-level field and set it to 1 in the command. To exclude fields that are included by default, specify the field and set to 0. You can specify either top-level or embedded fields. For example, the following operation suppresses the repl, metrics and locks information in the output. db.serverStatus( { repl: 0,  metrics: 0, locks: 0 } )  The following example includes all repl information in the output: db.serverStatus( { repl: 1 } )  \nINITIALIZATION \nThe statistics reported by db.serverStatus() are reset when the mongod server is restarted. The db.serverStatus() command does not report some statistics until they have been initialized by server events. For example, after restarting the mongod server, db.serverStatus() won't return any values for findAndModify. db.serverStatus().metrics.commands.findAndModify// No results returned  After you run an update query, subsequent calls to db.serverStatus() display the expected metrics. {   \"arrayFilters\" : NumberLong(0),   \"failed\" : NumberLong(0),   \"pipeline\" : NumberLong(0),   \"total\" : NumberLong(1)} \nNOTE The db.serverStatus() method returns an error if a specific object is queried before the counters have begun to increment. If there haven't been any document updates yet: db.serverStatus().metrics.commands.update.pipeline  Returns: TypeError: db.serverStatus(...).metrics.commands.update is undefined :@(shell):1:1 \nINCLUDE MIRROREDREADS \nBy default, the mirroredReads information (available starting in version 4.4) is not included in the output. To return mirroredReads information, you must explicitly specify the inclusion: db.serverStatus( { mirroredReads: 1 } )  \nINCLUDE LATCHANALYSIS \nBy default, the latchAnalysis information (available starting in version 4.4) is not included in the output. To return latchAnalysis information, you must explicitly specify the inclusion: db.serverStatus( { latchAnalysis: 1 } )  \nOUTPUT \nSee serverStatus Output for complete documentation of the output of this function. ←  db.serverCmdLineOpts()db.setLogLevel() → On this page  * Behavior\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.addKeyAlternateName/": " Docs Home → MongoDB Manual \nKEYVAULT.ADDKEYALTERNATENAME() \nOn this page    \n * Behavior\n   \n * Example New in version 4.2. KeyVault.addKeyAlternateName(UUID, keyAltName) \nAdds the keyAltName to the keyAltNames array of the data encryption key with UUID. keyAltName must be unique among all keys in the key vault. The getKeyVault() method automatically creates a unique index on the keyAltNames field with a partial index filter for only documents where keyAltNames exists. addKeyAlternateName() has the following syntax: keyVault = db.getMongo().getKeyVault()\nkeyVault.addKeyAlternateName(  UUID(\"<UUID string>\"),  \"keyAlternateName\")  Returns:Returns the previous version of the data encryption key document. Returns null if no data encryption key has the specified UUID(). \nBEHAVIOR  REQUIRES CONFIGURING CLIENT-SIDE FIELD LEVEL ENCRYPTION ON DATABASE CONNECTION \nThe mongosh client-side field level encryption methods require a database connection with client-side field level encryption enabled. If the current database connection was not initiated with client-side field level encryption enabled, either:      * Use the Mongo() constructor from the mongosh to establish a connection with the required client-side field level encryption options. The Mongo() method supports the following Key Management Service (KMS) providers for Customer Master Key (CMK) management:\n   \n   * Amazon Web Services KMS\n   \n   * Azure Key Vault\n   \n   * Google Cloud Platform KMS\n   \n   * Locally Managed Key\n   \n   or  * Use the mongosh command line options to establish a connection with the required options. The command line options only support the Amazon Web Services KMS provider for CMK management. \nEXAMPLE \nThe following example is intended for rapid evaluation of client-side field level encryption. For specific examples of using KeyVault.addKeyAlternateName() with each supported KMS provider, see Encryption Key Management. To configure client-side field level encryption for a locally managed key:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key export TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  Create the client-side field level encryption object using the generated local key string:  var autoEncryptionOpts = {   \"keyVaultNamespace\" : \"encryption.__dataKeys\",   \"kmsProviders\" : {     \"local\" : {       \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])     }   } }  Use the Mongo() constructor with the client-side field level encryption options configured to create a database connection. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",   autoEncryptionOpts)  Retrieve the KeyVault object and use the KeyVault.addKeyAlternateName() method to add a new key alternate name to the data encryption key with matching UUID. The specified key alternate name must be unique: keyVault = encryptedClient.getKeyVault()keyVault.addKeyAlternateName(UUID(\"b4b41b33-5c97-412e-a02b-743498346079\"),\"Other-Data-Encryption-Key\")  If successful, addKeyAlternateName() returns the previous version of data encryption key document:\n\n  To view the current version of the data encryption key document, use KeyVault.getKey() specifying the _id of the returned document or KeyVault.getKeyByAltName() specifying one of the keyAltNames. \nTIP \nSEE ALSO: Encryption Key Management ←  KeyVault.getKeys()KeyVault.removeKeyAlternateName() → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.findOne/": " Docs Home → MongoDB Manual \nDB.COLLECTION.FINDONE() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.findOne(query, projection, options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the find command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns one document that satisfies the specified query criteria on the collection or view. If multiple documents satisfy the query, this method returns the first document according to the natural order which reflects the order of documents on the disk. In capped collections, natural order is the same as insertion order. If no document satisfies the query, the method returns null. Parameter\nType\nDescription\nquery\ndocument\nOptional. Specifies query selection criteria using query operators.\nprojection\ndocument\nOptional. Specifies the fields to return using projection operators. Omit this parameter to return all fields in the matching document. For details, see Projection.\noptions\ndocument Optional. Specifies additional options for the query. These options modify query behavior and how results are returned. To see available options, see FindOptions. Returns:One document that satisfies the criteria specified as the first argument to this method. If you specify a projection parameter, findOne() returns a document that only contains the projection fields. The _id field is always included unless you explicitly exclude it. Although similar to the find() method, the findOne() method returns a document rather than a cursor. \nBEHAVIOR  CLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued db.collection.findOne() disconnects before the operation completes, MongoDB marks db.collection.findOne() for termination using killOp. \nPROJECTION  IMPORTANT \nLANGUAGE CONSISTENCY Starting in MongoDB 4.4, as part of making find() and findAndModify() projection consistent with aggregation's $project stage,  * The find() and findAndModify() projection can accept aggregation expressions and syntax.  * MongoDB enforces additional restrictions with regards to projections. See Projection Restrictions for details. The projection parameter determines which fields are returned in the matching documents. The projection parameter takes a document of the following form: { field1: <value>, field2: <value> ... } \nProjection\nDescription\n<field>: <1 or true>\nSpecifies the inclusion of a field. Non-zero integers are also treated as true.\n<field>: <0 or false>\nSpecifies the exclusion of a field.\n\"<field>.$\": <1 or true>\nWith the use of the $ array projection operator, you can specify the projection to return the first element that match the query condition on the array field; e.g. \"arrayField.$\" : 1. (Not available for views.) Non-zero integers are also treated as true.\n<field>: <array projection>\nUsing the array projection operators $elemMatch, $slice, specifies the array element(s) to include, thereby excluding those elements that do not meet the expressions. (Not available for views.)\n<field>: <$meta expression>\nUsing the $meta operator expression, specifies the inclusion of available per-document metadata. (Not available for views.)\n<field>: <aggregation expression> Specifies the value of the projected field. Starting in MongoDB 4.4, with the use of aggregation expressions and syntax, including the use of literals and aggregation variables, you can project new fields or project existing fields with new values. For example,  * If you specify a non-numeric, non-boolean literal (such as a literal string or an array or an operator expression) for the projection value, the field is projected with the new value; e.g.:\n   \n   * { field: [ 1, 2, 3, \"$someExistingField\" ] }\n   \n   * { field: \"New String Value\" }\n   \n   * { field: { status: \"Active\", total: { $sum: \"$existingArray\" } } }\n\n In versions 4.2 and earlier, any specification value (with the exception of the previously unsupported document value) is treated as either true or false to indicate the inclusion or exclusion of the field. New in version 4.4. EMBEDDED FIELD SPECIFICATION \nFor fields in an embedded documents, you can specify the field using either:  * dot notation; e.g. \"field.nestedfield\": <value>  * nested form; e.g. { field: { nestedfield: <value> } } (Starting in MongoDB 4.4) _ID FIELD PROJECTION \nThe _id field is included in the returned documents by default unless you explicitly specify _id: 0 in the projection to suppress the field. INCLUSION OR EXCLUSION \nA projection cannot contain both include and exclude specifications, with the exception of the _id field:  * In projections that explicitly include fields, the _id field is the only field that you can explicitly exclude.  * In projections that explicitly excludes fields, the _id field is the only field that you can explicitly include; however, the _id field is included by default. For more information on projection, see also:  * Projection  * Project Fields to Return from Query \nEXAMPLES  WITH EMPTY QUERY SPECIFICATION \nThe following operation returns a single document from the bios collection: db.bios.findOne()  \nWITH A QUERY SPECIFICATION \nThe following operation returns the first matching document from the bios collection where either the field first in the embedded document name starts with the letter G or where the field birth is less than new Date('01/01/1945'): db.bios.findOne(   {     $or: [            { 'name.first' : /^G/ },            { birth: { $lt: new Date('01/01/1945') } }          ]   })  \nWITH A PROJECTION \nThe projection parameter specifies which fields to return. The parameter contains either include or exclude specifications, not both, unless the exclude is for the _id field. SPECIFY THE FIELDS TO RETURN \nThe following operation finds a document in the bios collection and returns only the name, contribs and _id fields: db.bios.findOne(    { },    { name: 1, contribs: 1 })  RETURN ALL BUT THE EXCLUDED FIELDS \nThe following operation returns a document in the bios collection where the contribs field contains the element OOP and returns all fields except the _id field, the first field in the name embedded document, and the birth field: db.bios.findOne(   { contribs: 'OOP' },   { _id: 0, 'name.first': 0, birth: 0 })  \nTHE FINDONE RESULT DOCUMENT \nYou cannot apply cursor methods to the result of findOne() because a single document is returned. You have access to the document directly: var myDocument = db.bios.findOne();\nif (myDocument) {   var myName = myDocument.name;\n   print (tojson(myName));}  \nUSE VARIABLES IN LET OPTION \nYou can specify query options to modify query behavior and indicate how results are returned. For example, to define variables that you can access elsewhere in the findOne method, use the let option. To filter results using a variable, you must access the variable within the $expr operator. Create a collection cakeFlavors: db.cakeFlavors.insertMany( [   { _id: 1, flavor: \"chocolate\" },   { _id: 2, flavor: \"strawberry\" },   { _id: 3, flavor: \"cherry\" }] )  The following example defines a targetFlavor variable in let and uses the variable to retrieve the chocolate cake flavor: db.cakeFlavors.findOne(   { $expr: { $eq: [ \"$flavor\", \"$$targetFlavor\" ] } },   { _id: 0 },   { let : { targetFlavor: \"chocolate\" }} )  Output: { flavor: 'chocolate' }  To see all available query options, see FindOptions. ←  db.collection.findAndModify()db.collection.findOneAndDelete() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.setProfilingLevel/": " Docs Home → MongoDB Manual \nDB.SETPROFILINGLEVEL() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.setProfilingLevel(level, options) \nChanged in version 5.0. For a mongod instance, the method enables, disables, or configures the Database Profiler. The profiler captures and records data on the performance of write operations, cursors, and database commands on a running mongod instance. If the profiler is disabled, the method configures how slow operations are logged to the diagnostic log. If the database profiler level is 1 or 2 (specifically, the database profiler is enabled), the slowms, sampleRate affect the behavior of both the profiler and the diagnostic log. If the database profiler level is 0 (specifically, database profiler is disabled), the slowms and sampleRate, affect only the diagnostic log. With mongos instances, the method sets the slowms, sampleRate and filter configuration settings, which configure how operations get written to the diagnostic log. You cannot enable the Database Profiler on a mongos instance because mongos does not have any collections that the profiler can write to. The profile level must be 0 for a mongos instance. Starting in MongoDB 4.4.2, you can specify a filter on both mongod and mongos instances to control which operations are logged by the profiler. When you specify a filter for the profiler, the slowms, and sampleRate options are not used for profiling and slow-query log lines. db.setProfilingLevel() provides a wrapper around the profile command. Starting in MongoDB 5.0 (also available starting in 4.4.2, and 4.2.12), changes made to the database profiler level, slowms, sampleRate, or filter using the profile command or db.setProfilingLevel() wrapper method are recorded in the log file. \nSYNTAX \nThe db.setProfilingLevel() method has the following form: db.setProfilingLevel(<level>, <options>)  \nPARAMETERS \nParameter\nType\nDescription\nlevel\ninteger Configures the database profiler level. The following profiler levels are available: Level\nDescription\n0\nThe profiler is off and does not collect any data. This is the default profiler level.\n1 The profiler collects data for operations that take longer than the value of slowms or that match a filter. When a filter is set:      * The slowms and sampleRate options are not used for profiling.  * The profiler only captures operations that match the filter. 2\nThe profiler collects data for all operations. Because profiling is not available on mongos, db.setProfilingLevel() cannot be used to set the profiling level to a value other than 0 on a mongos instance. options\ndocument or integer Optional. Accepts an integer or an options document. If an integer value is passed as the options argument instead of a document, the value is assigned to slowms. The following options are available: slowms\nDefault: 100\nType: integer The slow operation time threshold, in milliseconds. Operations that run for longer than this threshold are considered slow. When logLevel is set to 0, MongoDB records slow operations to the diagnostic log at a rate determined by slowOpSampleRate. At higher logLevel settings, all operations appear in the diagnostic log regardless of their latency with the following exception: the logging of slow oplog entry messages by the secondaries. The secondaries log only the slow oplog entries; increasing the logLevel does not log all oplog entries. For mongod instances, the setting affects both the diagnostic log and, if enabled, the profiler. For mongos instances, the setting affects the diagnostic log only and not the profiler because profiling is not available on mongos. \nNOTE This argument affects the same setting as the configuration file option slowOpThresholdMs. sampleRate\nDefault: 1.0\nType: double The fraction of slow operations that should be profiled or logged. sampleRate accepts values between 0 and 1, inclusive. For mongod instances, the setting affects both the diagnostic log and, if enabled, the profiler. For mongos instances, the setting affects the diagnostic log only and not the profiler because profiling is not available on mongos. \nNOTE This argument affects the same setting as the configuration option slowOpSampleRate. filter Type: object A filter expression that controls which operations are profiled and logged. The field in the filter expression can be any field in the profiler output.\n\n For mongos instances, the setting affects the diagnostic log only and not the profiler because profiling is not available on mongos. For an example of a filter used to control logged operations, see Set a Filter to Determine Profiled Operations. New in version 4.4.2. \nNOTE When a profiling filter is set, the slowms and sampleRate options do not affect the diagnostic log or the profiler. \nRETURNS \nThe method returns a document that contains the previous values of the settings.  \nWhere:  * was is the previous level setting.  * slowms is the previous slowms setting.  * sampleRate is the previous sampleRate setting.  * filter is the previous filter setting. (New in MongoDB 4.4.2)  * note is a string explaining the behavior of filter. This field only appears in the output when filter is also present. (New in MongoDB 4.4.2) \nNOTE The filter and note fields only appear in the output if they were present in the previous level setting. To view the current profiling level, see db.getProfilingStatus(). \nBEHAVIOR  IMPORTANT Profiling can impact performance and shares settings with the system log. Carefully consider any performance and security implications before configuring and enabling the profiler on a production deployment. See Profiler Overhead for more information on potential performance degradation. \nEXAMPLES  ENABLE PROFILER AND SET SLOW OPERATION THRESHOLD AND SAMPLE RATE \nThe following example sets for a mongod instance:  * the profiling level to 1,  * the slow operation threshold slowms to 20 milliseconds, and  * the sampleRate to 0.42. db.setProfilingLevel(1, { slowms: 20, sampleRate: 0.42 })  The method returns a document with the previous values for the settings. To view the current profiling level, see db.getProfilingStatus(). \nDISABLE PROFILER AND SET SLOW OPERATION THRESHOLD AND SAMPLE RATE \nThe following example sets for a mongod or mongos instance:  * the profiling level to 0,  * the slow operation threshold slowms to 20 milliseconds, and  * the sampleRate to 0.42. db.setProfilingLevel(0, { slowms: 20, sampleRate: 0.42 })  The method returns a document with the previous values for the settings. To view the current profiling level, see db.getProfilingStatus(). \nSET A FILTER TO DETERMINE PROFILED OPERATIONS \nNew in version 4.4.2. The following example sets for a mongod instance:  * the profiling level to 1,  * a filter of { op: \"query\", millis: { $gt: 2000 } }, which causes the profiler to only record query operations that took longer than 2 seconds. db.setProfilingLevel( 1, { filter: { op: \"query\", millis: { $gt: 2000 } } } )  The method returns a document with the previous values for the settings. To view the current profiling level, see db.getProfilingStatus(). ←  db.setLogLevel()db.shutdownServer() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.distinct/": " Docs Home → MongoDB Manual \nDB.COLLECTION.DISTINCT() \nOn this page    \n * Definition\n   \n * Options\n * Behavior\n * Examples \nDEFINITION \ndb.collection.distinct(field, query, options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the distinct command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Finds the distinct values for a specified field across a single collection or view and returns the results in an array. This method takes the following parameters: Parameter\nType\nDescription\nfield\nstring\nThe field for which to return distinct values.\nquery\ndocument\nA query that specifies the documents from which to retrieve the distinct values.\noptions\ndocument\nOptional. A document that specifies the options. See Options. \nNOTE Results must not be larger than the maximum BSON size. If your results exceed the maximum BSON size, use the aggregation pipeline to retrieve distinct values using the $group operator, as described in Retrieve Distinct Values with the Aggregation Pipeline. The following diagram shows an example db.collection.distinct() call.  \nOPTIONS \n{ collation: <document> } \nField\nType\nDescription\ncollation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. \nBEHAVIOR \nIn a sharded cluster, the distinct command may return orphaned documents. \nARRAY FIELDS \nIf the value of the specified field is an array, db.collection.distinct() considers each element of the array as a separate value. For instance, if a field has as its value [ 1, [1], 1 ], then db.collection.distinct() considers 1, [1], and 1 as separate values. For an example, see Return Distinct Values for an Array Field. \nINDEX USE \nWhen possible, db.collection.distinct() operations can use indexes. Indexes can also cover db.collection.distinct() operations. See Covered Query for more information on queries covered by indexes. \nTRANSACTIONS \nTo perform a distinct operation within a transaction:  * For unsharded collections, you can use the db.collection.distinct() method/the distinct command as well as the aggregation pipeline with the $group stage.  * For sharded collections, you cannot use the db.collection.distinct() method or the distinct command.\n   \n   To find the distinct values for a sharded collection, use the aggregation pipeline with the $group stage instead. See Distinct Operation for details. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. \nCLIENT DISCONNECTION\n\n \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, distinct operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nEXAMPLES \nThe examples use the inventory collection that contains the following documents: { \"_id\": 1, \"dept\": \"A\", \"item\": { \"sku\": \"111\", \"color\": \"red\" }, \"sizes\": [ \"S\", \"M\" ] }{ \"_id\": 2, \"dept\": \"A\", \"item\": { \"sku\": \"111\", \"color\": \"blue\" }, \"sizes\": [ \"M\", \"L\" ] }{ \"_id\": 3, \"dept\": \"B\", \"item\": { \"sku\": \"222\", \"color\": \"blue\" }, \"sizes\": \"S\" }{ \"_id\": 4, \"dept\": \"A\", \"item\": { \"sku\": \"333\", \"color\": \"black\" }, \"sizes\": [ \"S\" ] }  \nRETURN DISTINCT VALUES FOR A FIELD \nThe following example returns the distinct values for the field dept from all documents in the inventory collection: db.inventory.distinct( \"dept\" )  The method returns the following array of distinct dept values: [ \"A\", \"B\" ]  \nRETURN DISTINCT VALUES FOR AN EMBEDDED FIELD \nThe following example returns the distinct values for the field sku, embedded in the item field, from all documents in the inventory collection: db.inventory.distinct( \"item.sku\" )  The method returns the following array of distinct sku values: [ \"111\", \"222\", \"333\" ]  \nTIP \nSEE ALSO: Dot Notation for information on accessing fields within embedded documents \nRETURN DISTINCT VALUES FOR AN ARRAY FIELD \nThe following example returns the distinct values for the field sizes from all documents in the inventory collection: db.inventory.distinct( \"sizes\" )  The method returns the following array of distinct sizes values: [ \"M\", \"S\", \"L\" ]  For information on distinct() and array fields, see the Behavior section. \nSPECIFY QUERY WITH DISTINCT \nThe following example returns the distinct values for the field sku, embedded in the item field, from the documents whose dept is equal to \"A\": db.inventory.distinct( \"item.sku\", { dept: \"A\" } )  The method returns the following array of distinct sku values: [ \"111\", \"333\" ]  \nSPECIFY A COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents: { _id: 1, category: \"café\", status: \"A\" }{ _id: 2, category: \"cafe\", status: \"a\" }{ _id: 3, category: \"cafE\", status: \"a\" }  The following aggregation operation includes the Collation option: db.myColl.distinct( \"category\", {}, { collation: { locale: \"fr\", strength: 1 } } )  For descriptions on the collation fields, see Collation Document. ←  db.collection.deleteMany()db.collection.drop() → On this page  * Definition\n * Options\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.allowPartialResults/": " Docs Home → MongoDB Manual \nCURSOR.ALLOWPARTIALRESULTS() \nOn this page    \n * Definition\n   \n * Syntax \nDEFINITION \ncursor.allowPartialResults()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. When used with db.collection.find() operations against a sharded collection, returns partial results, rather than an error, if one or more queried shards are unavailable. \nSYNTAX \nThe cursor.allowPartialResults() method has the following syntax: db.collection.find(<query>, <projection>).allowPartialResults() \n←  cursor.allowDiskUse()cursor.batchSize() → On this page  * Definition\n * Syntax Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.remove/": " Docs Home → MongoDB Manual \nRS.REMOVE() \nOn this page    \n * Definition\n   \n * Behavior \nDEFINITION \nrs.remove(hostname) \nRemoves the member described by the hostname parameter from the current replica set. This function will disconnect the shell briefly and forces a reconnection as the replica set renegotiates which member will be primary. As a result, the shell will display an error even if this command succeeds. The rs.remove() method has the following parameter: Parameter\nType\nDescription\nhostname\nstring\nThe hostname of a system in the replica set. \nNOTE Before running the rs.remove() operation, it is good practice to shut down the replica set member that you are removing. \nBEHAVIOR \nBy default, replica set members wait for 5 minutes before dropping connections to the removed member. In sharded replica sets, you can modify this timeout using the ShardingTaskExecutorPoolHostTimeoutMS server parameter. New in version 4.2: To immediately drop all outgoing connections from the replica set to the removed member, run the dropConnections administrative command on each remaining member on the replica set: db.adminCommand(  {    \"dropConnections\" : 1,    \"hostAndPort\" : [      \"<hostname>:<port>\"    ]  })  Replace <hostname> with the hostname of the removed member and <port> with the port the mongod listened on. ←  rs.reconfigForPSASet()rs.status() → On this page  * Definition\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.addShardTag/": " Docs Home → MongoDB Manual \nSH.ADDSHARDTAG() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \nsh.addShardTag(shard, tag) \nChanged in version 3.4: This method aliases to sh.addShardToZone() in MongoDB 3.4. The functionality specified below still applies to MongoDB 3.2. MongoDB 3.4 provides Zone sharding as the successor to tag-aware sharding. Associates a shard with a tag or identifier. MongoDB uses these identifiers to direct chunks that fall within a tagged range to specific shards. sh.addTagRange() associates chunk ranges with tag ranges. Parameter\nType\nDescription\nshard\nstring\nThe name of the shard to which to give a specific tag.\ntag\nstring\nThe name of the tag to add to the shard. Only issue sh.addShardTag() when connected to a mongos instance. \nTIP By defining the zones and the zone ranges before sharding an empty or a non-existing collection, the shard collection operation creates chunks for the defined zone ranges as well as any additional chunks to cover the entire range of the shard key values and performs an initial chunk distribution based on the zone ranges. This initial creation and distribution of chunks allows for faster setup of zoned sharding. After the initial distribution, the balancer manages the chunk distribution going forward. See Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection for an example. \nEXAMPLE \nThe following example adds three tags, NYC, LAX, and NRT, to three shards: sh.addShardTag(\"shard0000\", \"NYC\")sh.addShardTag(\"shard0001\", \"LAX\")sh.addShardTag(\"shard0002\", \"NRT\")  \nTIP \nSEE ALSO:      * sh.addTagRange()  * sh.removeShardTag() ←  sh.addShard()sh.addShardToZone() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-replication/": " Docs Home → MongoDB Manual \nREPLICATION METHODS  NOTE For details on a specific method, including syntax and examples, click on the link to the method's reference page. Name\nDescription\nrs.add()\nAdds a member to a replica set.\nrs.addArb()\nAdds an arbiter to a replica set.\nrs.conf()\nReturns the replica set configuration document.\nrs.freeze()\nPrevents the current member from seeking election as primary for a period of time.\nrs.help()\nReturns basic help text for replica set functions.\nrs.initiate()\nInitializes a new replica set.\nrs.printReplicationInfo()\nPrints a formatted report of the replica set status from the perspective of the primary.\nrs.printSecondaryReplicationInfo()\nPrints a formatted report of the replica set status from the perspective of the secondaries.\nrs.printSlaveReplicationInfo() Deprecated since version 4.4.1: Use rs.printSecondaryReplicationInfo() instead. rs.reconfig()\nReconfigures a replica set by applying a new replica set configuration object.\nrs.reconfigForPSASet()\nSafely perform some reconfiguration changes on a primary-secondary-arbiter (PSA) replica set or on a replica set that is changing to a PSA architecture.\nrs.remove()\nRemove a member from a replica set.\nrs.status()\nReturns a document with information about the state of the replica set.\nrs.stepDown()\nCauses the current primary to become a secondary which forces an election.\nrs.syncFrom()\nSets the member that this replica set member will sync from, overriding the default sync target selection logic. ←  db.updateRole()rs.add() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.balancerCollectionStatus/": " Docs Home → MongoDB Manual \nSH.BALANCERCOLLECTIONSTATUS() \nOn this page    \n * Definition\n   \n * Syntax\n * Access Control\n * Examples \nDEFINITION \nsh.balancerCollectionStatus(namespace) \nNew in version 4.4. Returns a document that contains information about whether the chunks of a sharded collection are balanced (i.e. do not need to be moved) as of the time the command is run or need to be moved because of draining shards, zone violation or imbalance of chunks across shards. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the balancerCollectionStatus command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 \nSYNTAX \nThe sh.balancerCollectionStatus() has the following form: sh.balancerCollectionStatus( <namespace> )  \nPARAMETER \nThe sh.balancerCollectionStatus() method takes the following parameter: Parameter\nType\nDescription\nnamespace\nString The namespace of the collection to shard in the form \"<database>.<collection>\". \nACCESS CONTROL \nWhen running with access control, the user must have the enableSharding privilege actions on database and/or collection to run the method. That is, a user must have a role that grants the following privilege: { resource: { db: <database>, collection: <collection> }, actions: [ \"enableSharding\" ] }  The built-in clusterManager role provides the appropriate privileges. \nEXAMPLES \nTo check whether the chunks of a sharded collection test.contacts is currently in balance, connect to a mongos instance and issue the following: sh.balancerCollectionStatus(\"test.contacts\")  If the chunks for the collection do not need to be moved, the method returns an output similar to the following: {   \"balancerCompliant\" : true,   \"ok\" : 1,   \"operationTime\" : Timestamp(1583193238, 1),   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1583193238, 1),      \"signature\" : {         \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"),         \"keyId\" : NumberLong(0)      }   }} \nONGOING DEFRAGMENTATION PROCESS \nIf the queried namespace is going through chunk defragmentation, the sh.balancerCollectionStatus method returns output similar to the following: {   \"balancerCompliant\": false,   \"firstComplianceViolation\": \"defragmentingChunks\",   \"details\": {      \"currentPhase\": \"moveAndMergeChunks\",      \"progress\": { \"remainingChunksToProcess\": 1 }   }} \nNOTE Chunk defragmentation occurs in multiple phases. The progress field only pertains to the current phase. For the description of the output, see balancerCollectionStatus Output. ←  sh.addTagRange()sh.commitReshardCollection() → On this page  * Definition\n * Syntax\n * Access Control\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.findAndModify/": " Docs Home → MongoDB Manual \nDB.COLLECTION.FINDANDMODIFY() \nOn this page    \n * Definition\n   \n * Return Data\n * Behavior\n * Examples \nDEFINITION \ndb.collection.findAndModify(document)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the findAndModify command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Modifies and returns a single document. By default, the returned document does not include the modifications made on the update. To return the document with the modifications made on the update, use the new option. Changed in version 5.0. The findAndModify() method has the following form: db.collection.findAndModify({    query: <document>,    sort: <document>,    remove: <boolean>,    update: <document or aggregation pipeline>, // Changed in MongoDB 4.2    new: <boolean>,    fields: <document>,    upsert: <boolean>,    bypassDocumentValidation: <boolean>,    writeConcern: <document>,    collation: <document>,    arrayFilters: [ <filterdocument1>, ... ],    let: <document> // Added in MongoDB 5.0});  The db.collection.findAndModify() method takes a document parameter with the following embedded document fields: Parameter\nType\nDescription\nquery\ndocument Optional. The selection criteria for the modification. The query field employs the same query selectors as used in the db.collection.find() method. Although the query may match multiple documents, db.collection.findAndModify() will only select one document to modify. If unspecified, defaults to an empty document. Starting in MongoDB 4.2 (and 4.0.12+, 3.6.14+, and 3.4.23+), the operation errors if the query argument is not a document. sort document Optional. Determines which document the operation modifies if the query selects multiple documents. db.collection.findAndModify() modifies the first document in the sort order specified by this argument. Starting in MongoDB 4.2 (and 4.0.12+, 3.6.14+, and 3.4.23+), the operation errors if the sort argument is not a document. MongoDB does not store documents in a collection in a particular order. When sorting on a field which contains duplicate values, documents containing those values may be returned in any order. If consistent sort order is desired, include at least one field in your sort that contains unique values. The easiest way to guarantee this is to include the _id field in your sort query. See Sort Consistency for more information. remove\nboolean\nMust specify either the remove or the update field. Removes the document specified in the query field. Set this to true to remove the selected document . The default is false.\nupdate\ndocument or array Must specify either the remove or the update field. Performs an update of the selected document.  * If passed a document with update operator expressions, db.collection.findAndModify() performs the specified modification.  * If passed a replacement document { <field1>: <value1>, ...}, the db.collection.findAndModify() performs a replacement.  * Starting in MongoDB 4.2, if passed an aggregation pipeline [ <stage1>, <stage2>, ... ], db.collection.findAndModify() modifies the document per the pipeline. The pipeline can consist of the following stages:\n   \n   * $addFields and its alias $set\n   \n   * $project and its alias $unset\n   \n   * $replaceRoot and its alias $replaceWith. new\nboolean\nOptional. When true, returns the modified document rather than the original. The db.collection.findAndModify() method ignores the new option for remove operations. The default is false.\nfields\ndocument Optional. A subset of fields to return. The fields document specifies an inclusion of a field with 1, as in: fields: { <field1>: 1, <field2>: 1, ... }.\n\n For more information on projection, see fields Projection. upsert\nboolean Optional. Used in conjunction with the update field. When true, findAndModify() either:  * Creates a new document if no documents match the query. For more details see upsert behavior.  * Updates a single document that matches the query. To avoid multiple upserts, ensure that the query field(s) are uniquely indexed. See Upsert with Unique Index for an example. Defaults to false, which does not insert a new document when no match is found. bypassDocumentValidation\nboolean\nOptional. Enables db.collection.findAndModify() to bypass document validation during the operation. This lets you update documents that do not meet the validation requirements.\nwriteConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. maxTimeMS\ninteger\nOptional. Specifies a time limit in milliseconds for processing the operation.\ncollation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. arrayFilters\narray Optional. An array of filter documents that determine which array elements to modify for an update operation on an array field. In the update document, use the $[<identifier>] filtered positional operator to define an identifier, which you then reference in the array filter documents. You cannot have an array filter document for an identifier if the identifier is not included in the update document. \nNOTE The <identifier> must begin with a lowercase letter and contain only alphanumeric characters. You can include the same identifier multiple times in the update document; however, for each distinct identifier ($[identifier]) in the update document, you must specify exactly one corresponding array filter document. That is, you cannot specify multiple array filter documents for the same identifier. For example, if the update statement includes the identifier x (possibly multiple times), you cannot specify the following for arrayFilters that includes 2 separate filter documents for x: // INVALID\n[  { \"x.a\": { $gt: 85 } },  { \"x.b\": { $gt: 80 } }] However, you can specify compound conditions on the same identifier in a single filter document, such as in the following examples: // Example 1[  { $or: [{\"x.a\": {$gt: 85}}, {\"x.b\": {$gt: 80}}] }]// Example 2[  { $and: [{\"x.a\": {$gt: 85}}, {\"x.b\": {$gt: 80}}] }]// Example 3[  { \"x.a\": { $gt: 85 }, \"x.b\": { $gt: 80 } }]  For examples, see Specify arrayFilters for an Array Update Operations. \nNOTE arrayFilters is not available for updates that use an aggregation pipeline. let\ndocument Optional. Specifies a document with a list of variables. This allows you to improve command readability by separating the variables from the query text. The document syntax is: { <variable_name_1>: <expression_1>,  ...,  <variable_name_n>: <expression_n> } The variable is set to the value returned by the expression, and cannot be changed afterwards. To access the value of a variable in the command, use the double dollar sign prefix ($$) together with your variable name in the form $$<variable_name>. For example: $$targetTotal. \nNOTE\n\n For a complete example using let and variables, see Use Variables in let. New in version 5.0. \nRETURN DATA \nFor remove operations, if the query matches a document, findAndModify() returns the removed document. If the query does not match a document to remove, findAndModify() returns null. For update operations, findAndModify() returns one of the following:  * If the new parameter is not set or is false:\n   \n   * the pre-modification document if the query matches a document;\n   \n   * otherwise, null.  * If new is true:\n   \n   * the modified document if the query returns a match;\n   \n   * the inserted document if upsert: true and no document matches the query;\n   \n   * otherwise, null. \nBEHAVIOR  FIELDS PROJECTION  IMPORTANT \nLANGUAGE CONSISTENCY Starting in MongoDB 4.4, as part of making find() and findAndModify() projection consistent with aggregation's $project stage,  * The find() and findAndModify() projection can accept aggregation expressions and syntax.  * MongoDB enforces additional restrictions with regards to projections. See Projection Restrictions for details. The fields option takes a document in the following form: { field1: <value>, field2: <value> ... } \nProjection\nDescription\n<field>: <1 or true>\nSpecifies the inclusion of a field. Non-zero integers are also treated as true.\n<field>: <0 or false>\nSpecifies the exclusion of a field.\n\"<field>.$\": <1 or true>\nWith the use of the $ array projection operator, you can specify the projection to return the first element that match the query condition on the array field; e.g. \"arrayField.$\" : 1. (Not available for views.) Non-zero integers are also treated as true.\n<field>: <array projection>\nUsing the array projection operators $elemMatch, $slice, specifies the array element(s) to include, thereby excluding those elements that do not meet the expressions. (Not available for views.)\n<field>: <aggregation expression> Specifies the value of the projected field. Starting in MongoDB 4.4, with the use of aggregation expressions and syntax, including the use of literals and aggregation variables, you can project new fields or project existing fields with new values. For example,  * If you specify a non-numeric, non-boolean literal (such as a literal string or an array or an operator expression) for the projection value, the field is projected with the new value; e.g.:\n   \n   * { field: [ 1, 2, 3, \"$someExistingField\" ] }\n   \n   * { field: \"New String Value\" }\n   \n   * { field: { status: \"Active\", total: { $sum: \"$existingArray\" } } }  * To project a literal value for a field, use the $literal aggregation expression; e.g.:\n   \n   * { field: { $literal: 5 } }\n   \n   * { field: { $literal: true } }\n   \n   * { field: { $literal: { fieldWithValue0: 0, fieldWithValue1: 1 } } } In versions 4.2 and earlier, any specification value (with the exception of the previously unsupported document value) is treated as either true or false to indicate the inclusion or exclusion of the field. New in version 4.4. EMBEDDED FIELD SPECIFICATION \nFor fields in an embedded documents, you can specify the field using either:  * dot notation; e.g. \"field.nestedfield\": <value>  * nested form; e.g. { field: { nestedfield: <value> } } (Starting in MongoDB 4.4) _ID FIELD PROJECTION \nThe _id field is included in the returned documents by default unless you explicitly specify _id: 0 in the projection to suppress the field. INCLUSION OR EXCLUSION \nA projection cannot contain both include and exclude specifications, with the exception of the _id field:  * In projections that explicitly include fields, the _id field is the only field that you can explicitly exclude.  * In projections that explicitly excludes fields, the _id field is the only field that you can explicitly include; however, the _id field is included by default. For more information on projection, see also:\n\n \nUPSERT WITH UNIQUE INDEX \nWhen using the upsert: true option with the findOneAndUpdate() method, and not using a unique index on the query field(s), multiple instances of a findOneAndUpdate() operation with similar query field(s) could result in duplicate documents being inserted in certain circumstances. Consider an example where no document with the name Andy exists and multiple clients issue the following command at roughly the same time: db.people.findAndModify(   {     query: { name: \"Andy\" },     update: { $inc: { score: 1 } },     upsert: true   })  If all findOneAndUpdate() operations finish the query phase before any client successfully inserts data, and there is no unique index on the name field, each findOneAndUpdate() operation may result in an insert, creating multiple documents with name: Andy. To ensure that only one such document is created, and the other findOneAndUpdate() operations update this new document instead, create a unique index on the name field. This guarantees that only one document with name: Andy is permitted in the collection. With this unique index in place, the multiple findOneAndUpdate() operations now exhibit the following behavior:  * Exactly one findOneAndUpdate() operation will successfully insert a new document.  * All other findOneAndUpdate() operations will update the newly-inserted document, incrementing the score value. \nSHARDED COLLECTIONS \nWhen using findAndModify against a sharded collection, the query must contain an equality condition on shard key. Starting in version 4.4, documents in a sharded collection can be missing the shard key fields. To target a document that is missing the shard key, you can use the null equality match in conjunction with another filter condition (such as on the _id field). For example: { _id: <value>, <shardkeyfield>: null } // _id of the document missing shard key  SHARD KEY MODIFICATION \nStarting in MongoDB 4.2, you can update a document's shard key value unless the shard key field is the immutable _id field. In MongoDB 4.2 and earlier, a document's shard key field value is immutable. \nWARNING Starting in version 4.4, documents in sharded collections can be missing the shard key fields. Take precaution to avoid accidentally removing the shard key when changing a document's shard key value. To modify the existing shard key value with db.collection.findAndModify():  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write.  * You must include an equality filter on the full shard key. MISSING SHARD KEY \nStarting in version 4.4, documents in a sharded collection can be missing the shard key fields. To use db.collection.findAndModify() to set the document's missing shard key:  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write if the new shard key value is not null.  * You must include an equality filter on the full shard key. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also:  * Missing Shard Key Fields \nDOCUMENT VALIDATION \nThe db.collection.findAndModify() method adds support for the bypassDocumentValidation option, which lets you bypass document validation when inserting or updating documents in a collection with validation rules. \nCOMPARISONS WITH THE UPDATE METHOD \nWhen updating a document, db.collection.findAndModify() and the updateOne() method operate differently:  * If multiple documents match the update criteria, for db.collection.findAndModify(), you can specify a sort to provide some measure of control on which document to update.\n   \n   updateOne() updates the first document that matches.\n\n When modifying a single document, both db.collection.findAndModify() and the updateOne() method atomically update the document. See Atomicity and Transactions for more details about interactions and order of operations of these methods. \nTRANSACTIONS \ndb.collection.findAndModify() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. UPSERT WITHIN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, db.collection.findAndModify() with upsert: true can be run on an existing collection or a non-existing collection. If run on a non-existing collection, the operation creates the collection. In MongoDB 4.2 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES  UPDATE AND RETURN \nThe following method updates and returns an existing document in the people collection where the document matches the query criteria: db.people.findAndModify({    query: { name: \"Tom\", state: \"active\", rating: { $gt: 10 } },    sort: { rating: 1 },    update: { $inc: { score: 1 } }})  This method performs the following actions:  1. The query finds a document in the people collection where the name field has the value Tom, the state field has the value active and the rating field has a value greater than 10.  2. The sort orders the results of the query in ascending order. If multiple documents meet the query condition, the method will select for modification the first document as ordered by this sort.  3. The update increments the value of the score field by 1.  4. The method returns the original (i.e. pre-modification) document selected for this update:\n    \n    {  \"_id\" : ObjectId(\"50f1e2c99beb36a0f45c6453\"),  \"name\" : \"Tom\",  \"state\" : \"active\",  \"rating\" : 100,  \"score\" : 5}\n    \n    \n    \n    To return the modified document, add the new:true option to the method.\n    \n    If no document matched the query condition, the method returns null. \nUPSERT \nThe following method includes the upsert: true option for the update operation to either update a matching document or, if no matching document exists, create a new document: db.people.findAndModify({    query: { name: \"Gus\", state: \"active\", rating: 100 },    sort: { rating: 1 },    update: { $inc: { score: 1 } },    upsert: true})  If the method finds a matching document, the method performs an update. If the method does not find a matching document, the method creates a new document. Because the method included the sort option, it returns an empty document { } as the original (pre-modification) document: { }  If the method did not include a sort option, the method returns null. null  \nRETURN NEW DOCUMENT \nThe following method includes both the upsert: true option and the new:true option. The method either updates a matching document and returns the updated document or, if no matching document exists, inserts a document and returns the newly inserted document in the value field. In the following example, no document in the people collection matches the query condition:\n\n  The method returns the newly inserted document: {   \"_id\" : ObjectId(\"50f49ad6444c11ac2448a5d6\"),   \"name\" : \"Pascal\",   \"rating\" : 25,   \"score\" : 1,   \"state\" : \"active\"}  \nSORT AND REMOVE \nBy including a sort specification on the rating field, the following example removes from the people collection a single document with the state value of active and the lowest rating among the matching documents: db.people.findAndModify(   {     query: { state: \"active\" },     sort: { rating: 1 },     remove: true   })  The method returns the deleted document: {   \"_id\" : ObjectId(\"52fba867ab5fdca1299674ad\"),   \"name\" : \"XYZ123\",   \"score\" : 1,   \"state\" : \"active\",   \"rating\" : 3}  \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents: { _id: 1, category: \"café\", status: \"A\" }{ _id: 2, category: \"cafe\", status: \"a\" }{ _id: 3, category: \"cafE\", status: \"a\" }  The following operation includes the collation option: db.myColl.findAndModify({    query: { category: \"cafe\", status: \"a\" },    sort: { category: 1 },    update: { $set: { status: \"Updated\" } },    collation: { locale: \"fr\", strength: 1 }});  The operation returns the following document: { \"_id\" : 1, \"category\" : \"café\", \"status\" : \"A\" }  \nSPECIFY ARRAYFILTERS FOR AN ARRAY UPDATE OPERATIONS  NOTE arrayFilters is not available for updates that use an aggregation pipeline. Starting in MongoDB 3.6, when updating an array field, you can specify arrayFilters that determine which array elements to update. UPDATE ELEMENTS MATCH ARRAYFILTERS CRITERIA  NOTE arrayFilters is not available for updates that use an aggregation pipeline. Create a collection students with the following documents: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] },   { \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] },   { \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }] )  To modify all elements that are greater than or equal to 100 in the grades array, use the filtered positional operator $[<identifier>] with the arrayFilters option in the db.collection.findAndModify() method: db.students.findAndModify({   query: { grades: { $gte: 100 } },   update: { $set: { \"grades.$[element]\" : 100 } },   arrayFilters: [ { \"element\": { $gte: 100 } } ]})  The operation updates the grades field for a single document, and after the operation, the collection has the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 100 ] }{ \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }  UPDATE SPECIFIC ELEMENTS OF AN ARRAY OF DOCUMENTS  NOTE arrayFilters is not available for updates that use an aggregation pipeline. Create a collection students2 with the following documents:\n\n  The following operation finds a document where the _id field equals 1 and uses the filtered positional operator $[<identifier>] with the arrayFilters to modify the mean for all elements in the grades array where the grade is greater than or equal to 85. db.students2.findAndModify({   query: { _id : 1 },   update: { $set: { \"grades.$[elem].mean\" : 100 } },   arrayFilters: [ { \"elem.grade\": { $gte: 85 } } ]})  The operation updates the grades field for a single document, and after the operation, the collection has the following documents: {   \"_id\" : 1,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 6 }   ]}{   \"_id\" : 2,   \"grades\" : [      { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },      { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }   ]}  \nUSE AN AGGREGATION PIPELINE FOR UPDATES \nStarting in MongoDB 4.2, db.collection.findAndModify() can accept an aggregation pipeline for the update. The pipeline can consist of the following stages:  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. Using the aggregation pipeline allows for a more expressive update statement, such as expressing conditional updates based on current field values or updating one field using the value of another field(s). For example, create a collection students2 with the following documents: db.students2.insertMany( [   {      \"_id\" : 1,      \"grades\" : [         { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 85, \"mean\" : 90, \"std\" : 4 },         { \"grade\" : 85, \"mean\" : 85, \"std\" : 6 }      ]   },   {      \"_id\" : 2,      \"grades\" : [         { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },         { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }      ]   }] )  The following operation finds a document where the _id field equals 1 and uses an aggregation pipeline to calculate a new field total from the grades field: db.students2.findAndModify( {   query: {  \"_id\" : 1 },   update: [ { $set: { \"total\" : { $sum: \"$grades.grade\" } } } ],  // The $set stage is an alias for ``$addFields`` stage   new: true} )  \nNOTE The $set used in the pipeline refers to the aggregation stage $set and not the update operator $set. The operation returns the updated document: {   \"_id\" : 1,   \"grades\" : [ { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 }, { \"grade\" : 85, \"mean\" : 90, \"std\" : 4 }, { \"grade\" : 85, \"mean\" : 85, \"std\" : 6 } ],   \"total\" : 250}  \nUSE VARIABLES IN LET \nNew in version 5.0. To define variables that you can access elsewhere in the command, use the let option. \nNOTE To filter results using a variable, you must access the variable within the $expr operator. Create a collection cakeFlavors:\n\n  The following example defines a targetFlavor variable in let and uses the variable to change the cake flavor from cherry to orange: db.cakeFlavors.findAndModify( {   query: {      $expr: { $eq: [ \"$flavor\", \"$$targetFlavor\" ] }   },   update: { flavor: \"orange\" },   let: { targetFlavor: \"cherry\" }} ) \n←  db.collection.find()db.collection.findOne() → On this page  * Definition\n * Return Data\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.commitReshardCollection/": " Docs Home → MongoDB Manual \nSH.COMMITRESHARDCOLLECTION() \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nsh.commitReshardCollection(namespace) \nNew in version 5.0. During a resharding operation, MongoDB does not block writes until the estimated duration to complete the resharding operation is below two seconds. If the current estimate is above two seconds but the time frame is acceptable to you, you can finish resharding faster. The sh.commitReshardCollection() method blocks writes early and forces the resharding operation to complete. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the commitReshardCollection command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 \nSYNTAX \nThe sh.commitReshardCollection() method has the following syntax: sh.commitReshardCollection( <namespace> )  \nPARAMETER \nThe sh.commitReshardCollection() method takes the following parameter: Parameter\nType\nDescription\nnamespace\nString The name of the collection to shard in the form \"<database>.<collection>\". \nEXAMPLE  COMMIT A RESHARDING OPERATION \nThe following command forces the resharding operation on the sales.orders to block writes and complete: sh.commitReshardCollection(\"sales.orders\")  \nTIP \nSEE ALSO: Reshard a Collection ←  sh.balancerCollectionStatus()sh.disableBalancing() → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.min/": " Docs Home → MongoDB Manual \nCURSOR.MIN() \nOn this page    \n * Definition\n   \n * Behaviors\n * Example \nDEFINITION \ncursor.min()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Specifies the inclusive lower bound for a specific index in order to constrain the results of find(). min() provides a way to specify lower bounds on compound key indexes. The min() method has the following parameter: Parameter\nType\nDescription\nindexBounds\ndocument\nThe inclusive lower bound for the index keys. The indexBounds parameter has the following prototype form: { field1: <min value>, field2: <min value2>, fieldN:<min valueN> }  \nNOTE \nINDEX USE To use an index with the max() method, you must use the hint() method to specify the index you want to use, exept when the find() query is an equality condition on the _id field. \nTIP \nSEE ALSO: max() min() exists primarily to support the mongos process. \nBEHAVIORS  INTERACTION WITH INDEX SELECTION \nBecause min() requires an index on a field, and forces the query to use this index, you may prefer the $gte operator for the query if possible. Consider the following example: db.products.find( { $in: [ 6, 7 ] } ).min( { price: NumberDecimal(\"1.39\") } ).hint( { price: 1 })  The query will use the index on the price field, even if the index on _id may be better. \nINDEX BOUNDS \nIf you use min() with max() to specify a range:      * the index bounds specified in min() and max() must both refer to the keys of the same index.  * the bound specified by max() must be greater than the bound specified by min(). \nMIN() WITHOUT MAX() \nThe min() and max() methods indicate that the system should avoid normal query planning. They construct an index scan where the index bounds are explicitly specified by the values given in min() and max(). \nWARNING If one of the two boundaries is not specified, the query plan will be an index scan that is unbounded on one side. This may degrade performance compared to a query containing neither operator, or one that uses both operators to more tightly constrain the index scan. \nEXAMPLE \nStarting in MongoDB 4.2, you must explicitly specify the particular index with the hint() method to run min() with the following exception: you do not need to hint if the find() query is an equality condition on the _id field { _id: <value> }. For the examples below, create a sample collection named products that holds the following documents:\n\n  Create the following indexes for the collection: db.products.createIndexes( [   { \"item\" : 1, \"type\" : 1 },   { \"item\" : 1, \"type\" : -1 },   { \"price\" : 1 }] )   * Using the ordering of the { item: 1, type: 1 } index, min() limits the query to the documents that are at or above the index key bound of item equal to apple and type equal to jonagold, as in the following:\n   \n   db.products.find().min( { item: 'apple', type: 'jonagold' } ).hint( { item: 1, type: 1 } )\n   \n   \n   \n   The query returns the following documents:\n   \n   { \"_id\" : 3, \"item\" : \"apple\", \"type\" : \"jonagold\", \"price\" : NumberDecimal(\"1.29\") }{ \"_id\" : 4, \"item\" : \"apple\", \"type\" : \"jonathan\", \"price\" : NumberDecimal(\"1.29\") }{ \"_id\" : 5, \"item\" : \"apple\", \"type\" : \"mcintosh\", \"price\" : NumberDecimal(\"1.29\") }{ \"_id\" : 7, \"item\" : \"orange\", \"type\" : \"cara cara\", \"price\" : NumberDecimal(\"2.99\") }{ \"_id\" : 10, \"item\" : \"orange\", \"type\" : \"navel\", \"price\" : NumberDecimal(\"1.39\") }{ \"_id\" : 9, \"item\" : \"orange\", \"type\" : \"satsuma\", \"price\" : NumberDecimal(\"1.99\") }{ \"_id\" : 8, \"item\" : \"orange\", \"type\" : \"valencia\", \"price\" : NumberDecimal(\"0.99\") }\n   \n     * Using the ordering of the index { price: 1 }, min() limits the query to the documents that are at or above the index key bound of price equal to 1.39 and max() limits the query to the documents that are below the index key bound of price equal to 1.99:\n   \n   \n   NOTE\n   \n   The bound specified by max() must be greater than the bound specified by min().\n   \n   db.products.find().min( { price: NumberDecimal(\"1.39\") } ).max( { price:  NumberDecimal(\"1.99\") } ).hint( { price: 1 } )\n   \n   \n   \n   The query returns the following documents:\n   \n   { \"_id\" : 10, \"item\" : \"orange\", \"type\" : \"navel\", \"price\" : NumberDecimal(\"1.39\") }\n   \n    ←  cursor.maxTimeMS()cursor.next() → On this page  * Definition\n * Behaviors\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getLastError/": " Docs Home → MongoDB Manual \nDB.GETLASTERROR() \nRemoved in MongoDB 5.1. Any code explicitly using getLastError, db.getLastError(), or db.getLastErrorObj() should instead use the CRUD API to issue the write with the desired write concern. Information about the success or failure of the write operation will be provided directly by the driver as a return value. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.splitAt/": " Docs Home → MongoDB Manual \nSH.SPLITAT() \nOn this page    \n * Definition\n   \n * Consideration\n * Behavior\n * Example \nDEFINITION \nsh.splitAt(namespace, query) \nSplits a chunk at the shard key value specified by the query. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the split command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The method takes the following arguments: Parameter\nType\nDescription\nnamespace\nstring\nThe namespace (i.e. <database>.<collection>) of the sharded collection that contains the chunk to split.\nquery\ndocument\nA query document that specifies the shard key value at which to split the chunk. \nCONSIDERATION \nIn most circumstances, you should leave chunk splitting to the automated processes within MongoDB. However, when initially deploying a sharded cluster, it may be beneficial to pre-split manually an empty collection using methods such as sh.splitAt(). \nBEHAVIOR \nsh.splitAt() splits the original chunk into two chunks. One chunk has a shard key range that starts with the original lower bound (inclusive) and ends at the specified shard key value (exclusive). The other chunk has a shard key range that starts with the specified shard key value (inclusive) as the lower bound and ends at the original upper bound (exclusive). To split a chunk at its median point instead, see sh.splitFind(). \nEXAMPLE \nFor the sharded collection test.foo, the following example splits a chunk at the shard key value x: 70. sh.splitAt( \"test.foo\", { x: 70 } ) \n←  sh.shardCollection()sh.splitFind() → On this page  * Definition\n * Consideration\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-sharding/": " Docs Home → MongoDB Manual \nSHARDING METHODS  NOTE For details on a specific method, including syntax and examples, click on the link to the method's reference page. Name\nDescription\nsh.abortReshardCollection() Aborts a resharding operation. New in version 5.0. sh.addShard()\nAdds a shard to a sharded cluster.\nsh.addShardTag()\nThis method aliases to sh.addShardToZone().\nsh.addShardToZone()\nAssociates a shard to a zone. Supports configuring zones in sharded clusters.\nsh.addTagRange()\nThis method aliases to sh.updateZoneKeyRange().\nsh.balancerCollectionStatus() Returns information on whether the chunks of a sharded collection are balanced. New in version 4.4. sh.commitReshardCollection() Forces a resharding operation to block writes and complete. New in version 5.0. sh.disableBalancing()\nDisable balancing on a single collection in a sharded database. Does not affect balancing of other collections in a sharded cluster.\nsh.enableBalancing()\nActivates the sharded collection balancer process if previously disabled using sh.disableBalancing().\nsh.disableAutoSplit() Disables auto-splitting for the sharded cluster. Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. sh.enableAutoSplit() Enables auto-splitting for the sharded cluster. Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. sh.enableSharding()\nCreates a database.\nsh.getBalancerState()\nReturns a boolean to report if the balancer is currently enabled.\nsh.removeTagRange()\nThis method aliases to sh.removeRangeFromZone().\nsh.removeRangeFromZone()\nRemoves an association between a range of shard keys and a zone. Supports configuring zones in sharded clusters.\nsh.help()\nReturns help text for the sh methods.\nsh.isBalancerRunning()\nReturns a document describing the status of the balancer.\nsh.moveChunk()\nMigrates a chunk in a sharded cluster.\nsh.removeShardTag()\nThis method aliases to sh.removeShardFromZone().\nsh.removeShardFromZone()\nRemoves the association between a shard and a zone. Use to manage zone sharding.\nsh.reshardCollection() Initiates a resharding operation to change the shard key for a collection, changing the distribution of your data. New in version 5.0. sh.setBalancerState()\nEnables or disables the balancer which migrates chunks between shards.\nsh.shardCollection()\nEnables sharding for a collection.\nsh.splitAt()\nDivides an existing chunk into two chunks using a specific value of the shard key as the dividing point.\nsh.splitFind()\nDivides an existing chunk that contains a document matching a query into two approximately equal chunks.\nsh.startBalancer()\nEnables the balancer and waits for balancing to start.\nsh.status()\nReports on the status of a sharded cluster, as db.printShardingStatus().\nsh.stopBalancer()\nDisables the balancer and waits for any in progress balancing rounds to complete.\nsh.waitForBalancer()\nInternal. Waits for the balancer state to change.\nsh.waitForBalancerOff()\nInternal. Waits until the balancer stops running.\nsh.waitForPingChange()\nInternal. Waits for a change in ping state from one of the mongos in the sharded cluster.\nsh.updateZoneKeyRange()\nAssociates a range of shard keys to a zone. Supports configuring zones in sharded clusters.\nconvertShardKeyToHashed()\nReturns the hashed value for the input. ←  rs.syncFrom()convertShardKeyToHashed → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.currentOp/": " Docs Home → MongoDB Manual \nDB.CURRENTOP() \nOn this page    \n * Definition\n   \n * Behavior\n * Access Control\n * Examples\n * Output Example\n * Output Fields \nDEFINITION \ndb.currentOp() \nReturns a document that contains information on in-progress operations for the database instance. The db.currentOp() method wraps the database command currentOp. \nNOTE Starting in MongoDB 5.0, the $currentOp aggregation stage is used when running the helper method db.currentOp() with mongosh. Given this, in the 5.0 version of the shell and with mongosh, db.currentOp() result sets are not subject to the 16MB BSON document return size document return size limit for documents of the previous legacy mongo versions. \nSYNTAX \ndb.currentOp() has the following form: db.currentOp(<operations>)  db.currentOp() can take the following optional argument: Parameter\nType\nDescription\noperations\nboolean or document Optional. Specifies the operations to report on. Can pass either a boolean or a document:      * Specify true to include operations on idle connections and system operations.  * Specify a document with query conditions to report only on those operations that match the conditions. See Behavior for details. \nBEHAVIOR \ndb.currentOp() can accept a filter document or a boolean parameter. If you pass a filter document to db.currentOp(), the output returns information only for the current operations that match the filter. The filter document can contain: Field\nDescription\n\"$ownOps\" Boolean. If set to true, returns information on the current user's operations only. On mongod instances, users can always run db.currentOp( { \"$ownOps\": true } ) to view their own operations. \"$all\" Boolean. If set to true, returns information on all operations, including operations on idle connections and system operations. If the document includes \"$all\": true along with Output Fields conditions, only the \"$all\":true applies. <filter> Specify filter conditions on the Output Fields. See Examples. If the document includes \"$all\": true along with Output Fields conditions, only the \"$all\": true applies. Passing in true to db.currentOp() is equivalent to passing in a document of { \"$all\": true }. The following operations are equivalent: db.currentOp(true)db.currentOp( { \"$all\": true } )  db.currentOp() and the database profiler report the same basic diagnostic information for all CRUD operations, including the following:  * aggregate  * count  * delete  * distinct  * find (OP_QUERY and command)  * findAndModify  * getMore (OP_GET_MORE and command)  * insert  * mapReduce  * update These operations are also included in the logging of slow queries (see slowOpThresholdMs for more information about slow query logging). \nACCESS CONTROL \nOn systems running with authorization, the user must have access that includes the inprog privilege action. Users can run db.currentOp( { \"$ownOps\": true } ) on mongod instances to view their own operations even without the inprog privilege action. \nTIP \nSEE ALSO: Create a Role to Manage Current Operations \nEXAMPLES \nThe following examples use the db.currentOp() method with various query documents to filter the output. \nWRITE OPERATIONS WAITING FOR A LOCK \nChanged in version 3.6. The following example returns information on all write operations that are waiting for a lock: db.currentOp(   {     \"waitingForLock\" : true,     $or: [        { \"op\" : { \"$in\" : [ \"insert\", \"update\", \"remove\" ] } },        { \"command.findandmodify\": { $exists: true } }    ]   })  \nACTIVE OPERATIONS WITH NO YIELDS \nThe following example returns information on all active running operations that have never yielded: db.currentOp(   {     \"active\" : true,     \"numYields\" : 0,     \"waitingForLock\" : false   })  \nACTIVE OPERATIONS ON A SPECIFIC DATABASE \nThe following example returns information on all active operations for database db1 that have been running longer than 3 seconds:\n\n  \nACTIVE INDEXING OPERATIONS \nChanged in version 3.6. The following example returns information on index creation operations: db.adminCommand(    {      currentOp: true,      $or: [        { op: \"command\", \"command.createIndexes\": { $exists: true }  },        { op: \"none\", \"msg\" : /^Index Build/ }      ]    })  \nOUTPUT EXAMPLE \nThe following is a prototype of db.currentOp() output.   OUTPUT FIELDS \nFor a complete list of db.currentOp() output fields, see currentOp. ←  db.createView()db.dropDatabase() → On this page  * Definition\n * Behavior\n * Access Control\n * Examples\n * Output Example\n * Output Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.update/": " Docs Home → MongoDB Manual \nBULK.FIND.UPDATE() \nOn this page    \n * Description\n   \n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.find.update(<update>) \nAdds a multi update operation to a bulk operations list. The method updates specific fields in existing documents. Use the Bulk.find() method to specify the condition that determines which documents to update. The Bulk.find.update() method updates all matching documents. To specify a single document update, see Bulk.find.updateOne(). Bulk.find.update() accepts the following parameter: Parameter\nType\nDescription\nupdate\ndocument or pipeline The modifications to apply. Can be one of the following: Update document\nContains only update operator expressions. Aggregation pipeline Starting in MongoDB 4.2 Contains only the following aggregation stages:      * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. For more information on the update modification parameter, see the db.collection.updateMany() reference page. The sum of the associated <query> document from the Bulk.find() and the update document must be less than or equal to the maximum BSON document size.  * To specify an upsert: true for this operation, use with Bulk.find.upsert(). With Bulk.find.upsert(), if no documents match the Bulk.find() query condition, the update operation inserts only a single document.  * To specify arrayFilters to update specific array elements, use with Bulk.find.arrayFilters().  * To specify the index to use for the associated Bulk.find(), see Bulk.find.hint().  * To replace a document wholesale, see Bulk.find.replaceOne(). \nEXAMPLE \nThe following example initializes a Bulk() operations builder for the items collection, and adds various multi update operations to the list of operations. var bulk = db.items.initializeUnorderedBulkOp();bulk.find( { status: \"D\" } ).update( { $set: { status: \"I\", points: \"0\" } } );bulk.find( { item: null } ).update( { $set: { item: \"TBD\" } } );bulk.execute();  \nUPDATE WITH AGGREGATION PIPELINE \nStarting in version 4.2, update methods can accept an aggregation pipeline. For example, the following uses:  * the $set stage which can provide similar behavior to the $set update operator expression,  * the aggregation variable NOW, which resolves to the current datetime and can provide similar behavior to a $currentDate update operator expression. To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes. var bulk = db.items.initializeUnorderedBulkOp();bulk.find( { status: \"P\" } ).update(   [      { $set: { points: 0, lastModified: \"$$NOW\" } }   ]);bulk.execute();  \nTIP \nSEE ALSO:  * db.collection.initializeUnorderedBulkOp()  * db.collection.initializeOrderedBulkOp()  * Bulk.find()  * Bulk.find.updateOne()  * Bulk.execute()  * All Bulk Methods ←  Bulk.find.updateOne()Bulk.find.upsert() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.help/": " Docs Home → MongoDB Manual \nRS.HELP() \nrs.help() \nReturns a basic help text for all of the replication related shell functions. ←  rs.freeze()rs.initiate() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.enableAutoSplit/": " Docs Home → MongoDB Manual \nSH.ENABLEAUTOSPLIT \nOn this page    \n * Description \nNOTE Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. \nDESCRIPTION \nsh.enableAutoSplit() \nEnables the autosplit flag in the config.settings collection. When auto-splitting is enabled for a sharded cluster, MongoDB automatically splits chunks based on the shard key values the chunk represents to keep the chunks from growing too large. You can only run sh.enableAutoSplit() from a mongosh session that is connected to a mongos instance. sh.enableAutoSplit() errors if run on a mongod instance. \nNOTE Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0, sh.startBalancer() also enables auto-splitting for the sharded cluster. \nTIP \nSEE ALSO:      * Manage Sharded Cluster Balancer  * Sharded Cluster Balancer ←  sh.disableAutoSplitsh.enableSharding() → On this page  * Description Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.findOneAndDelete/": " Docs Home → MongoDB Manual \nDB.COLLECTION.FINDONEANDDELETE() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.findOneAndDelete( filter, options )  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the delete command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Deletes a single document based on the filter and sort criteria, returning the deleted document. The findOneAndDelete() method has the following form: db.collection.findOneAndDelete(   <filter>,   {     writeConcern: <document>,     projection: <document>,     sort: <document>,     maxTimeMS: <number>,     collation: <document>   })  The findOneAndDelete() method takes the following parameters: Parameter\nType\nDescription\nfilter\ndocument The selection criteria for the deletion. The same query selectors as in the find() method are available. Specify an empty document { } to delete the first document returned in the collection. If unspecified, defaults to an empty document. Starting in MongoDB 4.2 (and 4.0.12+), the operation errors if the query argument is not a document. writeConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. { w: <value>, j: <boolean>, wtimeout: <number> }  See Delete A Document Using WriteConcern for usage. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. projection\ndocument Optional. A subset of fields to return. To return all fields in the returned document, omit this parameter. Starting in MongoDB 4.2 (and 4.0.12+), the operation errors if the projection argument is not a document. sort\ndocument Optional. Specifies a sorting order for the documents matched by the filter. Starting in MongoDB 4.2 (and 4.0.12+) the operation errors if the sort argument is not a document. See cursor.sort(). maxTimeMS\nnumber\nOptional. Specifies a time limit in milliseconds within which the operation must complete within. Throws an error if the limit is exceeded.\ncollation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. Returns:Returns the deleted document. \nBEHAVIOR  DOCUMENT MATCH \nfindOneAndDelete() deletes the first matching document in the collection that matches the filter. The sort parameter can be used to influence which document is deleted. \nPROJECTION  IMPORTANT \nLANGUAGE CONSISTENCY Starting in MongoDB 4.4, as part of making find() and findAndModify() projection consistent with aggregation's $project stage,  * The find() and findAndModify() projection can accept aggregation expressions and syntax.  * MongoDB enforces additional restrictions with regards to projections. See Projection Restrictions for details. The projection parameter takes a document in the following form: { field1: <value>, field2: <value> ... }\n\n Specifies the value of the projected field. Starting in MongoDB 4.4, with the use of aggregation expressions and syntax, including the use of literals and aggregation variables, you can project new fields or project existing fields with new values. For example,  * If you specify a non-numeric, non-boolean literal (such as a literal string or an array or an operator expression) for the projection value, the field is projected with the new value; e.g.:\n   \n   * { field: [ 1, 2, 3, \"$someExistingField\" ] }\n   \n   * { field: \"New String Value\" }\n   \n   * { field: { status: \"Active\", total: { $sum: \"$existingArray\" } } }  * To project a literal value for a field, use the $literal aggregation expression; e.g.:\n   \n   * { field: { $literal: 5 } }\n   \n   * { field: { $literal: true } }\n   \n   * { field: { $literal: { fieldWithValue0: 0, fieldWithValue1: 1 } } } In versions 4.2 and earlier, any specification value (with the exception of the previously unsupported document value) is treated as either true or false to indicate the inclusion or exclusion of the field. New in version 4.4. EMBEDDED FIELD SPECIFICATION \nFor fields in an embedded documents, you can specify the field using either:  * dot notation; e.g. \"field.nestedfield\": <value>  * nested form; e.g. { field: { nestedfield: <value> } } (Starting in MongoDB 4.4) _ID FIELD PROJECTION \nThe _id field is included in the returned documents by default unless you explicitly specify _id: 0 in the projection to suppress the field. INCLUSION OR EXCLUSION \nA projection cannot contain both include and exclude specifications, with the exception of the _id field:  * In projections that explicitly include fields, the _id field is the only field that you can explicitly exclude.  * In projections that explicitly excludes fields, the _id field is the only field that you can explicitly include; however, the _id field is included by default. For more information on projection, see also:  * Project Fields to Return from Query \nSHARDED COLLECTIONS \nWhen using db.collection.findOneAndDelete() against a sharded collection, the query must contain an equality condition on shard key. Starting in version 4.4, documents in a sharded collection can be missing the shard key fields. To target a document that is missing the shard key, you can use the null equality match in conjunction with another filter condition (such as on the _id field). For example: { _id: <value>, <shardkeyfield>: null } // _id of the document missing shard key  \nTRANSACTIONS \ndb.collection.findOneAndDelete() can be used inside multi-document transactions. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. \nEXAMPLES  DELETE A DOCUMENT \nThe scores collection contains documents similar to the following:\n\n  The following operation finds the first document where name : M. Tagnum and deletes it: db.scores.findOneAndDelete(   { \"name\" : \"M. Tagnum\" })  The operation returns the original document that has been deleted: { _id: 6312, name: \"M. Tagnum\", \"assignment\" : 5, \"points\" : 30 }  \nDELETE A DOCUMENT USING WRITECONCERN \nThe scores collection contains documents similar to the following: db.scores.insertMany( [   { _id: 6305, name : \"A. MacDyver\", \"assignment\" : 5, \"points\" : 24 },   { _id: 6308, name : \"B. Batlock\", \"assignment\" : 3, \"points\" : 22 },   { _id: 6312, name : \"M. Tagnum\", \"assignment\" : 5, \"points\" : 30 },   { _id: 6319, name : \"R. Stiles\", \"assignment\" : 2, \"points\" : 12 },   { _id: 6322, name : \"A. MacDyver\", \"assignment\" : 2, \"points\" : 14 },   { _id: 6234, name : \"R. Stiles\", \"assignment\" : 1, \"points\" : 10 }] )  The following operation uses a write concern document inside of the db.collection.findOneAndDelete() method with options:  * w:1 to requests acknowledgment that the write operation has propagated to the standalone mongod or the primary in a replica set.  * j:true to tell the number of MongoDB instances specified in w:1 to have the delete written to on-disk journel.  * wtimeout : 1000 to specify a time limit, in milliseconds, for the write concern. wtimeout is only applicable for w values greater than 1. db.scores.findOneAndDelete(   { name: \"A. MacDyver\" },   {      writeConcern: {         w : 1,         j : true,         wtimeout : 1000      }   })  The operation returns the following document: { _id: 6305, name: 'A. MacDyver', assignment: 5, points: 24 } The document is deleted with the writeConcern options specified. \nSORT AND DELETE A DOCUMENT \nThe scores collection contains documents similar to the following: db.scores.insertMany( [   { _id: 6305, name : \"A. MacDyver\", \"assignment\" : 5, \"points\" : 24 },   { _id: 6308, name : \"B. Batlock\", \"assignment\" : 3, \"points\" : 22 },   { _id: 6312, name : \"M. Tagnum\", \"assignment\" : 5, \"points\" : 30 },   { _id: 6319, name : \"R. Stiles\", \"assignment\" : 2, \"points\" : 12 },   { _id: 6322, name : \"A. MacDyver\", \"assignment\" : 2, \"points\" : 14 },   { _id: 6234, name : \"R. Stiles\", \"assignment\" : 1, \"points\" : 10 }] )  The following operation first finds all documents where name : \"A. MacDyver\". It then sorts by points ascending before deleting the document with the lowest points value: db.scores.findOneAndDelete(   { \"name\" : \"A. MacDyver\" },   { sort : { \"points\" : 1 } })  The operation returns the original document that has been deleted: { _id: 6322, name: \"A. MacDyver\", \"assignment\" : 2, \"points\" : 14 }  \nPROJECTING THE DELETED DOCUMENT \nThe following operation uses projection to only return the _id and assignment fields in the returned document: db.scores.findOneAndDelete(   { \"name\" : \"A. MacDyver\" },   { sort : { \"points\" : 1 }, projection: { \"assignment\" : 1 } })  The operation returns the original document with the assignment and _id fields: { _id: 6322, \"assignment\" : 2 }  \nUPDATE DOCUMENT WITH TIME LIMIT\n\n try {   db.scores.findOneAndDelete(      { \"name\" : \"A. MacDyver\" },      { sort : { \"points\" : 1 }, maxTimeMS : 5 }   )}catch(e){   print(e)}  If the operation exceeds the time limit, it returns: MongoServerError: operation exceeded time limit: { \"ok\": 0, \"code\" : 50, \"codeName\" : \"MaxTimeMSExpired\" }  \nNOTE This error message has been shortened for brevity. \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents: db.myColl.insertMany( [   { _id: 1, category: \"café\", status: \"A\" },   { _id: 2, category: \"cafe\", status: \"a\" },   { _id: 3, category: \"cafE\", status: \"a\" }] )  The following operation includes the collation option: db.myColl.findOneAndDelete(   { category: \"cafe\", status: \"a\" },   { collation: { locale: \"fr\", strength: 1 } });  The operation returns the following document: { \"_id\" : 1, \"category\" : \"café\", \"status\" : \"A\" } \n←  db.collection.findOne()db.collection.findOneAndReplace() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.readConcern/": " Docs Home → MongoDB Manual \nCURSOR.READCONCERN() \nOn this page    \n * Definition\n   \n * Considerations \nDEFINITION \ncursor.readConcern(level)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Specify a read concern for the db.collection.find() method. The readConcern() method has the following form: db.collection.find().readConcern(<level>)  The readConcern() method has the following parameter: Parameter\nType\nDescription\nlevel\nstring Read concern level. Possible read concern levels are:      * \"local\". This is the default read concern level for read operations against the primary and secondaries.  * \"available\". Available for read operations against the primary and secondaries. \"available\" behaves the same as \"local\" against the primary and non-sharded secondaries. The query returns the instance's most recent data.  * \"majority\". Available for replica sets that use WiredTiger storage engine.  * \"linearizable\". Available for read operations on the primary only. For more formation on the read concern levels, see Read Concern Levels. \nCONSIDERATIONS  READ YOUR OWN WRITES \nStarting in MongoDB 3.6, you can use causally consistent sessions to read your own writes, if the writes request acknowledgement. Prior to MongoDB 3.6, in order to read your own writes you must issue your write operation with { w: \"majority\" } write concern, and then issue your read operation with primary read preference, and either \"majority\" or \"linearizable\" read concern. \nLINEARIZABLE READ CONCERN PERFORMANCE \nWhen specifying linearizable read concern, always use maxTimeMS() in case a majority of data bearing members are unavailable. db.restaurants.find( { _id: 5 } ).readConcern(\"linearizable\").maxTimeMS(10000)  \nTIP \nSEE ALSO: Read Concern ←  cursor.pretty()cursor.readPref() → On this page  * Definition\n * Considerations Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.remove/": " Docs Home → MongoDB Manual \nBULK.FIND.REMOVE() \nOn this page    \n * Description\n   \n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.find.remove() \nStarting in mongosh 0.12.2, Bulk.find.remove() is an alias for Bulk.find.delete(). In new code, use Bulk.find.delete() instead of Bulk.find.remove(). \nEXAMPLE \nCreate the music collection: db.music.insertMany( [   { artist: \"DOA\", genre: \"punk\" },   { artist: \"Rick Astley\", genre: \"pop\" },   { artist: \"Black Flag\", genre: \"punk\" },   { artist: \"Justin Bieber\", genre: \"pop\" }] )  The following example:      * Initializes a Bulk() operations builder.  * Searches for the genre pop.  * Deletes pop music from the collection. var bulk = db.music.initializeOrderedBulkOp();bulk.find( { \"genre\": \"pop\" } ).remove();bulk.execute() \n←  Bulk.find.hint()Bulk.find.removeOne() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/SessionOptions/": " Docs Home → MongoDB Manual \nSESSIONOPTIONS \nOn this page    \n * Definition \nDEFINITION \nSessionOptions \nThe options for a session in mongosh. To access the SessionOptions() object, use the Session.getOptions() method. The session options available are: Option\nDescription\ncausalConsistency Boolean. Enables or disables causal consistency for the session. You can explicitly set the option when you start a session manually:      * Session = db.getMongo().startSession( { causalConsistency: true } ) readConcern Document. Specifies the read concern. In mongosh, you can set the option when you run Mongo.startSession(). You can also access the readConcern option via the following methods:  * Session.getOptions().getReadConcern()  * Session.getOptions().setReadConcern(<document>) readPreference Document. Specifies the read preference. In mongosh, you can set the option when you run Mongo.startSession(). You can also access the readPreference option via the following methods:  * Session.getOptions().getReadPreference()  * Session.getOptions().setReadPreference({ mode: <string>, tags: <array>}) retryWrites Boolean. Enables or disables the ability to retry writes upon encountering transient network errors, such as during failovers. To enable retry writes, start mongosh with the --retryWrites option. You can view whether retryWrites is enabled for a session via the following method:  * Session.getOptions().shouldRetryWrites() writeConcern Document. Specifies the write concern. In mongosh, you can set the options when you run Mongo.startSession(). You can also access the writeConcern option via the following methods:  * Session.getOptions().getWriteConcern()  * Session.getOptions().setWriteConcern(<document>) Verify which options are enabled for the session by running Session.getOptions(). ←  Session.withTransaction()Client-Side Field Level Encryption Methods → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.getShardVersion/": " Docs Home → MongoDB Manual \nDB.COLLECTION.GETSHARDVERSION() \ndb.collection.getShardVersion()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. This method returns information regarding the state of data in a sharded cluster that is useful when diagnosing underlying issues with a sharded cluster. For internal and diagnostic use only. ←  db.collection.getShardDistribution()db.collection.hideIndex() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.validate/": " Docs Home → MongoDB Manual \nDB.COLLECTION.VALIDATE() \nOn this page    \n * Description\n   \n * Syntax\n * Behavior\n * Examples \nDESCRIPTION \ndb.collection.validate(<documents>)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the validate command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Validates a collection. The method scans a collection data and indexes for correctness and returns the result. For details of the output, see Validate Output. Starting in version 5.0, the db.collection.validate() method can also fix inconsistencies in the collection. Index inconsistencies include:  * An index is multikey but there are no multikey fields.  * An index has multikeyPaths covering fields that are not multikey.  * An index does not have multikeyPaths but there are multikey documents (for indexes built before 3.4). If any inconsistencies are detected by the db.collection.validate() command, a warning is returned and the repair flag on the index is set to true. db.collection.validate() also validates any documents that violate the collection's schema validation rules. The db.collection.validate() method is a wrapper around the validate command. \nSYNTAX  NOTE \nCHANGED IN VERSION 4.4 db.collection.validate() no longer accepts just a boolean parameter. See db.collection.validate() Parameter Change. Changed in version 5.0. The db.collection.validate() method has the following syntax: db.collection.validate( {   full: <boolean>,          // Optional   repair: <boolean>         // Optional, added in MongoDB 5.0} )  \nPARAMETERS \nThe db.collection.validate() method can take the following optional document parameter with the fields: Field\nType\nDescription\nfull\nboolean Optional. A flag that determines whether the command performs a slower but more thorough check or a faster but less thorough check.  * If true, performs a more thorough check with the following exception:\n   \n   * Starting in MongoDB 4.4, full validation on the oplog for WiredTiger skips the more thorough check.  * If false, omits some checks for a faster but less thorough check. The default is false. Starting in MongoDB 3.6, for the WiredTiger storage engine, only the full validation process will force a checkpoint and flush all in-memory data to disk before verifying the on-disk data. In previous versions, the data validation process for the WT storage engine always forces a checkpoint. repair\nboolean Optional. A flag that determines whether the command performs a repair.  * If true, a repair is performed.  * If false, no repair is performed. The default is false. A repair can only be run on a standalone node. The repair fixes these issues:  * If missing index entries are found, the missing keys are inserted into the index.  * If extra index entries are found, the extra keys are removed from the index.  * If multikey documents are found for an index that is not a multikey index, the index is changed to a multikey index.  * If multikey documents are found that are not specified by an index's multikey paths, the index's multikey paths are updated.  * If corrupt documents with invalid BSON data are found, the documents are removed. \nTIP \nSEE ALSO: --repair option for mongod New in version 5.0. \nBEHAVIOR  PERFORMANCE \nThe db.collection.validate() method is potentially resource intensive and may impact the performance of your MongoDB instance, particularly on larger data sets. The db.collection.validate() method obtains an exclusive lock on the collection. This will block all reads and writes on the collection until the operation finishes. When run on a secondary, the operation can block all other operations on that secondary until it finishes. \nWARNING Validation has exclusive lock requirements that affect performance on primaries and on secondaries that are servicing reads. Consider only running db.collection.validate() on nodes that are not servicing reads or writes.\n\n To minimize the impact of the validation operation on client applications, run db.collection.validate() on a secondary node that is not servicing read requests. You can convert the current primary node to a secondary node, by running the rs.stepDown() method. To completely isolate the db.collection.validate() operation from client traffic, choose one of the following options:  * Isolate a replica set member by following the rolling maintenance procedure to temporarily remove it from the cluster.  * Convert a secondary node to a replica set hidden member and perform the validation on the hidden node. \nDATA THROUGHPUT METRICS \nStarting in version MongoDB 4.4,  * The $currentOp and the currentOp command include dataThroughputAverage and dataThroughputLastSecond information for validate operations in progress.  * The log messages for validate operations include dataThroughputAverage and dataThroughputLastSecond information. \nEXAMPLES \n * To validate a collection myCollection using the default validation setting (specifically, full: false):\n   \n   db.myCollection.validate()\n   db.myCollection.validate({ })\n   db.myCollection.validate( { full: false } )\n   \n     * To perform a full validation of collection myCollection, specify full: true:\n   \n   db.myCollection.validate( { full: true } )\n   \n     * To repair collection myCollection, specify repair: true:\n   \n   db.myCollection.validate( { repair: true } )\n   \n    For details of the output, see Validate Output. ←  db.collection.watch()Cursor Methods → On this page  * Description\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.serverBuildInfo/": " Docs Home → MongoDB Manual \nDB.SERVERBUILDINFO() \ndb.serverBuildInfo() \nProvides a wrapper around the buildInfo database command. buildInfo returns a document that contains an overview of parameters used to compile this mongod instance. ←  db.runCommand()db.serverCmdLineOpts() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.addKeyName/": " Docs Home → MongoDB Manual \nKEYVAULT.ADDKEYALTNAME() \nKeyVault.addKeyAltName(UUID, keyAltName) \nAdds the keyAltName to the keyAltNames array of the data encryption key with UUID. keyAltName must be unique among all keys in the key vault. Use the getKeyVault() method with a partial index filter on documents with keyAltNames to automatically create a unique index on the keyAltNames field. This method is an alias for addKeyAlternateName(). \nTIP \nSEE ALSO: addKeyAlternateName(). What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.dropAllUsers/": " Docs Home → MongoDB Manual \nDB.DROPALLUSERS() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.dropAllUsers(writeConcern) \nRemoves all users from the current database. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the dropAllUsersFromDatabase command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 \nWARNING The db.dropAllUsers() method removes all users from the database. The db.dropAllUsers() method takes the following arguments: Field\nType\nDescription\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. The db.dropAllUsers() method wraps the dropAllUsersFromDatabase command. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.dropAllUsers() is executed using \"majority\" write concern by default. \nREQUIRED ACCESS \nYou must have the dropUser action on a database to drop a user from that database. \nEXAMPLE \nThe following db.dropAllUsers() operation drops every user from the products database. use productsdb.dropAllUsers( {w: \"majority\", wtimeout: 5000} )  The n field in the results document shows the number of users removed: { \"n\" : 12, \"ok\" : 1 } \n←  db.dropUser()db.getUser() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.collation/": " Docs Home → MongoDB Manual \nBULK.FIND.COLLATION() \nOn this page    \n * Description\n   \n * Example \nDESCRIPTION \nBulk.find.collation(<document>) \nSpecifies the collation for the bulk writes. Append to Bulk.find() method to specify collation for the find operation. The Bulk.find.collation() accepts the following collation document: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. Field\nType\nDescription\nlocale\nstring The ICU locale. See Supported Languages and Locales for a list of supported locales. To specify simple binary comparison, specify locale value of \"simple\". strength\ninteger Optional. The level of comparison to perform. Corresponds to ICU Comparison Levels. Possible values are: Value\nDescription\n1\nPrimary level of comparison. Collation performs comparisons of the base characters only, ignoring other differences such as diacritics and case.\n2\nSecondary level of comparison. Collation performs comparisons up to secondary differences, such as diacritics. That is, collation performs comparisons of base characters (primary differences) and diacritics (secondary differences). Differences between base characters takes precedence over secondary differences.\n3 Tertiary level of comparison. Collation performs comparisons up to tertiary differences, such as case and letter variants. That is, collation performs comparisons of base characters (primary differences), diacritics (secondary differences), and case and variants (tertiary differences). Differences between base characters takes precedence over secondary differences, which takes precedence over tertiary differences. This is the default level. 4\nQuaternary Level. Limited for specific use case to consider punctuation when levels 1-3 ignore punctuation or for processing Japanese text.\n5\nIdentical Level. Limited for specific use case of tie breaker. See ICU Collation: Comparison Levels for details. caseLevel\nboolean Optional. Flag that determines whether to include case comparison at strength level 1 or 2. If true, include case comparison; i.e.      * When used with strength:1, collation compares base characters and case.  * When used with strength:2, collation compares base characters, diacritics (and possible other secondary differences) and case. If false, do not include case comparison at level 1 or 2. The default is false. For more information, see ICU Collation: Case Level. caseFirst\nstring Optional. A field that determines sort order of case differences during tertiary level comparisons. Possible values are: Value\nDescription\n\"upper\"\nUppercase sorts before lowercase.\n\"lower\"\nLowercase sorts before uppercase.\n\"off\"\nDefault value. Similar to \"lower\" with slight differences. See http://userguide.icu-project.org/collation/customization for details of differences. numericOrdering\nboolean Optional. Flag that determines whether to compare numeric strings as numbers or as strings. If true, compare as numbers; i.e. \"10\" is greater than \"2\". If false, compare as strings; i.e. \"10\" is less than \"2\". Default is false. alternate\nstring Optional. Field that determines whether collation should consider whitespace and punctuation as base characters for purposes of comparison. Possible values are: Value\nDescription\n\"non-ignorable\"\nWhitespace and punctuation are considered base characters.\n\"shifted\"\nWhitespace and punctuation are not considered base characters and are only distinguished at strength levels greater than 3. See ICU Collation: Comparison Levels for more information. Default is \"non-ignorable\". maxVariable\nstring Optional. Field that determines up to which characters are considered ignorable when alternate: \"shifted\". Has no effect if alternate: \"non-ignorable\" Possible values are: Value\nDescription\n\"punct\"\nBoth whitespaces and punctuation are \"ignorable\", i.e. not considered base characters.\n\"space\"\nWhitespace are \"ignorable\", i.e. not considered base characters. backwards\nboolean Optional. Flag that determines whether strings with diacritics sort from back of the string, such as with some French dictionary ordering. If true, compare from back to front.\n\n The default value is false. normalization\nboolean Optional. Flag that determines whether to check if text require normalization and to perform normalization. Generally, majority of text does not require this normalization processing. If true, check if fully normalized and perform normalization to compare text. If false, does not check. The default value is false. See http://userguide.icu-project.org/collation/concepts#TOC-Normalization for details. \nEXAMPLE \nThe following example initializes a Bulk() operations builder for the myColl collection and specifies a collation for the find filter. var bulk = db.myColl.initializeUnorderedBulkOp();bulk.find( { category: \"cafe\" } ).collation({ locale: \"fr\", strength: 1 }).update( { $set: { status: \"I\", points: \"0\" } } );bulk.execute();  \nTIP \nSEE ALSO:  * db.collection.initializeUnorderedBulkOp()  * db.collection.initializeOrderedBulkOp()  * Bulk.find()  * Bulk.find.updateOne()  * Bulk.execute()  * All Bulk Methods ←  Bulk.find.arrayFilters()Bulk.find.hint() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.removeOne/": " Docs Home → MongoDB Manual \nBULK.FIND.REMOVEONE() \nOn this page    \n * Description\n   \n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.find.removeOne() \nStarting in mongosh 0.12.2, Bulk.find.removeOne() is an alias for Bulk.find.deleteOne(). In new code, use Bulk.find.deleteOne() instead of Bulk.find.removeOne(). \nEXAMPLE \nCreate the music collection: db.music.insertMany( [   { artist: \"DOA\", genre: \"punk\" },   { artist: \"Rick Astley\", genre: \"pop\" },   { artist: \"Black Flag\", genre: \"punk\" },   { artist: \"Justin Bieber\", genre: \"pop\" }] )  The following example:      * Initializes a Bulk() operations builder.  * Searches for the genre pop.  * Deletes Rick Astley, the first matching pop artist, from the collection. var bulk = db.music.initializeOrderedBulkOp();bulk.find( { \"genre\": \"pop\" } ).removeOne();bulk.execute() \n←  Bulk.find.remove()Bulk.find.replaceOne() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/PlanCache.help/": " Docs Home → MongoDB Manual \nPLANCACHE.HELP() \nOn this page    \n * Definition \nDEFINITION \nPlanCache.help() \nDisplays the methods available to view and modify a collection's query plan cache. The method is only available from the plan cache object of a specific collection; i.e. db.collection.getPlanCache().help()  \nTIP \nSEE ALSO: db.collection.getPlanCache() ←  PlanCache.clearPlansByQuery()PlanCache.list() → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.abortReshardCollection/": " Docs Home → MongoDB Manual \nSH.ABORTRESHARDCOLLECTION() \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nsh.abortReshardCollection(namespace) \nNew in version 5.0. During a resharding operation, you can abort the operation with the sh.abortReshardCollection() method. You can abort a resharding operation at any point until the commit phase. If the resharding operation has reached the commit phase before you run the sh.abortReshardCollection() method, the method returns an error. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the abortReshardCollection command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 \nSYNTAX \nThe sh.abortReshardCollection() method has the following syntax: sh.abortReshardCollection( <namespace> )  \nPARAMETER \nThe sh.abortReshardCollection() method takes the following parameter: Parameter\nType\nDescription\nnamespace\nString The name of the collection to shard in the form \"<database>.<collection>\". \nEXAMPLE  ABORT A RESHARDING OPERATION \nThe following example aborts a running resharding operation on the sales.orders collection: sh.abortReshardCollection(\"sales.orders\")  \nTIP \nSEE ALSO: Reshard a Collection ←  convertShardKeyToHashedsh.addShard() → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getUsers/": " Docs Home → MongoDB Manual \nDB.GETUSERS() \nOn this page    \n * Definition\n   \n * Required Access\n * Examples \nDEFINITION \ndb.getUsers(<options>) \nReturns information for all the users in the database. db.getUsers() wraps the usersInfo: 1 command. The db.getUsers() method can take the following options: db.getUsers( {   showCredentials: <Boolean>,   showCustomData: <Boolean>,   filter: <document>} ) \nField\nType\nDescription\nshowCredentials\nboolean Optional. Set to true to display the user's password hash. By default, this field is false. showCustomData\nboolean Optional. Set to false to omit the user's customData from the output. By default, this field is true. New in version 5.2. filter\ndocument\nOptional. A document that specifies $match stage conditions to return information for users that match the filter conditions. For more information, see usersInfo. \nREQUIRED ACCESS \nTo view another user's information, you must have the viewUser action on the other user's database. Users can view their own information. \nEXAMPLES  VIEW ALL USERS FOR A DATABASE THAT MATCH THE SPECIFIED FILTER \nThe db.getUsers() method can accept a filter document to return information for users that match the filter condition. To view all users for the current database who have SCRAM-SHA-256 credentials: db.getUsers({ filter: { mechanisms: \"SCRAM-SHA-256\" } })  When viewing all users, you can specify the showCredentials option but not the showPrivileges or the showAuthenticationRestrictions options. \nOMIT CUSTOM DATA FROM OUTPUT \nNew in version 5.2: To omit users' custom data from the db.getUsers() output, set the showCustomData option to false. Use the createUser command to create a user named accountAdmin01 on the products database: db.getSiblingDB(\"products\").runCommand( {   createUser: \"accountAdmin01\",   pwd: passwordPrompt(),   customData: { employeeId: 12345 },   roles: [ { role: 'readWrite', db: 'products' } ]} )  The user contains a customData field of { employeeId: 12345 }. To retrieve the user but omit the custom data from the output, run db.getUsers() with showCustomData set to false: db.getSiblingDB(\"products\").getUsers( { showCustomData: false } )  Example output: {   users: [     {       _id: 'products.accountAdmin01',       userId: UUID(\"0955afc1-303c-4683-a029-8e17dd5501f4\"),       user: 'accountAdmin01',       db: 'products',       roles: [ { role: 'readWrite', db: 'products' } ],       mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]     }   ],   ok: 1} ←  db.getUser()db.grantRolesToUser() → On this page  * Definition\n * Required Access\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/hostname/": " Docs Home → MongoDB Manual \nHOSTNAME() \nhostname() \nReturns:The hostname of the system running mongosh. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.waitForBalancer/": " Docs Home → MongoDB Manual \nSH.WAITFORBALANCER() \nOn this page    \n * Definition \nDEFINITION \nsh.waitForBalancer(wait, timeout, interval) \nWaits for a change in the state of the balancer. sh.waitForBalancer() is an internal method, which takes the following arguments: Parameter\nType\nDescription\nwait\nboolean\nOptional. Set to true to ensure the balancer is now active. The default is false, which waits until balancing stops and becomes inactive.\ntimeout\ninteger\nMilliseconds to wait.\ninterval\ninteger\nMilliseconds to sleep. ←  sh.stopBalancer()sh.waitForBalancerOff() → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.mapReduce/": " Docs Home → MongoDB Manual \nDB.COLLECTION.MAPREDUCE() \nOn this page    \n * Syntax\n   \n * Output\n * Restrictions\n * Additional Information \nNOTE \nAGGREGATION PIPELINE AS ALTERNATIVE TO MAP-REDUCE Starting in MongoDB 5.0, map-reduce is deprecated:      * Instead of map-reduce, you should use an aggregation pipeline. Aggregation pipelines provide better performance and usability than map-reduce.  * You can rewrite map-reduce operations using aggregation pipeline stages, such as $group, $merge, and others.  * For map-reduce operations that require custom functionality, you can use the $accumulator and $function aggregation operators, available starting in version 4.4. You can use those operators to define custom aggregation expressions in JavaScript. For examples of aggregation pipeline alternatives to map-reduce, see:  * Map-Reduce to Aggregation Pipeline  * Map-Reduce Examples db.collection.mapReduce(map,reduce, { <options> })  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the mapReduce command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:  * mongo shell v4.4  * mongo shell v4.2 \nNOTE Views do not support map-reduce operations. \nSYNTAX  NOTE Starting in version 4.4, MongoDB ignores the verbose option. Starting in version 4.2, MongoDB deprecates:  * The map-reduce option to create a new sharded collection as well as the use of the sharded option for map-reduce. To output to a sharded collection, create the sharded collection first. MongoDB 4.2 also deprecates the replacement of an existing sharded collection.  * The explicit specification of nonAtomic: false option. db.collection.mapReduce() has the following syntax: db.collection.mapReduce(                         <map>,                         <reduce>,                         {                           out: <collection>,                           query: <document>,                           sort: <document>,                           limit: <number>,                           finalize: <function>,                           scope: <document>,                           jsMode: <boolean>,                           verbose: <boolean>,                           bypassDocumentValidation: <boolean>                         }                       )  db.collection.mapReduce() takes the following parameters: Parameter\nType\nDescription\nmap\nJavaScript or String A JavaScript function that associates or \"maps\" a value with a key and emits the key and value pair. You can specify the function as BSON type JavaScript (i.e. BSON type 13) or String (i.e. BSON type 2). See Requirements for the map Function for more information. reduce\nJavaScript or String\n\n See Requirements for the reduce Function for more information. options\ndocument\nA document that specifies additional parameters to db.collection.mapReduce(). The following table describes additional arguments that db.collection.mapReduce() can accept. Field\nType\nDescription\nout\nstring or document Specifies the location of the result of the map-reduce operation. You can output to a collection, output to a collection with an action, or output inline. You may output to a collection when performing map-reduce operations on the primary members of the set; on secondary members you may only use the inline output. See out Options for more information. query\ndocument\nSpecifies the selection criteria using query operators for determining the documents input to the map function.\nsort\ndocument\nSorts the input documents. This option is useful for optimization. For example, specify the sort key to be the same as the emit key so that there are fewer reduce operations. The sort key must be in an existing index for this collection.\nlimit\nnumber\nSpecifies a maximum number of documents for the input into the map function.\nfinalize\nJavascript or String Optional. A JavaScript function that modifies the output after the reduce function. You can specify the function as BSON type JavaScript (i.e. BSON type 13) or String (i.e. BSON type 2). See Requirements for the finalize Function for more information. scope\ndocument\nSpecifies global variables that are accessible in the map, reduce and finalize functions.\njsMode\nboolean Specifies whether to convert intermediate data into BSON format between the execution of the map and reduce functions. Defaults to false. If false:  * Internally, MongoDB converts the JavaScript objects emitted by the map function to BSON objects. These BSON objects are then converted back to JavaScript objects when calling the reduce function.  * The map-reduce operation places the intermediate BSON objects in temporary, on-disk storage. This allows the map-reduce operation to execute over arbitrarily large data sets. If true:  * Internally, the JavaScript objects emitted during map function remain as JavaScript objects. There is no need to convert the objects for the reduce function, which can result in faster execution.  * You can only use jsMode for result sets with fewer than 500,000 distinct key arguments to the mapper's emit() function. verbose\nboolean Specifies whether to include the timing information in the result information. Set verbose to true to include the timing information. Defaults to false. Starting in MongoDB 4.4, this option is ignored. The result information always excludes the timing information. You can view timing information by running db.collection.explain() with db.collection.mapReduce() in the \"executionStats\" or \"allPlansExecution\" verbosity modes. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. bypassDocumentValidation\nboolean\nOptional. Enables mapReduce to bypass document validation during the operation. This lets you insert documents that do not meet the validation requirements. \nNOTE map-reduce operations and $where operator expressions cannot access certain global functions or properties, such as db, that are available in mongosh. The following JavaScript functions and properties are available to map-reduce operations and $where operator expressions: Available Properties\nAvailable Functions\n\n \nREQUIREMENTS FOR THE MAP FUNCTION \nThe map function is responsible for transforming each input document into zero or more documents. It can access the variables defined in the scope parameter, and has the following prototype: function() {   ...   emit(key, value);}  The map function has the following requirements:  * In the map function, reference the current document as this within the function.  * The map function should not access the database for any reason.  * The map function should be pure, or have no impact outside of the function (i.e. side effects.)  * The map function may optionally call emit(key,value) any number of times to create an output document associating key with value.  * In MongoDB 4.2 and earlier, a single emit can only hold half of MongoDB's maximum BSON document size. MongoDB removes this restriction starting in version 4.4.  * Starting in MongoDB 4.4, mapReduce no longer supports the deprecated BSON type JavaScript code with scope (BSON type 15) for its functions. The map function must be either BSON type String (BSON type 2) or BSON type JavaScript (BSON type 13). To pass constant values which will be accessible in the map function, use the scope parameter.\n   \n   The use of JavaScript code with scope for the map function has been deprecated since version 4.2.1. The following map function will call emit(key,value) either 0 or 1 times depending on the value of the input document's status field: function() {    if (this.status == 'A')        emit(this.cust_id, 1);}  The following map function may call emit(key,value) multiple times depending on the number of elements in the input document's items field: function() {    this.items.forEach(function(item){ emit(item.sku, 1); });}   REQUIREMENTS FOR THE REDUCE FUNCTION \nThe reduce function has the following prototype: function(key, values) {   ...   return result;}  The reduce function exhibits the following behaviors:  * The reduce function should not access the database, even to perform read operations.  * The reduce function should not affect the outside system.  * MongoDB can invoke the reduce function more than once for the same key. In this case, the previous output from the reduce function for that key will become one of the input values to the next reduce function invocation for that key.  * The reduce function can access the variables defined in the scope parameter.  * The inputs to reduce must not be larger than half of MongoDB's maximum BSON document size. This requirement may be violated when large documents are returned and then joined together in subsequent reduce steps.  * Starting in MongoDB 4.4, mapReduce no longer supports the deprecated BSON type JavaScript code with scope (BSON type 15) for its functions. The reduce function must be either BSON type String (BSON type 2) or BSON type JavaScript (BSON type 13). To pass constant values which will be accessible in the reduce function, use the scope parameter.\n   \n   The use of JavaScript code with scope for the reduce function has been deprecated since version 4.2.1. Because it is possible to invoke the reduce function more than once for the same key, the following properties need to be true:  * the type of the return object must be identical to the type of the value emitted by the map function.  * the reduce function must be associative. The following statement must be true:\n   \n   reduce(key, [ C, reduce(key, [ A, B ]) ] ) == reduce( key, [ C, A, B ] )\n   \n     * the reduce function must be idempotent. Ensure that the following statement is true:\n   \n   reduce( key, [ reduce(key, valuesArray) ] ) == reduce( key, valuesArray )\n   \n     * the reduce function should be commutative: that is, the order of the elements in the valuesArray should not affect the output of the reduce function, so that the following statement is true:\n   \n   reduce( key, [ A, B ] ) == reduce( key, [ B, A ] )\n   \n     \nOUT OPTIONS \nYou can specify the following options for the out parameter: OUTPUT TO A COLLECTION \nThis option outputs to a new collection, and is not available on secondary members of replica sets. out: <collectionName>  OUTPUT TO A COLLECTION WITH AN ACTION  NOTE\n\n  * The map-reduce option to create a new sharded collection as well as the use of the sharded option for map-reduce. To output to a sharded collection, create the sharded collection first. MongoDB 4.2 also deprecates the replacement of an existing sharded collection.  * The explicit specification of nonAtomic: false option. This option is only available when passing a collection that already exists to out. It is not available on secondary members of replica sets. out: { <action>: <collectionName>        [, db: <dbName>]        [, sharded: <boolean> ]        [, nonAtomic: <boolean> ] }  When you output to a collection with an action, the out has the following parameters:  * <action>: Specify one of the following actions:\n   \n   * replace\n     \n     Replace the contents of the <collectionName> if the collection with the <collectionName> exists.\n   \n   * merge\n     \n     Merge the new result with the existing result if the output collection already exists. If an existing document has the same key as the new result, overwrite that existing document.\n   \n   * reduce\n     \n     Merge the new result with the existing result if the output collection already exists. If an existing document has the same key as the new result, apply the reduce function to both the new and the existing documents and overwrite the existing document with the result.  * db:\n   \n   Optional. The name of the database that you want the map-reduce operation to write its output. By default this will be the same database as the input collection.  * sharded:\n   \n   \n   NOTE\n   \n   Starting in version 4.2, the use of the sharded option is deprecated.\n   \n   Optional. If true and you have enabled sharding on output database, the map-reduce operation will shard the output collection using the _id field as the shard key.\n   \n   If true and collectionName is an existing unsharded collection, map-reduce fails.  * nonAtomic:\n   \n   \n   NOTE\n   \n   Starting in MongoDB 4.2, explicitly setting nonAtomic to false is deprecated.\n   \n   Optional. Specify output operation as non-atomic. This applies only to the merge and reduce output modes, which may take minutes to execute.\n   \n   By default nonAtomic is false, and the map-reduce operation locks the database during post-processing.\n   \n   If nonAtomic is true, the post-processing step prevents MongoDB from locking the database: during this time, other clients will be able to read intermediate states of the output collection. OUTPUT INLINE \nPerform the map-reduce operation in memory and return the result. This option is the only available option for out on secondary members of replica sets. out: { inline: 1 }  The result must fit within the maximum size of a BSON document.  \nREQUIREMENTS FOR THE FINALIZE FUNCTION \nThe finalize function has the following prototype: function(key, reducedValue) {   ...   return modifiedObject;}  The finalize function receives as its arguments a key value and the reducedValue from the reduce function. Be aware that:  * The finalize function should not access the database for any reason.  * The finalize function should be pure, or have no impact outside of the function (i.e. side effects.)  * The finalize function can access the variables defined in the scope parameter.  * Starting in MongoDB 4.4, mapReduce no longer supports the deprecated BSON type JavaScript code with scope (BSON type 15) for its functions. The finalize function must be either BSON type String (BSON type 2) or BSON type JavaScript (BSON type 13). To pass constant values which will be accessible in the finalize function, use the scope parameter.\n   \n   The use of JavaScript code with scope for the finalize function has been deprecated since version 4.2.1. \nMAP-REDUCE EXAMPLES \nThe examples in this section include aggregation pipeline alternatives without custom aggregation expressions. For alternatives that use custom expressions, see Map-Reduce to Aggregation Pipeline Translation Examples. Create a sample collection orders with these documents:\n\n  RETURN THE TOTAL PRICE PER CUSTOMER \nPerform the map-reduce operation on the orders collection to group by the cust_id, and calculate the sum of the price for each cust_id:  1. Define the map function to process each input document:\n    \n    * In the function, this refers to the document that the map-reduce operation is processing.\n    \n    * The function maps the price to the cust_id for each document and emits the cust_id and price.\n    \n    var mapFunction1 = function() {   emit(this.cust_id, this.price);};\n    \n      2. Define the corresponding reduce function with two arguments keyCustId and valuesPrices:\n    \n    * The valuesPrices is an array whose elements are the price values emitted by the map function and grouped by keyCustId.\n    \n    * The function reduces the valuesPrice array to the sum of its elements.\n    \n    var reduceFunction1 = function(keyCustId, valuesPrices) {   return Array.sum(valuesPrices);};\n    \n      3. Perform map-reduce on all documents in the orders collection using the mapFunction1 map function and the reduceFunction1 reduce function:\n    \n    db.orders.mapReduce(   mapFunction1,   reduceFunction1,   { out: \"map_reduce_example\" })\n    \n    \n    \n    This operation outputs the results to a collection named map_reduce_example. If the map_reduce_example collection already exists, the operation will replace the contents with the results of this map-reduce operation.  4. Query the map_reduce_example collection to verify the results:\n    \n    db.map_reduce_example.find().sort( { _id: 1 } )\n    \n    \n    \n    The operation returns these documents:\n    \n    { \"_id\" : \"Ant O. Knee\", \"value\" : 95 }{ \"_id\" : \"Busby Bee\", \"value\" : 125 }{ \"_id\" : \"Cam Elot\", \"value\" : 60 }{ \"_id\" : \"Don Quis\", \"value\" : 155 } AGGREGATION ALTERNATIVE \nUsing the available aggregation pipeline operators, you can rewrite the map-reduce operation without defining custom functions: db.orders.aggregate([   { $group: { _id: \"$cust_id\", value: { $sum: \"$price\" } } },   { $out: \"agg_alternative_1\" }])   1. The $group stage groups by the cust_id and calculates the value field (See also $sum). The value field contains the total price for each cust_id.\n    \n    The stage output the following documents to the next stage:\n    \n    { \"_id\" : \"Don Quis\", \"value\" : 155 }{ \"_id\" : \"Ant O. Knee\", \"value\" : 95 }{ \"_id\" : \"Cam Elot\", \"value\" : 60 }{ \"_id\" : \"Busby Bee\", \"value\" : 125 }  2. Then, the $out writes the output to the collection agg_alternative_1. Alternatively, you could use $merge instead of $out.  3. Query the agg_alternative_1 collection to verify the results:\n    \n    db.agg_alternative_1.find().sort( { _id: 1 } )\n    \n    \n    \n    The operation returns the following documents:\n    \n    { \"_id\" : \"Ant O. Knee\", \"value\" : 95 }{ \"_id\" : \"Busby Bee\", \"value\" : 125 }{ \"_id\" : \"Cam Elot\", \"value\" : 60 }{ \"_id\" : \"Don Quis\", \"value\" : 155 } \nTIP \nSEE ALSO: For an alternative that uses custom aggregation expressions, see Map-Reduce to Aggregation Pipeline Translation Examples.\n\n \nIn the following example, you will see a map-reduce operation on the orders collection for all documents that have an ord_date value greater than or equal to 2020-03-01. The operation in the example:  1. Groups by the item.sku field, and calculates the number of orders and the total quantity ordered for each sku.  2. Calculates the average quantity per order for each sku value and merges the results into the output collection. When merging results, if an existing document has the same key as the new result, the operation overwrites the existing document. If there is no existing document with the same key, the operation inserts the document. Example steps:  1. Define the map function to process each input document:\n    \n    * In the function, this refers to the document that the map-reduce operation is processing.\n    \n    * For each item, the function associates the sku with a new object value that contains the count of 1 and the item qty for the order and emits the sku (stored in the key) and the value.\n    \n     var mapFunction2 = function() {    for (var idx = 0; idx < this.items.length; idx++) {       var key = this.items[idx].sku;       var value = { count: 1, qty: this.items[idx].qty };\n           emit(key, value);    }};\n    \n      2. Define the corresponding reduce function with two arguments keySKU and countObjVals:\n    \n    * countObjVals is an array whose elements are the objects mapped to the grouped keySKU values passed by map function to the reducer function.\n    \n    * The function reduces the countObjVals array to a single object reducedValue that contains the count and the qty fields.\n    \n    * In reducedVal, the count field contains the sum of the count fields from the individual array elements, and the qty field contains the sum of the qty fields from the individual array elements.\n    \n    var reduceFunction2 = function(keySKU, countObjVals) {   reducedVal = { count: 0, qty: 0 };\n       for (var idx = 0; idx < countObjVals.length; idx++) {       reducedVal.count += countObjVals[idx].count;       reducedVal.qty += countObjVals[idx].qty;   }\n       return reducedVal;};\n    \n      3. Define a finalize function with two arguments key and reducedVal. The function modifies the reducedVal object to add a computed field named avg and returns the modified object:\n    \n    var finalizeFunction2 = function (key, reducedVal) {  reducedVal.avg = reducedVal.qty/reducedVal.count;  return reducedVal;};\n    \n    \n\n  5. Query the map_reduce_example2 collection to verify the results:\n    \n    db.map_reduce_example2.find().sort( { _id: 1 } )\n    \n    \n    \n    The operation returns these documents:\n    \n    { \"_id\" : \"apples\", \"value\" : { \"count\" : 4, \"qty\" : 35, \"avg\" : 8.75 } }{ \"_id\" : \"carrots\", \"value\" : { \"count\" : 2, \"qty\" : 15, \"avg\" : 7.5 } }{ \"_id\" : \"chocolates\", \"value\" : { \"count\" : 3, \"qty\" : 15, \"avg\" : 5 } }{ \"_id\" : \"oranges\", \"value\" : { \"count\" : 7, \"qty\" : 63, \"avg\" : 9 } }{ \"_id\" : \"pears\", \"value\" : { \"count\" : 1, \"qty\" : 10, \"avg\" : 10 } } AGGREGATION ALTERNATIVE \nUsing the available aggregation pipeline operators, you can rewrite the map-reduce operation without defining custom functions: db.orders.aggregate( [   { $match: { ord_date: { $gte: new Date(\"2020-03-01\") } } },   { $unwind: \"$items\" },   { $group: { _id: \"$items.sku\", qty: { $sum: \"$items.qty\" }, orders_ids: { $addToSet: \"$_id\" } }  },   { $project: { value: { count: { $size: \"$orders_ids\" }, qty: \"$qty\", avg: { $divide: [ \"$qty\", { $size: \"$orders_ids\" } ] } } } },   { $merge: { into: \"agg_alternative_3\", on: \"_id\", whenMatched: \"replace\",  whenNotMatched: \"insert\" } }] )   1. The $match stage selects only those documents with ord_date greater than or equal to new Date(\"2020-03-01\").\n\n  3. The $group stage groups by the items.sku, calculating for each sku:\n    \n    * The qty field. The qty field contains thetotal qty ordered per each items.sku (See $sum).\n    * The orders_ids array. The orders_ids field contains anarray of distinct order _id's for the items.sku (See $addToSet).\n    \n    { \"_id\" : \"chocolates\", \"qty\" : 15, \"orders_ids\" : [ 2, 5, 8 ] }{ \"_id\" : \"oranges\", \"qty\" : 63, \"orders_ids\" : [ 4, 7, 3, 2, 9, 1, 10 ] }{ \"_id\" : \"carrots\", \"qty\" : 15, \"orders_ids\" : [ 6, 9 ] }{ \"_id\" : \"apples\", \"qty\" : 35, \"orders_ids\" : [ 9, 8, 1, 6 ] }{ \"_id\" : \"pears\", \"qty\" : 10, \"orders_ids\" : [ 3 ] }  4. The $project stage reshapes the output document to mirror the map-reduce's output to have two fields _id and value. The $project sets:\n\n  6. The $group stage groups by the items.sku, calculating for each sku:\n    \n    * The qty field. The qty field contains the total qty ordered per each items.sku using $sum.\n    \n    * The orders_ids array. The orders_ids field contains an array of distinct order _id's for the items.sku using $addToSet.\n    \n    { \"_id\" : \"chocolates\", \"qty\" : 15, \"orders_ids\" : [ 2, 5, 8 ] }{ \"_id\" : \"oranges\", \"qty\" : 63, \"orders_ids\" : [ 4, 7, 3, 2, 9, 1, 10 ] }{ \"_id\" : \"carrots\", \"qty\" : 15, \"orders_ids\" : [ 6, 9 ] }{ \"_id\" : \"apples\", \"qty\" : 35, \"orders_ids\" : [ 9, 8, 1, 6 ] }{ \"_id\" : \"pears\", \"qty\" : 10, \"orders_ids\" : [ 3 ] }  7. The $project stage reshapes the output document to mirror the map-reduce's output to have two fields _id and value. The $project sets:\n    \n    * the value.count to the size of the orders_ids array using $size.\n    \n    * the value.qty to the qty field of input document.\n    \n    * the value.avg to the average number of qty per order using $divide and $size.\n    \n    { \"_id\" : \"apples\", \"value\" : { \"count\" : 4, \"qty\" : 35, \"avg\" : 8.75 } }{ \"_id\" : \"pears\", \"value\" : { \"count\" : 1, \"qty\" : 10, \"avg\" : 10 } }{ \"_id\" : \"chocolates\", \"value\" : { \"count\" : 3, \"qty\" : 15, \"avg\" : 5 } }{ \"_id\" : \"oranges\", \"value\" : { \"count\" : 7, \"qty\" : 63, \"avg\" : 9 } }{ \"_id\" : \"carrots\", \"value\" : { \"count\" : 2, \"qty\" : 15, \"avg\" : 7.5 } }  8. Finally, the $merge writes the output to the collection agg_alternative_3. If an existing document has the same key _id as the new result, the operation overwrites the existing document. If there is no existing document with the same key, the operation inserts the document.  9. Query the agg_alternative_3 collection to verify the results:\n    \n    db.agg_alternative_3.find().sort( { _id: 1 } )\n    \n    \n    \n    The operation returns the following documents:\n    \n    { \"_id\" : \"apples\", \"value\" : { \"count\" : 4, \"qty\" : 35, \"avg\" : 8.75 } }{ \"_id\" : \"carrots\", \"value\" : { \"count\" : 2, \"qty\" : 15, \"avg\" : 7.5 } }{ \"_id\" : \"chocolates\", \"value\" : { \"count\" : 3, \"qty\" : 15, \"avg\" : 5 } }{ \"_id\" : \"oranges\", \"value\" : { \"count\" : 7, \"qty\" : 63, \"avg\" : 9 } }{ \"_id\" : \"pears\", \"value\" : { \"count\" : 1, \"qty\" : 10, \"avg\" : 10 } } \nTIP \nSEE ALSO: For an alternative that uses custom aggregation expressions, see Map-Reduce to Aggregation Pipeline Translation Examples. \nOUTPUT \nThe output of the db.collection.mapReduce() method is identical to that of the mapReduce command. See the Output section of the mapReduce command for information on the db.collection.mapReduce() output. \nRESTRICTIONS\n\n \nADDITIONAL INFORMATION \n * Troubleshoot the Map Function  * Troubleshoot the Reduce Function  * mapReduce command  * Aggregation Operations  * Map-Reduce  * Perform Incremental Map-Reduce ←  db.collection.latencyStats()db.collection.reIndex() → On this page  * Syntax\n * Output\n * Restrictions\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.insertMany/": " Docs Home → MongoDB Manual \nDB.COLLECTION.INSERTMANY() \nOn this page    \n * Definition\n   \n * Behaviors\n * Examples \nDEFINITION \ndb.collection.insertMany()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the insert command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Inserts multiple documents into a collection. The insertMany() method has the following syntax: db.collection.insertMany(   [ <document 1> , <document 2>, ... ],   {      writeConcern: <document>,      ordered: <boolean>   }) \nParameter\nType\nDescription\ndocument\ndocument\nAn array of documents to insert into the collection.\nwriteConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. ordered\nboolean\nOptional. A boolean specifying whether the mongod instance should perform an ordered or unordered insert. Defaults to true. Returns:A document containing:  * An acknowledged boolean, set to true if the operation ran with write concern or false if write concern was disabled  * An insertedIds array, containing _id values for each successfully inserted document \nBEHAVIORS \nGiven an array of documents, insertMany() inserts each document in the array into the collection. \nEXECUTION OF OPERATIONS \nBy default documents are inserted in order. If ordered is set to false, documents are inserted in an unordered format and may be reordered by mongod to increase performance. Applications should not depend on ordering of inserts if using an unordered insertMany(). The number of operations in each group cannot exceed the value of the maxWriteBatchSize of the database. As of MongoDB 3.6, this value is 100,000. This value is shown in the hello.maxWriteBatchSize field. This limit prevents issues with oversized error messages. If a group exceeds this limit, the client driver divides the group into smaller groups with counts less than or equal to the value of the limit. For example, with the maxWriteBatchSize value of 100,000, if the queue consists of 200,000 operations, the driver creates 2 groups, each with 100,000 operations. \nNOTE The driver only divides the group into smaller groups when using the high-level API. If using db.runCommand() directly (for example, when writing a driver), MongoDB throws an error when attempting to execute a write batch which exceeds the limit. Starting in MongoDB 3.6, once the error report for a single batch grows too large, MongoDB truncates all remaining error messages to the empty string. Currently, begins once there are at least 2 error messages with total size greater than 1MB. The sizes and grouping mechanics are internal performance details and are subject to change in future versions. Executing an ordered list of operations on a sharded collection will generally be slower than executing an unordered list since with an ordered list, each operation must wait for the previous operation to finish. \nCOLLECTION CREATION \nIf the collection does not exist, then insertMany() creates the collection on successful write. \n_ID FIELD \nIf the document does not specify an _id field, then mongod adds the _id field and assign a unique ObjectId() for the document. Most drivers create an ObjectId and insert the _id field, but the mongod will create and populate the _id if the driver or application does not. If the document contains an _id field, the _id value must be unique within the collection to avoid duplicate key error. \nEXPLAINABILITY \ninsertMany() is not compatible with db.collection.explain(). \nERROR HANDLING \nInserts throw a BulkWriteError exception. Excluding Write Concern errors, ordered operations stop after an error, while unordered operations continue to process any remaining write operations in the queue.\n\n \nTRANSACTIONS \ndb.collection.insertMany() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. COLLECTION CREATION IN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, if you specify an insert on a non-existing collection in a transaction, the collection is implicitly created. In MongoDB 4.4 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES \nThe following examples insert documents into the products collection. \nINSERT SEVERAL DOCUMENT WITHOUT SPECIFYING AN _ID FIELD \nThe following example uses db.collection.insertMany() to insert documents that do not contain the _id field: try {   db.products.insertMany( [      { item: \"card\", qty: 15 },      { item: \"envelope\", qty: 20 },      { item: \"stamps\" , qty: 30 }   ] );} catch (e) {   print (e);}  The operation returns the following document: {   \"acknowledged\" : true,   \"insertedIds\" : [      ObjectId(\"562a94d381cb9f1cd6eb0e1a\"),      ObjectId(\"562a94d381cb9f1cd6eb0e1b\"),      ObjectId(\"562a94d381cb9f1cd6eb0e1c\")   ]}  Because the documents did not include _id, mongod creates and adds the _id field for each document and assigns it a unique ObjectId() value. The ObjectId values are specific to the machine and time when the operation is run. As such, your values may differ from those in the example. \nINSERT SEVERAL DOCUMENT SPECIFYING AN _ID FIELD \nThe following example/operation uses insertMany() to insert documents that include the _id field. The value of _id must be unique within the collection to avoid a duplicate key error. try {   db.products.insertMany( [      { _id: 10, item: \"large box\", qty: 20 },      { _id: 11, item: \"small box\", qty: 55 },      { _id: 12, item: \"medium box\", qty: 30 }   ] );} catch (e) {   print (e);}  The operation returns the following document: { \"acknowledged\" : true, \"insertedIds\" : [ 10, 11, 12 ] }  Inserting a duplicate value for any key that is part of a unique index, such as _id, throws an exception. The following attempts to insert a document with a _id value that already exists: try {   db.products.insertMany( [      { _id: 13, item: \"envelopes\", qty: 60 },      { _id: 13, item: \"stamps\", qty: 110 },      { _id: 14, item: \"packing tape\", qty: 38 }   ] );} catch (e) {   print (e);}  Since _id: 13 already exists, the following exception is thrown:\n\n  Note that one document was inserted: The first document of _id: 13 will insert successfully, but the second insert will fail. This will also stop additional documents left in the queue from being inserted. With ordered to false, the insert operation would continue with any remaining documents. \nUNORDERED INSERTS \nThe following attempts to insert multiple documents with _id field and ordered: false. The array of documents contains two documents with duplicate _id fields. try {   db.products.insertMany( [      { _id: 10, item: \"large box\", qty: 20 },      { _id: 11, item: \"small box\", qty: 55 },      { _id: 11, item: \"medium box\", qty: 30 },      { _id: 12, item: \"envelope\", qty: 100},      { _id: 13, item: \"stamps\", qty: 125 },      { _id: 13, item: \"tape\", qty: 20},      { _id: 14, item: \"bubble wrap\", qty: 30}   ], { ordered: false } );} catch (e) {   print (e);}  The operation throws the following exception: BulkWriteError({   \"writeErrors\" : [      {         \"index\" : 2,         \"code\" : 11000,         \"errmsg\" : \"E11000 duplicate key error collection: inventory.products index: _id_ dup key: { : 11.0 }\",         \"op\" : {            \"_id\" : 11,            \"item\" : \"medium box\",            \"qty\" : 30         }      },      {         \"index\" : 5,         \"code\" : 11000,         \"errmsg\" : \"E11000 duplicate key error collection: inventory.products index: _id_ dup key: { : 13.0 }\",         \"op\" : {            \"_id\" : 13,            \"item\" : \"tape\",            \"qty\" : 20         }      }   ],   \"writeConcernErrors\" : [ ],   \"nInserted\" : 5,   \"nUpserted\" : 0,   \"nMatched\" : 0,   \"nModified\" : 0,   \"nRemoved\" : 0,   \"upserted\" : [ ]})  While the document with item: \"medium box\" and item: \"tape\" failed to insert due to duplicate _id values, nInserted shows that the remaining 5 documents were inserted. \nUSING WRITE CONCERN \nGiven a three member replica set, the following operation specifies a w of majority and wtimeout of 100: try {   db.products.insertMany(      [         { _id: 10, item: \"large box\", qty: 20 },         { _id: 11, item: \"small box\", qty: 55 },         { _id: 12, item: \"medium box\", qty: 30 }      ],      { w: \"majority\", wtimeout: 100 }   );} catch (e) {   print (e);}  If the primary and at least one secondary acknowledge each write operation within 100 milliseconds, it returns:\n\n  If the total time required for all required nodes in the replica set to acknowledge the write operation is greater than wtimeout, the following writeConcernError is displayed when the wtimeout period has passed. This operation returns: WriteConcernError({   \"code\" : 64,   \"errmsg\" : \"waiting for replication timed out\",   \"errInfo\" : {     \"wtimeout\" : true,     \"writeConcern\" : {    // Added in MongoDB 4.4       \"w\" : \"majority\",       \"wtimeout\" : 100,       \"provenance\" : \"getLastErrorDefaults\"     }   }})  \nTIP \nSEE ALSO: WriteResult.writeConcernError ←  db.collection.insertOne()db.collection.isCapped() → On this page  * Definition\n * Behaviors\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.tryNext/": " Docs Home → MongoDB Manual \nCURSOR.NEXT() \nOn this page    \n * Behavior cursor.tryNext()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Returns:The next document in the cursor returned by the db.collection.find() method or null. \nBEHAVIOR \ncursor.tryNext() is a special case of the cursor.next() method that returns the next element in the iteration if available or else null. ←  cursor.toArray()Database Methods → On this page  * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.logout/": " Docs Home → MongoDB Manual \nDB.LOGOUT() \ndb.logout() \nDeprecated since version 5.0. Attempting to use the db.logout() method will write an error message to the log once per logout attempt. This method will be removed in a future release. Ends the current authentication session. This function has no effect if the current session is not authenticated. \nNOTE If you're not logged in and using authentication, db.logout() has no effect. Because MongoDB allows users defined in one database to have privileges on another database, you must call db.logout() while using the same database context that you authenticated to. If you authenticated to a database such as users or $external, you must issue db.logout() against this database in order to successfully log out. \nEXAMPLE Use the use <database-name> helper in mongosh, or the following db.getSiblingDB() method in an interactive mongosh session or in mongosh shell scripts to change the db object: db = db.getSiblingDB('<database-name>')  When you have set the database context and db object, you can use the db.logout() to log out of database as in the following operation: db.logout()  db.logout() function provides a wrapper around the database command logout. ←  db.listCommands()db.printCollectionStats() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.status/": " Docs Home → MongoDB Manual \nSH.STATUS() \nOn this page    \n * Definition\n   \n * Output Examples\n * Output Fields \nDEFINITION \nsh.status() \nWhen run on a mongos instance, prints a formatted report of the sharding configuration and the information regarding existing chunks in a sharded cluster. The default behavior suppresses the detailed chunk information if the total number of chunks is greater than or equal to 20. The sh.status() method has the following parameter: Parameter\nType\nDescription\nverbose\nboolean Optional. Determines the level of verbosity. If true, the method displays:      * Full details of the chunk distribution across shards even if you have 20 or more chunks, as well as the number of chunks on each shard.  * Details of active mongos instances. If false, the method displays:  * Full details of the chunk distribution across shards only if you have less than 20 chunks. If you have 20 or more chunks, the method instead returns a too many chunks to print ... message, showing only the number of chunks on each shard.  * Only the version and number of active mongos instances. The default verbose value is false. \nTIP \nSEE ALSO: db.printShardingStatus() \nOUTPUT EXAMPLES \nThe Sharding Version section displays information on the config database: --- Sharding Status ---  sharding version: {   \"_id\" : <num>,   \"minCompatibleVersion\" : <num>,   \"currentVersion\" : <num>,   \"clusterId\" : <ObjectId>}  The Shards section lists information on the shard(s). For each shard, the section displays the name, host, and the associated tags, if any. shards: { \"_id\" : <shard name1>, \"host\" : <string>, \"tags\" : [ <string> ... ], \"state\" : <num> } { \"_id\" : <shard name2>, \"host\" : <string>, \"tags\" : [ <string> ... ], \"state\" : <num> } ...  The Active mongos Instances section displays, by default, information on the version and count of mongos instances that have been active within the last 60 seconds: active mongoses:  <version> : <num>  If the method is run with the verbose parameter to true, the Active mongos Instances section displays additional information: active mongoses:{  \"_id\" : \"<hostname:port>\",  \"advisoryHostFQDNs\" : [ \"<name>\" ],  \"mongoVersion\" : <string>,  \"ping\" : <ISODate>,  \"up\" : <long>,  \"waiting\" : <boolean> }  The Autosplit displays information on whether autosplit is enabled: autosplit:  Currently enabled: <yes|no>  The Balancer section lists information about the state of the balancer. This provides insight into current balancer operation and can be useful when troubleshooting an unbalanced sharded cluster. balancer:      Currently enabled:  yes      Currently running:  yes      Collections with active migrations:              config.system.sessions started at Fri May 15 2020 17:38:12 GMT-0400 (EDT)      Failed balancer rounds in last 5 attempts:  0      Migration Results for the last 24 hours:             416 : Success             1 : Failed with error 'aborted', from shardA to shardB  The Databases section lists information on the database(s). It displays the database name and the primary shard for each database. databases: { \"_id\" : <dbname1>, \"primary\" : <string>, \"partitioned\" : <boolean>, \"version\": <document> } { \"_id\" : <dbname2>, \"primary\" : <string>, \"partitioned\" : <boolean>, \"version\": <document> } ...  The Sharded Collection section provides information on the sharding details for sharded collection(s). For each sharded collection, the section displays the shard key, the number of chunks per shard(s), the distribution of chunks across shards [1], and the tag information, if any, for shard key range(s).\n\n  \nOUTPUT FIELDS  SHARDING VERSION \nsh.status.sharding-version._id \nThe _id is an identifier for the version details. sh.status.sharding-version.minCompatibleVersion \nThe minCompatibleVersion is the minimum compatible version of the config server. sh.status.sharding-version.currentVersion \nThe currentVersion is the current version of the config server. sh.status.sharding-version.clusterId \nThe clusterId is the identification for the sharded cluster. \nACTIVE MONGOS INSTANCES \nsh.status.active-mongoses \nIf verbose is false, sh.status.active-mongoses lists the version and count of the active mongos instances. Active mongos instances are mongos instances that have been pinged within the last 60 seconds. If verbose is true, sh.status.active-mongoses returns a document for each active mongos instance containing the following fields: Field\nData Type\nDescription\n_id\nString\nThe hostname and port where the mongos is running. The _id is formatted as <hostname>:<port>.\nadvisoryHostFQDNs\nArray of strings\nArray of the mongos's fully qualified domain names (FQDNs).\ncreated\nDate When the mongos was started. New in version 5.2. mongoVersion\nString\nVersion of MongoDB that the mongos is running.\nping\nDate\nmongos instances send pings to the config server every 30 seconds. This field indicates the last ping time.\nup\nNumberLong\nNumber of seconds the mongos has been up as of the last ping.\nwaiting\nBoolean\nAs of MongoDB 3.4, this field is always true and is only present for backward compatibility. active mongoses:\n{   \"_id\" : \"<hostname:port>\",   \"advisoryHostFQDNs\" : [ \"<name>\" ],   \"created\" : <ISODate>,   \"mongoVersion\" : <string>,   \"ping\" : <ISODate>,   \"up\" : <long>,   \"waiting\" : <boolean>}...  \nAUTOSPLIT \nsh.status.autosplit \nsh.status.autosplit indicates whether autosplit is currently enabled. \nNOTE Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0:  * The balancerStart command and the mongo shell helper methods sh.startBalancer() and sh.setBalancerState(true) also enable auto-splitting for the sharded cluster.\n   \n   To disable auto-splitting when the balancer is enabled, you can use sh.disableAutoSplit().  * The balancerStop command and the mongo shell helper methods sh.stopBalancer() and sh.setBalancerState(false) also disable auto-splitting for the sharded cluster.\n   \n   To enable auto-splitting when the balancer is disabled, you can use sh.enableAutoSplit(). The mongo methods sh.enableBalancing(namespace) and sh.disableBalancing(namespace) have no affect on the auto-splitting. \nSHARDS \nsh.status.shards._id \nThe _id displays the name of the shard. sh.status.shards.host \nThe host displays the host location of the shard. sh.status.shards.tags \nThe tags displays all the tags for the shard. The field only displays if the shard has tags. sh.status.shards.state \nThe state displays:  * 0 if the shard is not shard aware.  * 1 if the shard is shard aware. \nBALANCER  NOTE Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0:  * The balancerStart command and the mongo shell helper methods sh.startBalancer() and sh.setBalancerState(true) also enable auto-splitting for the sharded cluster.\n   \n   To disable auto-splitting when the balancer is enabled, you can use sh.disableAutoSplit().\n\n The mongo methods sh.enableBalancing(namespace) and sh.disableBalancing(namespace) have no affect on the auto-splitting. sh.status.balancer.currently-enabled \ncurrently-enabled indicates if the balancer is currently enabled on the sharded cluster. sh.status.balancer.currently-running \ncurrently-running indicates whether the balancer is currently running, and therefore currently balancing the cluster. sh.status.balancer.collections-with-active-migrations \ncollections-with-active-migrations lists the names of any collections with active migrations, and specifies when the migration began. If there are no active migrations, this field will not appear in the sh.status() output. sh.status.balancer.failed-balancer-rounds-in-last-5-attempts \nfailed-balancer-rounds-in-last-5-attempts displays the number of balancer rounds that failed, from among the last five attempted rounds. A balancer round will fail when a chunk migration fails. sh.status.balancer.last-reported-error \nlast-reported-error lists the most recent balancer error message. If there have been no errors, this field will not appear in the sh.status() output. sh.status.balancer.time-of-reported-error \ntime-of-reported-error provides the date and time of the most recently-reported error. sh.status.balancer.migration-results-for-the-last-24-hours \nmigration-results-for-the-last-24-hours displays the number of migrations in the last 24 hours, and the error messages from failed migrations . If there have been no recent migrations, migration-results-for-the-last-24-hours displays No recent migrations. migration-results-for-the-last-24-hours includes all migrations, including those not initiated by the balancer. \nDATABASES \nsh.status.databases._id \nThe _id displays the name of the database. sh.status.databases.partitioned \nStarting in MongoDB 6.0 and feature compatibility version (FCV) 6.0, the partitioned flag only maintains backward compatibility with previous versions. By default, all databases are enabled for sharding regardless of the partitioned value. sh.status.databases.primary \nThe primary displays the primary shard for the database. sh.status.databases.version \nThe version displays the version information for the database: {  \"uuid\" : UUID(\"cc250b66-8638-49f7-a2e8-c6f1220b9d7a\"),  \"lastMod\" : 1 }  where:  * uuid is the database identifier.  * lastMod is the database version. \nSHARDED COLLECTION \nsh.status.databases.<collection>.shard-key \nThe shard-key displays the shard key specification document. sh.status.databases.<collection>.unique \nThe unique displays whether MongoDB enforces uniqueness on the shard key values (i.e. whether the underlying shard key index is unique). sh.status.databases.<collection>.balancing \nThe balancing displays whether balancing is enabled (true) or disabled (false) for the collection. sh.status.databases.<collection>.chunks \nThe chunks lists all the shards and the number of chunks that reside on each shard. sh.status.databases.<collection>.chunk-details \nThe chunk-details lists the details of the chunks [1]:  * The range of shard key values that define the chunk,  * The shard where the chunk resides, and  * The last modified timestamp for the chunk. sh.status.databases.<collection>.tag \nThe tag lists the details of the tags associated with a range of shard key values. \nTIP \nSEE ALSO: sh.balancerCollectionStatus() [1](1, 2) The sharded collection section, by default, displays the chunk information if the total number of chunks is less than 20. To display the information when you have 20 or more chunks, call the sh.status() methods with the verbose parameter set to true, i.e. sh.status(true). ←  sh.startBalancer()sh.stopBalancer() → On this page  * Definition\n * Output Examples\n * Output Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getFreeMonitoringStatus/": " Docs Home → MongoDB Manual \nDB.GETFREEMONITORINGSTATUS \nOn this page    \n * Access Control\n   \n * Output\n * Example db.getFreeMonitoringStatus() \nReturns information on free monitoring status. mongosh helper db.getFreeMonitoringStatus() is a wrapper around the getFreeMonitoringStatus command. \nTIP \nSEE ALSO: db.enableFreeMonitoring() \nACCESS CONTROL \nWhen running with access control, the user must have the checkFreeMonitoringStatus privilege actions on the cluster. That is, a user must have a role that grants the following privilege: { resource: { cluster : true }, actions: [ \"checkFreeMonitoringStatus\" ] }  The built-in role clusterMonitor role provides this privilege. \nOUTPUT \nThe method returns a document with the following fields: Field\nDescription\nstate\nThe free monitoring enablement state. Values are either: \"enabled\" or \"disabled\".\nmessage\nAny informational message related to your state.\nurl The unique URL at which your monitoring data can be accessed. \nNOTE Anyone with whom you share this unique URL can access your monitored data. Even when disabled, your unique URL is returned so that if you re-enable monitoring, you can access your previous metrics. userReminder\nAny informational message related to your state.\nok Status of the getFreeMonitoringStatus operation itself. Values are either:      * 1 if the operation was successful.  * 0 if the operation was not successful. \nEXAMPLE \nYou can use db.getFreeMonitoringStatus() to check your free monitoring status db.getFreeMonitoringStatus()  The method returns information on your free monitoring status. {   \"state\" : \"enabled\",   \"message\" : \"To see your monitoring data, navigate to the unique URL below.\\nAnyone you share the URL with will also be able to view this page.\\n\\nhttps://cloud.mongodb.com/freemonitoring/mongo/MSBjZTZhNTJmOS0yODg1\\n\\nYou can disable monitoring at any time by running db.disableFreeMonitoring().\",   \"url\" : \"https://cloud.mongodb.com/freemonitoring/mongo/MSBjZTZhNTJmOS0yODg1\",   \"userReminder\" : \"\",   \"ok\" : 1}  \nTIP \nSEE ALSO: freeMonitoring field returned from db.serverStatus() ←  db.enableFreeMonitoring()Object Constructors and Methods → On this page  * Access Control\n * Output\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.createIndex/": " Docs Home → MongoDB Manual \nDB.COLLECTION.CREATEINDEX() \nOn this page    \n * Definition\n   \n * Options\n * Behaviors\n * Examples\n * Additional Information \nDEFINITION \ndb.collection.createIndex(keys, options, commitQuorum)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the createIndexes command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Creates indexes on collections. To minimize the impact of building an index on replica sets and sharded clusters, use a rolling index build procedure as described on Rolling Index Builds on Replica Sets. db.collection.createIndex() takes the following parameters: Parameter\nType\nDescription\nkeys\ndocument A document that contains the field and value pairs where the field is the index key and the value describes the type of index for that field. For an ascending index on a field, specify a value of 1; for descending index, specify a value of -1. An asterisk (*) is not a valid index name. MongoDB supports several different index types including text, geospatial, and hashed indexes. See index types for more information. Changed in version 4.2: MongoDB 4.2 wildcard indexes support workloads where users query against custom fields or a large variety of fields in a collection:  * To create a wildcard index on all fields and subfields in a document, specify { \"$**\" : 1 } as the index key. You cannot specify a descending index key when creating a wildcard index.\n   \n   You can also either include or exclude specific fields and their subfields from the index using the optional wildcardProjection parameter.\n   \n   Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document:\n   \n   {  \"wildcardProjection\" : {    \"_id\" : 1,    \"<field>\" : 0|1  }}\n   \n   \n   \n   With the exception of explicitly including _id field, you cannot combine inclusion and exclusion statements in the wildcardProjection document.  * You can create a wildcard index on a specific field and its subpaths by specifying the full path to that field as the index key and append \"$**\" to the path:\n   \n   { \"path.to.field.$**\" : 1 }\n   \n   You cannot specify a descending index key when creating a wildcard index.\n   \n   The path-specific wildcard index syntax is incompatible with the wildcardProjection option. You cannot specify additional inclusions or exclusions on the specified path. The wildcard index key must use one of the syntaxes listed above. For example, you cannot specify a compound index key. For more complete documentation on wildcard indexes, including restrictions on their creation, see Wildcard Index Restrictions. The mongod featureCompatibilityVersion must be 4.2 to create wildcard indexes. For instructions on setting the fCV, see Set Feature Compatibility Version on MongoDB 6.0 Deployments. For examples of wildcard index creation, see Create a Wildcard Index. options\ndocument\nOptional. A document that contains a set of options that controls the creation of the index. See Options for details.\ncommitQuorum\ninteger or string Optional. The minimum number of data-bearing voting replica set members (i.e. commit quorum), including the primary, that must report a successful index build before the primary marks the indexes as ready. A \"voting\" member is any replica set member where members[n].votes is greater than 0. Supports the following values:  * \"votingMembers\" - all data-bearing voting replica set members (Default).  * \"majority\" - a simple majority of data-bearing voting replica set members.  * <int> - a specific number of data-bearing voting replica set members.\n\n  * A replica set tag name. New in version 4.4. \nOPTIONS \nThe options document contains a set of options that controls the creation of the index. Different index types can have additional options specific for that type. Multiple index options can be specified in the same document. However, if you specify multiple option documents the db.collection.createIndex() operation will fail. Consider the following db.collection.createIndex() operation: db.collection.createIndex(  {      \"a\": 1  },  {      unique: true,      sparse: true,      expireAfterSeconds: 3600  })  If the options specification had been split into multiple documents like this: { unique: true }, { sparse: true, expireAfterSeconds: 3600 } the index creation operation would have failed. \nOPTIONS FOR ALL INDEX TYPES \nThe following options are available for all index types unless otherwise specified: Parameter\nType\nDescription\nunique\nboolean Optional. Creates a unique index so that the collection will not accept insertion or update of documents where the index key value matches an existing value in the index. Specify true to create a unique index. The default value is false. The option is unavailable for hashed indexes. name\nstring\nOptional. The name of the index. If unspecified, MongoDB generates an index name by concatenating the names of the indexed fields and the sort order.\npartialFilterExpression\ndocument Optional. If specified, the index only references documents that match the filter expression. See Partial Indexes for more information. A filter expression can include:  * equality expressions (i.e. field: value or using the $eq operator),  * $exists: true expression,  * $gt, $gte, $lt, $lte expressions,  * $type expressions,  * $and operator,  * $or operator,  * $in operator You can specify a partialFilterExpression option for all MongoDB index types. sparse\nboolean Optional. If true, the index only references documents with the specified field. These indexes use less space but behave differently in some situations (particularly sorts). The default value is false. See Sparse Indexes for more information. The following index types are sparse by default and ignore this option:  * 2dsphere  * 2d  * geoHaystack  * text For a compound index that includes 2dsphere index key(s) along with keys of other types, only the 2dsphere index fields determine whether the index references a document. \nTIP Partial indexes offer a superset of the functionality of sparse indexes. Unless your application has a specific requirement, use partial indexes instead of sparse indexes. expireAfterSeconds\ninteger Optional. Specifies a value, in seconds, as a time to live (TTL) to control how long MongoDB retains documents in this collection. This option only applies to TTL indexes. See Expire Data from Collections by Setting TTL for more information. If you use TTL indexes created before MongoDB 5.0, or if you want to sync data created in MongDB 5.0 with a pre-5.0 installation, see Indexes Configured Using NaN to avoid misconfiguration issues. hidden\nboolean Optional. A flag that determines whether the index is hidden from the query planner. A hidden index is not evaluated as part of the query plan selection. Default is false. To use the hidden option, you must have featureCompatibilityVersion set to 4.4 or greater. However, once hidden, the index remains hidden even with featureCompatibilityVersion set to 4.2 on MongoDB 4.4 binaries. New in version 4.4. storageEngine\ndocument Optional. Allows users to configure the storage engine on a per-index basis when creating an index. The storageEngine option should take the following form: storageEngine: { <storage-engine-name>: <options> }  Storage engine configuration options specified when creating indexes are validated and logged to the oplog during replication to support replica sets with members that use different storage engines. \nOPTION FOR COLLATION \nParameter\nType\nDescription\ncollation\ndocument Optional. Specifies the collation for the index. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. If you have specified a collation at the collection level, then:  * If you do not specify a collation when creating the index, MongoDB creates the index with the collection's default collation.  * If you do specify a collation when creating the index, MongoDB creates the index with the specified collation. The collation option has the following syntax:\n\n  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. The following indexes only support simple binary comparison and do not support collation:  * text indexes,  * 2d indexes, and  * geoHaystack indexes. \nTIP To create a text, a 2d, or a geoHaystack index on a collection that has a non-simple collation, you must explicitly specify {collation: {locale: \"simple\"} } when creating the index. COLLATION AND INDEX USE \nIf you have specified a collation at the collection level, then:  * If you do not specify a collation when creating the index, MongoDB creates the index with the collection's default collation.  * If you do specify a collation when creating the index, MongoDB creates the index with the specified collation. \nTIP By specifying a collation strength of 1 or 2, you can create a case-insensitive index. Index with a collation strength of 1 is both diacritic- and case-insensitive. You can create multiple indexes on the same key(s) with different collations. To create indexes with the same key pattern but different collations, you must supply unique index names. To use an index for string comparisons, an operation must also specify the same collation. That is, an index with a collation cannot support an operation that performs string comparisons on the indexed fields if the operation specifies a different collation. For example, the collection myColl has an index on a string field category with the collation locale \"fr\". db.myColl.createIndex( { category: 1 }, { collation: { locale: \"fr\" } } )  The following query operation, which specifies the same collation as the index, can use the index: db.myColl.find( { category: \"cafe\" } ).collation( { locale: \"fr\" } )  However, the following query operation, which by default uses the \"simple\" binary collator, cannot use the index: db.myColl.find( { category: \"cafe\" } )  For a compound index where the index prefix keys are not strings, arrays, and embedded documents, an operation that specifies a different collation can still use the index to support comparisons on the index prefix keys. For example, the collection myColl has a compound index on the numeric fields score and price and the string field category; the index is created with the collation locale \"fr\" for string comparisons: db.myColl.createIndex(   { score: 1, price: 1, category: 1 },   { collation: { locale: \"fr\" } } )  The following operations, which use \"simple\" binary collation for string comparisons, can use the index: db.myColl.find( { score: 5 } ).sort( { price: 1 } )db.myColl.find( { score: 5, price: { $gt: NumberDecimal( \"10\" ) } } ).sort( { price: 1 } )  The following operation, which uses \"simple\" binary collation for string comparisons on the indexed category field, can use the index to fulfill only the score: 5 portion of the query: db.myColl.find( { score: 5, category: \"cafe\" } )  \nOPTIONS FOR TEXT INDEXES \nThe following options are available for text indexes only: Parameter\nType\nDescription\nweights\ndocument Optional. For text indexes, a document that contains field and weight pairs. The weight is an integer ranging from 1 to 99,999 and denotes the significance of the field relative to the other indexed fields in terms of the score. You can specify weights for some or all the indexed fields. See Control Search Results with Weights to adjust the scores. The default value is 1. Starting in MongoDB 5.0, the weights option is only allowed for text indexes. default_language\nstring\nOptional. For text indexes, the language that determines the list of stop words and the rules for the stemmer and tokenizer. See Text Search Languages for the available languages and Specify a Language for Text Index for more information and examples. The default value is english.\nlanguage_override\nstring\nOptional. For text indexes, the name of the field, in the collection's documents, that contains the override language for the document. The default value is language. See Use any Field to Specify the Language for a Document for an example.\ntextIndexVersion\ninteger Optional. The text index version number. Users can use this option to override the default version number. For available versions, see Versions. \nOPTIONS FOR 2DSPHERE INDEXES\n\n Parameter\nType\nDescription\n2dsphereIndexVersion\ninteger Optional. The 2dsphere index version number. Users can use this option to override the default version number. For the available versions, see Versions. \nOPTIONS FOR 2D INDEXES \nThe following options are available for 2d indexes only: Parameter\nType\nDescription\nbits\ninteger Optional. For 2d indexes, the number of precision of the stored geohash value of the location data. The bits value ranges from 1 to 32 inclusive. The default value is 26. min\nnumber\nOptional. For 2d indexes, the lower inclusive boundary for the longitude and latitude values. The default value is -180.0.\nmax\nnumber\nOptional. For 2d indexes, the upper inclusive boundary for the longitude and latitude values. The default value is 180.0. \nOPTIONS FOR GEOHAYSTACK INDEXES \nThe following option is available for geoHaystack indexes only: Parameter\nType\nDescription\nbucketSize\nnumber For geoHaystack indexes, specify the number of units within which to group the location values; i.e. group in the same bucket those location values that are within the specified number of units to each other. The value must be greater than 0. \nNOTE \nREMOVED IN MONGODB 5.0 MongoDB 5.0 removes the deprecated geoHaystack index and geoSearch command. Use a 2d index with $geoNear or one of the supported geospatial query operators instead. Upgrading your MongoDB instance to 5.0 and setting featureCompatibilityVersion to 5.0 will delete any pre-existing geoHaystack indexes. \nOPTIONS FOR WILDCARD INDEXES \nThe following option is available for wildcard indexes only: Parameter\nType\nDescription\nwildcardProjection\ndocument Optional. Allows users to include or exclude specific field paths from a wildcard index using the { \"$**\" : 1} key pattern. This option is only valid if creating a wildcard index on all document fields. You cannot specify this option if creating a wildcard index on a specific field path and its subfields, e.g. { \"path.to.field.$**\" : 1 } The wildcardProjection option takes the following form: wildcardProjection: {  \"path.to.field.a\" : <value>,  \"path.to.field.b\" : <value>}  The <value> can be either of the following:  * 1 or true to include the field in the wildcard index.  * 0 or false to exclude the field from the wildcard index. Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document: {  \"wildcardProjection\" : {    \"_id\" : 1,    \"<field>\" : 0|1  }}  With the exception of explicitly including _id field, you cannot combine inclusion and exclusion statements in the wildcardProjection document. \nBEHAVIORS  CONCURRENCY \nChanged in version 4.2. MongoDB uses an optimized build process that obtains and holds an exclusive lock on the specified collection at the start and end of the index build. All subsequent operations on the collection must wait until createIndex() releases the exclusive lock. createIndex() allows interleaving read and write operations during the majority of the index build. For more information on the locking behavior of createIndex(), see Index Builds on Populated Collections. \nRECREATING AN EXISTING INDEX \nIf you call db.collection.createIndex() for an index that already exists, MongoDB does not recreate the index. \nINDEX OPTIONS \nNON-COLLATION AND NON-HIDDEN OPTIONS \nWith the exception of the collation option, if you create an index with one set of index options and then try to recreate the same index but with different index options, MongoDB will not change the options nor recreate the index. The hidden option can be changed without dropping and recreating the index. See Hidden Option. To change the other index options, drop the existing index with db.collection.dropIndex() before running db.collection.createIndex() with the new options. COLLATION OPTION \nYou can create multiple indexes on the same key(s) with different collations. To create indexes with the same key pattern but different collations, you must supply unique index names. HIDDEN OPTION \nNew in version 4.4. \nNOTE\n\n To hide or unhide existing indexes, you can use the following mongosh methods:  * db.collection.hideIndex()  * db.collection.unhideIndex() For example,  * To change the hidden option for an index to true, use the db.collection.hideIndex() method:\n   \n   db.restaurants.hideIndex( { borough: 1, ratings: 1 } );\n   \n     * To change the hidden option for an index to false, use the db.collection.unhideIndex() method:\n   \n   db.restaurants.unhideIndex( { borough: 1, city: 1 } );\n   \n    \nTIP \nSEE ALSO: Hidden Indexes \nTRANSACTIONS \nChanged in version 4.4. Starting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. To use db.collection.createIndex() in a transaction, the transaction must use read concern \"local\". If you specify a read concern level other than \"local\", the transaction fails. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction \nEXAMPLES  CREATE AN ASCENDING INDEX ON A SINGLE FIELD \nThe following example creates an ascending index on the field orderDate. db.collection.createIndex( { orderDate: 1 } )  If the keys document specifies more than one field, then createIndex() creates a compound index. \nCREATE AN INDEX ON A MULTIPLE FIELDS \nThe following example creates a compound index on the orderDate field (in ascending order) and the zipcode field (in descending order.) db.collection.createIndex( { orderDate: 1, zipcode: -1 } )  Changed in version 4.4: Starting in MongoDB 4.4, compound indexes can include a single hashed field. Compound hashed indexes require featureCompatibilityVersion set to 4.4. The following example creates a compound index on the state field (in ascending order) and the zipcode field (hashed): db.collection.createIndex( { \"state\" : 1, \"zipcode\" : \"hashed\" } )  The order of fields in a compound index is important for supporting sort() operations using the index. \nTIP \nSEE ALSO:  * Sort on Multiple Fields  * Sort and Index Prefix \nCREATE INDEXES WITH COLLATION SPECIFIED \nThe following example creates an index named category_fr. The example creates the index with the collation that specifies the locale fr and comparison strength 2: db.collection.createIndex(   { category: 1 },   { name: \"category_fr\", collation: { locale: \"fr\", strength: 2 } })  The following example creates a compound index named date_category_fr with a collation. The collation applies only to the index keys with string values. db.collection.createIndex(   { orderDate: 1, category: 1 },   { name: \"date_category_fr\", collation: { locale: \"fr\", strength: 2 } })  The collation applies to the indexed keys whose values are string. For queries or sort operations on the indexed keys that uses the same collation rules, MongoDB can use the index. For details, see Collation and Index Use. \nCREATE A WILDCARD INDEX \nNew in version 4.2. The mongod featureCompatibilityVersion must be 4.2 to create wildcard indexes. For instructions on setting the fCV, see Set Feature Compatibility Version on MongoDB 6.0 Deployments.  * Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document:\n   \n   {  \"wildcardProjection\" : {    \"_id\" : 1,    \"<field>\" : 0|1  }}\n   \n   \n   \n   With the exception of explicitly including _id field, you cannot combine inclusion and exclusion statements in the wildcardProjection document.\n\n For complete documentation on Wildcard Indexes, see Wildcard Indexes. The following lists examples of wildcard index creation:  * Create a Wildcard Index on a Single Field Path  * Create a Wildcard Index on All Field Paths  * Include Specific Fields in Wildcard Index Coverage  * Omit Specific Fields from Wildcard Index Coverage CREATE A WILDCARD INDEX ON A SINGLE FIELD PATH \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index on the product_attributes field: use inventorydb.products_catalog.createIndex( { \"product_attributes.$**\" : 1 } )  With this wildcard index, MongoDB indexes all scalar values of product_attributes. If the field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The wildcard index can support arbitrary single-field queries on product_attributes or one of its nested fields: db.products_catalog.find( { \"product_attributes.superFlight\" : true } )db.products_catalog.find( { \"product_attributes.maxSpeed\" : { $gt : 20 } } )db.products_catalog.find( { \"product_attributes.elements\" : { $eq: \"water\" } } )  \nNOTE The path-specific wildcard index syntax is incompatible with the wildcardProjection option. See the parameter documentation for more information. CREATE A WILDCARD INDEX ON ALL FIELD PATHS \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index on all scalar fields (excluding the _id field): use inventorydb.products_catalog.createIndex( { \"$**\" : 1 } )  With this wildcard index, MongoDB indexes all scalar fields for each document in the collection. If a given field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The created index can support queries on any arbitrary field within documents in the collection: db.products_catalog.find( { \"product_price\" : { $lt : 25 } } )db.products_catalog.find( { \"product_attributes.elements\" : { $eq: \"water\" } } )  \nNOTE Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document. See parameter documentation for more information. INCLUDE SPECIFIC FIELDS IN WILDCARD INDEX COVERAGE\n\n {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index and uses the wildcardProjection option to include only scalar values of the product_attributes.elements and product_attributes.resistance fields in the index. use inventorydb.products_catalog.createIndex(  { \"$**\" : 1 },  {    \"wildcardProjection\" : {      \"product_attributes.elements\" : 1,      \"product_attributes.resistance\" : 1    }  })  While the key pattern \"$**\" covers all fields in the document, the wildcardProjection field limits the index to only the included fields. For complete documentation on wildcardProjection, see Options for wildcard indexes. If a field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The created index can support queries on any scalar field included in the wildcardProjection: db.products_catalog.find( { \"product_attributes.elements\" : { $eq: \"Water\" } } )db.products_catalog.find( { \"product_attributes.resistance\" : \"Bludgeoning\" } )  \nNOTE Wildcard indexes do not support mixing inclusion and exclusion statements in the wildcardProjection document except when explicitly including the _id field. For more information on wildcardProjection, see the parameter documentation. OMIT SPECIFIC FIELDS FROM WILDCARD INDEX COVERAGE \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index and uses the wildcardProjection document to index all scalar fields for each document in the collection, excluding the product_attributes.elements and product_attributes.resistance fields: use inventorydb.products_catalog.createIndex(  { \"$**\" : 1 },  {    \"wildcardProjection\" : {      \"product_attributes.elements\" : 0,      \"product_attributes.resistance\" : 0    }  })  While the key pattern \"$**\" covers all fields in the document, the wildcardProjection field excludes the specified fields from the index. For complete documentation on wildcardProjection, see Options for wildcard indexes. If a field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The created index can support queries on any scalar field except those excluded by wildcardProjection: db.products_catalog.find( { \"product_attributes.maxSpeed\" : { $gt: 25 } } )db.products_catalog.find( { \"product_attributes.superStrength\" : true } )  \nNOTE\n\n \nCREATE INDEX WITH COMMIT QUORUM  NOTE \nREQUIRES FEATURECOMPATIBILITYVERSION 4.4+ Each mongod in the replica set or sharded cluster must have featureCompatibilityVersion set to at least 4.4 to start index builds simultaneously across replica set members. MongoDB 4.4 running featureCompatibilityVersion: \"4.2\" builds indexes on the primary before replicating the index build to secondaries. Starting with MongoDB 4.4, index builds on a replica set or sharded cluster build simultaneously across all data-bearing replica set members. For sharded clusters, the index build occurs only on shards containing data for the collection being indexed. The primary requires a minimum number of data-bearing voting members (i.e commit quorum), including itself, that must complete the build before marking the index as ready for use. See Index Builds in Replicated Environments for more information. Specify the commitQuorum parameter to the createIndex() operation to set the minimum number of data-bearing voting members (i.e commit quorum), including the primary, which must complete the index build before the primary marks the indexes as ready. The default commit quorum is votingMembers, or all data-bearing voting replica set members. The following operation creates an index with a commit quorum of \"majority\", or a simple majority of data-bearing voting members: db.getSiblingDB(\"examples\").invoices.createIndex(  { \"invoices\" : 1 },  { },  \"majority\")  The primary marks index build as ready only after a simple majority of data-bearing voting members \"vote\" to commit the index build. For more information on index builds and the voting process, see Index Builds in Replicated Environments. \nADDITIONAL INFORMATION \n * The Indexes section of this manual for full documentation of indexes and indexing in MongoDB.  * db.collection.getIndexes() to view the specifications of existing indexes for a collection.  * Text Indexes for details on creating text indexes.  * Geospatial Indexes for geospatial queries.  * TTL Indexes for expiration of data. ←  db.collection.countDocuments()db.collection.createIndexes() → On this page  * Definition\n * Options\n * Behaviors\n * Examples\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.estimatedDocumentCount/": " Docs Home → MongoDB Manual \nDB.COLLECTION.ESTIMATEDDOCUMENTCOUNT() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \ndb.collection.estimatedDocumentCount(options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the count field returned by the collStats command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns the count of all documents in a collection or view. db.collection.estimatedDocumentCount( <options> ) \nParameter\nType\nDescription\noptions\ndocument\nOptional. Extra options that affect the count behavior. The options document can contain the following: Field\nType\nDescription\nmaxTimeMS\ninteger\nOptional. The maximum amount of time to allow the count to run. \nBEHAVIOR  MECHANICS \ndb.collection.estimatedDocumentCount() does not take a query filter and instead uses metadata to return the count for a collection. For a view:  * There is no metadata.  * The document count is calculated by executing the aggregation pipeline in the view definition.  * There is no fast estimated document count. \nSHARDED CLUSTERS \nOn a sharded cluster, the resulting count will not correctly filter out orphaned documents. \nUNCLEAN SHUTDOWN \nThis section only applies to collections. After an unclean shutdown, the count may be incorrect. After an unclean shutdown of a mongod using the Wired Tiger storage engine, count statistics reported by db.collection.estimatedDocumentCount() may be inaccurate. The amount of drift depends on the number of insert, update, or delete operations performed between the last checkpoint and the unclean shutdown. Checkpoints usually occur every 60 seconds. However, mongod instances running with non-default --syncdelay settings may have more or less frequent checkpoints. Run validate on each collection on the mongod to restore statistics after an unclean shutdown. After an unclean shutdown:  * validate updates the count statistic in the collStats output with the latest value.  * Other statistics like the number of documents inserted or removed in the collStats output are estimates. \nCLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued db.collection.estimatedDocumentCount() disconnects before the operation completes, MongoDB marks db.collection.estimatedDocumentCount() for termination using killOp. \nCOUNT AND TRANSACTIONS \nWhen you use db.collection.estimatedDocumentCount() in a transaction, the resulting count will not filter out any uncommitted multi-document transactions. \nEXAMPLE \nThe following example uses db.collection.estimatedDocumentCount() to retrieve the count of all documents in the orders collection: db.orders.estimatedDocumentCount({})  \nTIP \nSEE ALSO:  * db.collection.countDocuments()  * count  * collStats pipeline stage with the count option ←  db.collection.ensureIndex()db.collection.explain() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getProfilingStatus/": " Docs Home → MongoDB Manual \nDB.GETPROFILINGSTATUS() \ndb.getProfilingStatus() \nReturns:The current profile level, slowOpThresholdMs setting, and slowOpSampleRate setting. Starting in MongoDB 4.4.2, you can set a filter to control which operations are logged by the profiler. When set, any configured filters are also returned by db.getProfilingStatus(), along with a note explaining filter behavior. You can set the profiling filter with either:      * the db.setProfilingLevel() shell method, or  * the operationProfiling.filter configuration file option. ←  db.getName()db.getReplicationInfo() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.insert/": " Docs Home → MongoDB Manual \nDB.COLLECTION.INSERT() \nOn this page    \n * Definition\n   \n * Behaviors\n * Examples\n * WriteResult\n * BulkWriteResult \nIMPORTANT \nDEPRECATED MONGOSH METHOD This method is deprecated in mongosh. For alternative methods, see Compatibility Changes with Legacy mongo Shell. \nDEFINITION \ndb.collection.insert() \nInserts a document or documents into a collection. The insert() method has the following syntax: db.collection.insert(   <document or array of documents>,   {     writeConcern: <document>,     ordered: <boolean>   }) \nParameter\nType\nDescription\ndocument\ndocument or array\nA document or array of documents to insert into the collection.\nwriteConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. See Write Concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. ordered\nboolean Optional. If true, perform an ordered insert of the documents in the array, and if an error occurs with one of documents, MongoDB will return without processing the remaining documents in the array. If false, perform an unordered insert, and if an error occurs with one of documents, continue processing the remaining documents in the array. Defaults to true. The insert() returns an object that contains the status of the operation. Returns:\n   \n * A WriteResult object for single inserts.\n * A BulkWriteResult object for bulk inserts. \nBEHAVIORS  WRITE CONCERN \nThe insert() method uses the insert command, which uses the default write concern. To specify a different write concern, include the write concern in the options parameter. \nCREATE COLLECTION \nIf the collection does not exist, then the insert() method will create the collection. \n_ID FIELD \nIf the document does not specify an _id field, then MongoDB will add the _id field and assign a unique ObjectId() for the document before inserting. Most drivers create an ObjectId and insert the _id field, but the mongod will create and populate the _id if the driver or application does not. If the document contains an _id field, the _id value must be unique within the collection to avoid duplicate key error. \nTRANSACTIONS \ndb.collection.insert() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. COLLECTION CREATION IN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, if you specify an insert on a non-existing collection in a transaction, the collection is implicitly created. In MongoDB 4.4 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES \nThe following examples insert documents into the products collection. If the collection does not exist, the insert() method creates the collection. \nINSERT A DOCUMENT WITHOUT SPECIFYING AN _ID FIELD \nIn the following example, the document passed to the insert() method does not contain the _id field: db.products.insert( { item: \"card\", qty: 15 } )  During the insert, mongod will create the _id field and assign it a unique ObjectId() value, as verified by the inserted document: { \"_id\" : ObjectId(\"5063114bd386d8fadbd6b004\"), \"item\" : \"card\", \"qty\" : 15 }  The ObjectId values are specific to the machine and time when the operation is run. As such, your values may differ from those in the example. \nINSERT A DOCUMENT SPECIFYING AN _ID FIELD\n\n db.products.insert( { _id: 10, item: \"box\", qty: 20 } )  The operation inserts the following document in the products collection: { \"_id\" : 10, \"item\" : \"box\", \"qty\" : 20 }  \nINSERT MULTIPLE DOCUMENTS \nThe following example performs a bulk insert of three documents by passing an array of documents to the insert() method. By default, MongoDB performs an ordered insert. With ordered inserts, if an error occurs during an insert of one of the documents, MongoDB returns on error without processing the remaining documents in the array. The documents in the array do not need to have the same fields. For instance, the first document in the array has an _id field and a type field. Because the second and third documents do not contain an _id field, mongod will create the _id field for the second and third documents during the insert: db.products.insert(   [     { _id: 11, item: \"pencil\", qty: 50, type: \"no.2\" },     { item: \"pen\", qty: 20 },     { item: \"eraser\", qty: 25 }   ])  The operation inserted the following three documents: { \"_id\" : 11, \"item\" : \"pencil\", \"qty\" : 50, \"type\" : \"no.2\" }{ \"_id\" : ObjectId(\"51e0373c6f35bd826f47e9a0\"), \"item\" : \"pen\", \"qty\" : 20 }{ \"_id\" : ObjectId(\"51e0373c6f35bd826f47e9a1\"), \"item\" : \"eraser\", \"qty\" : 25 }  \nPERFORM AN UNORDERED INSERT \nThe following example performs an unordered insert of three documents. With unordered inserts, if an error occurs during an insert of one of the documents, MongoDB continues to insert the remaining documents in the array. db.products.insert(   [     { _id: 20, item: \"lamp\", qty: 50, type: \"desk\" },     { _id: 21, item: \"lamp\", qty: 20, type: \"floor\" },     { _id: 22, item: \"bulk\", qty: 100 }   ],   { ordered: false })  \nOVERRIDE DEFAULT WRITE CONCERN \nThe following operation to a replica set specifies a write concern of w: 2 with a wtimeout of 5000 milliseconds. This operation either returns after the write propagates to both the primary and one secondary, or times out after 5 seconds. db.products.insert(    { item: \"envelopes\", qty : 100, type: \"Clasp\" },    { writeConcern: { w: 2, wtimeout: 5000 } })  \nWRITERESULT \nWhen passed a single document, insert() returns a WriteResult object. \nSUCCESSFUL RESULTS \nThe insert() returns a WriteResult() object that contains the status of the operation. Upon success, the WriteResult() object contains information on the number of documents inserted: WriteResult({ \"nInserted\" : 1 })  \nWRITE CONCERN ERRORS \nIf the insert() method encounters write concern errors, the results include the WriteResult.writeConcernError field: WriteResult({  \"nInserted\" : 1,  \"writeConcernError\"({     \"code\" : 64,     \"errmsg\" : \"waiting for replication timed out\",     \"errInfo\" : {       \"wtimeout\" : true,       \"writeConcern\" : {    // Added in MongoDB 4.4         \"w\" : \"majority\",         \"wtimeout\" : 100,         \"provenance\" : \"getLastErrorDefaults\"       }     }})  \nTIP \nSEE ALSO: WriteResult.writeConcernError \nERRORS UNRELATED TO WRITE CONCERN \nIf the insert() method encounters a non-write concern error, the results include the WriteResult.writeError field:\n\n  \nBULKWRITERESULT \nWhen passed an array of documents, insert() returns a BulkWriteResult() object. See BulkWriteResult() for details. ←  db.collection.hideIndex()db.collection.insertOne() → On this page  * Definition\n * Behaviors\n * Examples\n * WriteResult\n * BulkWriteResult Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/ClientEncryption.encrypt/": " Docs Home → MongoDB Manual \nCLIENTENCRYPTION.ENCRYPT() \nOn this page    \n * Syntax\n   \n * Behavior\n * Examples New in version 4.2. ClientEncryption.encrypt(keyId, value, algorithm or encOptions) \nClientEncryption.encrypt() encrypts the value using the specified keyId and the algorithm specified by algorithm or encOptions. encrypt() supports explicit (manual) encryption of field values. Returns:A binary data object with subtype 6. \nSYNTAX \nclientEncryption = db.getMongo().getClientEncryption()\nclientEncryption.encrypt(  keyId,  value,  algorithm or encOptions,) \nParameter\nType\nDescription\nkeyId\nUUID The data encryption key to use for encrypting the value. The UUID is a BSON binary data object with subtype 4 that identifies a specific data encryption key. If the data encryption key does not exist in the key vault configured for the database connection, encrypt() returns an error. See Key Vault Collections for more information on key vaults and data encryption keys. value\nSee Unsupported BSON Types.\nThe value to encrypt.\nalgorithm or encOptions\nstring or document\n     * To explicitly encrypt fields with Client-Side Field Level Encryption:\n   \n   Specify the algorithm as a string or encOptions as a document containing the field algorithm.\n   \n   The supported algorithms are:\n   \n   * AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic\n   \n   * AEAD_AES_256_CBC_HMAC_SHA_512-Random\n   \n   For examples, see Set the Client-Side Field Level Encryption Algorithm.\n   \n   For complete documentation on the supported encryption algorithms, see Fields and Encryption Types.  * To explicitly encrypt fields with Queryable Encryption:\n   \n   Specify the algorithm as a string or encOptions as a document containing the fields:\n   \n   * algorithm: The encryption algorithm to use for encrypting the value. The supported algorithms are:\n     \n     * Indexed\n     \n     * Unindexed\n   \n   * contentionFactor: Required when algorithm is set to Indexed. Related to the frequency of the values for this field.\n   \n   * queryType: The only query type currently supported is \"equality\". queryType must be set when algorithm is not Indexed.\n   \n   For examples, see Set the Queryable Encryption Algorithm.\n   \n   For details on the supported encryption algorithms, see Algorithm Choice. \nBEHAVIOR  CREATE AN ENCRYPTED DATABASE CONNECTION \nThe mongosh client-side field level and queryable encryption methods require a database connection configured for client-side encryption. If the current database connection was not initiated with client-side field level encryption enabled, either:  * Use the Mongo() constructor from the mongosh to establish a connection with the required client-side field level encryption options. The Mongo() method supports the following Key Management Service (KMS) providers for Customer Master Key (CMK) management:  * Amazon Web Services KMS  * Azure Key Vault  * Google Cloud Platform KMS  * Locally Managed Key or  * Use the mongosh command line options to establish a connection with the required options. The command line options only support the Amazon Web Services KMS provider for CMK management. \nUNSUPPORTED BSON TYPES \nYou cannot use encrypt() to encrypt values with the following BSON types:  * minKey  * maxKey  * null  * undefined If encrypting a field using AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic, encrypt() does not support the following BSON types:  * double  * decimal128  * bool  * object  * array  * javascriptWithScope (Deprecated) \nEXAMPLES  CLIENT-SIDE FIELD LEVEL ENCRYPTION \nThe following example uses a locally managed KMS for the client-side field level encryption configuration. To configure client-side field level encryption for a locally managed key:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key\n\n  Create the client-side field level encryption object using the generated local key string:  var autoEncryptionOpts = {   \"keyVaultNamespace\" : \"encryption.__dataKeys\",   \"kmsProviders\" : {     \"local\" : {       \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])     }   } }  Use the Mongo() constructor with the client-side field level encryption options configured to create a database connection. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",   autoEncryptionOpts)  Retrieve the ClientEncryption object and use the ClientEncryption.encrypt() method to encrypt a value using a specific data encryption key UUID and encryption algorithm: clientEncryption = encryptedClient.getClientEncryption();\nclientEncryption.encrypt(  UUID(\"64e2d87d-f168-493c-bbdf-a394535a2cb9\"),  \"123-45-6789\",  \"AEAD_AES_256_CBC_HMAC_SHA_512-Random\")  You can also specify the algorithm using a document with an algorithm field: clientEncryption = encryptedClient.getClientEncryption();\nclientEncryption.encrypt(  UUID(\"64e2d87d-f168-493c-bbdf-a394535a2cb9\"),  \"123-45-6789\",  { algorithm: \"AEAD_AES_256_CBC_HMAC_SHA_512-Random\" })  If sucessful, encrypt() returns the encrypted value: BinData(6,\"AmTi2H3xaEk8u9+jlFNaLLkC3Q/+kmwDbbWrq+h9nuv9W+u7A5a0UnpULBNZH+Q21fAztPpU09wpKPrju9dKfpN1Afpj1/ZhFcH6LYZOWSBBOAuUNjPLxMNSYOOuITuuYWo=\") For complete documentation on initiating MongoDB connections with client-side field level encryption enabled, see Mongo(). \nQUERYABLE ENCRYPTION \nThe following example uses a locally managed KMS for the queryable encryption configuration. Configuring queryable encryption for a locally managed key requires specifying a base64-encoded 96-byte string with no line breaks. The following operation generates a key that meets the stated requirements and loads it into mongosh: TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  Create the client-side field level encryption object using the generated local key string: var autoEncryptionOpts = {  \"keyVaultNamespace\" : \"encryption.__keyVault\",  \"kmsProviders\" : {    \"local\" : {      \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])    }  }}  Use the Mongo() constructor to create a database connection with the queryable encryption options. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",  autoEncryptionOpts)  Retrieve the ClientEncryption object and use the ClientEncryption.encrypt() method to encrypt a value using a specific data encryption key UUID and encryption algorithm: const eDB = \"encrypted\"const eKV = \"__keyVault\"\nconst clientEncryption = encryptedClient.getClientEncryption();const keyVaultClient = Mongo().getDB(eDB).getCollection(eKV)\nconst dek = keyVaultClient.findOne({ keyAltNames: \"dataKey1\" })\nclientEncryption.encrypt(  dek._id,  \"123-45-6789\",  \"Unindexed\")  You can also specify the algorithm using a document containing the fields:  * algorithm  * queryType  * contentionFactor\n\n  If sucessful, encrypt() returns the encrypted value: Binary(Buffer.from(\"05b100000005640020000000005ab3581a43e39a8e855b1ac87013e841735c09d19ae86535eea718dd56122ba50573002000000000703d2cba9832d90436c6c92eb232aa5b968cdcd7a3138570bc87ef0a9eb3a0e905630020000000009cb61df010b1bb54670a5ad979f25f4c48889059dfd8920782cf03dd27d1a50b05650020000000003f5acea703ea357d3eea4c6a5b19139a580089341424a247839fd4d5cf0d312a12636d00040000000000000000\", \"hex\"), 6) ←  getClientEncryption()ClientEncryption.decrypt() → On this page  * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.insert/": " Docs Home → MongoDB Manual \nBULK.INSERT() \nOn this page    \n * Description\n   \n * Behavior\n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.insert(<document>) \nAdds an insert operation to a bulk operations list. Bulk.insert() accepts the following parameter: Parameter\nType\nDescription\ndoc\ndocument\nDocument to insert. The size of the document must be less than or equal to the maximum BSON document size. \nBEHAVIOR  INSERT INACCURACIES \nEven if you encounter a server error during an insert, some documents may have been inserted. After a successful insert, the system returns BulkWriteResult.nInserted, the number of documents inserted into the collection. If the insert operation is interrupted by a replica set state change, the system may continue inserting documents. As a result, BulkWriteResult.nInserted may report fewer documents than actually inserted. \nEXAMPLE \nThe following initializes a Bulk() operations builder for the items collection and adds a series of insert operations to add multiple documents: var bulk = db.items.initializeUnorderedBulkOp();bulk.insert( { item: \"abc123\", defaultQty: 100, status: \"A\", points: 100 } );bulk.insert( { item: \"ijk123\", defaultQty: 200, status: \"A\", points: 200 } );bulk.insert( { item: \"mop123\", defaultQty: 0, status: \"P\", points: 0 } );bulk.execute();  \nTIP \nSEE ALSO:      * db.collection.initializeUnorderedBulkOp()  * db.collection.initializeOrderedBulkOp()  * Bulk.execute() ←  Bulk.getOperations()Bulk.toJSON() → On this page  * Description\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.removeKeyAltName/": " Docs Home → MongoDB Manual \nKEYVAULT.REMOVEKEYALTNAME() \nKeyVault.removeKeyAltName(UUID, keyAltName) \nRemoves the specified keyAltName from the data encryption key with the specified UUID. The data encryption key must exist in the key vault associated with the database connection. This method is an alias for removeKeyAlternateName(). \nTIP \nSEE ALSO: removeKeyAlternateName(). What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.hello/": " Docs Home → MongoDB Manual \nDB.HELLO() \ndb.hello() \nNew in version 5.0: (and 4.4.2, 4.2.10, 4.0.21, and 3.6.21) Returns a document that describes the role of the mongod instance. If the mongod is a member of a replica set, then the isWritablePrimary and secondary fields report if the instance is the primary or if it is a secondary member of the replica set. \nTIP \nSEE: hello for the complete documentation of the output of db.hello(). ←  db.getSiblingDB()db.help() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.addShard/": " Docs Home → MongoDB Manual \nSH.ADDSHARD() \nOn this page    \n * Definition\n   \n * Considerations\n * Example \nDEFINITION \nsh.addShard(<url>) \nAdds a shard replica set to a sharded cluster. This method must be run on a mongos instance. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the addShard command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The sh.addShard() method has the following parameter: Parameter\nType\nDescription\nhost\nstring The replica set name, hostname, and port of at least one member of the shard's replica set. Any additional replica set member hostnames must be comma separated. For example: <replica_set>/<hostname><:port>,<hostname><:port>, ...  The sh.addShard() method has the following prototype form: sh.addShard(\"<replica_set>/<hostname><:port>\")  \nWARNING Do not use localhost for the hostname unless your config server is also running on localhost. mongos installed from official .deb and .rpm packages have the bind_ip configuration set to 127.0.0.1 by default. mongos uses \"majority\" for the addShard command and its helper sh.addShard(). \nCONSIDERATIONS  BALANCING \nWhen you add a shard to a sharded cluster, you affect the balance of chunks among the shards of a cluster for all existing sharded collections. The balancer will begin migrating chunks so that the cluster will achieve balance. See Balancer Internals for more information. Chunk migrations can have an impact on disk space, as the source shard automatically archives the migrated documents by default. For details, see moveChunk directory. \nHIDDEN MEMBERS  IMPORTANT You cannot include a hidden member in the seed list provided to sh.addShard(). \nEXAMPLE \nTo add a shard, specify the name of the replica set and the hostname of at least one member of the replica set, as a seed. If you specify additional hostnames, all must be members of the same replica set. The following example adds a replica set named repl0 and specifies one member of the replica set: sh.addShard(\"repl0/mongodb3.example.net:27327\") \n←  sh.abortReshardCollection()sh.addShardTag() → On this page  * Definition\n * Considerations\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.getOperations/": " Docs Home → MongoDB Manual \nBULK.GETOPERATIONS() \nOn this page    \n * Example\n   \n * Returned Fields Bulk.getOperations() \nReturns an array of write operations executed through Bulk.execute(). The returned write operations are in groups as determined by MongoDB for execution. For information on how MongoDB groups the list of bulk write operations, see Bulk.execute() Behavior. Only use Bulk.getOperations() after a Bulk.execute(). Calling Bulk.getOperations() before you call Bulk.execute() will result in an incomplete list. \nEXAMPLE \nThe following initializes a Bulk() operations builder on the items collection, adds a series of write operations, executes the operations, and then calls getOperations() on the bulk builder object: var bulk = db.items.initializeUnorderedBulkOp();\nfor (var i = 1; i <= 1500; i++) {    bulk.insert( { x: i } );}\nbulk.execute();bulk.getOperations();  The getOperations() method returns an array with the operations executed. The output shows that MongoDB divided the operations into 2 groups, one with 1000 operations and one with 500. For information on how MongoDB groups the list of bulk write operations, see Bulk.execute() Behavior Although the method returns all 1500 operations in the returned array, this page omits some of the results for brevity. [   {      \"originalZeroIndex\" : 0,      \"batchType\" : 1,      \"operations\" : [         { \"_id\" : ObjectId(\"53a8959f1990ca24d01c6165\"), \"x\" : 1 },\n         ... // Content omitted for brevity\n         { \"_id\" : ObjectId(\"53a8959f1990ca24d01c654c\"), \"x\" : 1000 }      ]   },   {      \"originalZeroIndex\" : 1000,      \"batchType\" : 1,      \"operations\" : [         { \"_id\" : ObjectId(\"53a8959f1990ca24d01c654d\"), \"x\" : 1001 },\n         ... // Content omitted for brevity\n         { \"_id\" : ObjectId(\"53a8959f1990ca24d01c6740\"), \"x\" : 1500 }      ]   }]  \nRETURNED FIELDS \nThe array contains documents with the following fields: originalZeroIndex \nSpecifies the order in which the operation was added to the bulk operations builder, based on a zero index; e.g. first operation added to the bulk operations builder will have originalZeroIndex value of 0. batchType \nSpecifies the write operations type. batchType\nOperation\n1\nInsert\n2\nUpdate\n3\nRemove operations \nArray of documents that contain the details of the operation. \nTIP \nSEE ALSO:      * Bulk()  * Bulk.execute() ←  Bulk.find.upsert()Bulk.insert() → On this page  * Example\n * Returned Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.removeKeyAlternateName/": " Docs Home → MongoDB Manual \nKEYVAULT.REMOVEKEYALTERNATENAME() \nOn this page    \n * Behavior\n   \n * Example KeyVault.removeKeyAlternateName(UUID, keyAltName) \nRemoves the specified keyAltName from the data encryption key with the specified UUID. The data encryption key must exist in the key vault associated to the database connection. removeKeyAlternateName() has the following syntax: keyVault = db.getMongo().getKeyVault()\nkeyVault.removeKeyAlternateName(  UUID(\"<UUID string>\"),  \"keyAltName\")  Returns:The data encryption key prior to updating the keyAltName. Returns null if no data encryption key has the specified UUID(). \nBEHAVIOR  REQUIRES CONFIGURING CLIENT-SIDE FIELD LEVEL ENCRYPTION ON DATABASE CONNECTION \nThe mongosh client-side field level encryption methods require a database connection with client-side field level encryption enabled. If the current database connection was not initiated with client-side field level encryption enabled, either:      * Use the Mongo() constructor from the mongosh to establish a connection with the required client-side field level encryption options. The Mongo() method supports the following Key Management Service (KMS) providers for Customer Master Key (CMK) management:\n   \n   * Amazon Web Services KMS\n   \n   * Azure Key Vault\n   \n   * Google Cloud Platform KMS\n   \n   * Locally Managed Key\n   \n   or  * Use the mongosh command line options to establish a connection with the required options. The command line options only support the Amazon Web Services KMS provider for CMK management. \nEXAMPLE \nThe following example is intended for rapid evaluation of client-side field level encryption. For specific examples of using KeyVault.removeKeyAlternateName() with each supported KMS provider, see Encryption Key Management. To configure client-side field level encryption for a locally managed key:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key export TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  Create the client-side field level encryption object using the generated local key string:  var autoEncryptionOpts = {   \"keyVaultNamespace\" : \"encryption.__dataKeys\",   \"kmsProviders\" : {     \"local\" : {       \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])     }   } }  Use the Mongo() constructor with the client-side field level encryption options configured to create a database connection. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",   autoEncryptionOpts)  Retrieve the keyVault object and use the KeyVault.removeKeyAlternateName() method to remove the specified key alternate name from the data encryption key with matching UUID: keyVault = encryptedClient.getKeyVault()keyVault.removeKeyAlternateName(UUID(\"b4b41b33-5c97-412e-a02b-743498346079\"),\"Other-Data-Encryption-Key\")  If successful, removeKeyAlternateName() returns the data encryption key prior to updating the keyAltName.\n\n \n←  KeyVault.addKeyAlternateName()KeyVault.getKeyByAltName() → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.count/": " Docs Home → MongoDB Manual \nCURSOR.COUNT() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ncursor.count()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. \nNOTE MongoDB drivers compatible with the 4.0 features deprecate their respective cursor and collection count() APIs in favor of new APIs that corresponds to countDocuments() and estimatedDocumentCount(). For the specific API names for a given driver, see the driver API documentation. Counts the number of documents referenced by a cursor. Append the count() method to a find() query to return the number of matching documents. The operation does not perform the query but instead counts the results that would be returned by the query. The count() method has the following prototype form: db.collection.find(<query>).count()  The count() method has the following parameter: Parameter\nType\nDescription\napplySkipLimit\nboolean\nOptional. Specifies whether to consider the effects of the cursor.skip() and cursor.limit() methods in the count. By default, the count() method ignores the effects of the cursor.skip() and cursor.limit(). Set applySkipLimit to true to consider the effect of these methods. MongoDB also provides an equivalent db.collection.count() as an alternative to the db.collection.find(<query>).count() construct. MongoDB supports the use of hint() with count(). See Specify the Index to Use for an example. \nTIP \nSEE ALSO: cursor.size() \nBEHAVIOR  INACCURATE COUNTS WITHOUT QUERY PREDICATE \nWhen you call count() on a find() operation which does not specify a query predicate, the count() method can return inaccurate document counts. These count() methods return results based on the collection's metadata, which may result in an approximate count. In particular,      * On a sharded cluster, the resulting count will not correctly filter out orphaned documents.  * After an unclean shutdown or file copy based initial sync, the count may be incorrect. For counts based on collection metadata, see also collStats pipeline stage with the count option. \nCOUNT AND TRANSACTIONS \nYou cannot use count and shell helpers count() and db.collection.count() in transactions. For details, see Transactions and Count Operations. \nSHARDED CLUSTERS \nOn a sharded cluster, count() without a query predicate in the find can result in an inaccurate count if orphaned documents exist or if a chunk migration is in progress. To avoid these situations, on a sharded cluster, use the db.collection.aggregate() method: You can use the $count stage to count the documents. For example, the following operation counts the documents in a collection: db.collection.aggregate( [   { $count: \"myCount\" }])  The $count stage is equivalent to the following $group + $project sequence: db.collection.aggregate( [   { $group: { _id: null, count: { $sum: 1 } } }   { $project: { _id: 0 } }] )  \nTIP \nSEE ALSO: $collStats to return an approximate count based on the collection's metadata. \nINDEX USE \nConsider a collection with the following index: { a: 1, b: 1 }  When performing a count, MongoDB can return the count using only the index if:  * the query can use an index,  * the query only contains conditions on the keys of the index, and  * the query predicates access a single contiguous range of index keys. For example, the following operations can return the count using only the index: db.collection.find( { a: 5, b: 5 } ).count()db.collection.find( { a: { $gt: 5 } } ).count()db.collection.find( { a: 5, b: { $gt: 10 } } ).count()  If, however, the query can use an index but the query predicates do not access a single contiguous range of index keys or the query also contains conditions on fields outside the index, then in addition to using the index, MongoDB must also read the documents to return the count.\n\n  In such cases, during the initial read of the documents, MongoDB pages the documents into memory such that subsequent calls of the same count operation will have better performance. \nEXAMPLES \nThe following are examples of the count() method. \nCOUNT ALL DOCUMENTS \nThe following operation counts the number of all documents in the orders collection: db.orders.find().count()  \nCOUNT DOCUMENTS THAT MATCH A QUERY \nThe following operation counts the number of the documents in the orders collection with the field ord_dt greater than new Date('01/01/2012'): db.orders.find( { ord_dt: { $gt: new Date('01/01/2012') } } ).count()  \nLIMIT DOCUMENTS IN COUNT \nThe following operation counts the number of the documents in the orders collection with the field ord_dt greater than new Date('01/01/2012') taking into account the effect of the limit(5): db.orders.find( { ord_dt: { $gt: new Date('01/01/2012') } } ).limit(5).count(true)  \nSPECIFY THE INDEX TO USE \nThe following operation uses the index named \"status_1\", which has the index key specification of { status: 1 }, to return a count of the documents in the orders collection with the field ord_dt greater than new Date('01/01/2012') and the status field is equal to \"D\": db.orders.find(   { ord_dt: { $gt: new Date('01/01/2012') }, status: \"D\" }).hint( \"status_1\" ).count() \n←  cursor.comment()cursor.explain() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.killOp/": " Docs Home → MongoDB Manual \nDB.KILLOP() \nOn this page    \n * Description\n   \n * Sharded Cluster\n * Access Control \nDESCRIPTION \ndb.killOp(opid) \nTerminates an operation as specified by the operation ID. To find operations and their corresponding IDs, see $currentOp or db.currentOp(). The db.killOp() method has the following parameter: Parameter\nType\nDescription\nop\nnumber\nAn operation ID. \nWARNING Terminate running operations with extreme caution. Only use db.killOp() to terminate operations initiated by clients and do not terminate internal database operations. \nSHARDED CLUSTER  KILL READ OPERATIONS \nThe db.killOp() method can be run on a mongos and can kill queries (read operations) that are running on more than one shard in a cluster. For example, to kill a query operation on a sharded cluster:   KILL WRITE OPERATIONS \nWithin a Session MongoDB drivers associate all operations with a server session, with the exception of unacknowledged writes. If the write operation is associated with a session, you can use the killSessions command on the mongos to kill the write operation across shards.       1. Run the aggregation pipeline $currentOp on the mongos to find the lsid (logical session id).\n    \n    use admindb.aggregate( [   { $currentOp : { allUsers: true, localOps: true } },   { $match : <filter condition> } // Optional.  Specify the condition to find the op.                                   // e.g. { \"op\" : \"update\", \"ns\": \"mydb.someCollection\" }] )\n    \n    \n    \n    1. Using the returned lsid information, issue the killSessions command on the mongos to kill the operation on the shards.\n       \n       db.adminCommand( { killSessions: [   { \"id\" : UUID(\"80e48c5a-f7fb-4541-8ac0-9e3a1ed224a4\"), \"uid\" : BinData(0,\"47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=\") }] } )\n       \n        Without a Session If the write operation is not associated with a session, you must find and kill the operation on all the shards associated with the write.  1. From a mongos, run the aggregation pipeline $currentOp to find the opid(s) of the query operation on the shards:\n    \n    use admindb.aggregate( [   { $currentOp : { allUsers: true } },   { $match : <filter condition> } // Optional.  Specify the condition to find the op.] )\n    \n    \n    \n    When run on a mongos, $currentOp returns the opids in the format of \"<shardName>:<opid on that shard>\"; e.g.\n    \n    {   \"shard\" : \"shardB\",   ..   \"opid\" : \"shardB:79214\",   ...},{   \"shard\" : \"shardA\",   ..   \"opid\" : \"shardA:100913\",   ...},\n    \n      2. Using the opid information, issue db.killOp() on the mongos to kill the operation on the shards.\n    \n    db.killOp(\"shardB:79014\");db.killOp(\"shardA:100813\");\n    \n     \nACCESS CONTROL \nOn systems running with authorization, to kill operations not owned by the user, the user must have access that includes the killop privilege action. On mongod instances, users can kill their own operations even without the killop privilege action. \nTIP \nSEE ALSO: $currentOp\n\n On this page  * Description\n * Sharded Cluster\n * Access Control Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.setLogLevel/": " Docs Home → MongoDB Manual \nDB.SETLOGLEVEL() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.setLogLevel() \nSets a single verbosity level for log messages. db.setLogLevel() has the following form: db.setLogLevel(<level>, <component>)  db.setLogLevel() takes the following parameters: Parameter\nType\nDescription\nlevel\nint The log verbosity level. The verbosity level can range from 0 to 5:      * 0 is the MongoDB's default log verbosity level, to include Informational messages.  * 1 to 5 increases the verbosity level to include Debug messages. To inherit the verbosity level of the component's parent, you can also specify -1. component\nstring Optional. The name of the component for which to specify the log verbosity level. The component name corresponds to the <name> from the corresponding systemLog.component.<name>.verbosity setting:  * accessControl  * command  * control  * ftdc  * geo  * index  * network  * query  * replication  * replication.election  * replication.heartbeats  * replication.initialSync  * replication.rollback  * recovery  * sharding  * storage  * storage.journal  * transaction  * write Omit to specify the default verbosity level for all components. \nBEHAVIOR \ndb.setLogLevel() sets a single verbosity level. To set multiple verbosity levels in a single operation, use either the setParameter command to set the logComponentVerbosity parameter. You can also specify the verbosity settings in the configuration file. See Configure Log Verbosity Levels for examples. \nNOTE Starting in version 4.2, MongoDB includes the Debug verbosity level (1-5) in the log messages. For example, if the verbosity level is 2, MongoDB logs D2. In previous versions, MongoDB log messages only specified D for Debug level. \nEXAMPLES  SET DEFAULT VERBOSITY LEVEL \nOmit the <component> parameter to set the default verbosity for all components; i.e. the systemLog.verbosity setting. The operation sets the default verbosity to 1: db.setLogLevel(1)  \nSET VERBOSITY LEVEL FOR A COMPONENT \nSpecify the <component> parameter to set the verbosity for the component. The following operation updates the systemLog.component.storage.journal.verbosity to 2: db.setLogLevel(2, \"storage.journal\" ) \n←  db.serverStatus()db.setProfilingLevel() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.enableBalancing/": " Docs Home → MongoDB Manual \nSH.ENABLEBALANCING() \nOn this page    \n * Description \nDESCRIPTION \nsh.enableBalancing(namespace) \nEnables the balancer for the specified namespace of the sharded collection. \nIMPORTANT You can only run sh.enableBalancing() on a mongos instance. sh.enableBalancing() errors if run on mongod instance. The sh.enableBalancing() method has the following parameter: Parameter\nType\nDescription\nnamespace\nstring\nThe namespace of the collection. \nIMPORTANT  For more information on the balancing process, see Manage Sharded Cluster Balancer and Sharded Cluster Balancer. ←  sh.disableBalancing()sh.disableAutoSplit → On this page  * Description Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Mongo.watch/": " Docs Home → MongoDB Manual \nMONGO.WATCH() \nOn this page    \n * Definition\n   \n * Availability\n * Deployment\n * Storage Engine\n * Read Concern majority Support\n * Behavior\n * Resumability\n * Full Document Lookup of Update Operations\n * Availability\n * Access Control\n * Cursor Iteration\n * Example \nDEFINITION \nMongo.watch( pipeline, options ) \nFor replica sets and sharded clusters only Opens a change stream cursor for a replica set or a sharded cluster to report on all its non-system collections across its databases, with the exception of the admin, local, and the config databases. Parameter\nType\nDescription\npipeline\narray Optional. An Aggregation Pipeline consisting of one or more of the following aggregation stages:      * $addFields  * $match  * $project  * $replaceRoot  * $replaceWith (Available starting in MongoDB 4.2)  * $redact  * $set (Available starting in MongoDB 4.2)  * $unset (Available starting in MongoDB 4.2) Specify a pipeline to filter/modify the change events output. Starting in MongoDB 4.2, change streams will throw an exception if the change stream aggregation pipeline modifies an event's _id field. options\ndocument\nOptional. Additional options that modify the behavior of Mongo.watch(). The options document can contain the following fields and values: Field\nType\nDescription\nresumeAfter\ndocument Optional. Directs Mongo.watch() to attempt resuming notifications starting after the operation specified in the resume token. Each change stream event document includes a resume token as the _id field. Pass the entire _id field of the change event document that represents the operation you want to resume after. resumeAfter is mutually exclusive with startAfter and startAtOperationTime. startAfter\ndocument Optional. Directs Mongo.watch() to attempt starting a new change stream after the operation specified in the resume token. Allows notifications to resume after an invalidate event. Each change stream event document includes a resume token as the _id field. Pass the entire _id field of the change event document that represents the operation you want to resume after. startAfter is mutually exclusive with resumeAfter and startAtOperationTime. New in version 4.2. fullDocument\nstring Optional. By default, Mongo.watch() returns the delta of those fields modified by an update operation, instead of the entire updated document. Set fullDocument to \"updateLookup\" to direct Mongo.watch() to look up the most current majority-committed version of the updated document. Mongo.watch() returns a fullDocument field with the document lookup in addition to the updateDescription delta. batchSize\nint Optional. Specifies the maximum number of change events to return in each batch of the response from the MongoDB cluster. Has the same functionality as cursor.batchSize(). maxAwaitTimeMS\nint Optional. The maximum amount of time in milliseconds the server waits for new data changes to report to the change stream cursor before returning an empty batch. Defaults to 1000 milliseconds. collation\ndocument Optional. Pass a collation document to specify a collation for the change stream cursor. If omitted, defaults to simple binary comparison. startAtOperationTime\nTimestamp Optional. The starting point for the change stream. If the specified starting point is in the past, it must be in the time range of the oplog. To check the time range of the oplog, see rs.printReplicationInfo(). startAtOperationTime is mutually exclusive with resumeAfter and startAfter. Returns:A cursor over the change event documents. See Change Events for examples of change event documents. \nTIP \nSEE ALSO: db.collection.watch() and db.watch() \nAVAILABILITY  DEPLOYMENT \nMongo.watch() is available for replica sets and sharded clusters:  * For a replica set, you can issue Mongo.watch() on any data-bearing member.  * For a sharded cluster, you must issue Mongo.watch() on a mongos instance. \nSTORAGE ENGINE \nYou can only use Mongo.watch() with the Wired Tiger storage engine. \nREAD CONCERN MAJORITY SUPPORT \nStarting in MongoDB 4.2, change streams are available regardless of the \"majority\" read concern support; that is, read concern majority support can be either enabled (default) or disabled to use change streams. In MongoDB 4.0 and earlier, change streams are available only if \"majority\" read concern support is enabled (default). \nBEHAVIOR \n * Mongo.watch() only notifies on data changes that have persisted to a majority of data-bearing members.\n\n \nRESUMABILITY \nUnlike the MongoDB Drivers, mongosh does not automatically attempt to resume a change stream cursor after an error. The MongoDB drivers make one attempt to automatically resume a change stream cursor after certain errors. Mongo.watch() uses information stored in the oplog to produce the change event description and generate a resume token associated to that operation. If the operation identified by the resume token passed to the resumeAfter or startAfter option has already dropped off the oplog, Mongo.watch() cannot resume the change stream. See Resume a Change Stream for more information on resuming a change stream. \nNOTE  * You cannot use resumeAfter to resume a change stream after an invalidate event (for example, a collection drop or rename) closes the stream. Starting in MongoDB 4.2, you can use startAfter to start a new change stream after an invalidate event.  * If the deployment is a sharded cluster, a shard removal may cause an open change stream cursor to close, and the closed change stream cursor may not be fully resumable. \nNOTE \nRESUME TOKEN The resume token _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (fcv) at the time of the change stream's opening/resumption (i.e. a change in fcv value does not affect the resume tokens for already opened change streams): MongoDB Version\nFeature Compatibility Version\nResume Token _data Type\nMongoDB 4.2 and later\n\"4.2\" or \"4.0\"\nHex-encoded string (v1)\nMongoDB 4.0.7 and later\n\"4.0\" or \"3.6\"\nHex-encoded string (v1)\nMongoDB 4.0.6 and earlier\n\"4.0\"\nHex-encoded string (v0)\nMongoDB 4.0.6 and earlier\n\"3.6\"\nBinData\nMongoDB 3.6\n\"3.6\"\nBinData HEX ENCODED TOKENS \nWith hex-encoded string resume tokens, you can compare and sort the resume tokens. Regardless of the fcv value, a 4.0 deployment can use either BinData resume tokens or hex string resume tokens to resume a change stream. As such, a 4.0 deployment can use a resume token from a change stream opened on a collection from a 3.6 deployment. New resume token formats introduced in a MongoDB version cannot be consumed by earlier MongoDB versions. DECODE RESUME TOKENS \nMongoDB provides a \"snippet\", an extension to mongosh, that decodes hex-encoded resume tokens. You can install and run the resumetoken snippet from mongosh: snippet install resumetokendecodeResumeToken('<RESUME TOKEN>')  You can also run resumetoken from the command line (without using mongosh) if npm is installed on your system: npx mongodb-resumetoken-decoder <RESUME TOKEN>  See the following for more details on:  * resumetoken  * using snippets in mongosh. \nFULL DOCUMENT LOOKUP OF UPDATE OPERATIONS \nBy default, the change stream cursor returns specific field changes/deltas for update operations. You can also configure the change stream to look up and return the current majority-committed version of the changed document. Depending on other write operations that may have occurred between the update and the lookup, the returned document may differ significantly from the document at the time of the update. Depending on the number of changes applied during the update operation and the size of the full document, there is a risk that the size of the change event document for an update operation is greater than the 16MB BSON document limit. If this occurs, the server closes the change stream cursor and returns an error. \nAVAILABILITY \nStarting in MongoDB 4.2, change streams are available regardless of the \"majority\" read concern support; that is, read concern majority support can be either enabled (default) or disabled to use change streams. In MongoDB 4.0 and earlier, change streams are available only if \"majority\" read concern support is enabled (default). \nACCESS CONTROL \nWhen running with access control, the user must have the find and changeStream privilege actions on all non-systems collections across all databases.. That is, a user must have a role that grants the following privilege: { resource: { db: \"\", collection: \"\" }, actions: [ \"find\", \"changeStream\" ] }  The built-in read role provides the appropriate privileges.\n\n \nMongoDB provides multiple ways to iterate on a cursor. The cursor.hasNext() method blocks and waits for the next event. To monitor the watchCursor cursor and iterate over the events, use hasNext() like this: while (!watchCursor.isClosed()) {   if (watchCursor.hasNext()) {     firstChange = watchCursor.next();     break;   }}  The cursor.tryNext() method is non-blocking. To monitor the watchCursor cursor and iterate over the events, use tryNext() like this: while (!watchCursor.isClosed()) {  let next = watchCursor.tryNext()  while (next !== null) {    printjson(next);    next = watchCursor.tryNext()  }}  \nEXAMPLE \nThe following operation in mongosh opens a change stream cursor on a replica set. The returned cursor reports on data changes to all the non-system collections across all databases except for the admin, local, and the config databases. watchCursor = db.getMongo().watch()  Iterate the cursor to check for new events. Use the cursor.isClosed() method with the cursor.tryNext() method to ensure the loop only exits if the change stream cursor is closed and there are no objects remaining in the latest batch: while (!watchCursor.isClosed()) {  let next = watchCursor.tryNext()  while (next !== null) {    printjson(next);    next = watchCursor.tryNext()  }}  For complete documentation on change stream output, see Change Events. ←  Mongo.setWriteConcern()Session → On this page  * Definition\n * Availability\n * Deployment\n * Storage Engine\n * Read Concern majority Support\n * Behavior\n * Resumability\n * Full Document Lookup of Update Operations\n * Availability\n * Access Control\n * Cursor Iteration\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.serverCmdLineOpts/": " Docs Home → MongoDB Manual \nDB.SERVERCMDLINEOPTS() \ndb.serverCmdLineOpts() \nWraps the getCmdLineOpts database command. Returns a document that reports on the arguments and configuration options used to start the mongod or mongos instance. See Configuration File Options, mongod, and mongos for additional information on available MongoDB runtime options. ←  db.serverBuildInfo()db.serverStatus() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-database/": " Docs Home → MongoDB Manual \nDATABASE METHODS  NOTE For details on a specific method, including syntax and examples, click on the link to the method's reference page. Name\nDescription\ndb.adminCommand()\nRuns a command against the admin database.\ndb.aggregate()\nRuns admin/diagnostic pipeline which does not require an underlying collection.\ndb.commandHelp()\nReturns help information for a database command.\ndb.createCollection()\nCreates a new collection or a view. Commonly used to create a capped collection.\ndb.createView()\nCreates a view.\ndb.currentOp()\nReports the current in-progress operations.\ndb.dropDatabase()\nRemoves the current database.\ndb.fsyncLock()\nFlushes writes to disk and locks the database to prevent write operations and assist backup operations. Wraps fsync.\ndb.fsyncUnlock()\nAllows writes to continue on a database locked with db.fsyncLock().\ndb.getCollection()\nReturns a collection or view object. Used to access collections with names that are not valid in mongosh.\ndb.getCollectionInfos()\nReturns collection information for all collections and views in the current database.\ndb.getCollectionNames()\nLists all collections and views in the current database.\ndb.getLogComponents()\nReturns the log message verbosity levels.\ndb.getMongo()\nReturns the Mongo() connection object for the current connection.\ndb.getName()\nReturns the name of the current database.\ndb.getProfilingStatus()\nReturns a document that reflects the current profiling level and the profiling threshold.\ndb.getReplicationInfo()\nReturns a document with replication statistics.\ndb.getSiblingDB()\nProvides access to the specified database.\ndb.hello()\nReturns a document that reports the state of the replica set.\ndb.help()\nDisplays descriptions of common db object methods.\ndb.hostInfo()\nReturns a document with information about the system MongoDB runs on. Wraps hostInfo\ndb.killOp()\nTerminates a specified operation.\ndb.listCommands()\nDisplays a list of common database commands.\ndb.logout()\nDeprecated. Ends an authenticated session.\ndb.printCollectionStats()\nPrints statistics from every collection. Wraps db.collection.stats().\ndb.printReplicationInfo()\nPrints a formatted report of the replica set status from the perspective of the primary.\ndb.printSecondaryReplicationInfo()\nPrints a formatted report of the replica set status from the perspective of the secondaries.\ndb.printShardingStatus()\nPrints a report of the sharding configuration and the chunk ranges.\ndb.printSlaveReplicationInfo() Deprecated since version 4.4.1: Use db.printSecondaryReplicationInfo() instead. db.resetError()\nRemoved in MongoDB 5.0. Resets the last error status.\ndb.rotateCertificates()\nPerforms online TLS certificate rotation. Wraps rotateCertificates.\ndb.runCommand()\nRuns a database command.\ndb.serverBuildInfo()\nReturns a document that displays the compilation parameters for the mongod instance. Wraps buildInfo.\ndb.serverCmdLineOpts()\nReturns a document with information about the runtime used to start the MongoDB instance. Wraps getCmdLineOpts.\ndb.serverStatus()\nReturns a document that provides an overview of the state of the database process.\ndb.setLogLevel()\nSets a single log message verbosity level.\ndb.setProfilingLevel()\nModifies the current level of database profiling.\ndb.shutdownServer()\nShuts down the current mongod or mongos process cleanly and safely.\ndb.stats()\nReturns a document that reports on the state of the current database.\ndb.version()\nReturns the version of the mongod instance.\ndb.watch()\nOpens a change stream cursor for a database to report on all its non-system collections. Cannot be opened on the admin, local or config databases. ←  cursor.next()db.adminCommand() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/ObjectId/": " Docs Home → MongoDB Manual \nOBJECTID \nOn this page    \n * Description\n   \n * Methods and Attributes\n * Examples \nDESCRIPTION \nObjectId(<value>) \nReturns a new ObjectId. The 12-byte ObjectId consists of:      * A 4-byte timestamp, representing the ObjectId's creation, measured in seconds since the Unix epoch.  * A 5-byte random value generated once per process. This random value is unique to the machine and process.  * A 3-byte incrementing counter, initialized to a random value. While the BSON format itself is little-endian, the timestamp and counter values are big-endian, the most significant bytes appear first in the byte sequence. If an integer value is used to create an ObjectId, the integer replaces the timestamp. ObjectId() can accept one of the following inputs: Input Type\nDescription\nhexadecimal\nOptional. A 24 character hexadecimal string value for the new ObjectId.\ninteger\nOptional. The integer value, in seconds, is added to the Unix epoch to create the new timestamp. \nMETHODS AND ATTRIBUTES \nObjectId() has the following attribute and methods: Attribute/Method\nDescription\nstr\nReturns the hexadecimal string representation of the object.\nObjectId.getTimestamp()\nReturns the timestamp portion of the object as a Date.\nObjectId.toString()\nReturns the JavaScript representation in the form of a string literal \"ObjectId(...)\".\nObjectId.valueOf()\nReturns the representation of the object as a hexadecimal string. The returned string is the str attribute. \nEXAMPLES  GENERATE A NEW OBJECTID \nTo generate a new ObjectId, use ObjectId() with no argument: x = ObjectId()  In this example, the value of x is: ObjectId(\"507f1f77bcf86cd799439011\")  \nSPECIFY A HEXADECIMAL STRING \nTo generate a new ObjectId using ObjectId() with a unique hexadecimal string: y = ObjectId(\"507f191e810c19729de860ea\")  In this example, the value of y would be: ObjectId(\"507f191e810c19729de860ea\")  \nACCESS THE HEXADECIMAL STRING \nAccess the str attribute of an ObjectId() object, as follows: ObjectId(\"507f191e810c19729de860ea\").str  This operation will return the following hexadecimal string: 507f191e810c19729de860ea  \nSPECIFY AN INTEGER STRING \nGenerate a new ObjectId using an integer. newObjectId = ObjectId(32)  The ObjectId resembles: ObjectId(\"00000020f51bb4362eee2a4d\")  The first four bytes of the ObjectId are the number of seconds since the Unix epoch. In this example 32 seconds, represented in hexadecimal as 00000020, are added. A five byte random element and a three byte counter make up the rest of the ObjectId. \nTIP \nSEE ALSO: ObjectId BSON Type ←  Date()ObjectId.getTimestamp() → On this page  * Description\n * Methods and Attributes\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.updateUser/": " Docs Home → MongoDB Manual \nDB.UPDATEUSER() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.updateUser( username, update, writeConcern ) \nUpdates the user's profile on the database on which you run the method. An update to a field completely replaces the previous field's values. This includes updates to the user's roles array. \nWARNING When you update the roles array, you completely replace the previous array's values. To add or remove roles without replacing all the user's existing roles, use the db.grantRolesToUser() or db.revokeRolesFromUser() methods. The db.updateUser() method uses the following syntax: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. db.updateUser(   \"<username>\",   {     customData : { <any information> },     roles : [       { role: \"<role>\", db: \"<database>\" } | \"<role>\",       ...     ],     pwd: passwordPrompt(),      // Or  \"<cleartext password>\"     authenticationRestrictions: [        {          clientSource: [\"<IP>\" | \"<CIDR range>\", ...],          serverAddress: [\"<IP>\", | \"<CIDR range>\", ...]        },        ...     ],     mechanisms: [ \"<SCRAM-SHA-1|SCRAM-SHA-256>\", ... ],     passwordDigestor: \"<server|client>\"   },   writeConcern: { <write concern> })  The db.updateUser() method has the following arguments. Parameter\nType\nDescription\nusername\nstring\nThe name of the user to update.\nupdate\ndocument\nA document containing the replacement data for the user. This data completely replaces the corresponding data for the user.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. The update document specifies the fields to update and their new values. All fields in the update document are optional, but must include at least one field. The update document has the following fields: Field\nType\nDescription\ncustomData\ndocument\nOptional. Any arbitrary information.\nroles\narray\nOptional. The roles granted to the user. An update to the roles array overrides the previous array's values.\npwd\nstring Optional. The user's password. The value can be either:      * the user's password in cleartext string, or  * passwordPrompt() to prompt for the user's password. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. authenticationRestrictions\narray\nOptional. The authentication restrictions the server enforces upon the user. Specifies a list of IP addresses and CIDR ranges from which the user is allowed to connect to the server or from which the server can accept users.\nmechanisms\narray Optional. The specific SCRAM mechanism or mechanisms for the user credentials. If authenticationMechanisms is specified, you can only specify a subset of the authenticationMechanisms. If updating the mechanisms field without the password, you can only specify a subset of the user's current mechanisms, and only the existing user credentials for the specified mechanism or mechanisms are retained. If updating the password along with the mechanisms, new set of credentials are stored for the user. Valid values are:  * \"SCRAM-SHA-1\"\n   \n   * Uses the SHA-1 hashing function.\n\n passwordDigestor\nstring Optional. Indicates whether the server or the client digests the password. Available values are:  * \"server\" (Default)The server receives undigested password from the client and digests the password.\n * \"client\" (Not compatible with SCRAM-SHA-256)The client digests the password and passes the digested password to the server. \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where db.updateUser() runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field: Field Name\nValue\nDescription\nclientSource\nArray of IP addresses and/or CIDR ranges\nIf present, when authenticating a user, the server verifies that the client's IP address is either in the given list or belongs to a CIDR range in the list. If the client's IP address is not present, the server does not authenticate the user.\nserverAddress\nArray of IP addresses and/or CIDR ranges\nA list of IP addresses or CIDR ranges to which the client can connect. If present, the server will verify that the client's connection was accepted via an IP address in the given list. If the connection was accepted via an unrecognized IP address, the server does not authenticate the user. \nIMPORTANT If a user inherits multiple roles with incompatible authentication restrictions, that user becomes unusable. For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. The db.updateUser() method wraps the updateUser command. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.updateUser() is executed using \"majority\" write concern by default. \nENCYPTION  WARNING By default, db.updateUser() sends all specified data to the MongoDB instance in cleartext, even if using passwordPrompt(). Use TLS transport encryption to protect communications between clients and the server, including the password sent by db.updateUser(). For instructions on enabling TLS transport encryption, see Configure mongod and mongos for TLS/SSL. MongoDB does not store the password in cleartext. The password is only vulnerable in transit between the client and the server, and only if TLS transport encryption is not enabled. \nREQUIRED ACCESS \nYou must have access that includes the revokeRole action on all databases in order to update a user's roles array. You must have the grantRole action on a role's database to add a role to a user. To change another user's pwd or customData field, you must have the changePassword and changeCustomData actions respectively on that user's database. To modify your own password and custom data, you must have privileges that grant changeOwnPassword and changeOwnCustomData actions respectively on the user's database. \nEXAMPLE \nGiven a user appClient01 in the products database with the following user info:\n\n  The following db.updateUser() method completely replaces the user's customData and roles data: use productsdb.updateUser( \"appClient01\",{   customData : { employeeId : \"0x3039\" },   roles : [      { role : \"read\", db : \"assets\"  }   ]} )  The user appClient01 in the products database now has the following user information: {   _id : \"products.appClient01\",   userId : UUID(\"c5d88855-3f1e-46cb-9c8b-269bef957986\"),   user : \"appClient01\",   db : \"products\",   customData : { employeeId : \"0x3039\" },   roles : [       {         role : \"read\",         db : \"assets\"       }   ],   mechanisms : [      \"SCRAM-SHA-1\",      \"SCRAM-SHA-256\"   ],   authenticationRestrictions : [ {      clientSource: [\"69.89.31.226\"],      serverAddress: [\"172.16.254.1\"]   } ]}  \nUPDATE USER TO USE SCRAM-SHA-256 CREDENTIALS ONLY  NOTE To use SCRAM-SHA-256, the featureCompatibilityVersion must be set to 4.0. For more information on featureCompatibilityVersion, see Get FeatureCompatibilityVersion and setFeatureCompatibilityVersion. The following operation updates a user who currently have both SCRAM-SHA-256 and SCRAM-SHA-1 credentials to have only SCRAM-SHA-256 credentials. \nNOTE  * If the password is not specified along with the mechanisms, you can only update the mechanisms to a subset of the current SCRAM mechanisms for the user.  * If the password is specified along with the mechanisms, you can specify any supported SCRAM mechanism or mechanisms.  * For SCRAM-SHA-256, the passwordDigestor must be the default value \"server\". use reportingdb.updateUser(   \"reportUser256\",   {     mechanisms: [ \"SCRAM-SHA-256\" ]   }) \n←  db.revokeRolesFromUser()passwordPrompt() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.changeUserPassword/": " Docs Home → MongoDB Manual \nDB.CHANGEUSERPASSWORD() \nOn this page    \n * Definition\n   \n * Required Access\n * Behavior\n * Example \nDEFINITION \ndb.changeUserPassword(username, password) \nUpdates a user's password. Run the method in the database where the user is defined, i.e. the database you created the user. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the updateUser command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Parameter\nType\nDescription\nusername\nstring\nThe name of the user whose password you wish to change.\npassword\nstring The user's password. The value can be either:  * the user's password in cleartext string, or  * passwordPrompt() to prompt for the user's password. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. writeConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. \nREQUIRED ACCESS \nTo modify the password of another user on a database, you must have the changePassword action on that database. \nBEHAVIOR  WARNING By default, db.changeUserPassword() sends all specified data to the MongoDB instance in cleartext, even if using passwordPrompt(). Use TLS transport encryption to protect communications between clients and the server, including the password sent by db.changeUserPassword(). For instructions on enabling TLS transport encryption, see Configure mongod and mongos for TLS/SSL. MongoDB does not store the password in cleartext. The password is only vulnerable in transit between the client and the server, and only if TLS transport encryption is not enabled. \nEXAMPLE \nThe following operation changes the password of the user named accountUser in the products database to SOh3TbYhx8ypJPxmt1oOfL: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. use productsdb.changeUserPassword(\"accountUser\", passwordPrompt())  When prompted in mongosh for the password, enter the new password. You can also pass the new password directly to db.changeUserPassword(): use productsdb.changeUserPassword(\"accountUser\", \"SOh3TbYhx8ypJPxmt1oOfL\") \n←  db.auth()db.createUser() → On this page  * Definition\n * Required Access\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.stats/": " Docs Home → MongoDB Manual \nDB.COLLECTION.STATS() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.stats(<option>)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the collStats command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns statistics about the collection. The method has the following format: db.collection.stats({   scale: <num>,                   // Optional   indexDetails: <boolean>,        // Optional   indexDetailsKey: <document>,    // Optional   indexDetailsName: <string>.     // Optional}) \nField\nType\nDescription\nscale\nnumber Optional. The scale factor for the various size data. The scale defaults to 1 to return size data in bytes. To display kilobytes rather than bytes, specify a scale value of 1024. If you specify a non-integer scale factor, MongoDB uses the integer part of the specified factor. For example, if you specify a scale factor of 1023.999, MongoDB uses 1023 as the scale factor. Starting in version 4.2, the output includes the scaleFactor used to scale the size values. indexDetails\nboolean Optional. If true, db.collection.stats() returns index details in addition to the collection stats. Only works for WiredTiger storage engine. Defaults to false. indexDetailsKey\ndocument Optional. If indexDetails is true, you can use indexDetailsKey to filter index details by specifying the index key specification. Only the index that exactly matches indexDetailsKey will be returned. If no match is found, indexDetails will display statistics for all indexes. Use getIndexes() to discover index keys. You cannot use indexDetailsKey with indexDetailsName. indexDetailsName\nstring Optional. If indexDetails is true, you can use indexDetailsName to filter index details by specifying the index name. Only the index name that exactly matches indexDetailsName will be returned. If no match is found, indexDetails will display statistics for all indexes. Use getIndexes() to discover index names. You cannot use indexDetailsName with indexDetailsField. To specify just the scale factor, MongoDB supports the legacy format: db.collection.stats(<number>)  Returns:A document that contains statistics on the specified collection. See collStats for a breakdown of the returned statistics. \nBEHAVIOR  SCALED SIZES \nUnless otherwise specified by the metric name (such as \"bytes currently in the cache\"), values related to size are displayed in bytes and can be overridden by scale. The scale factor rounds the affected size values to whole numbers. \nSTORAGE ENGINE \nDepending on the storage engine, the data returned may differ. For details on the fields, see output details. \nACCURACY AFTER UNEXPECTED SHUTDOWN \nAfter an unclean shutdown of a mongod using the Wired Tiger storage engine, count and size statistics reported by db.collection.stats() may be inaccurate. The amount of drift depends on the number of insert, update, or delete operations performed between the last checkpoint and the unclean shutdown. Checkpoints usually occur every 60 seconds. However, mongod instances running with non-default --syncdelay settings may have more or less frequent checkpoints. Run validate on each collection on the mongod to restore statistics after an unclean shutdown. After an unclean shutdown:  * validate updates the count statistic in the collStats output with the latest value.  * Other statistics like the number of documents inserted or removed in the collStats output are estimates. \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, collStats operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nINDEX FILTER BEHAVIOR\n\n The indexDetailsKey field takes a document of the following form: { '<string>' : <value>, '<string>' : <value>, ... }  Where <string>> is the field that is indexed and <value> is either the direction of the index, or the special index type such as text or 2dsphere. See index types for the full list of index types. \nUNEXPECTED SHUTDOWN AND COUNT \nFor MongoDB instances using the WiredTiger storage engine, after an unclean shutdown, statistics on size and count may off by up to 1000 documents as reported by collStats, dbStats, count. To restore the correct statistics for the collection, run validate on the collection. \nIN-PROGRESS INDEXES \nStarting in MongoDB 4.2, the db.collection.stats() includes information on indexes currently being built. For details, see:  * collStats.nindexes  * collStats.indexDetails  * collStats.indexBuilds  * collStats.totalIndexSize  * collStats.indexSizes \nEXAMPLES  NOTE You can find the collection data used for these examples in our primer-dataset.json \nBASIC STATS LOOKUP \nThe following operation returns stats on the restaurants collection in the test database: db.restaurants.stats()  The operation returns:\n\n  As stats was not give a scale parameter, all size values are in bytes. \nSTATS LOOKUP WITH SCALE \nThe following operation changes the scale of data from bytes to kilobytes by specifying a scale of 1024: db.restaurants.stats( { scale : 1024 } )  The operation returns: {   \"ns\" : \"test.restaurants\",   \"size\" : 7380,   \"count\" : 25359,   \"avgObjSize\" : 298,   \"storageSize\" : 9444,   \"freeStorageSize\" : 6504,   \"capped\" : false,   \"wiredTiger\" : {      ...   },   \"nindexes\" : 4,   \"indexBuilds\" : [ ],      // Available starting in MongoDB 4.2   \"totalIndexSize\" : 688,   \"totalSize\" : 10132,      // Available starting in MongoDB 4.4   \"indexSizes\" : {      \"_id_\" : 236,      \"cuisine_1\" : 144,      \"borough_1_cuisine_1\" : 156,      \"borough_1_address.zipcode_1\" : 152   },   \"scaleFactor\" : 1024,     // Available starting in MongoDB 4.2   \"ok\" : 1}  \nSTATISTICS LOOKUP WITH INDEX DETAILS \nThe following operation creates an indexDetails document that contains information related to each of the indexes within the collection: db.restaurants.stats( { indexDetails : true } )  The operation returns:\n\n  \nSTATISTICS LOOKUP WITH FILTERED INDEX DETAILS \nTo filter the indexes in the indexDetails field, you can either specify the index keys using the indexDetailsKey option or specify the index name using the indexDetailsName. To discover index keys and names for the collection, use db.collection.getIndexes(). Given the following index: {   \"ns\" : \"test.restaurants\",   \"v\" : 1,   \"key\" : {      \"borough\" : 1,      \"cuisine\" : 1   },   \"name\" : \"borough_1_cuisine_1\"}  The following operation filters the indexDetails document to a single index as defined by the indexDetailsKey document. db.restaurants.stats(   {      'indexDetails' : true,      'indexDetailsKey' :      {         'borough' : 1,         'cuisine' : 1      }   })  The following operation filters the indexDetails document to a single index as defined by the indexDetailsName document. db.restaurants.stats(   {      'indexDetails' : true,      'indexDetailsName' : 'borough_1_cuisine_1'   })  Both operations will return the same output:\n\n  For explanation of the output, see output details. \nTIP \nSEE ALSO: $collStats ←  db.collection.replaceOne()db.collection.storageSize() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/resetDbpath/": " Docs Home → MongoDB Manual \nRESETDBPATH() \nresetDbpath() \nFor internal use. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.printSlaveReplicationInfo/": " Docs Home → MongoDB Manual \nRS.PRINTSLAVEREPLICATIONINFO() \nOn this page    \n * Definition\n   \n * Output \nDEFINITION \nrs.printSlaveReplicationInfo() \nDeprecated since version 4.4.1: Use rs.printSecondaryReplicationInfo() instead. Returns a formatted report of the status of a replica set from the perspective of the secondary member of the set. The output is identical to that of db.printSlaveReplicationInfo(). \nOUTPUT \nThe following is example output from the rs.printSlaveReplicationInfo() method issued on a replica set with two secondary members: source: m1.example.net:27017    syncedTo: Thu Apr 10 2014 10:27:47 GMT-0400 (EDT)    0 secs (0 hrs) behind the primarysource: m2.example.net:27017    syncedTo: Thu Apr 10 2014 10:27:47 GMT-0400 (EDT)    0 secs (0 hrs) behind the primary  A delayed member may show as 0 seconds behind the primary when the inactivity period on the primary is greater than the members[n].secondaryDelaySecs value. A member may show a negative time value behind the primary when rs.printSlaveReplicationInfo() is called. This behavior is expected if the method is called after a secondary replicates a write following a period of inactivity, but before it has received a heartbeat from the primary with its latest optime. \nNOTE The lag reported by secondaries may not be representative of cluster health. Negative values do not indicate that the secondary is ahead of the primary. For the most up-to-date information on your replica set, it is generally advisable to run rs.printSlaveReplicationInfo() on the primary. ←  rs.printSecondaryReplicationInfo()rs.reconfig() → On this page  * Definition\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.hint/": " Docs Home → MongoDB Manual \nBULK.FIND.HINT() \nOn this page    \n * Description\n   \n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.find.hint() \nNew in version 4.2.1. Sets the hint option that specifies the index to support the Bulk.find() for:      * Bulk.find.replaceOne()  * Bulk.find.update()  * Bulk.find.updateOne(). The option can take an index specification document or the index name string. If you specify an index that does not exist, the operation errors. Bulk.find.hint() has no effect on Bulk.find.removeOne() \nEXAMPLE \nCreate an example collection orders: db.orders.insertMany( [   { \"_id\" : 1, \"item\" : \"abc\", \"price\" : NumberDecimal(\"12\"), \"quantity\" : 2, \"type\": \"apparel\" },   { \"_id\" : 2, \"item\" : \"jkl\", \"price\" : NumberDecimal(\"20\"), \"quantity\" : 1, \"type\": \"electronics\" },   { \"_id\" : 3, \"item\" : \"abc\", \"price\" : NumberDecimal(\"10\"), \"quantity\" : 5, \"type\": \"apparel\" },   { \"_id\" : 4, \"item\" : \"abc\", \"price\" : NumberDecimal(\"8\"), \"quantity\" : 10, \"type\": \"apparel\" },   { \"_id\" : 5, \"item\" : \"jkl\", \"price\" : NumberDecimal(\"15\"), \"quantity\" : 15, \"type\": \"electronics\" }] )  Create the following indexes on the example collection: db.orders.createIndex( { item: 1 } );db.orders.createIndex( { item: 1, quantity: 1 } );db.orders.createIndex( { item: 1, price: 1 } );  The following bulk operations specify different index to use for the various update/replace document operations: var bulk = db.orders.initializeUnorderedBulkOp();bulk.find({ item: \"abc\", price: { $gte: NumberDecimal(\"10\") }, quantity: { $lte: 10 } }).hint({item: 1, quantity: 1}).replaceOne( { item: \"abc123\", status: \"P\", points: 100 } );bulk.find({ item: \"abc\", price: { $gte: NumberDecimal(\"10\") }, quantity: { $lte: 10 } }).hint({item: 1, price: 1}).updateOne( { $inc: { points: 10 } } );bulk.execute();  To view the indexes used, you can use the $indexStats pipeline: db.orders.aggregate( [ { $indexStats: { } }, { $sort: { name: 1 } } ] )  \nTIP \nSEE ALSO:  * db.collection.initializeUnorderedBulkOp()  * db.collection.initializeOrderedBulkOp()  * Bulk.find()  * Bulk.execute()  * All Bulk Methods ←  Bulk.find.collation()Bulk.find.remove() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.forEach/": " Docs Home → MongoDB Manual \nCURSOR.FOREACH() \nOn this page    \n * Description\n   \n * Example \nDESCRIPTION \ncursor.forEach(function)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Iterates the cursor to apply a JavaScript function to each document from the cursor. The forEach() method has the following prototype form: db.collection.find().forEach(<function>)  The forEach() method has the following parameter: Parameter\nType\nDescription\nfunction\nJavaScript\nA JavaScript function to apply to each document from the cursor. The <function> signature includes a single argument that is passed the current document to process. \nEXAMPLE \nThe following example invokes the forEach() method on the cursor returned by find() to print the name of each user in the collection: db.users.find().forEach( function(myDoc) { print( \"user: \" + myDoc.name ); } );  \nTIP \nSEE ALSO: cursor.map() for similar functionality. ←  cursor.explain()cursor.hasNext() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-constructor/": " Docs Home → MongoDB Manual \nOBJECT CONSTRUCTORS AND METHODS  NOTE For details on a specific method, including syntax and examples, click on the link to the method's reference page. Name\nDescription\nBulkWriteResult()\nWrapper around the result set from Bulk.execute().\nDate()\nCreates a date object. By default creates a date object including the current date.\nObjectId()\nReturns an ObjectId.\nObjectId.getTimestamp()\nReturns the timestamp portion of an ObjectId.\nObjectId.toString()\nDisplays the string representation of an ObjectId.\nObjectId.valueOf()\nDisplays the str attribute of an ObjectId as a hexadecimal string.\nUUID()\nConverts a 32-byte hexadecimal string to the UUID BSON subtype.\nWriteResult()\nWrapper around the result set from write methods.\nWriteResult.hasWriteError()\nReturns a boolean specifying whether the results include WriteResult.writeError.\nWriteResult.hasWriteConcernError()\nReturns a boolean specifying whether whether the results include WriteResult.writeConcernError. ←  db.getFreeMonitoringStatusBulkWriteResult() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.explain/": " Docs Home → MongoDB Manual \nCURSOR.EXPLAIN() \nOn this page    \n * Definition\n   \n * Behavior\n * Output\n * Example \nDEFINITION \ncursor.explain(verbosity)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Provides information on the query plan for the db.collection.find() method. The explain() method has the following form: db.collection.find().explain()  The explain() method has the following parameter: Parameter\nType\nDescription\nverbose\nstring Optional. Specifies the verbosity mode for the explain output. The mode affects the behavior of explain() and determines the amount of information to return. The possible modes are: \"queryPlanner\", \"executionStats\", and \"allPlansExecution\". Default mode is \"queryPlanner\". For backwards compatibility with earlier versions of cursor.explain(), MongoDB interprets true as \"allPlansExecution\" and false as \"queryPlanner\". For more information on the modes, see Verbosity Modes. The explain() method returns a document with the query plan and, optionally, the execution statistics. \nBEHAVIOR  VERBOSITY MODES \nThe behavior of cursor.explain() and the amount of information returned depend on the verbosity mode. QUERYPLANNER MODE \nBy default, cursor.explain() runs in queryPlanner verbosity mode. MongoDB runs the query optimizer to choose the winning plan for the operation under evaluation. cursor.explain() returns the queryPlanner information for the evaluated method. EXECUTIONSTATS MODE \nMongoDB runs the query optimizer to choose the winning plan, executes the winning plan to completion, and returns statistics describing the execution of the winning plan. cursor.explain() returns the queryPlanner and executionStats information for the evaluated method. However, executionStats does not provide query execution information for the rejected plans. ALLPLANSEXECUTION MODE \nMongoDB runs the query optimizer to choose the winning plan and executes the winning plan to completion. In \"allPlansExecution\" mode, MongoDB returns statistics describing the execution of the winning plan as well as statistics for the other candidate plans captured during plan selection. cursor.explain() returns the queryPlanner and executionStats information for the evaluated method. The executionStats includes the completed query execution information for the winning plan. If the query optimizer considered more than one plan, executionStats information also includes the partial execution information captured during the plan selection phase for both the winning and rejected candidate plans. \nDB.COLLECTION.EXPLAIN().FIND() \ndb.collection.explain().find() is similar to db.collection.find().explain() with the following key differences:      * The db.collection.explain().find() construct allows for the additional chaining of query modifiers. For list of query modifiers, see db.collection.explain().find().help().  * The db.collection.explain().find() returns a cursor, which requires a call to .next(), or its alias .finish(), to return the explain() results. If run interactively in mongosh, mongosh automatically calls .finish() to return the results. For scripts, however, you must explicitly call .next(), or .finish(), to return the results. For list of cursor-related methods, see db.collection.explain().find().help(). See db.collection.explain() for more information. \nOUTPUT \ncursor.explain() operations can return information regarding:  * explainVersion, the output format version (for example, \"1\");  * command, which details the command being explained;  * queryPlanner, which details the plan selected by the query optimizer and lists the rejected plans;  * executionStats, which details the execution of the winning plan and the rejected plans;  * serverInfo, which provides information on the MongoDB instance; and  * serverParameters, which details internal parameters. The verbosity mode (i.e. queryPlanner, executionStats, allPlansExecution) determines whether the results include executionStats and whether executionStats includes data captured during plan selection. Explain output is limited by the maximum Nested Depth for BSON Documents, which is 100 levels of nesting. Explain output that exceeds the limit is truncated. For details on the output, see Explain Results. \nEXAMPLE\n\n db.products.find(   { quantity: { $gt: 50 }, category: \"apparel\" }).explain(\"executionStats\") \n←  cursor.count()cursor.forEach() → On this page  * Definition\n * Behavior\n * Output\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.replaceOne/": " Docs Home → MongoDB Manual \nDB.COLLECTION.REPLACEONE() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.replaceOne(filter, replacement, options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the update command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Replaces a single document within the collection based on the filter. The replaceOne() method has the following form: db.collection.replaceOne(   <filter>,   <replacement>,   {     upsert: <boolean>,     writeConcern: <document>,     collation: <document>,     hint: <document|string>                   // Available starting in 4.2.1   })  The replaceOne() method takes the following parameters: Parameter\nType\nDescription\nfilter\ndocument The selection criteria for the update. The same query selectors as in the find() method are available. Specify an empty document { } to replace the first document returned in the collection. replacement\ndocument The replacement document. Cannot contain update operators. upsert\nboolean Optional. When true, replaceOne() either:  * Inserts the document from the replacement parameter if no document matches the filter.  * Replaces the document that matches the filter with the replacement document. MongoDB will add the _id field to the replacement document if it is not specified in either the filter or replacement documents. If _id is present in both, the values must be equal. To avoid multiple upserts, ensure that the query fields are uniquely indexed. Defaults to false. writeConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. hint\ndocument Optional. A document or string that specifies the index to use to support the filter. The option can take an index specification document or the index name string. If you specify an index that does not exist, the operation errors. For an example, see Specify hint for replaceOne. New in version 4.2.1. Returns:A document containing:  * A boolean acknowledged as true if the operation ran with write concern or false if write concern was disabled  * matchedCount containing the number of matched documents  * modifiedCount containing the number of modified documents  * upsertedId containing the _id for the upserted document \nBEHAVIOR \nreplaceOne() replaces the first matching document in the collection that matches the filter, using the replacement document. \nUPSERT \nIf upsert: true and no documents match the filter, db.collection.replaceOne() creates a new document based on the replacement document.\n\n See Replace with Upsert. \nCAPPED COLLECTIONS \nIf a replacement operation changes the document size, the operation will fail. \nTIME SERIES COLLECTIONS \nYou cannot use the replaceOne() method on a time series collection. \nSHARDED COLLECTIONS \nStarting in MongoDB 4.2, db.collection.replaceOne() attempts to target a single shard, first by using the query filter. If the operation cannot target a single shard by the query filter, it then attempts to target by the replacement document. In earlier versions, the operation attempts to target using the replacement document. SHARD KEY REQUIREMENTS IN REPLACEMENT DOCUMENT \nStarting in MongoDB 4.4, the replacement document does not need to include the shard key. In MongoDB 4.2 and earlier, the replacement document must include the shard key. \nWARNING Starting in version 4.4, documents in sharded collections can be missing the shard key fields. Take precaution to avoid accidentally removing the shard key when changing a document's shard key value. UPSERT ON A SHARDED COLLECTION \nStarting in MongoDB 4.2, a db.collection.replaceOne() operation that includes upsert: true on a sharded collection must include the full shard key in the filter. However, starting in version 4.4, documents in a sharded collection can be missing the shard key fields. To target a document that is missing the shard key, you can use the null equality match in conjunction with another filter condition (such as on the _id field). For example: { _id: <value>, <shardkeyfield>: null } // _id of the document missing shard key  SHARD KEY MODIFICATION \nStarting in MongoDB 4.2, you can update a document's shard key value unless the shard key field is the immutable _id field. In MongoDB 4.2 and earlier, a document's shard key field value is immutable. \nWARNING Starting in version 4.4, documents in sharded collections can be missing the shard key fields. Take precaution to avoid accidentally removing the shard key when changing a document's shard key value. To modify the existing shard key value with db.collection.replaceOne():  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write.  * You must include an equality filter on the full shard key. MISSING SHARD KEY \nStarting in version 4.4, documents in a sharded collection can be missing the shard key fields. To use db.collection.replaceOne() to set the document's missing shard key, you must run on a mongos. Do not issue the operation directly on the shard. In addition, the following requirements also apply: Task\nRequirements\nTo set to null  * Requires equality filter on the full shard key if upsert: true is specified. To set to a non-null value  * Must be performed either inside a transaction or as a retryable write.  * Requires equality filter on the full shard key if either:\n   \n   * upsert: true, or\n   \n   * the new shard key value belongs to a different shard. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also:  * upsert on a Sharded Collection  * Missing Shard Key Fields \nTRANSACTIONS \ndb.collection.replaceOne() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. UPSERT WITHIN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, db.collection.replaceOne() with upsert: true can be run on an existing collection or a non-existing collection. If run on a non-existing collection, the operation creates the collection.\n\n \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES  REPLACE \nThe restaurant collection contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\" },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : 0 }  The following operation replaces a single document where name: \"Central Perk Cafe\": try {   db.restaurant.replaceOne(      { \"name\" : \"Central Perk Cafe\" },      { \"name\" : \"Central Pork Cafe\", \"Borough\" : \"Manhattan\" }   );} catch (e){   print(e);}  The operation returns: { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }  If no matches were found, the operation instead returns: { \"acknowledged\" : true, \"matchedCount\" : 0, \"modifiedCount\" : 0 }  Setting upsert: true would insert the document if no match was found. See Replace with Upsert \nREPLACE WITH UPSERT \nThe restaurant collection contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\",  \"violations\" : 3 },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : 0 }  The following operation attempts to replace the document with name : \"Pizza Rat's Pizzaria\", with upsert : true: try {   db.restaurant.replaceOne(      { \"name\" : \"Pizza Rat's Pizzaria\" },      { \"_id\": 4, \"name\" : \"Pizza Rat's Pizzaria\", \"Borough\" : \"Manhattan\", \"violations\" : 8 },      { upsert: true }   );} catch (e){   print(e);}  Since upsert : true the document is inserted based on the replacement document. The operation returns: {   \"acknowledged\" : true,   \"matchedCount\" : 0,   \"modifiedCount\" : 0,   \"upsertedId\" : 4}  The collection now contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\", \"violations\" : 3 },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : 0 },{ \"_id\" : 4, \"name\" : \"Pizza Rat's Pizzaria\", \"Borough\" : \"Manhattan\", \"violations\" : 8 }  \nREPLACE WITH WRITE CONCERN \nGiven a three member replica set, the following operation specifies a w of majority and wtimeout of 100: try {   db.restaurant.replaceOne(       { \"name\" : \"Pizza Rat's Pizzaria\" },       { \"name\" : \"Pizza Rat's Pub\", \"Borough\" : \"Manhattan\", \"violations\" : 3 },       { w: \"majority\", wtimeout: 100 }   );} catch (e) {   print(e);}  If the acknowledgement takes longer than the wtimeout limit, the following exception is thrown: Changed in version 4.4.\n\n  The following table explains the possible values of errInfo.writeConcern.provenance: Provenance\nDescription\nclientSupplied\nThe write concern was specified in the application.\ncustomDefault\nThe write concern originated from a custom defined default value. See setDefaultRWConcern.\ngetLastErrorDefaults\nThe write concern originated from the replica set's settings.getLastErrorDefaults field.\nimplicitDefault\nThe write concern originated from the server in absence of all other write concern specifications. \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents: { _id: 1, category: \"café\", status: \"A\" }{ _id: 2, category: \"cafe\", status: \"a\" }{ _id: 3, category: \"cafE\", status: \"a\" }  The following operation includes the collation option: db.myColl.replaceOne(   { category: \"cafe\", status: \"a\" },   { category: \"cafÉ\", status: \"Replaced\" },   { collation: { locale: \"fr\", strength: 1 } }\n);  \nSPECIFY HINT FOR REPLACEONE \nNew in version 4.2.1. Create a sample members collection with the following documents: db.members.insertMany([   { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"P\", \"points\" :  0,  \"misc1\" : null, \"misc2\" : null },   { \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60,  \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" },   { \"_id\" : 3, \"member\" : \"lmn123\", \"status\" : \"P\", \"points\" :  0,  \"misc1\" : null, \"misc2\" : null },   { \"_id\" : 4, \"member\" : \"pqr123\", \"status\" : \"D\", \"points\" : 20,  \"misc1\" : \"Deactivated\", \"misc2\" : null },   { \"_id\" : 5, \"member\" : \"ijk123\", \"status\" : \"P\", \"points\" :  0,  \"misc1\" : null, \"misc2\" : null },   { \"_id\" : 6, \"member\" : \"cde123\", \"status\" : \"A\", \"points\" : 86,  \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" }])  Create the following indexes on the collection: db.members.createIndex( { status: 1 } )db.members.createIndex( { points: 1 } )  The following update operation explicitly hints to use the index { status: 1 }: \nNOTE If you specify an index that does not exist, the operation errors. db.members.replaceOne(   { \"points\": { $lte: 20 }, \"status\": \"P\" },   { \"misc1\": \"using index on status\", status: \"P\", member: \"replacement\", points: \"20\"},   { hint: { status: 1 } })  The operation returns the following: { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }  To view the indexes used, you can use the $indexStats pipeline: db.members.aggregate( [ { $indexStats: { } }, { $sort: { name: 1 } } ] ) \n←  db.collection.renameCollection()db.collection.stats() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.reconfigForPSASet/": " Docs Home → MongoDB Manual \nRS.RECONFIGFORPSASET() \nOn this page    \n * Syntax\n   \n * Behavior\n * Example rs.reconfigForPSASet( memberIndex, config, { options } ) \nNew in version 5.0. Safely perform some reconfiguration changes on a primary-secondary-arbiter (PSA) replica set or on a replica set that is changing to a PSA architecture. You should only use this method in one of the following cases:      * You want to reconfigure a secondary in an existing three-member replica set with a PSA architecture to become a voting, data-bearing node with a non-zero priority.  * You want to add a new voting, data-bearing node with a non-zero priority to an existing two-member replica set that contains one voting, data-bearing node and one arbiter. \nWARNING If the secondary you are adding is lagged and the resulting replica set is a PSA configuration, the first configuration change will change the number of nodes that need to commit a change with \"majority\". In this case, your commit point will lag until the secondary has caught up. For details about the behavior of this method, see Behavior. \nSYNTAX \nThe rs.reconfigForPSASet() method has the following syntax: rs.reconfigForPSASet(  memberIndex: <num>,  config: <configuration>,  {    \"force\" : <boolean>,    \"maxTimeMS\" : <int>  }) \nParameter\nType\nDescription\nmemberIndex\ninteger\nThe index of the secondary that is being added or modified.\nconfig\ndocument\nA document that specifies the new configuration of a replica set.\nforce\nboolean Optional \nWARNING Running the rs.reconfigForPSASet() method with force: true is not recommended and can lead to committed writes being rolled back. Specify true to force the available replica set members to accept the new configuration. Defaults to false. Force reconfiguration can result in unexpected or undesired behavior, including rollback of \"majority\" committed writes. maxTimeMS\ninteger Optional Specifies a cumulative time limit in milliseconds for processing each reconfiguration during the rs.reconfigForPSASet() operation. By default, rs.reconfigForPSASet() waits indefinitely for the replica configurations to propagate to a majority of replica set members. \nBEHAVIOR \nThe rs.reconfigForPSASet() method reconfigures your replica set in two steps:  1. The method reconfigures your replica set to add or modify a secondary with { votes: 1, priority: 0 }.  2. Once the added or modified secondary has caught up with all committed writes, the method reconfigures the secondary to have the priority specified in the rs.reconfigForPSASet() command ({ votes: 1, priority: <num> }). The two-step approach avoids the possibility of rolling back committed writes in the case of a failover to the new secondary before the new secondary has all committed writes from the previous configuration. \nEXAMPLE \nA replica set named rs0 has the following configuration:\n\n  The following sequence of operations add a new secondary to the replica set. The operations are issued in the mongosh shell while connected to the primary. cfg = rs.conf();cfg[\"members\"] = [  {     \"_id\" : 0,     \"host\" : \"mongodb0.example.net:27017\",     \"arbiterOnly\" : false,     \"buildIndexes\" : true,     \"hidden\" : false,     \"priority\" : 1,     \"tags\" : {},     \"secondaryDelaySecs\" : Long(\"0\"),     \"votes\" : 1  },  {     \"_id\" : 1,     \"host\" : \"mongodb2.example.net:27017\",     \"arbiterOnly\" : false,     \"buildIndexes\" : true,     \"hidden\" : false,     \"priority\" : 2,     \"tags\" : {},     \"secondaryDelaySecs\" : Long(\"0\"),     \"votes\" : 1  },  {     \"_id\" : 2,     \"host\" : \"mongodb2.example.net:27017\",     \"arbiterOnly\" : true,     \"buildIndexes\" : true,     \"hidden\" : false,     \"priority\" : 0,     \"tags\" : {},     \"secondaryDelaySecs\" : Long(\"0\"),     \"votes\" : 1  }]rs.reconfigForPSASet(1, cfg);   1. The first statement uses the rs.conf() method to retrieve a document containing the current configuration for the replica set and stores the document in the local variable cfg.  2. The second statement adds the new secondary to the members array. In this configuration the new secondary is added at memberIndex 1. The memberIndex is the same as the array index. For additional settings, see replica set configuration settings.\n\n \nTIP \nSEE ALSO:  * rs.conf()  * rs.reconfig()  * Replica Set Configuration Fields  * Member Configuration Tutorials  * Replica Set Maintenance Tutorials ←  rs.reconfig()rs.remove() → On this page  * Syntax\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.unhideIndex/": " Docs Home → MongoDB Manual \nDB.COLLECTION.UNHIDEINDEX() \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.collection.unhideIndex()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the index.hidden collection option set using the collMod command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Unhides an existing index from the query planner. Once unhidden, the indexes are immediately available for use. \nSYNTAX \ndb.collection.unhideIndex(<index>)  \nPARAMETERS \nThe db.collection.unhideIndex() method takes the following parameter: Parameter\nType\nDescription\nindex\nstring or document Specifies the index to unhide from the query planner. You can specify the index either by the index name or by the index specification document. \nTIP You can use the db.collection.getIndexes() method to find the index name or the index specification document. To unhide a text index, specify the index name. The db.collection.unhideIndex() is a mongosh shell wrapper for the collMod command. \nBEHAVIOR  INDEX MODIFICATIONS RESET STATISTICS \nUnhiding a hidden index resets its $indexStats. \nNO-OP \nUnhiding an already unhidden index has no effect on the index. However, the operation will still generate an empty oplog entry. \nREQUIRED ACCESS \nIf the deployment enforces authentication/authorization, you must have the collMod privilege in the collection's database. The built-in role dbAdmin provides the required privileges. \nEXAMPLE \nThe following example unhides an existing index. First, use db.collection.createIndex() to create a hidden index: db.restaurants.createIndex( { borough: 1, ratings: 1 }, { hidden: true } );  To verify, run db.collection.getIndexes() on the restaurants collection: db.restaurants.getIndexes();  The operation returns the following information: [   {      \"v\" : 2,      \"key\" : {         \"_id\" : 1      },      \"name\" : \"_id_\"   },   {      \"v\" : 2,      \"key\" : {         \"borough\" : 1,         \"ratings\" : 1      },      \"name\" : \"borough_1_ratings_1\",      \"hidden\" : true   }]  The index option hidden is only returned if the value is true. To unhide the index, you can specify either the index key specification document or the index name to the db.collection.unhideIndex() method. The following specifies the index name: db.restaurants.unhideIndex( \"borough_1_ratings_1\" );  To verify, run db.collection.getIndexes() on the restaurants collection: db.restaurants.getIndexes()  The operation returns the following information: [   {      \"v\" : 2,      \"key\" : {         \"_id\" : 1      },      \"name\" : \"_id_\"   },   {      \"v\" : 2,      \"key\" : {         \"borough\" : 1,         \"ratings\" : 1      },      \"name\" : \"borough_1_ratings_1\"   }]  The index option hidden no longer appears as part of the borough_1_ratings_1 index since the field is only returned if the value is true. \nTIP \nSEE ALSO:  * db.collection.hideIndex()  * db.collection.createIndex()\n\n On this page  * Definition\n * Syntax\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.resetError/": " Docs Home → MongoDB Manual \nDB.RESETERROR() \ndb.resetError() \nRemoved in MongoDB 5.0. Provides a wrapper around the resetError command (also removed in MongoDB 5.0). ←  db.printSlaveReplicationInfo()db.rotateCertificates() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.printShardingStatus/": " Docs Home → MongoDB Manual \nDB.PRINTSHARDINGSTATUS() \nOn this page    \n * Definition \nDEFINITION \ndb.printShardingStatus() \nPrints a formatted report of the sharding configuration and the information regarding existing chunks in a sharded cluster. Only use db.printShardingStatus() when connected to a mongos instance. The db.printShardingStatus() method has the following parameter: Parameter\nType\nDescription\nverbose\nboolean Optional. Determines the level of verbosity. If true, the method displays:      * Full details of the chunk distribution across shards even if you have 20 or more chunks, as well as the number of chunks on each shard.  * Details of active mongos instances. If false, the method displays:  * Full details of the chunk distribution across shards only if you have less than 20 chunks. If you have 20 or more chunks, the method instead returns a too many chunks to print ... message, showing only the number of chunks on each shard.  * Only the version and number of active mongos instances. The default verbose value is false. See sh.status() for details of the output. \nNOTE The db.printShardingStatus() method run in mongosh does not return JSON. Use db.printShardingStatus() for manual inspection, and Config Database in scripts. \nTIP \nSEE ALSO: sh.status() ←  db.printSecondaryReplicationInfo()db.printSlaveReplicationInfo() → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.hideIndex/": " Docs Home → MongoDB Manual \nDB.COLLECTION.HIDEINDEX() \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Access Control\n * Example \nDEFINITION \ndb.collection.hideIndex() \nNew in version 4.4. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the index.hidden collection option set using the collMod command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Hides an existing index from the query planner. An index hidden from the query planner is not evaluated as part of query plan selection. By hiding an index from the planner, users can evaluate the potential impact of dropping an index without actually dropping the index. If the impact is negative, the user can unhide the index instead of having to recreate a dropped index. And because indexes are fully maintained while hidden, the indexes are immediately available for use once unhidden. For details, see Hidden Indexes. \nSYNTAX \ndb.collection.hideIndex(<index>)  \nPARAMETERS \nThe db.collection.hideIndex() method takes the following parameter: Parameter\nType\nDescription\nindex\nstring or document Specifies the index to hide from the query planner. You can specify the index either by the index name or by the index specification document. \nTIP You can use the db.collection.getIndexes() method to find the index name or the index specification document. To hide a text index, specify the index name. The db.collection.hideIndex() is a mongosh shell wrapper for the collMod command. \nBEHAVIOR  FEATURE COMPATIBILITY VERSION \nTo hide an index, you must have featureCompatibilityVersion set to 4.4 or greater. However, once hidden, the index remains hidden even with featureCompatibilityVersion set to 4.2 on MongoDB 4.4 binaries. \nRESTRICTIONS \nYou cannot hide the _id index. \nINDEX MODIFICATIONS RESET STATISTICS \nHiding an unhidden index resets its $indexStats. \nNO-OP \nHiding an already hidden index has no effect on the index. However, the operation will still generate an empty oplog entry. \nACCESS CONTROL \nIf the deployment enforces authentication/authorization, you must have the collMod privilege in the collection's database. The built-in role dbAdmin provides the required privileges. \nEXAMPLE \nThe following example hides an existing index. First, use db.collection.createIndex() to create an index without hiding: db.restaurants.createIndex( { borough: 1, ratings: 1 } );  To hide the index, you can specify either the index key specification document or the index name to the db.collection.hideIndex() method. The following specifies the index name: db.restaurants.hideIndex( \"borough_1_ratings_1\" );  To verify, run db.collection.getIndexes() on the restaurants collection: db.restaurants.getIndexes();  The operation returns the following information: [   {      \"v\" : 2,      \"key\" : {         \"_id\" : 1      },      \"name\" : \"_id_\"   },   {      \"v\" : 2,      \"key\" : {         \"borough\" : 1,         \"ratings\" : 1      },      \"name\" : \"borough_1_ratings_1\",      \"hidden\" : true   }]  The hidden index option is only returned if the value is true. \nTIP \nSEE ALSO:  * db.collection.unhideIndex()  * db.collection.createIndex() ←  db.collection.getShardVersion()db.collection.insert() → On this page  * Definition\n * Syntax\n * Behavior\n * Access Control\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.getShardDistribution/": " Docs Home → MongoDB Manual \nDB.COLLECTION.GETSHARDDISTRIBUTION() \nOn this page    \n * Definition\n   \n * Output \nDEFINITION \ndb.collection.getShardDistribution()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Prints the data distribution statistics for a sharded collection. \nTIP Before running the method, use the flushRouterConfig command to refresh the cached routing table to avoid returning stale distribution information for the collection. Once refreshed, run db.collection.getShardDistribution() for the collection you wish to build the index. For example: db.adminCommand( { flushRouterConfig: \"test.myShardedCollection\" } );db.getSiblingDB(\"test\").myShardedCollection.getShardDistribution();  \nTIP \nSEE ALSO: Sharding \nOUTPUT  SAMPLE OUTPUT \nThe following is a sample output for the distribution of a sharded collection: Shard shard-a at shard-a/MyMachine.local:30000,MyMachine.local:30001,MyMachine.local:30002data : 38.14Mb docs : 1000003 chunks : 2estimated data per chunk : 19.07Mbestimated docs per chunk : 500001\nShard shard-b at shard-b/MyMachine.local:30100,MyMachine.local:30101,MyMachine.local:30102data : 38.14Mb docs : 999999 chunks : 3estimated data per chunk : 12.71Mbestimated docs per chunk : 333333\nTotalsdata : 76.29Mb docs : 2000002 chunks : 5Shard shard-a contains 50% data, 50% docs in cluster, avg obj size on shard : 40bShard shard-b contains 49.99% data, 49.99% docs in cluster, avg obj size on shard : 40b \nOUTPUT FIELDS \nShard <shard-a> at <host-a> data : <size-a> docs : <count-a> chunks : <number of chunks-a> estimated data per chunk : <size-a>/<number of chunks-a> estimated docs per chunk : <count-a>/<number of chunks-a>\nShard <shard-b> at <host-b> data : <size-b> docs : <count-b> chunks : <number of chunks-b> estimated data per chunk : <size-b>/<number of chunks-b> estimated docs per chunk : <count-b>/<number of chunks-b>\nTotals data : <stats.size> docs : <stats.count> chunks : <calc total chunks> Shard <shard-a> contains  <estDataPercent-a>% data, <estDocPercent-a>% docs in cluster, avg obj size on shard : stats.shards[ <shard-a> ].avgObjSize Shard <shard-b> contains  <estDataPercent-b>% data, <estDocPercent-b>% docs in cluster, avg obj size on shard : stats.shards[ <shard-b> ].avgObjSize The output information displays:      * <shard-x> is a string that holds the shard name.  * <host-x> is a string that holds the host name(s).  * <size-x> is a number that includes the size of the data, including the unit of measure (e.g. b, Mb).  * <count-x> is a number that reports the number of documents in the shard.  * <number of chunks-x> is a number that reports the number of chunks in the shard.  * <size-x>/<number of chunks-x> is a calculated value that reflects the estimated data size per chunk for the shard, including the unit of measure (e.g. b, Mb).  * <count-x>/<number of chunks-x> is a calculated value that reflects the estimated number of documents per chunk for the shard.  * <stats.size> is a value that reports the total size of the data in the sharded collection, including the unit of measure.  * <stats.count> is a value that reports the total number of documents in the sharded collection.\n\n  * <estDataPercent-x> is a calculated value that reflects, for each shard, the data size as the percentage of the collection's total data size, for example:\n   \n   <estDataPercent-x> = <size-x>/<stats.size>\n   \n     * <estDocPercent-x> is a calculated value that reflects, for each shard, the number of documents as the percentage of the total number of documents for the collection, for example:\n   \n   <estDocPercent-x> = <count-x>/<stats.count>\n   \n     * stats.shards[ <shard-x> ].avgObjSize is a number that reflects the average object size, including the unit of measure, for the shard. ←  db.collection.getIndexes()db.collection.getShardVersion() → On this page  * Definition\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.getIndexes/": " Docs Home → MongoDB Manual \nDB.COLLECTION.GETINDEXES() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Output \nDEFINITION \ndb.collection.getIndexes()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the listIndexes command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns an array that holds a list of documents that identify and describe the existing indexes on the collection, including hidden indexes. You must call db.collection.getIndexes() on a collection. For example: db.collection.getIndexes()  Change collection to the name of the collection for which to return index information. \nBEHAVIOR  CLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued db.collection.getIndexes() disconnects before the operation completes, MongoDB marks db.collection.getIndexes() for termination using killOp. \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, listIndexes operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nWILDCARD INDEXES \nStarting in MongoDB 6.3, 6.0.5, and 5.0.16, the wildcardProjection field stores the index projection in its submitted form. Earlier versions of the server may have stored the projection in a normalized form. The server uses the index the same way, but you may notice a difference in the output of the listIndexes and db.collection.getIndexes() commands. \nREQUIRED ACCESS \nTo run db.collection.getIndexes() when access control is enforced, usesrs must have privileges to listIndexes on the collection. The built-in role read provides the required privileges to run db.collection.getIndexes() for the collections in a database. \nOUTPUT \ndb.collection.getIndexes() returns an array of documents that hold index information for the collection. For example: \nNOTE Starting in MongoDB 4.4, db.collection.getIndexes() no longer includes the ns field. [   {      \"v\" : 2,      \"key\" : {         \"_id\" : 1      },      \"name\" : \"_id_\"   },   {      \"v\" : 2,      \"key\" : {         \"status\" : 1      },      \"name\" : \"status_1\"   },   {      \"v\" : 2,      \"key\" : {         \"points\" : 1      },      \"name\" : \"points_1\"   }] Index information includes the keys and options used to create the index. The index option hidden, available starting in MongoDB 4.4, is only available if the value is true. For information on the keys and index options, see db.collection.createIndex(). ←  db.collection.findOneAndUpdate()db.collection.getShardDistribution() → On this page  * Definition\n * Behavior\n * Required Access\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.printReplicationInfo/": " Docs Home → MongoDB Manual \nDB.PRINTREPLICATIONINFO() \nOn this page    \n * Definition \nDEFINITION \ndb.printReplicationInfo() \nPrints a formatted report of the replica set member's oplog. The displayed report formats the data returned by db.getReplicationInfo(). The output of db.printReplicationInfo() is identical to that of rs.printReplicationInfo(). \nNOTE The db.printReplicationInfo() method run in mongosh does not return JSON. Use db.printReplicationInfo() for manual inspection, and db.getReplicationInfo() in scripts. \nOUTPUT EXAMPLE \nThe following example is a sample output from the db.printReplicationInfo() method run on the primary: configured oplog size:   192MBlog length start to end: 65422secs (18.17hrs)oplog first event time:  Mon Jun 23 2014 17:47:18 GMT-0400 (EDT)oplog last event time:   Tue Jun 24 2014 11:57:40 GMT-0400 (EDT)now:                     Thu Jun 26 2014 14:24:39 GMT-0400 (EDT)  \nOUTPUT FIELDS \ndb.printReplicationInfo() formats and prints the data returned by db.getReplicationInfo(): configured oplog sizeDisplays the db.getReplicationInfo.logSizeMB value.log length start to endDisplays the db.getReplicationInfo.timeDiff and db.getReplicationInfo.timeDiffHours values.oplog first event timeDisplays the db.getReplicationInfo.tFirst.oplog last event timeDisplays the db.getReplicationInfo.tLast.nowDisplays the db.getReplicationInfo.now. See db.getReplicationInfo() for description of the data. ←  db.printCollectionStats()db.printSecondaryReplicationInfo() → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find/": " Docs Home → MongoDB Manual \nBULK.FIND() \nOn this page    \n * Description\n   \n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.find(<query>) \nSpecifies a query condition for an update or a remove operation. Bulk.find() accepts the following parameter: Parameter\nType\nDescription\nquery\ndocument Specifies a query condition using Query Selectors to select documents for an update or a remove operation. To specify all documents, use an empty document {}. With update operations, the sum of the query document and the update document must be less than or equal to the maximum BSON document size. With remove operations, the query document must be less than or equal to the maximum BSON document size. Use Bulk.find() with the following write operations:      * Bulk.find.removeOne()  * Bulk.find.remove()  * Bulk.find.replaceOne()  * Bulk.find.updateOne()  * Bulk.find.update() \nEXAMPLE \nThe following example initializes a Bulk() operations builder for the items collection and adds a remove operation and an update operation to the list of operations. The remove operation and the update operation use the Bulk.find() method to specify a condition for their respective actions: var bulk = db.items.initializeUnorderedBulkOp();bulk.find( { status: \"D\" } ).delete();bulk.find( { status: \"P\" } ).update( { $set: { points: 0 } } )bulk.execute();  \nTIP \nSEE ALSO:  * db.collection.initializeUnorderedBulkOp()  * db.collection.initializeOrderedBulkOp()  * Bulk.execute() ←  Bulk.execute()Bulk.find.arrayFilters() → On this page  * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.dropIndex/": " Docs Home → MongoDB Manual \nDB.COLLECTION.DROPINDEX() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \ndb.collection.dropIndex(index)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the dropIndexes command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Drops or removes the specified index from a collection. \nNOTE  * You cannot drop the default index on the _id field.  * Starting in MongoDB 4.2, you cannot specify db.collection.dropIndex(\"*\") to drop all non-_id indexes. Use db.collection.dropIndexes() instead. To get the index name or the index specification document for the db.collection.dropIndex() method, use the db.collection.getIndexes() method. The db.collection.dropIndex() method takes the following parameter: Parameter\nType\nDescription\nindex\nstring or document Optional. Specifies the index to drop. You can specify the index either by the index name or by the index specification document. To drop a text index, specify the index name. Starting in MongoDB 4.2, you cannot specify \"*\" to drop all non-_id indexes. Use db.collection.dropIndexes() instead. New in version 4.4: If an index specified to db.collection.dropIndex() is still building, db.collection.dropIndex() attempts to stop the in-progress build. Stopping an index build has the same effect as dropping the built index. Prior to MongoDB 4.4, db.collection.dropIndex() returned an error if the specified index was still building. See Stop In-Progress Index Builds for more complete documentation. \nBEHAVIOR \nStarting in MongoDB 5.2, you can use db.collection.dropIndex() to drop existing indexes on the same collection even if there is a build in progress on another index. In earlier versions, attempting to drop a different index during an in-progress index build results in a BackgroundOperationInProgressForNamespace error. \nRESOURCE LOCKING \nChanged in version 4.2. db.collection.dropIndex() obtains an exclusive lock on the specified collection for the duration of the operation. All subsequent operations on the collection must wait until db.collection.dropIndex() releases the lock. Prior to MongoDB 4.2, db.collection.dropIndex() obtained an exclusive lock on the parent database, blocking all operations on the database and all its collections until the operation completed. \nSTOP IN-PROGRESS INDEX BUILDS \nStarting in MongoDB 4.4, if an index specified to db.collection.dropIndex() is still building, db.collection.dropIndex() attempts to stop the in-progress build. Stopping an index build has the same effect as dropping the built index. In versions earlier than MongoDB 4.4, db.collection.dropIndex() returns an error if there are any index builds in progress on the collection. For replica sets, run db.collection.dropIndex() on the primary. The primary stops the index build and creates an associated \"abortIndexBuild\" oplog entry. Secondaries which replicate the \"abortIndexBuild\" oplog entry stop the in-progress index build and discard the build job. See Index Build Process for detailed documentation on the index build process. Use currentOp to identify the index builds associated with a createIndexes or db.collection.createIndexes() operation. See Active Indexing Operations for an example. \nHIDDEN INDEXES \nStarting in version 4.4, MongoDB adds the ability to hide or unhide indexes from the query planner. By hiding an index from the planner, users can evaluate the potential impact of dropping an index without actually dropping the index. If after the evaluation, the user decides to drop the index, the user can drop the hidden index; i.e. you do not need to unhide it first to drop it. If, however, the impact is negative, the user can unhide the index instead of having to recreate a dropped index. And because indexes are fully maintained while hidden, the indexes are immediately available for use once unhidden. For more information on hidden indexes, see Hidden Indexes. \nEXAMPLE\n\n [   {      \"v\" : 2,      \"key\" : {         \"_id\" : 1      },      \"name\" : \"_id_\"   },   {      \"v\" : 2,      \"key\" : {         \"cat\" : -1      },      \"name\" : \"catIdx\"   },   {      \"v\" : 2,      \"key\" : {         \"cat\" : 1,         \"dog\" : -1      },      \"name\" : \"cat_1_dog_-1\"   }]  The single field index on the field cat has the user-specified name of catIdx [1] and the index specification document of { \"cat\" : -1 }. To drop the index catIdx, you can use either the index name: db.pets.dropIndex( \"catIdx\" )  Or you can use the index specification document { \"cat\" : -1 }: db.pets.dropIndex( { \"cat\" : -1 } )  [1] During index creation, if the user does not specify an index name, the system generates the name by concatenating the index key field and value with an underscore, e.g. cat_1. ←  db.collection.drop()db.collection.dropIndexes() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.totalSize/": " Docs Home → MongoDB Manual \nDB.COLLECTION.TOTALSIZE() \ndb.collection.totalSize()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the totalSize field returned by the collStats command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns:The total size in bytes of the data in the collection plus the size of every index on the collection. If collection data is compressed (which is the default for WiredTiger), the returned size reflects the compressed size of the collection data. If an index uses prefix compression (which is the default for WiredTiger), the returned size reflects the compressed size of the index. The value returned is the sum of db.collection.storageSize() and db.collection.totalIndexSize() in bytes. Starting in MongoDB 4.4, to run on a replica set member, collStats operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. ←  db.collection.totalIndexSize()db.collection.unhideIndex() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getLogComponents/": " Docs Home → MongoDB Manual \nDB.GETLOGCOMPONENTS() \nOn this page    \n * Definition\n   \n * Output \nDEFINITION \ndb.getLogComponents() \nReturns the current verbosity settings. The verbosity settings determine the amount of Log Messages that MongoDB produces for each log message component. If a component inherits the verbosity level of its parent, db.getLogComponents() displays -1 for the component's verbosity. \nOUTPUT \nThe db.getLogComponents() returns a document with the verbosity settings. For example: {   \"verbosity\" : 0,   \"accessControl\" : {      \"verbosity\" : -1   },   \"command\" : {      \"verbosity\" : -1   },   \"control\" : {      \"verbosity\" : -1   },   \"geo\" : {      \"verbosity\" : -1   },   \"index\" : {      \"verbosity\" : -1   },   \"network\" : {      \"verbosity\" : -1   },   \"query\" : {      \"verbosity\" : 2   },   \"replication\" : {      \"verbosity\" : -1,      \"election\" : {         \"verbosity\" : -1      },      \"heartbeats\" : {         \"verbosity\" : -1      },      \"initialSync\" : {         \"verbosity\" : -1      },      \"rollback\" : {         \"verbosity\" : -1      }   },   \"sharding\" : {      \"verbosity\" : -1   },   \"storage\" : {      \"verbosity\" : 2,      \"recovery\" : {         \"verbosity\" : -1      },      \"journal\" : {         \"verbosity\" : -1      }   },   \"write\" : {      \"verbosity\" : -1   }}  To modify these settings, you can configure the systemLog.verbosity and systemLog.component.<name>.verbosity settings in the configuration file or set the logComponentVerbosity parameter using the setParameter command or use the db.setLogLevel() method. For examples, see Configure Log Verbosity Levels. ←  db.getCollectionNames()db.getMongo() → On this page  * Definition\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.runCommand/": " Docs Home → MongoDB Manual \nDB.RUNCOMMAND() \nOn this page    \n * Definition\n   \n * Behavior\n * Response \nDEFINITION \ndb.runCommand(command) \nProvides a helper to run specified database commands. This is the preferred method to issue database commands, as it provides a consistent interface between the shell and drivers. Parameter\nType\nDescription\ncommand\ndocument or string\n\"A database command, specified either in document form or as a string. If specified as a string, db.runCommand() transforms the string into a document.\" To specify a time limit in milliseconds, see Terminate Running Operations. \nBEHAVIOR \ndb.runCommand() runs the command in the context of the current database. Some commands are only applicable in the context of the admin database, and you must change your db object to before running these commands or use db.adminCommand(). \nRESPONSE \nThe method returns a response document that contains the following fields: Field\nDescription\n<command result>\nResult fields specific to the command that was run.\nok\nA number that indicates whether the command has succeeded (1) or failed (0).\noperationTime The logical time of the operation. MongoDB uses the logical time to order operations. Only for replica sets and sharded clusters. If the command does not generate an oplog entry, e.g. a read operation, then the operation does not advance the logical clock. In this case, operationTime returns:      * For read concern \"local\", the timestamp of the most recent entry in the oplog.  * For read concern \"majority\" and \"linearizable\", the timestamp of the most recent majority-acknowledged entry in the oplog. For operations associated with causally consistent sessions, MongoDB drivers use this time to automatically set the Read Operations and afterClusterTime. $clusterTime A document that returns the signed cluster time. Cluster time is a logical time used for ordering of operations. Only for replica sets and sharded clusters. For internal use only. The document contains the following fields:  * clusterTime: timestamp of the highest known cluster time for the member.  * signature: a document that contains the hash of the cluster time and the id of the key used to sign the cluster time. ←  db.rotateCertificates()db.serverBuildInfo() → On this page  * Definition\n * Behavior\n * Response Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.deleteOne/": " Docs Home → MongoDB Manual \nDB.COLLECTION.DELETEONE() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.deleteOne()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the delete command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Removes a single document from a collection. db.collection.deleteOne(   <filter>,   {      writeConcern: <document>,      collation: <document>,      hint: <document|string>        // Available starting in MongoDB 4.4   }) \nParameter\nType\nDescription\nfilter\ndocument Specifies deletion criteria using query operators. Specify an empty document { } to delete the first document returned in the collection. writeConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. hint\ndocument Optional. A document or string that specifies the index to use to support the query predicate. The option can take an index specification document or the index name string. If you specify an index that does not exist, the operation errors. For an example, see Specify hint for Delete Operations. New in version 4.4. Returns:A document containing:  * A boolean acknowledged as true if the operation ran with write concern or false if write concern was disabled  * deletedCount containing the number of deleted documents \nBEHAVIOR  DELETION ORDER \ndb.collection.deleteOne() deletes the first document that matches the filter. Use a field that is part of a unique index such as _id for precise deletions. \nTIME SERIES COLLECTIONS \ndb.collection.deleteOne() returns a WriteError exception if used on a time series collection. \nSHARDED COLLECTIONS \ndb.collection.deleteOne() operations on a sharded collection must include the shard key or the _id field in the query specification. db.collection.deleteOne() operations in a sharded collection which do not contain either the shard key or the _id field return an error. \nTRANSACTIONS \ndb.collection.deleteOne() can be used inside multi-document transactions. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. \nEXAMPLES \n\n \nThe orders collection has documents with the following structure: {   _id: ObjectId(\"563237a41a4d68582c2509da\"),   stock: \"Brent Crude Futures\",   qty: 250,   type: \"buy-limit\",   limit: 48.90,   creationts: ISODate(\"2015-11-01T12:30:15Z\"),   expiryts: ISODate(\"2015-11-01T12:35:15Z\"),   client: \"Crude Traders Inc.\"}  The following operation deletes the order with _id: ObjectId(\"563237a41a4d68582c2509da\") : try {   db.orders.deleteOne( { \"_id\" : ObjectId(\"563237a41a4d68582c2509da\") } );} catch (e) {   print(e);}  The operation returns: { \"acknowledged\" : true, \"deletedCount\" : 1 }  The following operation deletes the first document with expiryts greater than ISODate(\"2015-11-01T12:40:15Z\") try {   db.orders.deleteOne( { \"expiryts\" : { $lt: ISODate(\"2015-11-01T12:40:15Z\") } } );} catch (e) {   print(e);}  The operation returns: { \"acknowledged\" : true, \"deletedCount\" : 1 }  \nDELETEONE() WITH WRITE CONCERN \nGiven a three member replica set, the following operation specifies a w of majority, wtimeout of 100: try {   db.orders.deleteOne(       { \"_id\" : ObjectId(\"563237a41a4d68582c2509da\") },       { w : \"majority\", wtimeout : 100 }   );} catch (e) {   print (e);}  If the acknowledgement takes longer than the wtimeout limit, the following exception is thrown: WriteConcernError({   \"code\" : 64,   \"errmsg\" : \"waiting for replication timed out\",   \"errInfo\" : {     \"wtimeout\" : true,     \"writeConcern\" : {    // Added in MongoDB 4.4       \"w\" : \"majority\",       \"wtimeout\" : 100,       \"provenance\" : \"getLastErrorDefaults\"     }   }})  \nTIP \nSEE ALSO: WriteResult.writeConcernError \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents: { _id: 1, category: \"café\", status: \"A\" }{ _id: 2, category: \"cafe\", status: \"a\" }{ _id: 3, category: \"cafE\", status: \"a\" }  The following operation includes the collation option: db.myColl.deleteOne(   { category: \"cafe\", status: \"A\" },   { collation: { locale: \"fr\", strength: 1 } })  \nSPECIFY HINT FOR DELETE OPERATIONS \nNew in version 4.4. In mongosh, create a members collection with the following documents:\n\n  Create the following indexes on the collection: db.members.createIndex( { status: 1 } )db.members.createIndex( { points: 1 } )  The following delete operation explicitly hints to use the index { status: 1 }: db.members.deleteOne(   { \"points\": { $lte: 20 }, \"status\": \"P\" },   { hint: { status: 1 } })  \nNOTE If you specify an index that does not exist, the operation errors. The delete command returns the following: { \"acknowledged\" : true, \"deletedCount\" : 1 }  To view the indexes used, you can use the $indexStats pipeline: db.members.aggregate( [ { $indexStats: { } }, { $sort: { name: 1 } } ] )  The accesses.ops field in the $indexStats output indicates the number of operations that used the index. \nTIP \nSEE ALSO: To delete multiple documents, see db.collection.deleteMany() ←  db.collection.dataSize()db.collection.deleteMany() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.updateOne/": " Docs Home → MongoDB Manual \nDB.COLLECTION.UPDATEONE() \nOn this page    \n * Definition\n   \n * Syntax\n * Access Control\n * Behavior\n * Examples \nDEFINITION \ndb.collection.updateOne(filter, update, options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the update command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Updates a single document within the collection based on the filter. \nSYNTAX \nThe updateOne() method has the following syntax: db.collection.updateOne(   <filter>,   <update>,   {     upsert: <boolean>,     writeConcern: <document>,     collation: <document>,     arrayFilters: [ <filterdocument1>, ... ],     hint:  <document|string>        // Available starting in MongoDB 4.2.1   })  \nPARAMETERS \nThe db.collection.updateOne() method takes the following parameters: Parameter\nType\nDescription\nfilter\ndocument The selection criteria for the update. The same query selectors as in the find() method are available. Specify an empty document { } to update the first document returned in the collection. update\ndocument or pipeline The modifications to apply. Can be one of the following: Update document Contains only update operator expressions. For more information, see Update with an Update Operator Expressions Document Aggregation pipeline (Starting in MongoDB 4.2) Contains only the following aggregation stages:  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. For more information, see Update with an Aggregation Pipeline. To update with a replacement document, see db.collection.replaceOne(). upsert\nboolean Optional. When true, updateOne() either:  * Creates a new document if no documents match the filter. For more details see upsert behavior.  * Updates a single document that matches the filter. To avoid multiple upserts, ensure that the filter field(s) are uniquely indexed. Defaults to false, which does not insert a new document when no match is found. writeConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. arrayFilters\narray Optional. An array of filter documents that determine which array elements to modify for an update operation on an array field. In the update document, use the $[<identifier>] filtered positional operator to define an identifier, which you then reference in the array filter documents. You cannot have an array filter document for an identifier if the identifier is not included in the update document. \nNOTE The <identifier> must begin with a lowercase letter and contain only alphanumeric characters.\n\n // INVALID\n[  { \"x.a\": { $gt: 85 } },  { \"x.b\": { $gt: 80 } }] However, you can specify compound conditions on the same identifier in a single filter document, such as in the following examples: // Example 1[  { $or: [{\"x.a\": {$gt: 85}}, {\"x.b\": {$gt: 80}}] }]// Example 2[  { $and: [{\"x.a\": {$gt: 85}}, {\"x.b\": {$gt: 80}}] }]// Example 3[  { \"x.a\": { $gt: 85 }, \"x.b\": { $gt: 80 } }]  For examples, see Specify arrayFilters for an Array Update Operations. hint\nDocument or string Optional. A document or string that specifies the index to use to support the query predicate. The option can take an index specification document or the index name string. If you specify an index that does not exist, the operation errors. For an example, see Specify hint for Update Operations. New in version 4.2.1. \nRETURNS \nThe method returns a document that contains:  * matchedCount containing the number of matched documents  * modifiedCount containing the number of modified documents  * upsertedId containing the _id for the upserted document.  * A boolean acknowledged as true if the operation ran with write concern or false if write concern was disabled \nACCESS CONTROL \nOn deployments running with authorization, the user must have access that includes the following privileges:  * update action on the specified collection(s).  * find action on the specified collection(s).  * insert action on the specified collection(s) if the operation results in an upsert. The built-in role readWrite provides the required privileges. \nBEHAVIOR  UPDATES A SINGLE DOCUMENT \ndb.collection.updateOne() finds the first document that matches the filter and applies the specified update modifications. \nUPDATE WITH AN UPDATE OPERATOR EXPRESSIONS DOCUMENT \nFor the update specifications, the db.collection.updateOne() method can accept a document that only contains update operator expressions. For example: db.collection.updateOne(   <query>,   { $set: { status: \"D\" }, $inc: { quantity: 2 } },   ...) \nUPDATE WITH AN AGGREGATION PIPELINE \nStarting in MongoDB 4.2, the db.collection.updateOne() method can accept an aggregation pipeline [ <stage1>, <stage2>, ... ] that specifies the modifications to perform. The pipeline can consist of the following stages:  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. Using the aggregation pipeline allows for a more expressive update statement, such as expressing conditional updates based on current field values or updating one field using the value of another field(s). For example: db.collection.updateOne(   <query>,   [      { $set: { status: \"Modified\", comments: [ \"$misc1\", \"$misc2\" ] } },      { $unset: [ \"misc1\", \"misc2\" ] }   ]   ...) \nNOTE The $set and $unset used in the pipeline refers to the aggregation stages $set and $unset respectively, and not the update operators $set and $unset. For examples, see Update with Aggregation Pipeline. \nUPSERT \nIf upsert: true and no documents match the filter, db.collection.updateOne() creates a new document based on the filter criteria and update modifications. See Update with Upsert. If you specify upsert: true on a sharded collection, you must include the full shard key in the filter. For additional db.collection.updateOne() behavior on a sharded collection, see Sharded Collections. \nCAPPED COLLECTION \nIf an update operation changes the document size, the operation will fail. \nSHARDED COLLECTIONS \nUPSERT ON A SHARDED COLLECTION \nTo use db.collection.updateOne() on a sharded collection:  * If you don't specify upsert: true, you must include an exact match on the _id field or target a single shard (such as by including the shard key in the filter).  * If you specify upsert: true, the filter must include the shard key.\n\n { _id: <value>, <shardkeyfield>: null } // _id of the document missing shard key  SHARD KEY MODIFICATION \nStarting in MongoDB 4.2, you can update a document's shard key value unless the shard key field is the immutable _id field. In MongoDB 4.2 and earlier, a document's shard key field value is immutable. \nWARNING Starting in version 4.4, documents in sharded collections can be missing the shard key fields. Take precaution to avoid accidentally removing the shard key when changing a document's shard key value. To modify the existing shard key value with db.collection.updateOne():  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write.  * You must include an equality filter on the full shard key. See also upsert on a Sharded Collection. MISSING SHARD KEY \nStarting in version 4.4, documents in a sharded collection can be missing the shard key fields. To use db.collection.updateOne() to set the document's missing shard key, you must run on a mongos. Do not issue the operation directly on the shard. In addition, the following requirements also apply: Task\nRequirements\nTo set to null  * Requires equality filter on the full shard key if upsert: true. To set to a non-null value  * Must be performed either inside a transaction or as a retryable write.  * Requires equality filter on the full shard key if upsert: true. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also:  * upsert on a Sharded Collection  * Missing Shard Key Fields \nEXPLAINABILITY \nupdateOne() is not compatible with db.collection.explain(). \nTRANSACTIONS \ndb.collection.updateOne() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. UPSERT WITHIN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, db.collection.updateOne() with upsert: true can be run on an existing collection or a non-existing collection. If run on a non-existing collection, the operation creates the collection. In MongoDB 4.2 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES  UPDATE USING UPDATE OPERATOR EXPRESSIONS \nThe restaurant collection contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\" },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : 0 }  The following operation updates a single document where name: \"Central Perk Cafe\" with the violations field: try {   db.restaurant.updateOne(      { \"name\" : \"Central Perk Cafe\" },      { $set: { \"violations\" : 3 } }   );} catch (e) {   print(e);}  The operation returns: { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }  If no matches were found, the operation instead returns: { \"acknowledged\" : true, \"matchedCount\" : 0, \"modifiedCount\" : 0 } \n\n \nUPDATE WITH AGGREGATION PIPELINE \nStarting in MongoDB 4.2, the db.collection.updateOne() can use an aggregation pipeline for the update. The pipeline can consist of the following stages:  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. Using the aggregation pipeline allows for a more expressive update statement, such as expressing conditional updates based on current field values or updating one field using the value of another field(s). EXAMPLE 1 \nThe following examples uses the aggregation pipeline to modify a field using the values of the other fields in the document. Create a members collection with the following documents: db.members.insertMany( [    { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"A\", \"points\" : 2, \"misc1\" : \"note to self: confirm status\", \"misc2\" : \"Need to activate\", \"lastUpdate\" : ISODate(\"2019-01-01T00:00:00Z\") },    { \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60, comments: [ \"reminder: ping me at 100pts\", \"Some random comment\" ], \"lastUpdate\" : ISODate(\"2019-01-01T00:00:00Z\") }] )  Assume that instead of separate misc1 and misc2 fields in the first document, you want to gather these into a comments field, like the second document. The following update operation uses an aggregation pipeline to:  * add the new comments field and set the lastUpdate field.  * remove the misc1 and misc2 fields for all documents in the collection. db.members.updateOne(   { _id: 1 },   [      { $set: { status: \"Modified\", comments: [ \"$misc1\", \"$misc2\" ], lastUpdate: \"$$NOW\" } },      { $unset: [ \"misc1\", \"misc2\" ] }   ])  \nNOTE The $set and $unset used in the pipeline refers to the aggregation stages $set and $unset respectively, and not the update operators $set and $unset. First Stage The $set stage:  * creates a new array field comments whose elements are the current content of the misc1 and misc2 fields and  * sets the field lastUpdate to the value of the aggregation variable NOW. The aggregation variable NOW resolves to the current datetime value and remains the same throughout the pipeline. To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes. Second StageThe $unset stage removes the misc1 and misc2 fields. After the command, the collection contains the following documents: { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"Modified\", \"points\" : 2, \"lastUpdate\" : ISODate(\"2020-01-23T05:21:59.321Z\"), \"comments\" : [ \"note to self: confirm status\", \"Need to activate\" ] }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60, \"comments\" : [ \"reminder: ping me at 100pts\", \"Some random comment\" ], \"lastUpdate\" : ISODate(\"2019-01-01T00:00:00Z\") }  EXAMPLE 2 \nThe aggregation pipeline allows the update to perform conditional updates based on the current field values as well as use current field values to calculate a separate field value. For example, create a students3 collection with the following documents: db.students3.insertMany( [   { \"_id\" : 1, \"tests\" : [ 95, 92, 90 ], \"average\" : 92, \"grade\" : \"A\", \"lastUpdate\" : ISODate(\"2020-01-23T05:18:40.013Z\") },   { \"_id\" : 2, \"tests\" : [ 94, 88, 90 ], \"average\" : 91, \"grade\" : \"A\", \"lastUpdate\" : ISODate(\"2020-01-23T05:18:40.013Z\") },   { \"_id\" : 3, \"tests\" : [ 70, 75, 82 ], \"lastUpdate\" : ISODate(\"2019-01-01T00:00:00Z\") }] ) \n\n db.students3.updateOne(   { _id: 3 },   [     { $set: { average: { $trunc: [  { $avg: \"$tests\" }, 0 ] }, lastUpdate: \"$$NOW\" } },     { $set: { grade: { $switch: {                           branches: [                               { case: { $gte: [ \"$average\", 90 ] }, then: \"A\" },                               { case: { $gte: [ \"$average\", 80 ] }, then: \"B\" },                               { case: { $gte: [ \"$average\", 70 ] }, then: \"C\" },                               { case: { $gte: [ \"$average\", 60 ] }, then: \"D\" }                           ],                           default: \"F\"     } } } }   ])  \nNOTE The $set used in the pipeline refers to the aggregation stage $set, and not the update operators $set. First Stage The $set stage:  * calculates a new field average based on the average of the tests field. See $avg for more information on the $avg aggregation operator and $trunc for more information on the $trunc truncate aggregation operator.  * sets the field lastUpdate to the value of the aggregation variable NOW. The aggregation variable NOW resolves to the current datetime value and remains the same throughout the pipeline. To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes. Second StageThe $set stage calculates a new field grade based on the average field calculated in the previous stage. See $switch for more information on the $switch aggregation operator. After the command, the collection contains the following documents: { \"_id\" : 1, \"tests\" : [ 95, 92, 90 ], \"average\" : 92, \"grade\" : \"A\", \"lastUpdate\" : ISODate(\"2020-01-23T05:18:40.013Z\") }{ \"_id\" : 2, \"tests\" : [ 94, 88, 90 ], \"average\" : 91, \"grade\" : \"A\", \"lastUpdate\" : ISODate(\"2020-01-23T05:18:40.013Z\") }{ \"_id\" : 3, \"tests\" : [ 70, 75, 82 ], \"lastUpdate\" : ISODate(\"2020-01-24T17:33:30.674Z\"), \"average\" : 75, \"grade\" : \"C\" }  \nTIP \nSEE ALSO: Updates with Aggregation Pipeline \nUPDATE WITH UPSERT \nThe restaurant collection contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\", \"violations\" : 3 },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : \"0\" }  The following operation attempts to update the document with name : \"Pizza Rat's Pizzaria\", while upsert: true : try {   db.restaurant.updateOne(      { \"name\" : \"Pizza Rat's Pizzaria\" },      { $set: {\"_id\" : 4, \"violations\" : 7, \"borough\" : \"Manhattan\" } },      { upsert: true }   );} catch (e) {   print(e);}  Since upsert:true the document is inserted based on the filter and update criteria. The operation returns:\n\n  The collection now contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\", \"violations\" : 3 },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : 4 },{ \"_id\" : 4, \"name\" : \"Pizza Rat's Pizzaria\", \"Borough\" : \"Manhattan\", \"violations\" : 7 }  The name field was filled in using the filter criteria, while the update operators were used to create the rest of the document. The following operation updates the first document with violations that are greater than 10: try {   db.restaurant.updateOne(      { \"violations\" : { $gt: 10} },      { $set: { \"Closed\" : true } },      { upsert: true }   );} catch (e) {   print(e);}  The operation returns: {   \"acknowledged\" : true,   \"matchedCount\" : 0,   \"modifiedCount\" : 0,   \"upsertedId\" : ObjectId(\"56310c3c0c5cbb6031cafaea\")}  The collection now contains the following documents: { \"_id\" : 1, \"name\" : \"Central Perk Cafe\", \"Borough\" : \"Manhattan\", \"violations\" : 3 },{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"Borough\" : \"Queens\", \"violations\" : 2 },{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"Borough\" : \"Brooklyn\", \"violations\" : 4 },{ \"_id\" : 4, \"name\" : \"Pizza Rat's Pizzaria\", \"Borough\" : \"Manhattan\", \"grade\" : 7 }{ \"_id\" : ObjectId(\"56310c3c0c5cbb6031cafaea\"), \"Closed\" : true }  Since no documents matched the filter, and upsert was true, updateOne() inserted the document with a generated _id and the update criteria only. \nUPDATE WITH WRITE CONCERN \nGiven a three member replica set, the following operation specifies a w of majority, wtimeout of 100: try {   db.restaurant.updateOne(       { \"name\" : \"Pizza Rat's Pizzaria\" },       { $inc: { \"violations\" : 3}, $set: { \"Closed\" : true } },       { w: \"majority\", wtimeout: 100 }   );} catch (e) {   print(e);}  If the primary and at least one secondary acknowledge each write operation within 100 milliseconds, it returns: { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }  If the acknowledgement takes longer than the wtimeout limit, the following exception is thrown: Changed in version 4.4. WriteConcernError({   \"code\" : 64,   \"errmsg\" : \"waiting for replication timed out\",   \"errInfo\" : {     \"wtimeout\" : true,     \"writeConcern\" : {       \"w\" : \"majority\",       \"wtimeout\" : 100,       \"provenance\" : \"getLastErrorDefaults\"     }   }})  The following table explains the possible values of errInfo.writeConcern.provenance: Provenance\nDescription\nclientSupplied\nThe write concern was specified in the application.\ncustomDefault\nThe write concern originated from a custom defined default value. See setDefaultRWConcern.\ngetLastErrorDefaults\nThe write concern originated from the replica set's settings.getLastErrorDefaults field.\nimplicitDefault\nThe write concern originated from the server in absence of all other write concern specifications. \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents:\n\n  The following operation includes the collation option: db.myColl.updateOne(   { category: \"cafe\" },   { $set: { status: \"Updated\" } },   { collation: { locale: \"fr\", strength: 1 } });  \nSPECIFY ARRAYFILTERS FOR AN ARRAY UPDATE OPERATIONS \nStarting in MongoDB 3.6, when updating an array field, you can specify arrayFilters that determine which array elements to update. UPDATE ELEMENTS MATCH ARRAYFILTERS CRITERIA \nCreate a collection students with the following documents: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] },   { \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] },   { \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }] )  To modify all elements that are greater than or equal to 100 in the grades array, use the filtered positional operator $[<identifier>] with the arrayFilters option in the db.collection.updateOne() method: db.students.updateOne(   { grades: { $gte: 100 } },   { $set: { \"grades.$[element]\" : 100 } },   { arrayFilters: [ { \"element\": { $gte: 100 } } ] })  The operation updates the grades field of a single document, and after the operation, the collection has the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 100 ] }{ \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }  UPDATE SPECIFIC ELEMENTS OF AN ARRAY OF DOCUMENTS \nCreate a collection students2 with the following documents: db.students2.insertMany( [   {      \"_id\" : 1,      \"grades\" : [         { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 85, \"mean\" : 90, \"std\" : 4 },         { \"grade\" : 85, \"mean\" : 85, \"std\" : 6 }      ]   },   {      \"_id\" : 2,      \"grades\" : [         { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },         { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }      ]   }] )  To modify the value of the mean field for all elements in the grades array where the grade is greater than or equal to 85, use the filtered positional operator $[<identifier>] with the arrayFilters in the db.collection.updateOne() method: db.students2.updateOne(   { },   { $set: { \"grades.$[elem].mean\" : 100 } },   { arrayFilters: [ { \"elem.grade\": { $gte: 85 } } ] })  The operation updates the array of a single document, and after the operation, the collection has the following documents: {   \"_id\" : 1,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 6 }    ]}{   \"_id\" : 2,   \"grades\" : [      { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },      { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }   ]}  \nSPECIFY HINT FOR UPDATE OPERATIONS \nNew in version 4.2.1. Create a sample members collection with the following documents:\n\n  Create the following indexes on the collection: db.members.createIndex( { status: 1 } )db.members.createIndex( { points: 1 } )  The following update operation explicitly hints to use the index { status: 1 }: \nNOTE If you specify an index that does not exist, the operation errors. db.members.updateOne(   { \"points\": { $lte: 20 }, \"status\": \"P\" },   { $set: { \"misc1\": \"Need to activate\" } },   { hint: { status: 1 } })  The update command returns the following: { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }  To view the indexes used, you can use the $indexStats pipeline: db.members.aggregate( [ { $indexStats: { } }, { $sort: { name: 1 } } ] )  \nTIP \nSEE ALSO: To update multiple documents, see db.collection.updateMany(). ←  db.collection.update()db.collection.updateMany() → On this page  * Definition\n * Syntax\n * Access Control\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.revokeRolesFromUser/": " Docs Home → MongoDB Manual \nDB.REVOKEROLESFROMUSER() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.revokeRolesFromUser() \nRemoves one or more roles from a user on the current database. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the revokeRolesFromUser command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.revokeRolesFromUser() method uses the following syntax: db.revokeRolesFromUser( \"<username>\", [ <roles> ], { <writeConcern> } )  The db.revokeRolesFromUser() method takes the following arguments: Parameter\nType\nDescription\nuser\nstring\nThe name of the user from whom to revoke roles.\nroles\narray\nThe roles to remove from the user.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. In the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where db.revokeRolesFromUser() runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. The db.revokeRolesFromUser() method wraps the revokeRolesFromUser command. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.revokeRolesFromUser() is executed using \"majority\" write concern by default. \nREQUIRED ACCESS \nYou must have the revokeRole action on a database to revoke a role on that database. \nEXAMPLE \nThe accountUser01 user in the products database has the following roles: \"roles\" : [    { \"role\" : \"assetsReader\",      \"db\" : \"assets\"    },    { \"role\" : \"read\",      \"db\" : \"stock\"    },    { \"role\" : \"readWrite\",      \"db\" : \"products\"    }]  The following db.revokeRolesFromUser() method removes the two of the user's roles: the read role on the stock database and the readWrite role on the products database, which is also the database on which the method runs: use productsdb.revokeRolesFromUser( \"accountUser01\",                        [ { role: \"read\", db: \"stock\" }, \"readWrite\" ],                        { w: \"majority\" }                      )  The user accountUser01 user in the products database now has only one remaining role: \"roles\" : [    { \"role\" : \"assetsReader\",      \"db\" : \"assets\"    }] \n←  db.removeUser()db.updateUser() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.noCursorTimeout/": " Docs Home → MongoDB Manual \nCURSOR.NOCURSORTIMEOUT()  DEFINITION \ncursor.noCursorTimeout()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Instructs the server to avoid closing a cursor automatically after a period of inactivity. The noCursorTimeout() method has the following prototype form: db.collection.find(<query>).noCursorTimeout()  \nBEHAVIOR  SESSION IDLE TIMEOUT OVERRIDES NOCURSORTIMEOUT \nMongoDB drivers and mongosh associate all operations with a server session, with the exception of unacknowledged write operations. For operations not explicitly associated with a session (i.e. using Mongo.startSession()), MongoDB drivers and mongosh create an implicit session and associate it with the operation. If a session is idle for longer than 30 minutes, the MongoDB server marks that session as expired and may close it at any time. When the MongoDB server closes the session, it also kills any in-progress operations and open cursors associated with the session. This includes cursors configured with noCursorTimeout() or a maxTimeMS() greater than 30 minutes. Consider an application that issues a db.collection.find() with cursor.noCursorTimeout(). The server returns a cursor along with a batch of documents defined by the cursor.batchSize() of the find(). The session refreshes each time the application requests a new batch of documents from the server. However, if the application takes longer than 30 minutes to process the current batch of documents, the session is marked as expired and closed. When the server closes the session, it also kills the cursor despite the cursor being configured with noCursorTimeout(). When the application requests the next batch of documents, the server returns an error. For operations that return a cursor, if the cursor may be idle for longer than 30 minutes, issue the operation within an explicit session using Mongo.startSession() and periodically refresh the session using the refreshSessions command. For example: var session = db.getMongo().startSession()var sessionId = session.getSessionId().id\nvar cursor = session.getDatabase(\"examples\").getCollection(\"data\").find().noCursorTimeout()var refreshTimestamp = new Date() // take note of time at operation start\nwhile (cursor.hasNext()) {\n  // Check if more than 5 minutes have passed since the last refresh  if ( (new Date()-refreshTimestamp)/1000 > 300 ) {    print(\"refreshing session\")    db.adminCommand({\"refreshSessions\" : [sessionId]})    refreshTimestamp = new Date()  }\n  // process cursor normally\n}  In the example operation, the db.collection.find() method is associated with an explicit session. The cursor is configured with cursor.noCursorTimeout() to prevent the server from closing the cursor if idle. The while loop includes a block that uses refreshSessions to refresh the session every 5 minutes. Since the session will never exceed the 30 minute idle timeout, the cursor can remain open indefinitely. For MongoDB drivers, defer to the driver documentation for instructions and syntax for creating sessions. ←  cursor.next()cursor.objsLeftInBatch() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getCollection/": " Docs Home → MongoDB Manual \nDB.GETCOLLECTION() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \ndb.getCollection(name) \nReturns a collection or a view object that is functionally equivalent to using the db.<collectionName> syntax. The method is useful for a collection or a view whose name might interact with mongosh itself, such as names that begin with _ or that match a database shell method. The db.getCollection() method has the following parameter: Parameter\nType\nDescription\nname\nstring\nThe name of the collection. \nBEHAVIOR \nThe db.getCollection() object can access any collection methods. The collection specified may or may not exist on the server. If the collection does not exist, MongoDB creates it implicitly as part of write operations like db.collection.insertOne(). \nEXAMPLE \nThe following example uses db.getCollection() to access the auth collection and insert a document into it. var authColl = db.getCollection(\"auth\")\nauthColl.insertOne(   {       usrName : \"John Doe\",       usrDept : \"Sales\",       usrTitle : \"Executive Account Manager\",       authLevel : 4,       authDept : [ \"Sales\", \"Customers\"]   })  This returns: {   \"acknowledged\" : true,   \"insertedId\" : ObjectId(\"569525e144fe66d60b772763\")}  The previous example requires the use of db.getCollection(\"auth\") because of a name conflict with the database method db.auth(). Calling db.auth directly to perform an insert operation would reference the db.auth() method and would error. The following example attempts the same operation, but without using the db.getCollection() method: db.auth.insertOne(   {       usrName : \"John Doe\",       usrDept : \"Sales\",       usrTitle : \"Executive Account Manager\",       authLevel : 4,       authDept : [ \"Sales\", \"Customers\"]   })  The operation errors as db.auth() method has no insertOne method. \nTIP \nSEE ALSO: Collection Methods ←  db.fsyncUnlock()db.getCollectionInfos() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-native/": " Docs Home → MongoDB Manual \nNATIVE METHODS IN MONGOSH \nThe methods listed in this section are mongosh functional replacements for the native methods that were available in the legacy mongo shell. These methods are not exact replacements: output formats and some functionality may differ from the corresponding legacy methods. In addition to these methods, the mongocompat snippet provides access to legacy mongo shell APIs. \nNOTE In the following table <path> and <filename> are strings and should be in quotes. // process.chdir( <path> )\nprocess.chdir( \"./data/incoming\" ) Legacy Method Name\nReplacement\ncat() Returns the contents of the specified file fs.readFileSync( <filename>, 'utf8' )  The legacy useBinaryMode option is not supported. Emulate the useBinaryMode = false option with: fs.readFileSync( <filename>, 'utf8' ).replace( /\\r\\n/g, '\\n' ) \ncd() Changes the current working directory to the specified path. process.chdir( <path> ) \ngetHostName() Returns the hostname of the system running mongosh. os.hostname() \ngetMemInfo() Returns a document that reports memory used by the shell. process.memoryUsage() \nhostname() Returns the hostname of the computer running the shell. os.hostname() \nisInteractive() Returns a boolean indicating whether mongosh is running in interactive or script mode. isInteractive() \nlistFiles() Returns an array of documents that give the name and type of each object in the directory. fs.readdirSync( <path>, { withFileTypes: true } ) \nload() Loads and runs a JavaScript file in the shell. load() is available in mongosh. See also Differences Between require() and load(). ls() Returns a list of the files in the current directory. fs.readdirSync( <path> ) \nmd5sumFile() Returns the md5 hash of the specified file. crypto.createHash( 'md5' ).update( fs.readFileSync( <filename> ) ).digest( 'hex' ) \nmkdir() Creates a directory at the specified path. fs.mkdirSync( <path>, { recursive: true } ) \npwd() Returns the current directory. process.cwd() \nquit() Exits the current shell session. quit() \nremoveFile() Removes the specified file from the local file system. fs.unlinkSync( <filename> ) \nsleep() Sleep for the specified number of milliseconds. sleep( <number> ) \nversion() Returns the current version of mongosh instance. version() \n_isWindows() Returns true if the shell in running on Windows. process.platform === 'win32' \n_rand() Returns a random number between 0 and 1. Math.random()  What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.count/": " Docs Home → MongoDB Manual \nDB.COLLECTION.COUNT() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.count(query, options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the count command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 \nNOTE MongoDB drivers compatible with the 4.0 features deprecate their respective cursor and collection count() APIs in favor of new APIs for countDocuments() and estimatedDocumentCount(). For the specific API names for a given driver, see the driver documentation. Returns the count of documents that would match a find() query for the collection or view. The db.collection.count() method does not perform the find() operation but instead counts and returns the number of results that match a query. Parameter\nType\nDescription\nquery\ndocument\nThe query selection criteria.\noptions\ndocument\nOptional. Extra options for modifying the count. The options document contains the following fields: Field\nType\nDescription\nlimit\ninteger\nOptional. The maximum number of documents to count.\nskip\ninteger\nOptional. The number of documents to skip before counting.\nhint\nstring or document\nOptional. An index name hint or specification for the query.\nmaxTimeMS\ninteger\nOptional. The maximum amount of time to allow the query to run.\nreadConcern\nstring Optional. Specifies the read concern. The default level is \"local\". To ensure that a single thread can read its own writes, use \"majority\" read concern and \"majority\" write concern against the primary of the replica set. To use a read concern level of \"majority\", you must specify a nonempty query condition. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. count() is equivalent to the db.collection.find(query).count() construct. \nTIP \nSEE ALSO:  * cursor.count()  * db.collection.estimatedDocumentCount()  * db.collection.countDocuments() \nBEHAVIOR  INACCURATE COUNTS WITHOUT QUERY PREDICATE \nWhen you call count() without a query predicate, you may receive inaccurate document counts. Without a query predicate, count() methods return results based on the collection's metadata, which may result in an approximate count. In particular,  * On a sharded cluster, the resulting count will not correctly filter out orphaned documents.  * After an unclean shutdown or file copy based initial sync, the count may be incorrect. For counts based on collection metadata, see also collStats pipeline stage with the count option. \nCOUNT AND TRANSACTIONS \nYou cannot use count and shell helpers count() and db.collection.count() in transactions. For details, see Transactions and Count Operations. \nSHARDED CLUSTERS \nOn a sharded cluster, db.collection.count() without a query predicate can result in an inaccurate count if orphaned documents exist or if a chunk migration is in progress. To avoid these situations, on a sharded cluster, use the db.collection.aggregate() method: You can use the $count stage to count the documents. For example, the following operation counts the documents in a collection:\n\n  The $count stage is equivalent to the following $group + $project sequence: db.collection.aggregate( [   { $group: { _id: null, count: { $sum: 1 } } }   { $project: { _id: 0 } }] )  \nTIP \nSEE ALSO: $collStats to return an approximate count based on the collection's metadata. \nINDEX USE \nConsider a collection with the following index: { a: 1, b: 1 }  When performing a count, MongoDB can return the count using only the index if:  * the query can use an index,  * the query only contains conditions on the keys of the index, and  * the query predicates access a single contiguous range of index keys. For example, the following operations can return the count using only the index: db.collection.find( { a: 5, b: 5 } ).count()db.collection.find( { a: { $gt: 5 } } ).count()db.collection.find( { a: 5, b: { $gt: 10 } } ).count()  If, however, the query can use an index but the query predicates do not access a single contiguous range of index keys or the query also contains conditions on fields outside the index, then in addition to using the index, MongoDB must also read the documents to return the count. db.collection.find( { a: 5, b: { $in: [ 1, 2, 3 ] } } ).count()db.collection.find( { a: { $gt: 5 }, b: 5 } ).count()db.collection.find( { a: 5, b: 5, c: 5 } ).count()  In such cases, during the initial read of the documents, MongoDB pages the documents into memory such that subsequent calls of the same count operation will have better performance. \nACCURACY AFTER UNEXPECTED SHUTDOWN \nAfter an unclean shutdown of a mongod using the Wired Tiger storage engine, count statistics reported by count() may be inaccurate. The amount of drift depends on the number of insert, update, or delete operations performed between the last checkpoint and the unclean shutdown. Checkpoints usually occur every 60 seconds. However, mongod instances running with non-default --syncdelay settings may have more or less frequent checkpoints. Run validate on each collection on the mongod to restore statistics after an unclean shutdown. After an unclean shutdown:  * validate updates the count statistic in the collStats output with the latest value.  * Other statistics like the number of documents inserted or removed in the collStats output are estimates. \nNOTE This loss of accuracy only applies to count() operations that do not include a query predicate. \nCLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued db.collection.count() disconnects before the operation completes, MongoDB marks db.collection.count() for termination using killOp. \nEXAMPLES  COUNT ALL DOCUMENTS IN A COLLECTION \nTo count the number of all documents in the orders collection, use the following operation: db.orders.count()  This operation is equivalent to the following: db.orders.find().count()  \nCOUNT ALL DOCUMENTS THAT MATCH A QUERY \nCount the number of the documents in the orders collection with the field ord_dt greater than new Date('01/01/2012'): db.orders.count( { ord_dt: { $gt: new Date('01/01/2012') } } )  The query is equivalent to the following: db.orders.find( { ord_dt: { $gt: new Date('01/01/2012') } } ).count() \n←  db.collection.bulkWrite()db.collection.countDocuments() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.createRole/": " Docs Home → MongoDB Manual \nDB.CREATEROLE() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.createRole(role, writeConcern) \nCreates a role in a database. You can specify privileges for the role by explicitly listing the privileges or by having the role inherit privileges from other roles or both. The role applies to the database on which you run the method. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the createRole command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.createRole() method accepts the following arguments: Parameter\nType\nDescription\nrole\ndocument\nA document containing the name of the role and the role definition.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. The role document has the following form: {  role: \"<name>\",  privileges: [     { resource: { <resource> }, actions: [ \"<action>\", ... ] },     ...  ],  roles: [     { role: \"<role>\", db: \"<database>\" } | \"<role>\",      ...  ],  authenticationRestrictions: [    {      clientSource: [\"<IP>\" | \"<CIDR range>\", ...],      serverAddress: [\"<IP>\" | \"<CIDR range>\", ...]    },    ...  ]}  The role document has the following fields: Field\nType\nDescription\nrole\nstring\nThe name of the new role.\nprivileges\narray The privileges to grant the role. A privilege consists of a resource and permitted actions. For the syntax of a privilege, see the privileges array. You must include the privileges field. Use an empty array to specify no privileges. roles\narray An array of roles from which this role inherits privileges. You must include the roles field. Use an empty array to specify no roles to inherit from. authenticationRestrictions\narray Optional. The authentication restrictions the server enforces on the role. Specifies a list of IP addresses and CIDR ranges users granted this role are allowed to connect to and/or which they can connect from. \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where db.createRole() runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field: Field Name\nValue\nDescription\nclientSource\nArray of IP addresses and/or CIDR ranges\nIf present, when authenticating a user, the server verifies that the client's IP address is either in the given list or belongs to a CIDR range in the list. If the client's IP address is not present, the server does not authenticate the user.\nserverAddress\nArray of IP addresses and/or CIDR ranges\nA list of IP addresses or CIDR ranges to which the client can connect. If present, the server will verify that the client's connection was accepted via an IP address in the given list. If the connection was accepted via an unrecognized IP address, the server does not authenticate the user. \nIMPORTANT If a user inherits multiple roles with incompatible authentication restrictions, that user becomes unusable. For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.createRole() is executed using \"majority\" write concern by default.\n\n \nExcept for roles created in the admin database, a role can only include privileges that apply to its database and can only inherit from other roles in its database. A role created in the admin database can include privileges that apply to the admin database, other databases or to the cluster resource, and can inherit from roles in other databases as well as the admin database. The db.createRole() method returns a duplicate role error if the role already exists in the database. \nREQUIRED ACCESS \nTo create a role in a database, you must have:  * the createRole action on that database resource.  * the grantRole action on that database to specify privileges for the new role as well as to specify roles to inherit from. Built-in roles userAdmin and userAdminAnyDatabase provide createRole and grantRole actions on their respective resources. To create a role with authenticationRestrictions specified, you must have the setAuthenticationRestriction action on the database resource which the role is created. \nEXAMPLE \nThe following db.createRole() method creates the myClusterwideAdmin role on the admin database: use admindb.createRole(   {     role: \"myClusterwideAdmin\",     privileges: [       { resource: { cluster: true }, actions: [ \"addShard\" ] },       { resource: { db: \"config\", collection: \"\" }, actions: [ \"find\", \"update\", \"insert\", \"remove\" ] },       { resource: { db: \"users\", collection: \"usersCollection\" }, actions: [ \"update\", \"insert\", \"remove\" ] },       { resource: { db: \"\", collection: \"\" }, actions: [ \"find\" ] }     ],     roles: [       { role: \"read\", db: \"admin\" }     ]   },   { w: \"majority\" , wtimeout: 5000 }) \n←  Role Management Methodsdb.dropRole() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.rotateCertificates/": " Docs Home → MongoDB Manual \nDB.ROTATECERTIFICATES() \nOn this page    \n * Definition\n   \n * Output\n * Behavior\n * Logging\n * Required Access\n * Example \nDEFINITION \nNew in version 5.0. db.rotateCertificates(message) \nRotates the currently used TLS certificates for a mongod or mongos to use the updated values for these certificates defined in the configuration file. db.rotateCertificates(message)  The db.rotateCertificates() method takes the following optional argument: Parameter\nType\nDescription\nmessage\nstring\noptional A message logged by the server to the log file and audit file. The db.rotateCertificates() method wraps the rotateCertificates command. \nOUTPUT \nThe db.rotateCertificates() method returns a document with the following field: Field\nType\nDescription\nok\nbool\nContains the command's execution status. true on success, or false if an error occurred. If false, an errmsg field is additionally provided with a detailed error message. \nBEHAVIOR \nRotation includes the following certificates:      * TLS Certificates  * CRL (Certificate Revocation List) files (on Linux and Windows platforms)  * CA (Certificate Authority) files To rotate one or more of these certificates:  1. Replace the certificate or certificates you wish to rotate on the filesystem, noting the following constraints:\n    \n    * Each new certificate must have the same filename and same filepath as the certificate it is replacing.\n    \n    * If rotating an encrypted TLS Certificate, its password must be the same as the password for the old certificate (as specified to the certificateKeyFilePassword configuration file setting). Certificate rotation does not support the interactive password prompt.  2. Connect mongosh to the mongod or mongos instance that you wish to perform certificate rotation on.  3. Run db.rotateCertificates() to rotate the certificates used by the the mongod or mongos instance. When certificate rotation takes place:  * Existing connections to the mongod or mongos instance are not terminated, and will continue to use the old certificates.  * Any new connections will use the new certificates. If you have configured OCSP for your deployment, the db.rotateCertificates() method will also fetch stapled OCSP responses during rotation. The db.rotateCertificates() method may be run on a running mongod or mongos regardless of replication status. Only one instance of db.rotateCertificates() or rotateCertificates may run on each mongod or mongos process at a time. Attempting to initiate a second instance while one is already running will result in an error. Incorrect, expired, revoked, or missing certificate files will cause the certificate rotation to fail, but will not invalidate the existing TLS configuration or terminate the running mongod or mongos process. If the mongod or mongos is running with --tlsCertificateSelector set to thumbprint, db.rotateCertificates() will fail and write a warning message to the log file. \nLOGGING \nOn successful rotation, the subject names, thumbprints, and the validity period of the server and cluster certificate thumbprints are logged to the configured log destination. If auditing is enabled, this information is also written to the audit log. On Linux and Windows platforms, if a CRL file is present, its thumbprint and validity period are also logged to these locations. \nREQUIRED ACCESS \nYou must have the rotateCertificates action in order to use the db.rotateCertificates() method. The rotateCertificates action is part of the hostManager role. \nEXAMPLE \nThe following operation rotates the certificates on a running mongod instance, after having made the appropriate updates to the configuration file to specify the updated certificate information: db.rotateCertificates()  The following performs the same as above, but also writes a custom log message at rotation time to the log file and audit file: db.rotateCertificates(\"message\": \"Rotating certificates\") \n←  db.resetError()db.runCommand() → On this page  * Definition\n * Output\n * Behavior\n * Logging\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.isCapped/": " Docs Home → MongoDB Manual \nDB.COLLECTION.ISCAPPED() \ndb.collection.isCapped()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the capped field returned by the collStats command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns:Returns true if the collection is a capped collection, otherwise returns false. \nTIP \nSEE ALSO: Capped Collections ←  db.collection.insertMany()db.collection.latencyStats() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.commandHelp/": " Docs Home → MongoDB Manual \nDB.COMMANDHELP() \nOn this page    \n * Description \nDESCRIPTION \ndb.commandHelp(command) \nDisplays help text for the specified database command. See the Database Commands. The db.commandHelp() method has the following parameter: Parameter\nType\nDescription\ncommand\nstring\nThe name of a database command. ←  db.aggregate()db.createCollection() → On this page  * Description Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.updateZoneKeyRange/": " Docs Home → MongoDB Manual \nSH.UPDATEZONEKEYRANGE() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \nsh.updateZoneKeyRange(namespace, minimum, maximum, zone) \nAssociates a range of shard key values with a zone. Starting in MongoDB 4.0.2, you can run updateZoneKeyRange database command and its helpers sh.updateZoneKeyRange() and sh.addTagRange() on an unsharded collection or a non-existing collection. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the updateZoneKeyRange command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 sh.updateZoneKeyRange() takes the following arguments: Parameter\nType\nDescription\nnamespace\nstring The namespace of the sharded collection associate with the zone. The collection must be sharded for the operation to succeed. minimum\ndocument The inclusive lower bound of the range of shard key values. Specify each field of the shard key in the form of <fieldname> : <value>. The value must be of the same BSON type or types as the shard key. \nNOTE To use hashed sharding, the field value needs to be of type NumberLong. maximum\ndocument The exclusive upper bound of the range of shard key values. Specify each field of the shard key in the form of <fieldname> : <value>. The value must be of the same BSON type or types as the shard key. \nNOTE To use hashed sharding, the field value needs to be of type NumberLong. zone\nstring\nThe name of the zone to associate with the range of shard key values bounded by minimum and maximum. Only issue sh.updateZoneKeyRange() when connected to a mongos instance. \nBEHAVIOR \nYou cannot create a range of shard key values whose lower and upper boundaries overlap with an existing range for the sharded collection. For example, given an existing range of 1 to 10, you cannot create a new range of 5 to 20, as the new range would overlap with the existing range. A zone can have multiple ranges of data associated with it, but a range can at most be associated with a single zone. See the zone manual page for more information on zones in sharded clusters. \nINITIAL CHUNK DISTRIBUTION FOR EMPTY OR NON-EXISTING COLLECTIONS \nIf you are considering performing zone sharding on an empty or non-existent collection, use sh.updateZoneKeyRange() to create the zones and zone ranges before sharding the collection (since 4.0.2). Starting in version 4.0.3, creating zones and zone ranges on empty or non-existing collections allows MongoDB to optimize the initial chunk creation and distribution process when sharding the collection. This optimized process supports faster setup of zoned sharding with less balancer overhead than creating zones after sharding. The balancer performs all chunk management after the optimized initial chunk creation and distribution. For an example of defining zones and zone ranges for initial chunk distribution, see Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection. INITIAL CHUNK DISTRIBUTION WITH COMPOUND HASHED SHARD KEYS \nStarting in version 4.4, MongoDB supports sharding collections on compound hashed indexes. MongoDB can perform optimized initial chunk creation and distribution when sharding the empty or non-existing collection on a compound hashed shard key.  \nFor a more complete example of defining zones and zone ranges for initial chunk distribution on a compound hashed shard key, see Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection. \nTIP \nSEE ALSO: sh.balancerCollectionStatus() \nBALANCER \nAfter associating a range to a zone, the balancer must first run in order to migrate any chunks whose ranges are covered by the zone to shards inside of that zone. Until balancing completes, some chunks may reside on the wrong shard given the configured zones for the sharded cluster. See Balancer for more information. See the sharded cluster balancer manual page for more information on how migrations work in a sharded cluster. \nBOUNDS \nZone ranges are always inclusive of the lower boundary and exclusive of the upper boundary.\n\n \nDropping a collection deletes its associated zone/tag ranges. In earlier versions, MongoDB does not remove the tag associations for a dropped collection, and if you later create a new collection with the same name, the old tag associations will apply to the new collection. \nSECURITY \nFor sharded clusters running with authentication, you must authenticate as either:  * a user whose privileges include the specified actions on various collections in the config database:\n   \n   * find on the config.shards collection\n   \n   * find and update on the config.tags collection;\n   \n   or, alternatively,  * a user whose privileges include enableSharding on the cluster resource (available starting in version 4.2.2, 4.0.14, 3.6.16). The clusterAdmin or clusterManager built-in roles have the appropriate permissions for issuing sh.updateZoneKeyRange(). See the documentation page for Role-Based Access Control for more information. \nEXAMPLES \nGiven a sharded collection exampledb.collection with a shard key of { a : 1 }, the following operation creates a range with a lower bound of 1 and an upper bound of 10 on the alpha zone: sh.updateZoneKeyRange(   \"exampledb.collection\",   { a : 1 },   { a : 10 },   \"alpha\")  The following operation removes the previously created range by passing null to the zone field. sh.updateZoneKeyRange(   \"exampledb.collection\",   { a : 1 },   { a : 10 },   null)  The min and max must match exactly the bounds of the target range. The following operation attempts to remove the previously created range, but specifies { a : 0 } as the min bound: sh.updateZoneKeyRange(   \"exampledb.collection\",   { a : 0 },   { a : 10 },   null)  While the range of { a : 0 } and { a : 10 } encompasses the existing range, it is not an exact match and therefore updateZoneKeyRange does not remove anything. \nCOMPOUND SHARD KEY \nGiven a sharded collection exampledb.collection with a shard key of { a : 1, b : 1 }, the following operation creates a range covering the lower bound of { a: 1, b : 1 } and an upper bound of { a : 10, b : 10} and associates it with the alpha zone: sh.updateZoneKeyRange(   \"exampledb.collection\",   { a : 1, b : 1 },   { a : 10, b : 10 },   \"alpha\")  \nPRE-DEFINE ZONES AND ZONE RANGES FOR AN EMPTY OR NON-EXISTING COLLECTION \nStarting in version 4.2, creating zones and zone ranges on empty or non-existing collections allows MongoDB to optimize the initial chunk creation and distribution process when sharding the collection. This optimized process supports faster setup of zoned sharding with less balancer overhead than creating zones after sharding. The balancer performs all chunk management after the optimized initial chunk creation and distribution. See Initial Chunk Distribution with Compound Hashed Shard Keys for more information. The sections below contain examples for three different shard key types. Consider the following examples, which explore pre-defining zones or zone ranges for three different shard key types:  * Single or Compound Shard Keys  * Compound Hashed Shard Key with Hashed Prefix  * Compound Hashed Shard Key with Non-Prefix Hashed Field SINGLE OR COMPOUND SHARD KEYS  NOTE This example only applies to single-field or compound shard keys without a hashed field. For example, { \"zip\" : 1 } or { \"zip\" : 1, \"account\" : 1} 1 CREATE THE ZONES \nUse sh.addShardToZone() to create the zones: sh.addShardToZone(\"shardA\", \"DC1\")sh.addShardToZone(\"shardB\", \"DC2\") \n2 CREATE THE ZONE RANGES \nUse sh.updateZoneKeyRange() to create the ranges for the empty contacts collection in the exampledb database: sh.updateZoneKeyRange(    \"exampledb.contacts\",    { zip: 10001 },    { zip: 10090 },    \"DC1\");\nsh.updateZoneKeyRange(    \"exampledb.contacts\",    { zip: 90001 },    { zip: 96054 },    \"DC2\"); \n3\n\n  NOTE If the collection does not exist, the sharding operation creates the collection. If the collection is empty and no index exists to support the shard key, the sharding operation creates the index. Use sh.shardCollection() to shard the collection contacts: sh.shardCollection(\"exampledb.contacts\",  { zip: 1 } ); \n4 REVIEW THE CREATED CHUNKS AND DISTRIBUTION \nTo see the created chunks and distribution, run the sh.status() operation: sh.status()  The method returns:\n\n  For the collection, sharding operation created 5 chunks (two chunks that correspond to the zone ranges and the other three to cover all other values) across shardA and shardB. COMPOUND HASHED SHARD KEY WITH HASHED PREFIX  NOTE This example only applies to compound hashed shard keys where the hashed field is the prefix of the shard key (i.e. the first field in the shard key is hashed). For example, { \"_id\" : \"hashed\", \"facility\" : 1 } Starting in version 4.4, MongoDB supports sharding collections on compound hashed indexes. When sharding on a compound hashed shard key, MongoDB can perform optimized initial chunk creation and distribution on the empty or non-existing collection only if the defined zone ranges meet additional requirements. Consider an empty collection examples.metrics which will store analytics from one of two manufacturing facilities. The planned shard key is { \"_id\" : \"hashed\", \"facility\" : 1}, where the hashed field is the shard key prefix. 1 CREATE THE ZONES \nThe planned shard key is { \"_id\" : \"hashed\", \"facility\" : 1 }. Since the hashed field is the prefix (i.e. the first field in the shard key), create a single zone using sh.addShardToZone(): sh.addShardToZone(\"shardA\", \"FacilityAlpha\")sh.addShardToZone(\"shardB\", \"FacilityAlpha\") \n2 CREATE THE ZONE RANGES \nInitial chunk distribution on a compound hashed shard key with a hashed prefix requires a single zone range with MinKey for all lower-bound fields and MaxKey for all upper-bound fields. Use sh.updateZoneKeyRange() to create a single range: sh.updateZoneKeyRange(   \"examples.metrics\",   { \"_id\" : MinKey, \"facility\" : MinKey },   { \"_id\" : MaxKey, \"facility\" : MaxKey },   \"FacilityAlpha\"); \n3 SHARD THE COLLECTION  NOTE If the collection does not exist, the sharding operation creates the collection. If the collection is empty and no index exists to support the shard key, the sharding operation creates the index. Use sh.shardCollection() with presplitHashedZones: true to shard the collection and perform initial chunk creation and distribution: sh.shardCollection(  \"examples.metrics\",  { \"_id\" : \"hashed\", \"facility\" : 1 },  false,  { presplitHashedZones: true }) \n4 REVIEW THE CREATED CHUNKS AND DISTRIBUTION \nTo see the created chunks and distribution, run the sh.status() operation: sh.status()  The output resembles the following (content omitted for readability):\n\n  The sharding operation produced 4 chunks in total. Two chunks correspond to the absolute lower and upper bounds. One zone was created on shardA and shardB corresponding to FacilityAlpha. The zone was subdivided into 2 chunks using the hashed field. COMPOUND HASHED SHARD KEY WITH NON-PREFIX HASHED FIELD  NOTE This example only applies to compound hashed shard keys where the hashed field is not the prefix of the shard key (i.e. the first field in the shard key is not hashed). For example, { \"facility\" : 1, \"_id\" : \"hashed\" } Starting in version 4.4, MongoDB supports sharding collections on compound hashed indexes. When sharding on a compound hashed shard key, MongoDB can perform optimized initial chunk creation and distribution on the empty or non-existing collection only if the defined zone ranges meet additional requirements. Consider an empty collection examples.metrics which will store analytics from one of two manufacturing facilities. The planned shard key is { \"facility\" : 1, \"_id\" : \"hashed\" }, where the hashed field is not the shard key prefix.  * The facility field stores the name of the facility: \"FacilityAlpha\" or \"FacilityBaker\". The collection requires zone ranges on facility to help isolate data for each facility to specific shards.  * The _id field compensates for the low-cardinality of the facility field. Hashing compensates for the monotonically-increasing nature of the _id field. 1 CREATE THE ZONES \nUse the sh.addShardToZone() command to create the zones. sh.addShardToZone(\"shardA\", \"FacilityAlpha\")sh.addShardToZone(\"shardB\", \"FacilityBaker\") \n2 CREATE THE ZONE RANGES \nThe planned shard key is {\"facility\" : 1, \"_id\" : \"hashed\"}. The facility field has two possible values: FacilityAlpha and FacilityBaker. Initial chunk distribution on a compound hashed shard key where the hashed field is not the prefix requires one zone range for each combination of distinct prefix field values (i.e. all fields preceding the hashed field). Since facility has two distinct prefix values, the collection requires exactly two zone ranges that cover those values.  * The lower bound range specifies MinKey for all non-prefix fields.  * The upper-bound range has at least one prefix field that differs from its lower-bound counterpart. Use sh.updateZoneKeyRange() to create the range for \"facility\": \"FacilityAlpha\": sh.updateZoneKeyRange(   \"examples.metrics\",   { \"facility\": \"FacilityAlpha\", \"_id\" : MinKey },   { \"facility\": \"FacilityBaker\", \"_id\" : MinKey },   \"FacilityAlpha\");   * Since zone range upper bounds are exclusive, this range only covers documents with the distinct shard key prefix value \"facilty\" : \"FacilityAlpha\" and all possible values of _id. Use sh.updateZoneKeyRange() to create the range for \"facility\": \"FacilityBaker\": sh.updateZoneKeyRange(   \"examples.metrics\",   { \"facility\": \"FacilityBaker\", \"_id\" : MinKey },   { \"facility\": MaxKey, \"_id\" : MinKey },   \"FacilityBaker\");   * While the upper bound of this range can technically capture other values of facility, the initial chunk distribution logic relies on the assumption that no other distinct values for facility exist. Since the collection only contains documents where facility is FacilityAlpha or FacilityBaker, this range only covers documents with the distinct shard key prefix value \"facility\" : \"FacilityBaker\" and all possible values of _id. 3 SHARD THE COLLECTION  NOTE If the collection does not exist, the sharding operation creates the collection. If the collection is empty and no index exists to support the shard key, the sharding operation creates the index. Use sh.shardCollection() with presplitHashedZones: true to shard the collection and perform initial chunk creation and distribution: sh.shardCollection(  \"examples.metrics\",  { \"facility\" : 1, \"_id\" : \"hashed\"},  false,  { presplitHashedZones: true }) \n4 REVIEW THE CREATED CHUNKS AND DISTRIBUTION \nTo see the created chunks and distribution, run the sh.status() operation: sh.status()  The output resembles the following (content omitted for readability):\n\n  The sharding operation produced 6 chunks in total. Two chunks correspond to the absolute lower and upper bounds. Two zones were created, one on shardA and one on shardB, corresponding to FacilityAlpha and FacilityBaker. Each of these zones has been further subdivided into 2 chunks using the hashed field. \nTIP \nSEE ALSO:  * sh.addShardToZone()  * sh.removeRangeFromZone() ←  sh.waitForPingChange()Free Monitoring Methods → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.addShardToZone/": " Docs Home → MongoDB Manual \nSH.ADDSHARDTOZONE() \nOn this page    \n * Definition\n   \n * Behavior\n * Security\n * Example \nDEFINITION \nsh.addShardToZone(shard, zone) \nAssociates a shard with a zone. MongoDB associates this shard with the given zone. Chunks that are covered by the zone are assigned to shards associated with the zone. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the addShardToZone command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 This method has the following parameter: Parameter\nType\nDescription\nshard\nstring The name of the shard to which to associate the zone. zone\nstring The name of the zone to associate with the shard. Only issue sh.addShardToZone() when connected to a mongos instance. \nBEHAVIOR \nYou can associate a zone with multiple shards, and a shard can associate with multiple zones. See the zone manual page for more information on zones in sharded clusters. \nRANGES \nMongoDB effectively ignores zones that do not have at least one range of shard key values associated with it. To associate a range of shard key values with a zone, use the sh.updateZoneKeyRange() method. Starting in MongoDB 4.0.2, you can run updateZoneKeyRange database command and its helpers sh.updateZoneKeyRange() and sh.addTagRange() on an unsharded collection or a non-existing collection. \nTIP By defining the zones and the zone ranges before sharding an empty or a non-existing collection, the shard collection operation creates chunks for the defined zone ranges as well as any additional chunks to cover the entire range of the shard key values and performs an initial chunk distribution based on the zone ranges. This initial creation and distribution of chunks allows for faster setup of zoned sharding. After the initial distribution, the balancer manages the chunk distribution going forward. See Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection for an example. \nSECURITY \nFor sharded clusters that enforce access control, you must authenticate as a user whose privileges include either:  * update on the shards collection in the config database; or, alternatively,  * enableSharding on the cluster resource (Starting in version 4.2.2). The clusterAdmin or clusterManager built-in roles have the appropriate permissions for issuing sh.addShardToZone(). See the Role-Based Access Control manual page for more information. \nEXAMPLE \nThe following example adds three zones, NYC, LAX, and NRT, associating each to a shard: sh.addShardToZone(\"shard0000\", \"JFK\")sh.addShardToZone(\"shard0001\", \"LAX\")sh.addShardToZone(\"shard0002\", \"NRT\")  A shard can associate with multiple zones. The following example associates LGA to shard0000: sh.addShardToZone(\"shard0000\", \"LGA\")  shard0000 associates with both the LGA zone and the JFK zone. In a balanced cluster, MongoDB routes reads and writes covered by either zone to shard0000. \nTIP \nSEE ALSO:  * sh.updateZoneKeyRange()  * sh.removeShardFromZone() ←  sh.addShardTag()sh.addTagRange() → On this page  * Definition\n * Behavior\n * Security\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.drop/": " Docs Home → MongoDB Manual \nDB.COLLECTION.DROP() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \ndb.collection.drop(<options>)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the drop command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Removes a collection or view from the database. The method also removes any indexes associated with the dropped collection. The method provides a wrapper around the drop command. db.collection.drop() has the form: db.collection.drop( { writeConcern: <document> } )  db.collection.drop() takes an optional document with the following field: Field\nDescription\nwriteConcern Optional. A document expressing the write concern of the db.collection.drop() operation. Omit to use the default write concern. When issued on a sharded cluster, mongos converts the write concern of the drop command and its helper db.collection.drop() to \"majority\". Returns:\n * true when successfully drops a collection.\n * false when collection to drop does not exist. \nBEHAVIOR \n * The db.collection.drop() method and drop command create an invalidate for any Change Streams opened on dropped collection.  * Starting in MongoDB 4.4, the db.collection.drop() method and drop command abort any in-progress index builds on the target collection before dropping the collection. Prior to MongoDB 4.4, attempting to drop a collection with in-progress index builds results in an error, and the collection is not dropped.\n   \n   For replica sets or shard replica sets, aborting an index on the primary does not simultaneously abort secondary index builds. MongoDB attempts to abort the in-progress builds for the specified indexes on the primary and if successful creates an associated abort oplog entry. Secondary members with replicated in-progress builds wait for a commit or abort oplog entry from the primary before either committing or aborting the index build.  * Dropping a collection deletes its associated zone/tag ranges.  * Starting in MongoDB 5.0, the drop command and the db.collection.drop() method return an error if you try to drop a collection in the admin database or the config database from a mongos. To drop these collections, connect to the config server and run the command there.\n   \n   \n   WARNING\n   \n   Dropping collections in the admin database or the config database can leave your cluster in an unusable state.  * Starting in MongoDB 6.0, the db.collection.drop() method drops the specified collection and any internal collections related to encrypted fields.\n   \n   \n   WARNING\n   \n   The db.collection.drop() method's behavior differs from the driver's drop method's behavior. The driver's connection must have automatic encryption enabled in order to drop both the specified collection and any internal collections related to encrypted fields. mongosh always drops the specified collection and any internal collections related to encrypted fields. \nREUSING DROPPED COLLECTION NAMES ON SHARDED CLUSTERS \nOn a sharded cluster, if you create a collection that has the same name as a previously deleted collection prior to MongoDB 5.0, mongos may forward operations to the wrong shard. To avoid this situation use the version-specific instructions below: For a sharded cluster running MongoDB 5.0 or later, no special action is required. Use the drop() method and then create a new collection with the same name. For a sharded cluster running MongoDB 4.4 or earlier, if you use the drop() method and then create a new collection with the same name, you must either:  * Flush the cached routing table on every mongos using flushRouterConfig.  * Use db.collection.remove() to remove the existing documents and reuse the collection. Flushing the cached routing tables is the preferred procedure because it is faster than removing sharded collections with db.collection.remove(). Only use the remove() approach if you want to avoid flushing the cache. \nRESOURCE LOCKING \nChanged in version 4.2.\n\n Prior to MongoDB 4.2, db.collection.drop() obtained an exclusive lock on the parent database, blocking all operations on the database and all its collections until the operation completed. \nEXAMPLE  DROP A COLLECTION USING DEFAULT WRITE CONCERN \nThe following operation drops the students collection in the current database. db.students.drop()  \nDROP A COLLECTION USING W: 1 WRITE CONCERN \ndb.collection.drop() accepts an options document. The following operation drops the students collection in the current database. The operation uses the 1 write concern: db.students.drop( { writeConcern: { w: 1 } } ) \n←  db.collection.distinct()db.collection.dropIndex() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.shutdownServer/": " Docs Home → MongoDB Manual \nDB.SHUTDOWNSERVER() \nOn this page    \n * Behavior\n   \n * Access Control\n * Examples Changed in version 5.0. db.shutdownServer() \nShuts down the current mongod or mongos process cleanly and safely. You must issue the db.shutdownServer() operation against the admin database. db.shutdownServer() has this syntax: db.shutdownServer({  force: <boolean>,  timeoutSecs: <int>})  The method takes these fields: Field\nDescription\nforce Optional. Specify true to force the mongod or mongos to shut down. Force shutdown interrupts any ongoing operations on the mongod or mongos and may result in unexpected behavior. timeoutSecs Optional. Starting in MongoDB 5.0, mongod and mongos enter a quiesce period to allow any ongoing database operations to complete before shutting down. If a mongod primary receives a shut down request, the primary:       1. Attempts to step down to a secondary.\n    \n    If the step down fails and a:\n    \n    * shutdown or db.shutdownServer() command was run, mongod only continues the shut down steps if the force field is true, or a\n    \n    * SIGTERM signal was sent to mongod, mongod always continues the shut down steps.  2. Enters the quiesce period.  3. Ends any remaining database operations.  4. Shuts down. For a mongod secondary or mongos shut down request, the quiesce period is entered after a shut down was requested. The quiesce period is specified by the:  * timeoutSecs field if a shutdown or db.shutdownServer() command was run, or  * shutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongod, or  * mongosShutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongos. The timeoutSecs field defaults to 15 seconds in MongoDB 5.0. Clients cannot open new connections to a mongod or mongos that is shutting down. In MongoDB 4.4 and earlier, for a mongod primary, timeoutSecs is the number of seconds the primary should wait for a secondary to catch up. If no secondaries catch up within the specified time, the command fails. Defaults to 10 seconds. This operation provides a wrapper around the shutdown command. \nBEHAVIOR \nFor a mongod started with Authentication, you must run db.shutdownServer() over an authenticated connection. See Access Control for more information. For a mongod started without Authentication, you must run db.shutdownServer() from a client connected to the localhost interface. For example, run mongosh with the --host \"127.0.0.1\" option on the same host machine as the mongod. \nDB.SHUTDOWNSERVER() ON REPLICA SET MEMBERS \ndb.shutdownServer() fails if the mongod replica set member is running certain operations such as index builds. You can specify force: true to force the member to interrupt those operations and shut down. SHUTTING DOWN THE REPLICA SET PRIMARY, SECONDARY, OR MONGOS \nStarting in MongoDB 5.0, mongod and mongos enter a quiesce period to allow any ongoing database operations to complete before shutting down. If a mongod primary receives a shut down request, the primary:  1. Attempts to step down to a secondary.\n    \n    If the step down fails and a:\n    \n    * shutdown or db.shutdownServer() command was run, mongod only continues the shut down steps if the force field is true, or a\n    \n    * SIGTERM signal was sent to mongod, mongod always continues the shut down steps.  2. Enters the quiesce period.  3. Ends any remaining database operations.  4. Shuts down. For a mongod secondary or mongos shut down request, the quiesce period is entered after a shut down was requested. The quiesce period is specified by the:  * timeoutSecs field if a shutdown or db.shutdownServer() command was run, or  * shutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongod, or\n\n The timeoutSecs field defaults to 15 seconds in MongoDB 5.0. Clients cannot open new connections to a mongod or mongos that is shutting down. In MongoDB 4.4 and earlier, if running db.shutdownServer() against the replica set primary, the operation implicitly uses replSetStepDown to step down the primary before shutting down the mongod. If no secondary in the replica set can catch up to the primary within 10 seconds, the shutdown operation fails. You can issue db.shutdownServer() with force: true to shut down the primary even if the step down fails. \nWARNING Force shutdown of the primary can result in the rollback of any writes not yet replicated to a secondary. \nACCESS CONTROL \nTo run db.shutdownServer() on a mongod enforcing Authentication, the authenticated user must have the db.shutdownServer() privilege. For example, a user with the built-in role hostManager has the appropriate permissions. \nEXAMPLES  SHUT DOWN A MONGOD \ndb.getSiblingDB(\"admin\").shutdownServer()  \nFORCE SHUT DOWN A MONGOD \ndb.getSiblingDB(\"admin\").shutdownServer({ \"force\" : true })  \nSHUT DOWN A PRIMARY MONGOD WITH LONGER TIMEOUT \ndb.getSiblingDB(\"admin\").shutdownServer({ \"timeoutSecs\": 60 }) \n←  db.setProfilingLevel()db.stats() → On this page  * Behavior\n * Access Control\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.auth/": " Docs Home → MongoDB Manual \nDB.AUTH() \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example \nDEFINITION \ndb.auth() \nAllows a user to authenticate to the database from within the shell. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. Starting in MongoDB 4.4, if you use the db.auth(<username>, <password>) syntax and omit the password, the user is prompted to enter a password. \nSYNTAX \nThe db.auth() has the following syntax forms: \nDB.AUTH(<USERNAME>, <PASSWORD>)   \nDB.AUTH(<USER DOCUMENT>) \ndb.auth( {   user: <username>,   pwd: passwordPrompt(),   // Or \"<cleartext password>\"   mechanism: <authentication mechanism>,   digestPassword: <boolean>} ) \nParameter\nType\nDescription\nuser\nstring\nThe name of the user with access privileges for this database.\npwd\nstring The user's password. The value can be either:      * the user's password in cleartext string, or  * passwordPrompt() to prompt for the user's password. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. When using the user document syntax, you cannot omit the pwd. mechanism\nstring Optional. The authentication mechanism to use. For available mechanisms, see authentication mechanisms. If unspecified, uses the hello command to determine the SASL mechanism or mechanisms for the specified user. See saslSupportedMechs. digestPassword\nboolean Optional. Determines whether or not the supplied password should be pre-hashed before being used with the specified authentication mechanism.  * For SCRAM-SHA-1, although you may specify true, setting this value to true does not improve security and may interfere with credentials using other mechanisms.  * For all other methods, this value must be set to false (default value). Any other value will result in authentication failure since those methods do not understand MongoDB pre-hashing. The default value is false. \nNOTE mongosh excludes all db.auth() operations from the saved history. Returnsdb.auth() returns 0 when authentication is not successful, and 1 when the operation is successful. \nBEHAVIOR  CLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued db.auth() disconnects before the operation completes, MongoDB marks db.auth() for termination using killOp. \nEXAMPLE  TIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. Starting in MongoDB 4.4, if you use the db.auth(<username>, <password>) syntax and omit the password, the user is prompted to enter a password. \nAUTHENTICATE AFTER CONNECTING TO THE SHELL \nTo authenticate after connecting mongosh, issue db.auth() in the user's authentication database: use testdb.auth( \"myTestDBUser\", passwordPrompt() )  Starting in MongoDB 4.4, you can omit the password value entirely to prompt the user to enter their password: use testdb.auth( \"myTestDBUser\" )  Starting in MongoDB 5.0, if your connection specifies the --apiStrict option, you may not use the db.auth() method to:  * Authenticate again as the same user on the same database.  * Authenticate as a different user when previously authenticated on the same database.  * Authenticate with a new database when previously authenticated on a different database. \nAUTHENTICATE WHEN CONNECTING TO THE SHELL\n\n mongosh --username \"myTestDBUser\" --password --authenticationDatabase test --authenticationMechanism SCRAM-SHA-256 \n←  User Management Methodsdb.changeUserPassword() → On this page  * Definition\n * Syntax\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.removeTagRange/": " Docs Home → MongoDB Manual \nSH.REMOVETAGRANGE() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \nsh.removeTagRange(namespace, minimum, maximum, tag) \nChanged in version 3.4: This method aliases to sh.removeRangeFromZone() in MongoDB 3.4. The functionality specified below still applies to MongoDB 3.2. MongoDB 3.4 provides Zone sharding as the successor to tag-aware sharding. Removes a range of shard key values to a shard tag created using the sh.addShardTag() method. sh.removeTagRange() takes the following arguments: Parameter\nType\nDescription\nnamespace\nstring\nThe namespace of the sharded collection to tag.\nminimum\ndocument\nThe minimum value of the shard key from the tag. Specify the minimum value in the form of <fieldname>:<value>. This value must be of the same BSON type or types as the shard key.\nmaximum\ndocument\nThe maximum value of the shard key range from the tag. Specify the maximum value in the form of <fieldname>:<value>. This value must be of the same BSON type or types as the shard key.\ntag\nstring\nThe name of the tag attached to the range specified by the minimum and maximum arguments to. Use sh.removeShardTag() to ensure that unused or out of date ranges are removed and hence chunks are balanced as required. Only issue sh.removeTagRange() when connected to a mongos instance. \nEXAMPLE \nGiven a shard key of {state: 1, zip: 1}, the following operation removes an existing tag range covering zip codes in New York State: sh.removeTagRange( \"exampledb.collection\",                { state: \"NY\", zip: MinKey },                { state: \"NY\", zip: MaxKey },                \"NY\"              ) \n←  sh.removeShardFromZone()sh.reshardCollection() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-plan-cache/": " Docs Home → MongoDB Manual \nQUERY PLAN CACHE METHODS \nThe PlanCache methods are only accessible from a collection's plan cache object. To retrieve the plan cache object, use the db.collection.getPlanCache() method. \nNOTE For details on a specific method, including syntax and examples, click on the link to the method's reference page. Name\nDescription\ndb.collection.getPlanCache()\nReturns an interface to access the query plan cache object and associated PlanCache methods for a collection.\nPlanCache.clear()\nClears all the cached query plans for a collection. Accessible through the plan cache object of a specific collection, i.e. db.collection.getPlanCache().clear().\nPlanCache.clearPlansByQuery()\nClears the cached query plans for the specified query shape. Accessible through the plan cache object of a specific collection, i.e. db.collection.getPlanCache().clearPlansByQuery()\nPlanCache.help()\nDisplays the methods available for a collection's query plan cache. Accessible through the plan cache object of a specific collection, i.e. db.collection.getPlanCache().help().\nPlanCache.list() Returns the plan cache information for a collection. Accessible through the plan cache object of a specific collection, i.e. db.collection.getPlanCache().list(). New in version 4.4. ←  db.watch()db.collection.getPlanCache() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.createView/": " Docs Home → MongoDB Manual \nDB.CREATEVIEW() \nOn this page    \n * Syntax\n   \n * Examples\n * Behavior db.createView()  NOTE \nDISAMBIGUATION This page discusses standard views. For discussion of on-demand materialized views, see On-Demand Materialized Views. To understand the differences between the view types, see Comparison with On-Demand Materialized Views. Creates a view as the result of the applying the specified aggregation pipeline to the source collection or view. Views act as read-only collections, and are computed on demand during read operations. You must create views in the same database as the source collection. MongoDB executes read operations on views as part of the underlying aggregation pipeline. A view definition pipeline cannot include the $out or the $merge stage. This restriction also applies to embedded pipelines, such as pipelines used in $lookup or $facet stages. \nSYNTAX \ndb.createView() has the following syntax: db.createView(<view>, <source>, <pipeline>, <collation>)  The method accepts the following parameters: Parameter\nType\nDescription\nview\nstring\nThe name of the view to create.\nsource\nstring\nThe name of the source collection or view from which to create the view. The name does not include the database name and implies the same database as the view to create; it is not the full namespace of the collection or view. You must create views in the same database as the source collection.\npipeline\narray An array that consists of the aggregation pipeline stage(s). db.createView() creates the view by applying the specified pipeline to the source collection or view. A view definition pipeline cannot include the $out or the $merge stage. This restriction also applies to embedded pipelines, such as pipelines used in $lookup or $facet stages. The view definition is public; i.e. db.getCollectionInfos() and explain operations on the view will include the pipeline that defines the view. As such, avoid referring directly to sensitive fields and values in view definitions. collation\ndocument Optional. Specifies the default collation for the view. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. If the underlying source is a collection, the view does not inherit the collection's collation settings. If no collation is specified, the view's default collation is the \"simple\" binary comparison collator. If the underlying source is another view, the view must specify the same collation settings. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. The db.createView() method wraps the following create command operation: db.runCommand( { create: <view>, viewOn: <source>, pipeline: <pipeline>, collation: <collation> } )  \nIMPORTANT \nVIEW NAMES ARE INCLUDED IN COLLECTION LIST OUTPUT Operations that list collections, such as db.getCollectionInfos() and db.getCollectionNames(), include views in their outputs. The view definition is public; i.e. db.getCollectionInfos() and explain operations on the view will include the pipeline that defines the view. As such, avoid referring directly to sensitive fields and values in view definitions. \nEXAMPLES \nTo see examples of creating a view, see the following pages:      * Create and Query a View  * Use a View to Join Two Collections  * Create a View with Default Collation \nBEHAVIOR \nTo see behavioral details of views, see Behavior. ←  db.createCollection()db.currentOp() → On this page  * Syntax\n * Examples\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.dropUser/": " Docs Home → MongoDB Manual \nDB.DROPUSER() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.dropUser(username, writeConcern) \nRemoves the user from the current database. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the dropUser command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.dropUser() method takes the following arguments: Parameter\nType\nDescription\nusername\nstring\nThe name of the user to remove from the database.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. The db.dropUser() method wraps the dropUser command. Before dropping a user who has the userAdminAnyDatabase role, ensure you have at least another user with user administration privileges. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.dropUser() is executed using \"majority\" write concern by default. \nREQUIRED ACCESS \nYou must have the dropUser action on a database to drop a user from that database. \nEXAMPLE \nThe following db.dropUser() operation drops the reportUser1 user on the products database. use productsdb.dropUser(\"reportUser1\", {w: \"majority\", wtimeout: 5000}) \n←  db.createUser()db.dropAllUsers() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.fsyncLock/": " Docs Home → MongoDB Manual \nDB.FSYNCLOCK() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \ndb.fsyncLock() \nForces the mongod to flush all pending write operations to disk and locks the entire mongod instance to prevent additional writes until the user releases the lock with a corresponding db.fsyncUnlock() command. \nIMPORTANT The db.fsyncLock() and db.fsyncUnlock() operations maintain a lock count. db.fsyncLock() increments the lock count, and db.fsyncUnlock() decrements the lock count. To unlock a mongod instance for writes, the lock count must be zero. That is, for a given number of db.fsyncLock() operations, you must issue a corresponding number of db.fsyncUnlock() operations to unlock the instance for writes. db.fsyncLock() has the syntax: db.fsyncLock()  The operation returns a document with the following fields:      * info - Information on the status of the operation  * lockCount (New in version 3.4)- The number of locks currently on the instance.  * seeAlso - Link to the fsync command documentation.  * ok - The status code. This command provides a simple wrapper around a fsync database command with the following syntax: { fsync: 1, lock: true }  db.fsyncLock() is an administrative command. You can use this operation to locks the database and create a window for backup operations. \nBEHAVIOR \ndb.fsyncLock() ensures that the data files are safe to copy using low-level backup utilities such as cp, scp, or tar. A mongod started using the copied files contains user-written data that is indistinguishable from the user-written data on the locked mongod. The data files of a locked mongod may change due to operations such as journaling syncs or WiredTiger snapshots. While this has no affect on the logical data (e.g. data accessed by clients), some backup utilities may detect these changes and emit warnings or fail with errors. For more information on MongoDB- recommended backup utilities and procedures, see MongoDB Backup Methods. \nEXAMPLE \nThe following operation runs db.fsyncLock(): db.fsyncLock()  The operation returns the following status document that includes the lockCount: {   \"info\" : \"now locked against writes, use db.fsyncUnlock() to unlock\",   \"lockCount\" : NumberLong(1),   \"seeAlso\" : \"http://dochub.mongodb.org/core/fsynccommand\",   \"ok\" : 1}  If you run db.fsyncLock() again, the operation increments the lockCount: {   \"info\" : \"now locked against writes, use db.fsyncUnlock() to unlock\",   \"lockCount\" : NumberLong(2),   \"seeAlso\" : \"http://dochub.mongodb.org/core/fsynccommand\",   \"ok\" : 1}  To unlock the instance for writes, you must run db.fsyncUnlock() twice to reduce the lockCount to 0. ←  db.dropDatabase()db.fsyncUnlock() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/js-connection/": " Docs Home → MongoDB Manual \nCONNECTION METHODS  NOTE For details on a specific method, including syntax and examples, click on the link to the method's reference page. Name\nDescription\nconnect()\nConnects to a MongoDB instance and to a specified database on that instance.\nMongo()\nCreates a new connection object.\nMongo.getDB()\nReturns a database object.\nMongo.getDBNames()\nReturns a list of databases.\nMongo.getDBs()\nReturns a document with a list of databases and metadata.\nMongo.getReadPrefMode()\nReturns the current read preference mode for the MongoDB connection.\nMongo.getReadPrefTagSet()\nReturns the read preference tag set for the MongoDB connection.\nMongo.getWriteConcern()\nReturns the write concern for the connection object.\nMongo.setCausalConsistency()\nEnables or disables causal consistency on the connection object.\nMongo.setReadPref()\nSets the read preference for the MongoDB connection.\nMongo.setWriteConcern()\nSets the write concern for the connection object.\nMongo.startSession()\nStarts a session on the connection object.\nMongo.watch()\nOpens a change stream cursor for a deployment to report on all its non-system collections across all its databases, excluding the internal admin, local, and config databases.\nSession()\nThe session object.\nSessionOptions()\nThe options object for the session. ←  WriteResult.hasWriteConcernError()connect() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.renameCollection/": " Docs Home → MongoDB Manual \nDB.COLLECTION.RENAMECOLLECTION() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \ndb.collection.renameCollection(target, dropTarget)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the renameCollection command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Renames a collection. Provides a wrapper for the renameCollection database command. Parameter\nType\nDescription\ntarget\nstring\nThe new name of the collection. Enclose the string in quotes. See Naming Restrictions.\ndropTarget\nboolean\nOptional. If true, mongod drops the target of renameCollection prior to renaming the collection. The default value is false. \nBEHAVIOR \nThe db.collection.renameCollection() method operates within a collection by changing the metadata associated with a given collection. Refer to the documentation renameCollection for additional warnings and messages. \nWARNING The db.collection.renameCollection() method and renameCollection command will invalidate open cursors which interrupts queries that are currently returning data. For Change Streams, the db.collection.renameCollection() method and renameCollection command create an invalidate for any existing Change Streams opened on the source or target collection.  * The method has the following limitations:\n   \n   * db.collection.renameCollection() cannot move a collection between databases. Use renameCollection for these rename operations.\n   \n   * You cannot rename views.\n   \n   * db.collection.renameCollection() is not supported on time series collections.\n   \n   * You cannot rename a collection to itself. If you try to rename a collection to itself an IllegalOperation error is thrown. \nRESOURCE LOCKING IN SHARDED CLUSTERS \nChanged in version 5.0. When renaming a sharded or unsharded collection in a sharded cluster, the source and target collections are exclusively locked on every shard. Subsequent operations on the source and target collections must wait until the rename operation completes. For more information on locking in MongoDB, see FAQ: Concurrency. \nRESOURCE LOCKING IN REPLICA SETS \nChanged in version 4.2. renameCollection() obtains an exclusive lock on the source and target collections for the duration of the operation. All subsequent operations on the collections must wait until renameCollection() completes. Prior to MongoDB 4.2, renaming a collection within the same database with renameCollection required obtaining an exclusive database lock. \nINTERACTION WITH MONGODUMP \nA mongodump started with --oplog fails if a client issues db.collection.renameCollection() during the dump process. See mongodump.--oplog for more information. \nEXAMPLE \nCall the db.collection.renameCollection() method on a collection object. For example: db.rrecord.renameCollection(\"record\")  This operation will rename the rrecord collection to record. If the target name (i.e. record) is the name of an existing collection, then the operation will fail. ←  db.collection.remove()db.collection.replaceOne() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.map/": " Docs Home → MongoDB Manual \nCURSOR.MAP() \nOn this page    \n * Behavior\n   \n * Examples cursor.map(function)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Applies a function to each document visited by the cursor and collects the return values from successive applications of the function into a Cursor object. The cursor.map() method has the following parameter: Parameter\nType\nDescription\nfunction\nfunction\nA function to apply to each document visited by the cursor. \nBEHAVIOR \ncursor.map() returns a Cursor object. Note that .map() only converts the type, it does not create a new cursor. You can convert the Cursor object to an Array with .toArray(). \nEXAMPLES \nThese examples refer to the products collection: db.products.insertMany([   { _id: 1, name: 'widget', price: 10.89 },   { _id: 2, name: 'thing', price: 11.24 },   { _id: 3, name: 'moppet', price: 8 },   { _id: 4, name: 'cosa', price: 24.19 }])  \nRETURN A VALUE FROM A COLLECTION \nGet the product names. db.products.find().map( function(p) { return p.name; } ) ;  \nRETURN RESULTS AS AN ARRAY \nCalculate a discounted sale price and return the results as an array. var salePrices = db.products.find().map( function(p) { return p.price * .9 } ).toArray() ;  Confirm that the output is an Array salePrices.constructor.name  \nTIP \nSEE ALSO: cursor.forEach() for similar functionality. ←  cursor.limit()cursor.max() → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.addOption/": " Docs Home → MongoDB Manual \nCURSOR.ADDOPTION() \nOn this page    \n * Definition\n   \n * Flags\n * Example \nDEFINITION \ncursor.addOption(flag)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. \nNOTE \nDEPRECATED SINCE V3.2 Starting in v3.2, the cursor.addOption() operator is deprecated in mongo. Use available cursor methods instead. Used to change query behavior by setting the flags listed below. The cursor.addOption() method has the following parameter: Parameter\nType\nDescription\nflag\nflag\nFor mongosh, you can use the cursor flags listed below. For the driver-specific list, see your driver documentation. \nFLAGS \nmongosh provides several additional cursor flags to modify the behavior of the cursor. Flag\nDescription\nDBQuery.Option.tailable Sets the cursor not to close once the last data is received, allowing the query to continue returning data added after the initial results were exhausted.\nDBQuery.Option.slaveOk Allows querying of a replica secondary.\nDBQuery.Option.noTimeout Prevents the server from timing out idle cursors.\nDBQuery.Option.awaitData For use with .. data:: DBQuery.Option.tailable; sets the cursor to block and await data for a while rather than returning no data. The cursor will return no data once the timeout has expired.\nDBQuery.Option.exhaust Sets the cursor to return all data returned by the query at once rather than splitting the results into batches.\nDBQuery.Option.partial Sets the cursor to return partial data from a query against a sharded cluster in which some shards do not respond rather than throwing an error. \nEXAMPLE \nThe following example adds the DBQuery.Option.tailable flag and the DBQuery.Option.awaitData flag to ensure that the query returns a tailable cursor. The sequence creates a cursor that will wait for few seconds after returning the full result set so that it can capture and return additional data added during the query: var t = db.myCappedCollection;var cursor = t.find().addOption(DBQuery.Option.tailable).                      addOption(DBQuery.Option.awaitData)  \nWARNING Adding incorrect wire protocol flags can cause problems and/or extra server load. ←  Cursor Methodscursor.allowDiskUse() → On this page  * Definition\n * Flags\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.insertOne/": " Docs Home → MongoDB Manual \nDB.COLLECTION.INSERTONE() \nOn this page    \n * Definition\n   \n * Behaviors\n * Examples \nDEFINITION \ndb.collection.insertOne()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the insert command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Inserts a single document into a collection. The insertOne() method has the following syntax: db.collection.insertOne(   <document>,   {      writeConcern: <document>   }) \nParameter\nType\nDescription\ndocument\ndocument\nA document to insert into the collection.\nwriteConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. Returns:A document containing:  * A boolean acknowledged as true if the operation ran with write concern or false if write concern was disabled.  * A field insertedId with the _id value of the inserted document. \nBEHAVIORS  COLLECTION CREATION \nIf the collection does not exist, then the insertOne() method creates the collection. \n_ID FIELD \nIf the document does not specify an _id field, then mongod will add the _id field and assign a unique ObjectId() for the document before inserting. Most drivers create an ObjectId and insert the _id field, but the mongod will create and populate the _id if the driver or application does not. If the document contains an _id field, the _id value must be unique within the collection to avoid duplicate key error. \nEXPLAINABILITY \ninsertOne() is not compatible with db.collection.explain(). \nERROR HANDLING \nOn error, db.collection.insertOne() throws either a writeError or writeConcernError exception. \nTRANSACTIONS \ndb.collection.insertOne() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. COLLECTION CREATION IN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, if you specify an insert on a non-existing collection in a transaction, the collection is implicitly created. In MongoDB 4.4 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES  INSERT A DOCUMENT WITHOUT SPECIFYING AN _ID FIELD \nIn the following example, the document passed to the insertOne() method does not contain the _id field: try {   db.products.insertOne( { item: \"card\", qty: 15 } );} catch (e) {   print (e);};  The operation returns the following document: {   \"acknowledged\" : true,   \"insertedId\" : ObjectId(\"56fc40f9d735c28df206d078\")}  Because the documents did not include _id, mongod creates and adds the _id field and assigns it a unique ObjectId() value. The ObjectId values are specific to the machine and time when the operation is run. As such, your values may differ from those in the example. \nINSERT A DOCUMENT SPECIFYING AN _ID FIELD\n\n try {   db.products.insertOne( { _id: 10, item: \"box\", qty: 20 } );} catch (e) {   print (e);}  The operation returns the following: { \"acknowledged\" : true, \"insertedId\" : 10 }  Inserting an duplicate value for any key that is part of a unique index, such as _id, throws an exception. The following attempts to insert a document with a _id value that already exists: try {   db.products.insertOne( { _id: 10, \"item\" : \"packing peanuts\", \"qty\" : 200 } );} catch (e) {   print (e);}  Since _id: 10 already exists, the following exception is thrown: WriteError({   \"index\" : 0,   \"code\" : 11000,   \"errmsg\" : \"E11000 duplicate key error collection: inventory.products index: _id_ dup key: { : 10.0 }\",   \"op\" : {      \"_id\" : 10,      \"item\" : \"packing peanuts\",      \"qty\" : 200   }})  \nINCREASE WRITE CONCERN \nGiven a three member replica set, the following operation specifies a w of majority, wtimeout of 100: try {   db.products.insertOne(       { \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" },       { writeConcern: { w : \"majority\", wtimeout : 100 } }   );} catch (e) {   print (e);}  If the acknowledgement takes longer than the wtimeout limit, the following exception is thrown: WriteConcernError({   \"code\" : 64,   \"errmsg\" : \"waiting for replication timed out\",   \"errInfo\" : {     \"wtimeout\" : true,     \"writeConcern\" : {    // Added in MongoDB 4.4       \"w\" : \"majority\",       \"wtimeout\" : 100,       \"provenance\" : \"getLastErrorDefaults\"     }   }})  \nTIP \nSEE ALSO:  * To insert multiple documents, see db.collection.insertMany()  * WriteResult.writeConcernError ←  db.collection.insert()db.collection.insertMany() → On this page  * Definition\n * Behaviors\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.countDocuments/": " Docs Home → MongoDB Manual \nDB.COLLECTION.COUNTDOCUMENTS() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ndb.collection.countDocuments(query, options)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the $group aggregation stage and the $sum expression called by the aggregate command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Returns the count of documents that match the query for a collection or view. This method is available for use in Transactions. db.collection.countDocuments( <query>, <options> ) \nParameter\nType\nDescription\nquery\ndocument\nThe query selection criteria. To count all documents, specify an empty document. See also Query Restrictions.\noptions\ndocument\nOptional. Extra options that affects the count behavior. The options document can contain the following: Field\nType\nDescription\nlimit\ninteger\nOptional. The maximum number of documents to count.\nskip\ninteger\nOptional. The number of documents to skip before counting.\nhint\nstring or document\nOptional. An index name or the index specification to use for the query.\nmaxTimeMS\ninteger\nOptional. The maximum amount of time to allow the count to run. \nBEHAVIOR  MECHANICS \nUnlike db.collection.count(), db.collection.countDocuments() does not use the metadata to return the count. Instead, it performs an aggregation of the document to return an accurate count, even after an unclean shutdown or in the presence of orphaned documents in a sharded cluster. db.collection.countDocuments() wraps the following aggregation operation and returns just the value of n: db.collection.aggregate([   { $match: <query> },   { $group: { _id: null, n: { $sum: 1 } } }])  \nEMPTY OR NON-EXISTING COLLECTIONS AND VIEWS \nStarting in version 4.2.1, db.collection.countDocuments() returns 0 on an empty or non-existing collection or view. In earlier versions of MongoDB, db.collection.countDocuments() errors on an empty or non-existing collection or view. \nQUERY RESTRICTIONS \nYou cannot use the following query operators as part of the query expression for db.collection.countDocuments(): Restricted Operator\nAlternative\n$where\nAs an alternative, use $expr instead.\n$near As an alternative, use $geoWithin with $center; i.e. { $geoWithin: { $center: [ [ <x>, <y> ], <radius> ] } } $nearSphere As an alternative, use $geoWithin with $centerSphere; i.e. { $geoWithin: { $centerSphere: [ [ <x>, <y> ], <radius> ] } } \nTRANSACTIONS \ndb.collection.countDocuments() can be used inside multi-document transactions. When you use db.collection.countDocuments() in a transaction, the resulting count will not filter out any uncommitted multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. \nCLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued db.collection.countDocuments() disconnects before the operation completes, MongoDB marks db.collection.countDocuments() for termination using killOp. \nEXAMPLES  COUNT ALL DOCUMENTS IN A COLLECTION \nTo count the number of all documents in the orders collection, use the following operation: db.orders.countDocuments({})  \nCOUNT ALL DOCUMENTS THAT MATCH A QUERY \nCount the number of the documents in the orders collection with the field ord_dt greater than new Date('01/01/2012'):\n\n  \nTIP \nSEE ALSO:  * db.collection.estimatedDocumentCount()  * $group and $sum  * count  * collStats pipeline stage with the count option ←  db.collection.count()db.collection.createIndex() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.maxTimeMS/": " Docs Home → MongoDB Manual \nCURSOR.MAXTIMEMS() \nOn this page    \n * Definition\n   \n * Behaviors\n * Examples \nDEFINITION \ncursor.maxTimeMS(<time limit>)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Specifies a cumulative time limit in milliseconds for processing operations on a cursor. The maxTimeMS() method has the following prototype form: db.collection.find(   { <query> },   { <projection> }).maxTimeMS( <milliseconds> )  The maxTimeMS() method has the following parameter: Parameter\nType\nDescription\nmilliseconds\ninteger\nSpecifies a cumulative time limit in milliseconds for processing operations on the cursor. \nIMPORTANT maxTimeMS() is not related to the NoCursorTimeout query flag. maxTimeMS() relates to processing time, while NoCursorTimeout relates to idle time. A cursor's idle time does not contribute towards its processing time. The maxAwaitTimeMS() method sets a limit on how long a tailable cursor waits for the next response. It does not set a limit on total processing time. \nBEHAVIORS \nMongoDB targets operations for termination if the associated cursor exceeds its allotted time limit. MongoDB terminates operations that exceed their allotted time limit using the same mechanism as db.killOp(). MongoDB only terminates an operation at one of its designated interrupt points. MongoDB does not count network latency between the client and the server towards a cursor's time limit. For a sharded cluster, however, MongoDB does include the latency between the mongos and mongod instances towards this time limit. Queries that generate multiple batches of results continue to return batches until the cursor exceeds its allotted time limit. \nSESSION IDLE TIMEOUT OVERRIDES MAXTIMEMS \nMongoDB drivers and mongosh associate all operations with a server session, with the exception of unacknowledged write operations. For operations not explicitly associated with a session (i.e. using Mongo.startSession()), MongoDB drivers and mongosh create an implicit session and associate it with the operation. If a session is idle for longer than 30 minutes, the MongoDB server marks that session as expired and may close it at any time. When the MongoDB server closes the session, it also kills any in-progress operations and open cursors associated with the session. This includes cursors configured with noCursorTimeout() or a maxTimeMS() greater than 30 minutes. For example, consider a find() operation with the maxTimeMS() configured for a timeout of 31 minutes. The server returns a cursor along with a batch of documents defined by the cursor.batchSize() of the find(). The session refreshes each time the application requests a new batch of documents from the server. However, if the application takes longer than 30 minutes to process the current batch of documents, the session is marked as expired and closed. When the server closes the session, it also kills the cursor despite the cursor being configured with maxTimeMS() greater than 30 minutes. When the application requests the next batch of documents, the server returns an error. For operations that return a cursor, if the cursor may be idle for longer than 30 minutes, issue the operation within an explicit session using Mongo.startSession() and periodically refresh the session using the refreshSessions command. For example: var session = db.getMongo().startSession()var sessionId = session.getSessionId().id\nvar cursor = session.getDatabase(\"examples\").getCollection(\"data\").find().noCursorTimeout()var refreshTimestamp = new Date() // take note of time at operation start\nwhile (cursor.hasNext()) {\n  // Check if more than 5 minutes have passed since the last refresh  if ( (new Date()-refreshTimestamp)/1000 > 300 ) {    print(\"refreshing session\")    db.adminCommand({\"refreshSessions\" : [sessionId]})    refreshTimestamp = new Date()  }\n  // process cursor normally\n} \n\n For MongoDB drivers, defer to the driver documentation for instructions and syntax for creating sessions. \nTIP \nSEE ALSO: Session Idle Timeout \nEXAMPLES  EXAMPLE \nTHE FOLLOWING QUERY SPECIFIES A TIME LIMIT OF 50 MILLISECONDS: db.collection.find({description: /August [0-9]+, 1969/}).maxTimeMS(50) \n←  cursor.maxAwaitTimeMS()cursor.min() → On this page  * Definition\n * Behaviors\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.help/": " Docs Home → MongoDB Manual \nSH.HELP() \nsh.help() \nReturns:a basic help text for all sharding related shell functions. ←  sh.getBalancerState()sh.isBalancerRunning() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getLastErrorObj/": " Docs Home → MongoDB Manual \nDB.GETLASTERROROBJ() \nRemoved in MongoDB 5.1. Any code explicitly using getLastError, db.getLastError(), or db.getLastErrorObj() should instead use the CRUD API to issue the write with the desired write concern. Information about the success or failure of the write operation will be provided directly by the driver as a return value. What is MongoDB? → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.conf/": " Docs Home → MongoDB Manual \nRS.CONF() \nOn this page    \n * Definition\n   \n * Output Example \nDEFINITION \nrs.conf() \nReturns a document that contains the current replica set configuration. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the replSetGetConfig command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The replSetGetConfig command can specify a new option commitmentStatus: true when run on the primary. When run with the option, the command includes in the output a commitmentStatus field. This output field indicates whether the replica set's previous reconfig has been committed, so that the replica set is ready to be reconfigured again. The option is only available with the replSetGetConfig command. \nOUTPUT EXAMPLE \nThe following document provides a representation of a replica set configuration document. The configuration of your replica set may include only a subset of these settings: {  _id: <string>,  version: <int>,  term: <int>,  protocolVersion: <number>,  writeConcernMajorityJournalDefault: <boolean>,  configsvr: <boolean>,  members: [    {      _id: <int>,      host: <string>,      arbiterOnly: <boolean>,      buildIndexes: <boolean>,      hidden: <boolean>,      priority: <number>,      tags: <document>,      secondaryDelaySecs: <int>,      votes: <number>    },    ...  ],  settings: {    chainingAllowed : <boolean>,    heartbeatIntervalMillis : <int>,    heartbeatTimeoutSecs: <int>,    electionTimeoutMillis : <int>,    catchUpTimeoutMillis : <int>,    getLastErrorModes : <document>,    getLastErrorDefaults : <document>,    replicaSetId: <ObjectId>  }}  For description of the configuration settings, see Replica Set Configuration. rs.config() \nrs.config() is an alias of rs.conf(). ←  rs.addArb()rs.freeze() → On this page  * Definition\n * Output Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.getMongo/": " Docs Home → MongoDB Manual \nDB.GETMONGO()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. db.getMongo() \nReturns:The current database connection. db.getMongo() runs when the shell initiates. Use this command to test that mongosh has a connection to the proper database instance. \nNOTE The legacy mongo shell has a sub-command, db.getMongo().setSecondaryOk(), which is not available in mongosh. In mongosh, use Mongo.setReadPref() instead. ←  db.getLogComponents()db.getName() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.update/": " Docs Home → MongoDB Manual \nDB.COLLECTION.UPDATE() \nOn this page    \n * Definition\n   \n * Syntax\n * Access Control\n * Behavior\n * Examples\n * WriteResult \nIMPORTANT \nDEPRECATED MONGOSH METHOD This method is deprecated in mongosh. For alternative methods, see Compatibility Changes with Legacy mongo Shell. \nDEFINITION \ndb.collection.update(query, update, options) \nModifies an existing document or documents in a collection. The method can modify specific fields of an existing document or documents or replace an existing document entirely, depending on the update parameter. By default, the db.collection.update() method updates a single document. Include the option multi: true to update all documents that match the query criteria. \nSYNTAX \nChanged in version 5.0. The db.collection.update() method has the following form: db.collection.update(   <query>,   <update>,   {     upsert: <boolean>,     multi: <boolean>,     writeConcern: <document>,     collation: <document>,     arrayFilters: [ <filterdocument1>, ... ],     hint:  <document|string>, // Added in MongoDB 4.2     let: <document> // Added in MongoDB 5.0   })  \nPARAMETERS \nThe db.collection.update() method takes the following parameters: Parameter\nType\nDescription\nquery\ndocument The selection criteria for the update. The same query selectors as in the find() method are available. When you execute an update() with upsert: true and the query matches no existing document, MongoDB will refuse to insert a new document if the query specifies conditions on the _id field using dot notation. update\ndocument or pipeline The modifications to apply. Can be one of the following: Update document Contains only update operator expressions. Replacement document Contains only <field1>: <value1> pairs. Aggregation pipeline (Starting in MongoDB 4.2) Contains only the following aggregation stages:      * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. For details and examples, see Examples. upsert\nboolean Optional. When true, update() either:  * Creates a new document if no documents match the query. For more details see upsert behavior.  * Updates a single document that matches the query. If both upsert and multi are true and no documents match the query, the update operation inserts only a single document. To avoid multiple upserts, ensure that the query field(s) are uniquely indexed. See Upsert with Unique Index for an example. Defaults to false, which does not insert a new document when no match is found. multi\nboolean Optional. If set to true, updates multiple documents that meet the query criteria. If set to false, updates one document. The default value is false. For additional information, see Update Multiple Documents Examples. writeConcern\ndocument Optional. A document expressing the write concern. Omit to use the default write concern w: 1. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. For an example using writeConcern, see Override Default Write Concern. collation\ndocument Optional. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. For an example using collation, see Specify Collation. arrayFilters\narray Optional. An array of filter documents that determine which array elements to modify for an update operation on an array field. In the update document, use the $[<identifier>] to define an identifier to update only those array elements that match the corresponding filter document in the arrayFilters. \nNOTE You cannot have an array filter document for an identifier if the identifier is not included in the update document. For examples, see Specify arrayFilters for Array Update Operations. hint\nDocument or string Optional. A document or string that specifies the index to use to support the query predicate. The option can take an index specification document or the index name string. If you specify an index that does not exist, the operation errors. For an example, see Specify hint for Update Operations. New in version 4.2. let\ndocument Optional. Specifies a document with a list of variables. This allows you to improve command readability by separating the variables from the query text. The document syntax is:\n\n The variable is set to the value returned by the expression, and cannot be changed afterwards. To access the value of a variable in the command, use the double dollar sign prefix ($$) together with your variable name in the form $$<variable_name>. For example: $$targetTotal. \nNOTE To use a variable to filter results, you must access the variable within the $expr operator. For a complete example using let and variables, see Use Variables in let. New in version 5.0. \nRETURNS \nThe method returns a WriteResult document that contains the status of the operation. \nACCESS CONTROL \nOn deployments running with authorization, the user must have access that includes the following privileges:  * update action on the specified collection(s).  * find action on the specified collection(s).  * insert action on the specified collection(s) if the operation results in an upsert. The built-in role readWrite provides the required privileges. \nBEHAVIOR  USING $EXPR IN AN UPDATE WITH UPSERT \nAttempting to use the $expr operator with the upsert flag set to true will generate an error. \nSHARDED COLLECTIONS \nTo use db.collection.update() with multi: false on a sharded collection, you must include an exact match on the _id field or target a single shard (such as by including the shard key). When the db.collection.update() performs update operations (and not document replacement operations), db.collection.update() can target multiple shards. \nTIP \nSEE ALSO: findAndModify() REPLACE DOCUMENT OPERATIONS ON A SHARDED COLLECTION \nStarting in MongoDB 4.2, replace document operations attempt to target a single shard, first by using the query filter. If the operation cannot target a single shard by the query filter, it then attempts to target by the replacement document. In earlier versions, the operation attempts to target using the replacement document. UPSERT ON A SHARDED COLLECTION \nFor a db.collection.update() operation that includes upsert: true and is on a sharded collection, you must include the full shard key in the filter:  * For an update operation.  * For a replace document operation (starting in MongoDB 4.2). However, starting in version 4.4, documents in a sharded collection can be missing the shard key fields. To target a document that is missing the shard key, you can use the null equality match in conjunction with another filter condition (such as on the _id field). For example: { _id: <value>, <shardkeyfield>: null } // _id of the document missing shard key  SHARD KEY MODIFICATION \nStarting in MongoDB 4.2, you can update a document's shard key value unless the shard key field is the immutable _id field. In MongoDB 4.2 and earlier, a document's shard key field value is immutable. To modify the existing shard key value with db.collection.update():  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write.  * You must specify multi: false.  * You must include an equality query filter on the full shard key. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also upsert on a Sharded Collection. MISSING SHARD KEY \nStarting in version 4.4, documents in a sharded collection can be missing the shard key fields. To use db.collection.update() to set the document's missing shard key, you must run on a mongos. Do not issue the operation directly on the shard. In addition, the following requirements also apply: Task\nRequirements\nTo set to null  * Can specify multi: true.  * Requires equality filter on the full shard key if upsert: true. To set to a non-null value  * Must be performed either inside a transaction or as a retryable write.  * Must specify multi: false.  * Requires equality filter on the full shard key if either:\n   \n   * upsert: true, or\n   \n   * if using a replacement document and the new shard key value belongs to a different shard. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also:  * upsert on a Sharded Collection\n\n \nTRANSACTIONS \ndb.collection.update() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. UPSERT WITHIN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, db.collection.update() with upsert: true can be run on an existing collection or a non-existing collection. If run on a non-existing collection, the operation creates the collection. In MongoDB 4.2 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES \nThe following tabs showcase a variety of common update() operations. In mongosh, create a books collection which contains the following documents. This command first removes all previously existing documents from the books collection: db.books.remove({});\ndb.books.insertMany([  {    \"_id\" : 1,    \"item\" : \"TBD\",    \"stock\" : 0,    \"info\" : { \"publisher\" : \"1111\", \"pages\" : 430 },    \"tags\" : [ \"technology\", \"computer\" ],    \"ratings\" : [ { \"by\" : \"ijk\", \"rating\" : 4 }, { \"by\" : \"lmn\", \"rating\" : 5 } ],    \"reorder\" : false   },   {    \"_id\" : 2,    \"item\" : \"XYZ123\",    \"stock\" : 15,    \"info\" : { \"publisher\" : \"5555\", \"pages\" : 150 },    \"tags\" : [ ],    \"ratings\" : [ { \"by\" : \"xyz\", \"rating\" : 5 } ],    \"reorder\" : false   }]);   \nINSERT A NEW DOCUMENT IF NO MATCH EXISTS (UPSERT) \nWhen you specify the option upsert: true:  * If document(s) match the query criteria, db.collection.update() performs an update.  * If no document matches the query criteria, db.collection.update() inserts a single document.\n   \n   \n   NOTE\n   \n   If multiple, identical upserts are issued at roughly the same time, it is possible for update() used with upsert: true to create duplicate documents. See Upsert with Unique Index for more information. If you specify upsert: true on a sharded collection, you must include the full shard key in the filter. For additional db.collection.update() behavior on a sharded collection, see Sharded Collections. The following tabs showcase a variety of uses of the upsert modifier with update().  \nUPSERT WITH UNIQUE INDEX \nWhen using the upsert: true option with the update() method, and not using a unique index on the query field(s), multiple instances of a update() operation with similar query field(s) could result in duplicate documents being inserted in certain circumstances. Consider an example where no document with the name Andy exists and multiple clients issue the following command at roughly the same time: db.people.update(   { name: \"Andy\" },   { $inc: { score: 1 } },   {     upsert: true,     multi: true   })  If all update() operations finish the query phase before any client successfully inserts data, and there is no unique index on the name field, each update() operation may result in an insert, creating multiple documents with name: Andy.\n\n With this unique index in place, the multiple update() operations now exhibit the following behavior:  * Exactly one update() operation will successfully insert a new document.  * All other update() operations will update the newly-inserted document, incrementing the score value. \nTIP \nSEE ALSO: $setOnInsert \nUPDATE WITH AGGREGATION PIPELINE \nStarting in MongoDB 4.2, the db.collection.update() method can accept an aggregation pipeline [ <stage1>, <stage2>, ... ] that specifies the modifications to perform. The pipeline can consist of the following stages:  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. Using the aggregation pipeline allows for a more expressive update statement, such as expressing conditional updates based on current field values or updating one field using the value of another field(s). MODIFY A FIELD USING THE VALUES OF THE OTHER FIELDS IN THE DOCUMENT \nCreate a members collection with the following documents: db.members.insertMany( [   { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"A\", \"points\" : 2, \"misc1\" : \"note to self: confirm status\", \"misc2\" : \"Need to activate\", \"lastUpdate\" : ISODate(\"2019-01-01T00:00:00Z\") },   { \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60, \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\", \"lastUpdate\" : ISODate(\"2019-01-01T00:00:00Z\") }] )  Assume that instead of separate misc1 and misc2 fields, you want to gather these into a new comments field. The following update operation uses an aggregation pipeline to:  * add the new comments field and set the lastUpdate field.  * remove the misc1 and misc2 fields for all documents in the collection. db.members.update(   { },   [      { $set: { status: \"Modified\", comments: [ \"$misc1\", \"$misc2\" ], lastUpdate: \"$$NOW\" } },      { $unset: [ \"misc1\", \"misc2\" ] }   ],   { multi: true })  \nNOTE The $set and $unset used in the pipeline refers to the aggregation stages $set and $unset respectively, and not the update operators $set and $unset. First Stage The $set stage:  * creates a new array field comments whose elements are the current content of the misc1 and misc2 fields and  * sets the field lastUpdate to the value of the aggregation variable NOW. The aggregation variable NOW resolves to the current datetime value and remains the same throughout the pipeline. To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes. Second StageThe $unset stage removes the misc1 and misc2 fields. After the command, the collection contains the following documents: { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"Modified\", \"points\" : 2, \"lastUpdate\" : ISODate(\"2020-01-23T05:11:45.784Z\"), \"comments\" : [ \"note to self: confirm status\", \"Need to activate\" ] }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"Modified\", \"points\" : 60, \"lastUpdate\" : ISODate(\"2020-01-23T05:11:45.784Z\"), \"comments\" : [ \"reminder: ping me at 100pts\", \"Some random comment\" ] }  \nTIP \nSEE ALSO: Updates with Aggregation Pipeline PERFORM CONDITIONAL UPDATES BASED ON CURRENT FIELD VALUES \nCreate a students3 collection with the following documents:\n\n  Using an aggregation pipeline, you can update the documents with the calculated grade average and letter grade. db.students3.update(   { },   [     { $set: { average : { $trunc: [ { $avg: \"$tests\" }, 0 ] }, lastUpdate: \"$$NOW\" } },     { $set: { grade: { $switch: {                           branches: [                               { case: { $gte: [ \"$average\", 90 ] }, then: \"A\" },                               { case: { $gte: [ \"$average\", 80 ] }, then: \"B\" },                               { case: { $gte: [ \"$average\", 70 ] }, then: \"C\" },                               { case: { $gte: [ \"$average\", 60 ] }, then: \"D\" }                           ],                           default: \"F\"     } } } }   ],   { multi: true })  \nNOTE The $set used in the pipeline refers to the aggregation stage $set, and not the update operators $set. First Stage The $set stage:  * calculates a new field average based on the average of the tests field. See $avg for more information on the $avg aggregation operator and $trunc for more information on the $trunc truncate aggregation operator.  * sets the field lastUpdate to the value of the aggregation variable NOW. The aggregation variable NOW resolves to the current datetime value and remains the same throughout the pipeline. To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes. Second StageThe $set stage calculates a new field grade based on the average field calculated in the previous stage. See $switch for more information on the $switch aggregation operator. After the command, the collection contains the following documents: { \"_id\" : 1, \"tests\" : [ 95, 92, 90 ], \"lastUpdate\" : ISODate(\"2020-01-24T17:29:35.340Z\"), \"average\" : 92, \"grade\" : \"A\" }{ \"_id\" : 2, \"tests\" : [ 94, 88, 90 ], \"lastUpdate\" : ISODate(\"2020-01-24T17:29:35.340Z\"), \"average\" : 90, \"grade\" : \"A\" }{ \"_id\" : 3, \"tests\" : [ 70, 75, 82 ], \"lastUpdate\" : ISODate(\"2020-01-24T17:29:35.340Z\"), \"average\" : 75, \"grade\" : \"C\" }  \nTIP \nSEE ALSO: Updates with Aggregation Pipeline \nSPECIFY ARRAYFILTERS FOR ARRAY UPDATE OPERATIONS \nIn the update document, use the $[<identifier>] filtered positional operator to define an identifier, which you then reference in the array filter documents. You cannot have an array filter document for an identifier if the identifier is not included in the update document. \nNOTE The <identifier> must begin with a lowercase letter and contain only alphanumeric characters. You can include the same identifier multiple times in the update document; however, for each distinct identifier ($[identifier]) in the update document, you must specify exactly one corresponding array filter document. That is, you cannot specify multiple array filter documents for the same identifier. For example, if the update statement includes the identifier x (possibly multiple times), you cannot specify the following for arrayFilters that includes 2 separate filter documents for x: // INVALID\n[  { \"x.a\": { $gt: 85 } },  { \"x.b\": { $gt: 80 } }] However, you can specify compound conditions on the same identifier in a single filter document, such as in the following examples:\n\n  arrayFilters is not available for updates that use an aggregation pipeline. UPDATE ELEMENTS MATCH ARRAYFILTERS CRITERIA \nTo update all array elements which match a specified criteria, use the arrayFilters parameter. In mongosh, create a students collection with the following documents: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] },   { \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] },   { \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }] )  To update all elements that are greater than or equal to 100 in the grades array, use the filtered positional operator $[<identifier>] with the arrayFilters option: db.students.update(   { grades: { $gte: 100 } },   { $set: { \"grades.$[element]\" : 100 } },   {     multi: true,     arrayFilters: [ { \"element\": { $gte: 100 } } ]   })  After the operation, the collection contains the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 100 ] }{ \"_id\" : 3, \"grades\" : [ 95, 100, 100 ] }  UPDATE SPECIFIC ELEMENTS OF AN ARRAY OF DOCUMENTS \nYou can also use the arrayFilters parameter to update specific document fields within an array of documents. In mongosh, create a students2 collection with the following documents: db.students2.insertMany( [  {    \"_id\" : 1,    \"grades\" : [       { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },       { \"grade\" : 85, \"mean\" : 90, \"std\" : 4 },       { \"grade\" : 85, \"mean\" : 85, \"std\" : 6 }    ]  }  {     \"_id\" : 2,     \"grades\" : [        { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },        { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },        { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }     ]  }] )  To modify the value of the mean field for all elements in the grades array where the grade is greater than or equal to 85, use the filtered positional operator $[<identifier>] with the arrayFilters: db.students2.update(   { },   { $set: { \"grades.$[elem].mean\" : 100 } },   {     multi: true,     arrayFilters: [ { \"elem.grade\": { $gte: 85 } } ]   })  After the operation, the collection has the following documents: {   \"_id\" : 1,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 6 }   ]}{   \"_id\" : 2,   \"grades\" : [      { \"grade\" : 90, \"mean\" : 100, \"std\" : 6 },      { \"grade\" : 87, \"mean\" : 100, \"std\" : 3 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 }   ]}  \nSPECIFY HINT FOR UPDATE OPERATIONS \nNew in version 4.2. In mongosh, create a members collection with the following documents:\n\n  Create the following indexes on the collection: db.members.createIndex( { status: 1 } )db.members.createIndex( { points: 1 } )  The following update operation explicitly hints to use the index {status: 1 }: \nNOTE If you specify an index that does not exist, the operation errors. db.members.update(   { points: { $lte: 20 }, status: \"P\" },     // Query parameter   { $set: { misc1: \"Need to activate\" } },   // Update document   { multi: true, hint: { status: 1 } }       // Options)  The update command returns the following: WriteResult({ \"nMatched\" : 3, \"nUpserted\" : 0, \"nModified\" : 3 })  To see the index used, run explain on the operation: db.members.explain().update(   { \"points\": { $lte: 20 }, \"status\": \"P\" },   { $set: { \"misc1\": \"Need to activate\" } },   { multi: true, hint: { status: 1 } })  The db.collection.explain().update() does not modify the documents. \nUSE VARIABLES IN LET \nNew in version 5.0. To define variables that you can access elsewhere in the command, use the let option. \nNOTE To filter results using a variable, you must access the variable within the $expr operator. Create a collection cakeFlavors: db.cakeFlavors.insertMany( [   { _id: 1, flavor: \"chocolate\" },   { _id: 2, flavor: \"strawberry\" },   { _id: 3, flavor: \"cherry\" }] )  The following example defines targetFlavor and newFlavor variables in let and uses the variables to change the cake flavor from cherry to orange: db.cakeFlavors.update(   { $expr: { $eq: [ \"$flavor\", \"$$targetFlavor\" ] } },   [ { $set: { flavor: \"$$newFlavor\" } } ],   { let : { targetFlavor: \"cherry\", newFlavor: \"orange\" } })  \nOVERRIDE DEFAULT WRITE CONCERN \nThe following operation to a replica set specifies a write concern of w: 2 with a wtimeout of 5000 milliseconds. This operation either returns after the write propagates to both the primary and one secondary, or times out after 5 seconds. db.books.update(   { stock: { $lte: 10 } },   { $set: { reorder: true } },   {     multi: true,     writeConcern: { w: 2, wtimeout: 5000 }   })  \nSPECIFY COLLATION \nSpecifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. In mongosh, create a collection named myColl with the following documents: db.myColl.insertMany( [    { _id: 1, category: \"café\", status: \"A\" },    { _id: 2, category: \"cafe\", status: \"a\" },    { _id: 3, category: \"cafE\", status: \"a\" }] )  The following operation includes the collation option and sets multi to true to update all matching documents:\n\n  The write result of the operation returns the following document, indicating that all three documents in the collection were updated: WriteResult({ \"nMatched\" : 3, \"nUpserted\" : 0, \"nModified\" : 3 }) After the operation, the collection contains the following documents: { \"_id\" : 1, \"category\" : \"café\", \"status\" : \"Updated\" }{ \"_id\" : 2, \"category\" : \"cafe\", \"status\" : \"Updated\" }{ \"_id\" : 3, \"category\" : \"cafE\", \"status\" : \"Updated\" }  \nWRITERESULT  SUCCESSFUL RESULTS \nThe db.collection.update() method returns a WriteResult() object that contains the status of the operation. Upon success, the WriteResult() object contains the number of documents that matched the query condition, the number of documents inserted by the update, and the number of documents modified: WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })  \nTIP \nSEE:  * WriteResult.nMatched  * WriteResult.nUpserted  * WriteResult.nModified \nWRITE CONCERN ERRORS \nIf the db.collection.update() method encounters write concern errors, the results include the WriteResult.writeConcernError field: Changed in version 4.4. WriteResult({   \"nMatched\" : 1,   \"nUpserted\" : 0,   \"nModified\" : 1,   \"writeConcernError\": {     \"code\" : 64,     \"errmsg\" : \"waiting for replication timed out\",     \"errInfo\" : {       \"wtimeout\" : true,       \"writeConcern\" : {         \"w\" : \"majority\",         \"wtimeout\" : 100,         \"provenance\" : \"getLastErrorDefaults\"       }   }})  The following table explains the possible values of WriteResult.writeConcernError.provenance: Provenance\nDescription\nclientSupplied\nThe write concern was specified in the application.\ncustomDefault\nThe write concern originated from a custom defined default value. See setDefaultRWConcern.\ngetLastErrorDefaults\nThe write concern originated from the replica set's settings.getLastErrorDefaults field.\nimplicitDefault\nThe write concern originated from the server in absence of all other write concern specifications. \nTIP \nSEE ALSO: WriteResult.hasWriteConcernError() \nERRORS UNRELATED TO WRITE CONCERN \nIf the db.collection.update() method encounters a non-write concern error, the results include the WriteResult.writeError field: WriteResult({   \"nMatched\" : 0,   \"nUpserted\" : 0,   \"nModified\" : 0,   \"writeError\" : {      \"code\" : 7,      \"errmsg\" : \"could not contact primary for replica set shard-a\"   }})  \nTIP \nSEE ALSO: WriteResult.hasWriteError() ←  db.collection.unhideIndex()db.collection.updateOne() → On this page  * Definition\n * Syntax\n * Access Control\n * Behavior\n * Examples\n * WriteResult Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.hint/": " Docs Home → MongoDB Manual \nCURSOR.HINT() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ncursor.hint(index)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Call this method on a query to override MongoDB's default index selection and query optimization process. Use db.collection.getIndexes() to return the list of current indexes on a collection. The cursor.hint() method has the following parameter: Parameter\nType\nDescription\nindex\nstring or document The index to \"hint\" or force MongoDB to use when performing the query. Specify the index either by the index name or by the index specification document. You can also specify { $natural : 1 } to force the query to perform a forwards collection scan, or { $natural : -1 } for a reverse collection scan. \nBEHAVIOR \n     * When an index filter exists for the query shape, MongoDB ignores the hint().  * You cannot use hint() if the query includes a $text query expression.  * You cannot use hint() on a hidden index.  * On a time series collections, you can only specify hints using the index name, not the index key pattern. \nEXAMPLES  SPECIFY AN INDEX \nThe following example returns all documents in the collection named users using the index on the age field. db.users.find().hint( { age: 1 } )  You can also specify the index using the index name: db.users.find().hint( \"age_1\" )  \nFORCE COLLECTION SCANS \nYou can specify { $natural : 1 } to force the query to perform a forwards collection scan: db.users.find().hint( { $natural : 1 } )  You can also specify { $natural : -1 } to force the query to perform a reverse collection scan: db.users.find().hint( { $natural : -1 } )  \nTIP \nSEE ALSO:  * Indexes  * Query Plans  * Index Filters ←  cursor.hasNext()cursor.isExhausted() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.dropAllRoles/": " Docs Home → MongoDB Manual \nDB.DROPALLROLES() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ndb.dropAllRoles( writeConcern ) \nDeletes all user-defined roles on the database where you run the method. \nWARNING The db.dropAllRoles() method removes all user-defined roles from the database. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the dropAllRolesFromDatabase command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.dropAllRoles() method takes the following argument: Field\nType\nDescription\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. Returns:The number of user-defined roles dropped. \nBEHAVIOR  REPLICA SET \nIf run on a replica set, db.dropAllRoles() is executed using \"majority\" write concern by default. \nREQUIRED ACCESS \nYou must have the dropRole action on a database to drop a role from that database. \nEXAMPLE \nThe following operations drop all user-defined roles from the products database and uses a write concern of majority. use productsdb.dropAllRoles( { w: \"majority\" } )  The method returns the number of roles dropped: 4 \n←  db.dropRole()db.getRole() → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.disableAutoSplit/": " Docs Home → MongoDB Manual \nSH.DISABLEAUTOSPLIT \nOn this page    \n * Description \nNOTE Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. \nDESCRIPTION \nsh.disableAutoSplit() \nDisables the autosplit flag in the config.settings collection. When auto-splitting is enabled for a sharded cluster, MongoDB automatically splits chunks based on the shard key values the chunk represents to keep the chunks from growing too large. You can only run sh.disableAutoSplit() from a mongosh session that is connected to a mongos instance. sh.disableAutoSplit() errors if run on a mongod instance. \nNOTE Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0, sh.stopBalancer() also disables auto-splitting for the sharded cluster. \nTIP \nSEE ALSO:      * Manage Sharded Cluster Balancer  * Sharded Cluster Balancer ←  sh.enableBalancing()sh.enableAutoSplit → On this page  * Description Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Date/": " Docs Home → MongoDB Manual \nDATE() \nOn this page    \n * Behavior\n   \n * Examples Date() \nReturns a date either as a string or as a Date object.      * Date() returns the current date as a string in mongosh.  * new Date() returns the current date as a Date object. mongosh wraps the Date object with the ISODate helper. The ISODate is in UTC. You can specify a particular date by passing an ISO-8601 date string with a year within the inclusive range 0 through 9999 to the new Date() constructor or the ISODate() function. These functions accept the following formats:  * new Date(\"<YYYY-mm-dd>\") returns the ISODate with the specified date.  * new Date(\"<YYYY-mm-ddTHH:MM:ss>\") specifies the datetime in the client's local timezone and returns the ISODate with the specified datetime in UTC.  * new Date(\"<YYYY-mm-ddTHH:MM:ssZ>\") specifies the datetime in UTC and returns the ISODate with the specified datetime in UTC.  * new Date(<integer>) specifies the datetime as milliseconds since the UNIX epoch (Jan 1, 1970), and returns the resulting ISODate instance. \nBEHAVIOR \nInternally, Date objects are stored as a signed 64-bit integer representing the number of milliseconds since the Unix epoch (Jan 1, 1970). Not all database operations and drivers support the full 64-bit range. You may safely work with dates with years within the inclusive range 0 through 9999. \nEXAMPLES  USE DATE IN A QUERY \nIf no document with _id equal to 1 exists in the products collection, the following operation inserts a document with the field dateAdded set to the current date: db.products.updateOne(   { _id: 1 },   {     $set: { item: \"apple\" },     $setOnInsert: { dateAdded: new Date() }   },   { upsert: true })  \nTIP \nSEE ALSO:  * $currentDate  * NOW aggregation variable usage to update with aggregation pipeline \nRETURN DATE AS A STRING \nTo return the date as a string, use the Date() method, as in the following example: var myDateString = Date();  \nRETURN DATE AS DATE OBJECT \nmongosh wraps objects of Date type with the ISODate helper; however, the objects remain of type Date. The following example uses new Date() to return Date object with the specified UTC datetime. var myDate = new Date(\"2016-05-18T16:00:00Z\");  \nTIP \nSEE ALSO:  * BSON Date  * mongo Shell Date \nINSERT AND RETURN ISODATE OBJECTS \nYou can specify dates as ISODate objects. The following example creates a cakeSales collection with ISODate objects in the orderDate field: db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new ISODate(\"2020-05-18T14:10:30Z\") },   { _id: 1, type: \"strawberry\", orderDate: new ISODate(\"2021-03-20T11:30:05Z\") },   { _id: 2, type: \"vanilla\", orderDate: new ISODate(\"2021-01-15T06:31:15Z\") }] )  The following example returns documents where the orderDate is less than the ISODate specified in the $lt operator: db.cakeSales.find( { orderDate: { $lt: ISODate(\"2021-02-25T10:03:46.000Z\") } } )  Example output: [   {      _id: 0,      type: 'chocolate',      orderDate: ISODate(\"2020-05-18T14:10:30.000Z\")   },   {      _id: 2,      type: 'vanilla',      orderDate: ISODate(\"2021-01-15T06:31:15.000Z\")   }] ←  BulkWriteResult()ObjectId → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.enableSharding/": " Docs Home → MongoDB Manual \nSH.ENABLESHARDING() \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example \nDEFINITION \nsh.enableSharding(database, primaryShard)  NOTE Changed in version 6.0. Starting in MongoDB 6.0, this method is not required to shard a collection. Explicitly creates a database. Use the mongosh method sh.shardCollection() to shard collections on the database. The mongosh method sh.enableSharding() wraps the enableSharding command. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the enableSharding command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 \nSYNTAX \nThe sh.enableSharding() has the following form: sh.enableSharding(   <database>,   <primary shard>  // Optional. Available starting in MongoDB 4.2.2 (and 4.0.14))  \nPARAMETER \nThe sh.enableSharding() method takes the following parameter: Parameter\nType\nDescription\ndatabase\nString The name of the database you want to create. primaryShard\nString Optional. The primary shard for the database; the primary shard contains the unsharded collection for this database. In general, rather than explicitly specifying the primary shard, it is recommended to let the balancer select the primary shard instead. \nWARNING \nTIP In general, you should not need to specify the primary shard. Allow the balancer to select the primary shard instead. Available starting in MongoDB 4.2.2 (and 4.0.14) \nBEHAVIOR  WRITE CONCERN \nmongos uses \"majority\" for the enableSharding command and its helper sh.enableSharding(). \nPRIMARY SHARD \nIn general, you should not need to specify the primaryShard in the command. Instead, allow the balancer to select the primary shard. However, if you do specify the primary shard for a database and the database is already enabled for sharding with a different primary shard, the operation returns an error and the primary shard for the database remains as before. To change the primary shard for a database, use the movePrimary command instead. \nEXAMPLE \nThe following example, run from a mongos, explicitly creates the shardTest database: sh.enableSharding(\"shardTest\")  To verify, run sh.status(). \nTIP \nSEE ALSO:  * sh.shardCollection()  * Deploy a Sharded Cluster ←  sh.enableAutoSplitsh.getBalancerState() → On this page  * Definition\n * Syntax\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/rs.addArb/": " Docs Home → MongoDB Manual \nRS.ADDARB() \nOn this page    \n * Description\n   \n * IP Binding \nDESCRIPTION \nrs.addArb(host) \nAdds a new arbiter to an existing replica set. rs.add(<host>, true) is functionally the same as rs.addArb(<host>). You can use these commands interchangeably. \nWARNING Avoid deploying more than one arbiter in a replica set. See Concerns with Multiple Arbiters. To add an arbiter to an existing replica set:      * Typically, if there are two or fewer data-bearing members in the replica set, you might need to first set the cluster wide write concern for the replica set.  * See cluster wide write concern for more information on why you might need to set the cluster wide write concern. You do not need to change the cluster wide write concern before starting a new replica set with an arbiter. \nTIP \nSEE ALSO: Default write concern formula The rs.addArb() method takes the following parameter: Parameter\nType\nDescription\nhost\nstring\nSpecifies the hostname and optionally the port number of the arbiter member to add to replica set. For the following MongoDB versions, pv1 increases the likelihood of w:1 rollbacks compared to pv0 (no longer supported in MongoDB 4.0+) for replica sets with arbiters:  * MongoDB 3.4.1  * MongoDB 3.4.0  * MongoDB 3.2.11 or earlier See Replica Set Protocol Version. \nIP BINDING  WARNING Before binding to a non-localhost (e.g. publicly accessible) IP address, ensure you have secured your cluster from unauthorized access. For a complete list of security recommendations, see Security Checklist. At minimum, consider enabling authentication and hardening network infrastructure. MongoDB binaries, mongod and mongos, bind to localhost by default. If the net.ipv6 configuration file setting or the --ipv6 command line option is set for the binary, the binary additionally binds to the localhost IPv6 address. By default mongod and mongos that are bound to localhost only accept connections from clients that are running on the same computer. This binding behavior includes mongosh and other members of your replica set or sharded cluster. Remote clients cannot connect to binaries that are bound only to localhost. To override the default binding and bind to other IP addresses, use the net.bindIp configuration file setting or the --bind_ip command-line option to specify a list of hostnames or IP addresses. \nWARNING Starting in MongDB 5.0, split horizon DNS nodes that are only configured with an IP address fail startup validation and report an error. See disableSplitHorizonIPCheck. For example, the following mongod instance binds to both the localhost and the hostname My-Example-Associated-Hostname, which is associated with the IP address 198.51.100.1: mongod --bind_ip localhost,My-Example-Associated-Hostname  In order to connect to this instance, remote clients must specify the hostname or its associated IP address 198.51.100.1: mongosh --host My-Example-Associated-Hostname\nmongosh --host 198.51.100.1  \nIMPORTANT To avoid configuration updates due to IP address changes, use DNS hostnames instead of IP addresses. It is particularly important to use a DNS hostname instead of an IP address when configuring replica set members or sharded cluster members. Use hostnames instead of IP addresses to configure clusters across a split network horizon. Starting in MongoDB 5.0, nodes that are only configured with an IP address will fail startup validation and will not start. ←  rs.add()rs.conf() → On this page  * Description\n * IP Binding Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.reIndex/": " Docs Home → MongoDB Manual \nDB.COLLECTION.REINDEX() \nOn this page    \n * Definition\n   \n * Behavior \nDEFINITION \ndb.collection.reIndex() \nDeprecated since version 6.0. Attempting to run the db.collection.reIndex() method writes a warning message to the log. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the reIndex command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.collection.reIndex() drops all indexes on a collection and recreates them. This operation may be expensive for collections that have a large amount of data and/or a large number of indexes. \nWARNING  * db.collection.reIndex() may only be run on standalone instances.  * For most users, the db.collection.reIndex() command is unnecessary. \nBEHAVIOR \nFor MongoDB 5.0 or later, db.collection.reIndex() may only be run on standalone instances. \nRESOURCE LOCKING \nChanged in version 4.2.2. For MongoDB 4.2.2 and later, db.collection.reIndex() obtains an exclusive (W) lock on the collection and blocks other operations on the collection until it completes. For more information on locking in MongoDB, see FAQ: Concurrency. \nTIP \nSEE ALSO: Indexes ←  db.collection.mapReduce()db.collection.remove() → On this page  * Definition\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.pretty/": " Docs Home → MongoDB Manual \nCURSOR.PRETTY() \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \ncursor.pretty()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Configures the cursor to display results in a format that is easy to read. The pretty() method has the following prototype form: db.collection.find(<query>).pretty()  \nBEHAVIOR \nThe pretty() method:      * Does not change the output format in mongosh.  * Changes the output format in the legacy mongo shell. \nEXAMPLES \nConsider the following document: db.books.save({    \"_id\" : ObjectId(\"54f612b6029b47909a90ce8d\"),    \"title\" : \"A Tale of Two Cities\",    \"text\" : \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\",    \"authorship\" : \"Charles Dickens\"})  By default, db.collection.find() returns data in a dense format: db.books.find(){ \"_id\" : ObjectId(\"54f612b6029b47909a90ce8d\"), \"title\" : \"A Tale of Two Cities\", \"text\" : \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\", \"authorship\" : \"Charles Dickens\" }  By using cursor.pretty() you can set the cursor to return data in a format that is easier to read: db.books.find().pretty(){    \"_id\" : ObjectId(\"54f612b6029b47909a90ce8d\"),    \"title\" : \"A Tale of Two Cities\",    \"text\" : \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\",    \"authorship\" : \"Charles Dickens\"} \n←  cursor.objsLeftInBatch()cursor.readConcern() → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.removeShardFromZone/": " Docs Home → MongoDB Manual \nSH.REMOVESHARDFROMZONE() \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \nsh.removeShardFromZone(shard, zone) \nRemoves the association between a zone and a shard. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the removeShardFromZone command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 This method has the following parameters: Parameter\nType\nDescription\nshard\nstring The name of the shard from which to remove the zone association. zone\nstring The name of the zone whose association with the shard you want to remove. Only issue sh.removeShardFromZone() when connected to a mongos instance. \nBEHAVIOR \nsh.removeShardFromZone() does not remove ranges associated with the zone. To completely remove a zone from the cluster, you must run sh.removeShardFromZone() on each shard associated with the zone. If the shard specified is the last shard associated with the zone, you must ensure there are no remaining ranges associated with the zone. Use updateZoneKeyRange to remove any existing ranges associated to the zone before running sh.removeShardFromZone(). See the zone manual page for more information on zones in sharded clusters. \nBALANCER \nRemoving the association between a zone and a shard removes the constraints keeping chunks covered by the zone on the shard. During the next balancer round, the balancer may migrate chunks that previously covered by the zone. See the sharded cluster balancer manual page for more information on how migrations work in a sharded cluster. \nSECURITY \nFor sharded clusters that enforce access control, you must authenticate either as:  * a user whose privileges include:\n   \n   * update on the shards collection in the config database, and\n   \n   * find on the tags collection in the config database;\n   \n   or, alternatively,  * a user whose privileges include enableSharding on the cluster resource (available starting in version 4.2.2, 4.0.14, 3.6.16). The clusterAdmin or clusterManager built-in roles have the appropriate permissions for issuing sh.removeShardFromZone(). See the Role-Based Access Control manual page for more information. \nEXAMPLE \nThe following example removes the association between shard0000 and the NYC zone: sh.removeShardFromZone(\"shard0000\", \"NYC\")  \nTIP \nSEE ALSO: sh.addShardToZone() sh.updateZoneKeyRange() sh.removeRangeFromZone() ←  sh.removeShardTag()sh.removeTagRange() → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.watch/": " Docs Home → MongoDB Manual \nDB.WATCH() \nOn this page    \n * Definition\n   \n * Availability\n * Deployment\n * Storage Engine\n * Read Concern majority Support\n * Behavior\n * Resumability\n * Full Document Lookup of Update Operations\n * Access Control\n * Cursor Iteration\n * Example \nDEFINITION \ndb.watch( pipeline, options ) \nFor replica sets and sharded clusters only Opens a change stream cursor for a database to report on all its non-system collections. Parameter\nType\nDescription\npipeline\narray Optional. An Aggregation Pipeline consisting of one or more of the following aggregation stages:      * $addFields  * $match  * $project  * $replaceRoot  * $replaceWith (Available starting in MongoDB 4.2)  * $redact  * $set (Available starting in MongoDB 4.2)  * $unset (Available starting in MongoDB 4.2) Specify a pipeline to filter/modify the change events output. Starting in MongoDB 4.2, change streams will throw an exception if the change stream aggregation pipeline modifies an event's _id field. options\ndocument\nOptional. Additional options that modify the behavior of db.watch(). The options document can contain the following fields and values: Field\nType\nDescription\nresumeAfter\ndocument Optional. Directs db.watch() to attempt resuming notifications starting after the operation specified in the resume token. Each change stream event document includes a resume token as the _id field. Pass the entire _id field of the change event document that represents the operation you want to resume after. resumeAfter is mutually exclusive with startAfter and startAtOperationTime. startAfter\ndocument Optional. Directs db.watch() to attempt starting a new change stream after the operation specified in the resume token. Allows notifications to resume after an invalidate event. Each change stream event document includes a resume token as the _id field. Pass the entire _id field of the change event document that represents the operation you want to resume after. startAfter is mutually exclusive with resumeAfter and startAtOperationTime. New in version 4.2. fullDocument\nstring Optional. By default, db.watch() returns the delta of those fields modified by an update operation, instead of the entire updated document. Set fullDocument to \"updateLookup\" to direct db.watch() to look up the most current majority-committed version of the updated document. db.watch() returns a fullDocument field with the document lookup in addition to the updateDescription delta. Starting in MongoDB 6.0, you can set fullDocument to:  * \"whenAvailable\" to output the document post-image, if available, after the document was inserted, replaced, or updated.  * \"required\" to output the document post-image after the document was inserted, replaced, or updated. Raises an error if the post-image is not available. fullDocumentBeforeChange\nstring Optional. Default is \"off\". Starting in MongoDB 6.0, you can use the new fullDocumentBeforeChange field and set it to:  * \"whenAvailable\" to output the document pre-image, if available, before the document was replaced, updated, or deleted.  * \"required\" to output the document pre-image before the document was replaced, updated, or deleted. Raises an error if the pre-image is not available.  * \"off\" to suppress the document pre-image. \"off\" is the default. batchSize\nint Optional. Specifies the maximum number of change events to return in each batch of the response from the MongoDB cluster. Has the same functionality as cursor.batchSize(). maxAwaitTimeMS\nint Optional. The maximum amount of time in milliseconds the server waits for new data changes to report to the change stream cursor before returning an empty batch. Defaults to 1000 milliseconds. collation\ndocument Optional. Pass a collation document to specify a collation for the change stream cursor. If omitted, defaults to simple binary comparison. startAtOperationTime\nTimestamp Optional. The starting point for the change stream. If the specified starting point is in the past, it must be in the time range of the oplog. To check the time range of the oplog, see rs.printReplicationInfo(). startAtOperationTime is mutually exclusive with resumeAfter and startAfter. Returns:A cursor over the change event documents. See Change Events for examples of change event documents. \nTIP \nSEE ALSO: db.collection.watch() and Mongo.watch() \nAVAILABILITY  DEPLOYMENT \ndb.watch() is available for replica sets and sharded clusters:\n\n  * For a sharded cluster, you must issue db.watch() on a mongos instance. \nSTORAGE ENGINE \nYou can only use db.watch() with the Wired Tiger storage engine. \nREAD CONCERN MAJORITY SUPPORT \nStarting in MongoDB 4.2, change streams are available regardless of the \"majority\" read concern support; that is, read concern majority support can be either enabled (default) or disabled to use change streams. In MongoDB 4.0 and earlier, change streams are available only if \"majority\" read concern support is enabled (default). \nBEHAVIOR \n * You cannot run db.watch() on the admin, local, or config database.  * db.watch() only notifies on data changes that have persisted to a majority of data-bearing members.  * The change stream cursor remains open until one of the following occurs:\n   \n   * The cursor is explicitly closed.\n   \n   * An invalidate event occurs; for example, a collection drop or rename.\n   \n   * The connection to the MongoDB deployment is closed.\n   \n   * If the deployment is a sharded cluster, a shard removal may cause an open change stream cursor to close, and the closed change stream cursor may not be fully resumable.  * You can run db.watch() for a database that does not exist. However, once the database is created and you drop the database, the change stream cursor closes. \nRESUMABILITY \nUnlike the MongoDB Drivers, mongosh does not automatically attempt to resume a change stream cursor after an error. The MongoDB drivers make one attempt to automatically resume a change stream cursor after certain errors. db.watch() uses information stored in the oplog to produce the change event description and generate a resume token associated to that operation. If the operation identified by the resume token passed to the resumeAfter or startAfter option has already dropped off the oplog, db.watch() cannot resume the change stream. See Resume a Change Stream for more information on resuming a change stream. \nNOTE  * You cannot use resumeAfter to resume a change stream after an invalidate event (for example, a collection drop or rename) closes the stream. Starting in MongoDB 4.2, you can use startAfter to start a new change stream after an invalidate event.  * If the deployment is a sharded cluster, a shard removal may cause an open change stream cursor to close, and the closed change stream cursor may not be fully resumable. \nNOTE You cannot use resumeAfter to resume a change stream after an invalidate event (for example, a collection drop or rename) closes the stream. Starting in MongoDB 4.2, you can use startAfter to start a new change stream after an invalidate event. \nFULL DOCUMENT LOOKUP OF UPDATE OPERATIONS \nBy default, the change stream cursor returns specific field changes/deltas for update operations. You can also configure the change stream to look up and return the current majority-committed version of the changed document. Depending on other write operations that may have occurred between the update and the lookup, the returned document may differ significantly from the document at the time of the update. Depending on the number of changes applied during the update operation and the size of the full document, there is a risk that the size of the change event document for an update operation is greater than the 16MB BSON document limit. If this occurs, the server closes the change stream cursor and returns an error. \nACCESS CONTROL \nWhen running with access control, the user must have the find and changeStream privilege actions on the database resource. That is, a user must have a role that grants the following privilege: { resource: { db: <dbname>, collection: \"\" }, actions: [ \"find\", \"changeStream\"] }  The built-in read role provides the appropriate privileges. \nCURSOR ITERATION \nMongoDB provides multiple ways to iterate on a cursor. The cursor.hasNext() method blocks and waits for the next event. To monitor the watchCursor cursor and iterate over the events, use hasNext() like this: while (!watchCursor.isClosed()) {   if (watchCursor.hasNext()) {     firstChange = watchCursor.next();     break;   }}  The cursor.tryNext() method is non-blocking. To monitor the watchCursor cursor and iterate over the events, use tryNext() like this:\n\n  \nEXAMPLE \nThe following operation in mongosh opens a change stream cursor on the hr database. The returned cursor reports on data changes to all the non-system collections in that database. watchCursor = db.getSiblingDB(\"hr\").watch()  Iterate the cursor to check for new events. Use the cursor.isClosed() method with the cursor.tryNext() method to ensure the loop only exits if the change stream cursor is closed and there are no objects remaining in the latest batch: while (!watchCursor.isClosed()) {  let next = watchCursor.tryNext()  while (next !== null) {    printjson(next);    next = watchCursor.tryNext()  }}  For complete documentation on change stream output, see Change Events. ←  db.version()Query Plan Cache Methods → On this page  * Definition\n * Availability\n * Deployment\n * Storage Engine\n * Read Concern majority Support\n * Behavior\n * Resumability\n * Full Document Lookup of Update Operations\n * Access Control\n * Cursor Iteration\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.printCollectionStats/": " Docs Home → MongoDB Manual \nDB.PRINTCOLLECTIONSTATS() \ndb.printCollectionStats() \nProvides a wrapper around the db.collection.stats() method. Returns statistics from every collection separated by three hyphen characters. \nNOTE The db.printCollectionStats() method run in mongosh does not return JSON. Use db.printCollectionStats() for manual inspection, and db.collection.stats() in scripts. \nTIP \nSEE ALSO: collStats ←  db.logout()db.printReplicationInfo() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.rewrapManyDataKey/": " Docs Home → MongoDB Manual \nKEYVAULT.REWRAPMANYDATAKEY() \nOn this page    \n * Behavior\n   \n * Example KeyVault.rewrapManyDataKey(filter, options) \nDecrypts multiple Data Encryption Keys (DEK) and re-encrypts them with a new Customer Master Key (CMK). Use this method to rotate the CMK that encrypts your DEKs. To learn more about CMKs and DEKs, see Keys. You specify a CMK through the masterKey parameter. If you do not include a masterKey argument, the method decrypts and encrypts each DEK with the CMK referenced in that DEK's metadata. To learn more about the metadata of DEKs, see Metadata Used for Decryption. KeyVault.rewrapManyDataKey has the following syntax: let keyVault = db.getMongo().getKeyVault()\nkeyVault.rewrapManyDataKey(  <filter>,  <options>) \nParameter\nType\nDescription\nfilter\nquery filter document\nThe query filter for the keyvault collection\noptions\ndocument This document has two fields:      * provider: A KMS provider (AWS KMS, Azure Key Vault, GCP KMS, the local provider, or KMIP)  * masterKey: A KMS-specific key used to encrypt the new data key Returns:A BulkWriteResult object that reports how many data keys were affected. \nWARNING \nBACK-UP YOUR KEY VAULT COLLECTION Before you rotate your Data Encryption Keys, ensure you create a backup of your Key Vault collection. If you lose access to your Data Encryption Keys, you will lose all your encrypted data. To learn how to create a backup of a collection, see Back Up and Restore with MongoDB Tools. \nIMPORTANT \nKEY ROTATION SUPPORT To view your driver's dependencies for the key rotation API, see Compatibility. \nBEHAVIOR \nThis operation is not atomic and should not be run in parallel with other key management operations. \nREQUIRES CONFIGURING CLIENT-SIDE FIELD LEVEL ENCRYPTION ON DATABASE CONNECTION \nThe mongosh client-side field level encryption methods require a database connection with client-side field level encryption enabled. If the current database connection was not initiated with client-side field level encryption enabled, either:  * Use the Mongo() constructor from the mongosh to establish a connection with the required client-side field level encryption options. The Mongo() method supports the following Key Management Service (KMS) providers for Customer Master Key (CMK) management:\n   \n   * Amazon Web Services KMS\n   \n   * Azure Key Vault\n   \n   * Google Cloud Platform KMS\n   \n   * Locally Managed Key\n   \n   or  * Use the mongosh command line options to establish a connection with the required options. The command line options only support the Amazon Web Services KMS provider for CMK management. \nEXAMPLE \nThese examples allow you to rapidly evaluate client-side field level encryption. For specific examples using each supported KMS provider, see Encryption Key Management. To configure client-side field level encryption for a locally managed key:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key export TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  Create the client-side field level encryption object using the generated local key string:  var autoEncryptionOpts = {   \"keyVaultNamespace\" : \"encryption.__dataKeys\",   \"kmsProviders\" : {     \"local\" : {       \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])     }   } }  Use the Mongo() constructor with the client-side field level encryption options configured to create a database connection. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",   autoEncryptionOpts) \n\n \nREWRAP DATA KEYS WITH CURRENT MASTERKEY \nThe following example shows how you can rewrap each data key with its respective current masterKey: let keyVault = mongo.getKeyVault()\nkeyVault.rewrapManyDataKey()  \nMIGRATE TO A NEW MASTERKEY \nThe following example shows how you can use KeyVault.rewrapManyDataKey() to migrate to a new masterKey: let keyVault = mongo.getKeyVault()\nkeyVault.rewrapManyDataKey({}, {  provider: 'aws',  masterKey: {    region: 'us-east-2',    key: 'arn:aws:kms:us-east-2:...'  }})  \nREWRAP DATA KEYS THAT HAVE NOT BEEN REWRAPPED RECENTLY \nThe following example shows how to rewrap data keys that have not been rewrapped in the previous thirty days. let keyVault = mongo.getKeyVault()\nconst thirtyDaysAgo = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);\nkeyVault.rewrapManyDataKey({ updateDate: { $lt: thirtyDaysAgo } });  \nOUTPUT \nKeyVault.rewrapManyDataKey() returns a BulkWriteResult object detailing how many data keys were affected: {  bulkWriteResult: BulkWriteResult {    result: {      ok: 1,      writeErrors: [],      writeConcernErrors: [],      insertedIds: [],      nInserted: 0,      nUpserted: 0,      nMatched: 3,      nModified: 3,      nRemoved: 0,      upserted: [],      opTime: { ts: Timestamp({ t: 1655840760, i: 3 }), t: 23 }    }  }} ←  KeyVault.getKeyByAltName()getClientEncryption() → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.updateOne/": " Docs Home → MongoDB Manual \nBULK.FIND.UPDATEONE() \nOn this page    \n * Description\n   \n * Behavior\n * Example \nTIP Starting in version 3.2, MongoDB also provides the db.collection.bulkWrite() method for performing bulk write operations. \nDESCRIPTION \nBulk.find.updateOne(<update>) \nAdds a single document update operation to a bulk operations list. Use the Bulk.find() method to specify the condition that determines which document to update. The Bulk.find.updateOne() method limits the update to a single document. To update multiple documents, see Bulk.find.update(). Bulk.find.updateOne() accepts the following parameter: Parameter\nType\nDescription\nupdate\ndocument or pipeline The modifications to apply. Can be one of the following: A replacement document Contains only field and value pairs. See also Bulk.find.replaceOne(). Update document\nContains only update operator expressions. Aggregation pipeline Starting in MongoDB 4.2 Contains only the following aggregation stages:      * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. For more information on the update modification parameter, see the db.collection.updateOne() reference page. The sum of the associated <query> document from the Bulk.find() and the update document must be less than or equal to the maximum BSON document size.  * To specify an upsert: true for this operation, use with Bulk.find.upsert().  * To specify arrayFilters to update specific array elements, use with Bulk.find.arrayFilters().  * To specify the index to use for the associated Bulk.find(), see Bulk.find.hint().  * To replace a document wholesale, see also Bulk.find.replaceOne(). \nBEHAVIOR \nIf the <update> document contains only update operator expressions, as in: {  $set: { status: \"D\" },  $inc: { points: 2 }}  Then, Bulk.find.updateOne() updates only the corresponding fields, status and points, in the document. \nEXAMPLE \nThe following example initializes a Bulk() operations builder for the items collection, and adds various updateOne() operations to the list of operations. var bulk = db.items.initializeUnorderedBulkOp();bulk.find( { status: \"D\" } ).updateOne( { $set: { status: \"I\", points: \"0\" } } );bulk.execute();  \nUPDATE WITH AGGREGATION PIPELINE \nStarting in version 4.2, update methods can accept an aggregation pipeline. For example, the following uses:  * the $set stage which can provide similar behavior to the $set update operator expression,  * the aggregation variable NOW, which resolves to the current datetime and can provide similar behavior to a $currentDate update operator expression. To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes. var bulk = db.items.initializeUnorderedBulkOp();bulk.find( {  status: \"P\" } ).updateOne(   [      { $set: { points: 0, lastModified: \"$$NOW\" } }   ]);bulk.execute();  \nTIP \nSEE ALSO:  * db.collection.initializeUnorderedBulkOp()  * db.collection.initializeOrderedBulkOp()  * Bulk.find()  * Bulk.find.update()  * Bulk.find.replaceOne()  * Bulk.execute()  * All Bulk Methods ←  Bulk.find.replaceOne()Bulk.find.update() → On this page  * Description\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/ObjectId.valueOf/": " Docs Home → MongoDB Manual \nOBJECTID.VALUEOF() \nOn this page    \n * Example ObjectId.valueOf() \nReturns the value of the ObjectId() as a lowercase hexadecimal string. This value is the str attribute of the ObjectId() object. \nEXAMPLE \nThe following example calls the valueOf() method on an ObjectId(): ObjectId(\"507c7f79bcf86cd7994f6c0e\").valueOf()  This will return the following string: 507c7f79bcf86cd7994f6c0e  You can confirm the type of this object using the following operation: typeof ObjectId(\"507c7f79bcf86cd7994f6c0e\").valueOf() \n←  ObjectId.toString()UUID() → On this page  * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.objsLeftInBatch/": " Docs Home → MongoDB Manual \nCURSOR.OBJSLEFTINBATCH() \ncursor.objsLeftInBatch()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. cursor.objsLeftInBatch() returns the number of documents remaining in the current batch. The MongoDB instance returns response in batches. To retrieve all the documents from a cursor may require multiple batch responses from the MongoDB instance. When there are no more documents remaining in the current batch, the cursor will retrieve another batch to get more documents until the cursor exhausts. ←  cursor.noCursorTimeout()cursor.pretty() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.showRecordId/": " Docs Home → MongoDB Manual \nCURSOR.SHOWRECORDID() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \ncursor.showRecordId()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Appends the $recordId field to documents returned by a query. $recordId is the internal key that uniquely identifies a document in a collection. $recordId format: '$recordId': Long(<int>) Returns:A modified cursor object that contains the document fields and the appended $recordId field. \nEXAMPLE \nThe example uses this pizzas collection: db.pizzas.insertMany( [   { type: \"pepperoni\", size: \"small\", price: 4 },   { type: \"cheese\", size: \"medium\", price: 7 },   { type: \"vegan\", size: \"large\", price: 8 }] )  The following find() example uses showRecordId() to append the $recordId to the pizza document fields in the output: db.pizzas.find().showRecordId()  Example output: [   {      _id: ObjectId(\"62ffc70660b33b68e8f30435\"),      type: 'pepperoni',      size: 'small',      price: 4,      '$recordId': Long(\"1\")   },   {      _id: ObjectId(\"62ffc70660b33b68e8f30436\"),      type: 'cheese',      size: 'medium',      price: 7,      '$recordId': Long(\"2\")   },   {      _id: ObjectId(\"62ffc70660b33b68e8f30437\"),      type: 'vegan',      size: 'large',      price: 8,      '$recordId': Long(\"3\")   }] ←  cursor.returnKey()cursor.size() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.find.deleteOne/": " Docs Home → MongoDB Manual \nBULK.FIND.DELETEONE() \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nBulk.find.deleteOne() \nAdds a single document remove operation to a bulk operations list. Use the Bulk.find() method to specify the condition that determines which document to remove. Bulk.find.deleteOne() only deletes the first matching document. To remove multiple documents, see Bulk.find.delete(). \nSYNTAX \nThe command has the following syntax: Bulk.find( <filter document> ).deleteOne()  For details on the find() method see: Bulk.find() \nEXAMPLE \nCreate the music collection: db.music.insertMany( [   { artist: \"DOA\", genre: \"punk\" },   { artist: \"Rick Astley\", genre: \"pop\" },   { artist: \"Black Flag\", genre: \"punk\" },   { artist: \"Justin Bieber\", genre: \"pop\" }] )  The following example:      * Initializes a Bulk() operations builder.  * Searches for the genre pop.  * Deletes Rick Astley, the first matching pop artist, from the collection. var bulk = db.music.initializeOrderedBulkOp();bulk.find( { \"genre\": \"pop\" } ).deleteOne();bulk.execute()  To delete all \"pop\" music, use Bulk.find.delete() instead. What is MongoDB? → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.next/": " Docs Home → MongoDB Manual \nCURSOR.NEXT() \ncursor.next()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Returns:The next document in the cursor returned by the db.collection.find() method. See cursor.hasNext() related functionality. ←  cursor.min()cursor.noCursorTimeout() → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Mongo.getDBNames/": " Docs Home → MongoDB Manual \nMONGO.GETDBNAMES() \nOn this page    \n * Description\n   \n * Examples \nDESCRIPTION \nMongo.getDBNames() \nReturns a list of available databases. Mongo.getDBNames() calls the listDatabases command. The Mongo.getDBNames() method doesn't take any parameters. \nEXAMPLES  LIST DATABASES \nList the available databases for the current MongoDB instance: db.getMongo().getDBNames()  The db.getMongo() method creates a connection to the instance. Mongo.getDBNames() returns: [ 'admin', 'config', 'local', 'test' ] \nMAP DATABASE LIST TO ANOTHER METHOD \nUse Mongo.getDBNames() to get a list of collections: db.getMongo().getDBNames().map(   name => db.getSiblingDB( name ).getCollectionNames())  Example output: [   [ 'system.users', 'system.keys', 'system.version' ],   [     'settings',     'tenantMigrationRecipients',     'system.sessions',     'transactions',     'external_validation_keys',     'image_collection',     'tenantMigrationDonors',     'system.indexBuilds'   ],   [     'replset.minvalid',     'system.views',     'oplog.rs',     'replset.initialSyncId',     'startup_log',     'system.replset',     'system.rollback.id',     'replset.oplogTruncateAfterPoint',     'replset.election',     'system.tenantMigration.oplogView'   ],   [     'feedback',     'inventory',     'engineers',     'clothes'   ]]      * Mongo.getDBNames() returns a list of databases.  * map defines a function that iterates over the list of databases. Each iteration of map:\n   \n   * assigns a database to the name variable,\n   \n   * connects to the database currently stored in name using db.getSiblingDB(),\n   \n   * returns the collections in the current database using db.getCollectionNames(). ←  Mongo.getDB()Mongo.getDBs() → On this page  * Description\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.tailable/": " Docs Home → MongoDB Manual \nCURSOR.TAILABLE() \nOn this page    \n * Definition\n   \n * Behavior \nDEFINITION \ncursor.tailable()  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Marks the cursor as tailable. For use against a capped collection only. Using tailable() against a non-capped collection will return an error. cursor.tailable() uses the following syntax: cursor.tailable( { awaitData : <boolean> } )  tailable() has the following parameter: Parameter\nType\nDescription\nawaitData\nboolean Optional. When true, enables awaitData. awaitData is false by default. Returns:The cursor that tailable() is attached to. \nBEHAVIOR \nA tailable cursor performs a collection scan over a capped collection. It remains open even after reaching the end of the collection. Applications can continue to iterate the tailable cursor as new data is inserted into the collection. If awaitData is set to true, when the cursor reaches the end of the capped collection, MongoDB blocks the query thread for a period of time waiting for new data to arrive. When new data is inserted into the capped collection, the blocked thread is signaled to wake up and return the next batch to the client. See Tailable Cursors. ←  cursor.sort()cursor.toArray() → On this page  * Definition\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.getKey/": " Docs Home → MongoDB Manual \nKEYVAULT.GETKEY() \nOn this page    \n * Behavior\n   \n * Example New in version 4.2. KeyVault.getKey(UUID) \nGets a data encryption key with the specified UUID. The data encryption key must exist in the key vault associated to the database connection. getKey() has the following syntax: keyVault = db.getMongo().getKeyVault()\nkeyVault.getKey(UUID(\"<UUID String>\"))  The UUID is a BSON binary data object with subtype 4. Returns:Document representing a matching data encryption key. If no matching data encryption key is found, KeyVault.getKey() returns an object containing a hint that no key was found. \nBEHAVIOR  REQUIRES CONFIGURING CLIENT-SIDE FIELD LEVEL ENCRYPTION ON DATABASE CONNECTION \nThe mongosh client-side field level encryption methods require a database connection with client-side field level encryption enabled. If the current database connection was not initiated with client-side field level encryption enabled, either:      * Use the Mongo() constructor from the mongosh to establish a connection with the required client-side field level encryption options. The Mongo() method supports the following Key Management Service (KMS) providers for Customer Master Key (CMK) management:\n   \n   * Amazon Web Services KMS\n   \n   * Azure Key Vault\n   \n   * Google Cloud Platform KMS\n   \n   * Locally Managed Key\n   \n   or  * Use the mongosh command line options to establish a connection with the required options. The command line options only support the Amazon Web Services KMS provider for CMK management. \nEXAMPLE \nThe following example uses a locally managed KMS for the client-side field level encryption configuration. To configure client-side field level encryption for a locally managed key:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key export TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  Create the client-side field level encryption object using the generated local key string:  var autoEncryptionOpts = {   \"keyVaultNamespace\" : \"encryption.__dataKeys\",   \"kmsProviders\" : {     \"local\" : {       \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])     }   } }  Use the Mongo() constructor with the client-side field level encryption options configured to create a database connection. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",   autoEncryptionOpts)  Retrieve the keyVault object and use the KeyVault.getKey() to retrieve a data encryption key using its UUID: keyVault = encryptedClient.getKeyVault()keyVault.getKey(UUID(\"b4b41b33-5c97-412e-a02b-743498346079\"))  getKey() returns the data encryption key, with output similar to the following:\n\n \n←  KeyVault.deleteKey()KeyVault.getKeys() → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/PlanCache.list/": " Docs Home → MongoDB Manual \nPLANCACHE.LIST() \nOn this page    \n * Definition\n   \n * Consideration\n * Required Access\n * Examples \nDEFINITION \nPlanCache.list(<pipeline>) \nNew in version 4.4. Returns an array of plan cache entries for a collection. The method is only available from the plan cache object of a specific collection; i.e. db.collection.getPlanCache().list(<pipeline>) \nParameter\nType\nDescription\npipeline\nArray Optional. Aggregation pipeline to filter/process the query shapes. You can run this method from a mongod or mongos instance. When run on a sharded cluster, the method returns plan cache entry information from a single member in each shard replica set. This member is identified with the shard and host fields. See also Read Preference. The method PlanCache.list() wraps the $planCacheStats aggregation pipeline. That is, db.collection.getPlanCache().list([<stage1>, <stage2>, ...] )  is equivalent to db.collection.aggregate([ <$planCacheStats stage>, <stage1>, <stage2>, ... ]).toArray();  For details on the output, see $planCacheStats output. Not all queries automatically place a query plan in the cache. PlanCache.list() returns an empty array if there are currently no query shapes with cached query plans. \nTIP \nSEE ALSO:      * $planCacheStats  * db.collection.aggregate()  * toArray() \nCONSIDERATION  RESTRICTIONS \nPlanCache.list() is not allowed in transactions. \nQUERY HASH AND QUERY SHAPES \nTo help identify slow queries with the same query shape, starting in MongoDB 4.2, each query shape is associated with a queryHash. The queryHash is a hexadecimal string that represents a hash of the query shape and is dependent only on the query shape. \nNOTE As with any hash function, two different query shapes may result in the same hash value. However, the occurrence of hash collisions between different query shapes is unlikely. The query optimizer only caches the plans for those query shapes that can have more than one viable plan. Each entry in the plan cache is associated with a queryHash. \nREAD PREFERENCE \nPlanCache.list() observes the read preference in selecting the host(s) from which to return the plan cache information. Applications may target different members of a replica set. As such, each replica set member might receive different read commands and have plan cache information that differs from other members. Nevertheless, running PlanCache.list() on a replica set or a sharded cluster obeys the normal read preference rules. That is, on a replica set, the operation gathers plan cache information from just one member of replica set, and on a sharded cluster, the operation gathers plan cache information from just one member of each shard replica set. \nREQUIRED ACCESS \nOn systems running with authorization, the user must have the planCacheRead privilege for the collection. \nEXAMPLES  NOTE  * Not all queries automatically place a query plan in the cache. PlanCache.list() returns an empty array if there are currently no query shapes with cached query plans.  * For details on the output, see $planCacheStats output. The examples in this section use the following orders collection: db.orders.insertMany( [   { \"_id\" : 1, \"item\" : \"abc\", \"price\" : NumberDecimal(\"12\"), \"quantity\" : 2, \"type\": \"apparel\" },   { \"_id\" : 2, \"item\" : \"jkl\", \"price\" : NumberDecimal(\"20\"), \"quantity\" : 1, \"type\": \"electronics\" },   { \"_id\" : 3, \"item\" : \"abc\", \"price\" : NumberDecimal(\"10\"), \"quantity\" : 5, \"type\": \"apparel\" },   { \"_id\" : 4, \"item\" : \"abc\", \"price\" : NumberDecimal(\"8\"), \"quantity\" : 10, \"type\": \"apparel\" },   { \"_id\" : 5, \"item\" : \"jkl\", \"price\" : NumberDecimal(\"15\"), \"quantity\" : 15, \"type\": \"electronics\" }] )  Create the following indexes on the collection:\n\n  \nNOTE Index { item: 1, price: 1 } is a partial index and only indexes documents with price field greater than or equal to NumberDecimal(\"10\"). Run some queries against the collection: db.orders.find( { item: \"abc\", price: { $gte: NumberDecimal(\"10\") } } )db.orders.find( { item: \"abc\", price: { $gte: NumberDecimal(\"5\") } } )db.orders.find( { quantity: { $gte: 20 } } )db.orders.find( { quantity: { $gte: 5 }, type: \"apparel\" } )  \nRETURN INFORMATION FOR ALL ENTRIES IN THE QUERY CACHE \nThe following returns the query shapes that have cached plans for the orders collection: db.orders.getPlanCache().list()  The method returns an array of the query shapes currently in the cache. In the example, the orders collection had cached query plans associated with the following shapes:\n\n For details on the output, see $planCacheStats output. \nLIST QUERY SHAPES \nMongoDB 4.4 removes the deprecated planCacheListQueryShapes command and its helper method PlanCache.listQueryShapes(). As an alternative, you can use the PlanCache.list() to obtain a list of all of the query shapes for which there is a cached plan. For example, the following operation passes in a pipeline with a $project stage to only output the createdFromQuery field and the queryHash field. db.orders.getPlanCache().list( [ { $project: {createdFromQuery: 1, queryHash: 1 } } ] )  The operation returns the following query shapes: [   { \"createdFromQuery\" : { \"query\" : { \"quantity\" : { \"$gte\" : 5 }, \"type\" : \"apparel\" }, \"sort\" : { }, \"projection\" : { } }, \"queryHash\" : \"4D151C4C\" },   { \"createdFromQuery\" : { \"query\" : { \"quantity\" : { \"$gte\" : 20 } }, \"sort\" : { }, \"projection\" : { } }, \"queryHash\" : \"23B19B75\" },   { \"createdFromQuery\" : { \"query\" : { \"item\" : \"abc\", \"price\" : { \"$gte\" : NumberDecimal(\"5\") } }, \"sort\" : { }, \"projection\" : { } }, \"queryHash\" : \"117A6B10\" },   { \"createdFromQuery\" : { \"query\" : { \"item\" : \"abc\", \"price\" : { \"$gte\" : NumberDecimal(\"10\") } }, \"sort\" : { }, \"projection\" : { } }, \"queryHash\" : \"117A6B10\" }] For details on the output, see $planCacheStats output. \nFIND CACHE ENTRY DETAILS FOR A QUERY SHAPE \nTo return plan cache information for a particular query shape, pass in a pipeline with a $match on the planCacheKey field. db.orders.getPlanCache().list([ { $match: { planCacheKey: \"DD67E353\"} } ] )  The operation returns the following:\n\n For details on the output, see $planCacheStats output. ←  PlanCache.help()Bulk Operation Methods → On this page  * Definition\n * Consideration\n * Required Access\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/Bulk.tojson/": " Docs Home → MongoDB Manual \nBULK.TOJSON() \nOn this page    \n * Example Bulk.toJSON() \nReturns a JSON document that contains the number of operations and batches in the Bulk() object. \nEXAMPLE \nThe following initializes a Bulk() operations builder on the items collection, adds a series of write operations, and calls Bulk.toJSON() on the bulk builder object. var bulk = db.items.initializeOrderedBulkOp();bulk.insert( { item: \"abc123\", status: \"A\", defaultQty: 500, points: 5 } );bulk.insert( { item: \"ijk123\", status: \"A\", defaultQty: 100, points: 10 } );bulk.find( { status: \"D\" } ).deleteOne();bulk.toJSON();  The Bulk.toJSON() returns the following JSON document {  acknowledged: true,  insertedCount: 2,  insertedIds: [    { index: 0, _id: ObjectId(\"627bf77e5e19ff3518448887\") },    { index: 1, _id: ObjectId(\"627bf77e5e19ff3518448888\") }  ],  matchedCount: 0,  modifiedCount: 0,  deletedCount: 0,  upsertedCount: 0,  upsertedIds: []}  \nTIP \nSEE ALSO: Bulk() ←  Bulk.insert()Bulk.toString() → On this page  * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.collection.findOneAndReplace/": " Docs Home → MongoDB Manual \nDB.COLLECTION.FINDONEANDREPLACE() \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \ndb.collection.findOneAndReplace( filter, replacement, options )  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the update command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 Replaces a single document based on the specified filter. \nSYNTAX \nThe findOneAndReplace() method has the following form: db.collection.findOneAndReplace(   <filter>,   <replacement>,   {     projection: <document>,     sort: <document>,     maxTimeMS: <number>,     upsert: <boolean>,     returnDocument: <string>,     returnNewDocument: <boolean>,     collation: <document>   })  \nFIELDS AND OPTIONS \nThe findOneAndReplace() method takes the following fields and options: Field\nType\nDescription\nfilter\ndocument The selection criteria for the update. The same query selectors as in the find() method are available. To replace the first document returned in the collection, specify an empty document { }. If unspecified, defaults to an empty document. Starting in MongoDB 4.2, the operation returns an error if the query argument is not a document. replacement\ndocument The replacement document. Cannot contain update operators. The <replacement> document cannot specify an _id value that differs from the replaced document. projection\ndocument Optional. A subset of fields to return. To return all fields in the matching document, omit this field. Starting in MongoDB 4.2, the operation returns an error if the projection field is not a document. sort\ndocument Optional. Specifies a sorting order for the documents matched by the filter. Starting in MongoDB 4.2, the operation returns an error if the sort field is not a document. See cursor.sort(). maxTimeMS\nnumber\nOptional. Specifies a time limit in milliseconds within which the operation must complete. Returns an error if the limit is exceeded.\nupsert\nboolean Optional. When true, findOneAndReplace() either:  * Inserts the document from the replacement parameter if no document matches the filter. Returns null after inserting the new document, unless returnNewDocument is true.  * Replaces the document that matches the filter with the replacement document. MongoDB will add the _id field to the replacement document if it is not specified in either the filter or replacement documents. If _id is present in both, the values must be equal. To avoid multiple upserts, ensure that the query fields are uniquely indexed. Defaults to false. returnDocument\nstring Optional. Starting in mongosh 0.13.2, returnDocument is an alternative for returnNewDocument. If both options are set, returnDocument takes precedence. returnDocument: \"before\" returns the original document. returnDocument: \"after\" returns the updated document. returnNewDocument\nboolean Optional. When true, returns the replacement document instead of the original document. Defaults to false. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons.\n\n \nRETURNS \nReturns the original document by default. Returns the updated document if returnDocument is set to after or returnNewDocument is set to true. \nBEHAVIOR  DOCUMENT MATCH \ndb.collection.findOneAndReplace() replaces the first matching document in the collection that matches the filter. The sort field can be used to influence which document is modified. \nPROJECTION  IMPORTANT \nLANGUAGE CONSISTENCY Starting in MongoDB 4.4, as part of making find() and findAndModify() projection consistent with aggregation's $project stage,  * The find() and findAndModify() projection can accept aggregation expressions and syntax.  * MongoDB enforces additional restrictions with regards to projections. See Projection Restrictions for details. The projection field takes a document in the following form: { field1: <value>, field2: <value> ... } \nProjection\nDescription\n<field>: <1 or true>\nSpecifies the inclusion of a field. Non-zero integers are also treated as true.\n<field>: <0 or false>\nSpecifies the exclusion of a field.\n\"<field>.$\": <1 or true>\nWith the use of the $ array projection operator, you can specify the projection to return the first element that match the query condition on the array field; e.g. \"arrayField.$\" : 1. (Not available for views.) Non-zero integers are also treated as true.\n<field>: <array projection>\nUsing the array projection operators $elemMatch, $slice, specifies the array element(s) to include, thereby excluding those elements that do not meet the expressions. (Not available for views.)\n<field>: <aggregation expression> Specifies the value of the projected field. Starting in MongoDB 4.4, with the use of aggregation expressions and syntax, including the use of literals and aggregation variables, you can project new fields or project existing fields with new values. For example,  * If you specify a non-numeric, non-boolean literal (such as a literal string or an array or an operator expression) for the projection value, the field is projected with the new value; e.g.:\n   \n   * { field: [ 1, 2, 3, \"$someExistingField\" ] }\n   \n   * { field: \"New String Value\" }\n   \n   * { field: { status: \"Active\", total: { $sum: \"$existingArray\" } } }  * To project a literal value for a field, use the $literal aggregation expression; e.g.:\n   \n   * { field: { $literal: 5 } }\n   \n   * { field: { $literal: true } }\n   \n   * { field: { $literal: { fieldWithValue0: 0, fieldWithValue1: 1 } } } In versions 4.2 and earlier, any specification value (with the exception of the previously unsupported document value) is treated as either true or false to indicate the inclusion or exclusion of the field. New in version 4.4. EMBEDDED FIELD SPECIFICATION \nFor fields in an embedded documents, you can specify the field using either:  * dot notation; e.g. \"field.nestedfield\": <value>  * nested form; e.g. { field: { nestedfield: <value> } } (Starting in MongoDB 4.4) _ID FIELD PROJECTION \nThe _id field is included in the returned documents by default unless you explicitly specify _id: 0 in the projection to suppress the field. INCLUSION OR EXCLUSION \nA projection cannot contain both include and exclude specifications, with the exception of the _id field:  * In projections that explicitly include fields, the _id field is the only field that you can explicitly exclude.  * In projections that explicitly excludes fields, the _id field is the only field that you can explicitly include; however, the _id field is included by default. For more information on projection, see also:  * Project Fields to Return from Query \nSHARDED COLLECTIONS \nTo use db.collection.findOneAndReplace() on a sharded collection, the query filter must include an equality condition on the shard key. Starting in version 4.4, documents in a sharded collection can be missing the shard key fields. To target a document that is missing the shard key, you can use the null equality match in conjunction with another filter condition (such as on the _id field). For example:\n\n  SHARD KEY MODIFICATION \nStarting in MongoDB 4.2, you can update a document's shard key value unless the shard key field is the immutable _id field. In MongoDB 4.2 and earlier, a document's shard key field value is immutable. \nWARNING Starting in version 4.4, documents in sharded collections can be missing the shard key fields. Take precaution to avoid accidentally removing the shard key when changing a document's shard key value. To modify the existing shard key value with db.collection.findOneAndReplace():  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write.  * You must include an equality filter on the full shard key. MISSING SHARD KEY \nStarting in version 4.4, documents in a sharded collection can be missing the shard key fields. To use db.collection.findOneAndReplace() to set the document's missing shard key,  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write if the new shard key value is not null.  * You must include an equality filter on the full shard key. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also:  * Missing Shard Key Fields \nTRANSACTIONS \ndb.collection.findOneAndReplace() can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. UPSERT WITHIN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, db.collection.findOneAndReplace() with upsert: true can be run on an existing collection or a non-existing collection. If run on a non-existing collection, the operation creates the collection. In MongoDB 4.2 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES  REPLACE A DOCUMENT \nCreate a sample scores collection with the following documents: db.scores.insertMany([   { \"_id\" : 1, \"team\" : \"Fearful Mallards\", \"score\" : 25000 },   { \"_id\" : 2, \"team\" : \"Tactful Mooses\", \"score\" : 23500 },   { \"_id\" : 3, \"team\" : \"Aquatic Ponies\", \"score\" : 19250 },   { \"_id\" : 4, \"team\" : \"Cuddly Zebras\", \"score\" : 15235 },   { \"_id\" : 5, \"team\" : \"Garrulous Bears\", \"score\" : 18000 }]);  The following operation finds a document with score less than 20000 and replaces it: db.scores.findOneAndReplace(   { \"score\" : { $lt : 20000 } },   { \"team\" : \"Observant Badgers\", \"score\" : 20000 })  The operation returns the original document that has been replaced: { \"_id\" : 3, \"team\" : \"Aquatic Ponies\", \"score\" : 19250 }  If returnNewDocument was true, the operation would return the replacement document instead. Although multiple documents meet the filter criteria, db.collection.findOneAndReplace() replaces only one document. \nSORT AND REPLACE A DOCUMENT \nCreate a sample scores collection with the following documents:\n\n  By including an ascending sort on the score field, the following example replaces the document with the lowest score among those documents that match the filter: db.scores.findOneAndReplace(   { \"score\" : { $lt : 20000 } },   { \"team\" : \"Observant Badgers\", \"score\" : 20000 },   { sort: { \"score\" : 1 } })  The operation returns the original document that has been replaced: { \"_id\" : 4, \"team\" : \"Cuddly Zebras\", \"score\" : 15235 }  See Replace A Document for the non-sorted result of this command. \nPROJECT SPECIFIC FIELDS IN RETURN DOCUMENT \nCreate a sample scores collection with the following documents: db.scores.insertMany([   { \"_id\" : 1, \"team\" : \"Fearful Mallards\", \"score\" : 25000 },   { \"_id\" : 2, \"team\" : \"Tactful Mooses\", \"score\" : 23500 },   { \"_id\" : 3, \"team\" : \"Aquatic Ponies\", \"score\" : 19250 },   { \"_id\" : 4, \"team\" : \"Cuddly Zebras\", \"score\" : 15235 },   { \"_id\" : 5, \"team\" : \"Garrulous Bears\", \"score\" : 18000 }])  The following operation uses projection to only display the team field in the returned document: db.scores.findOneAndReplace(   { \"score\" : { $lt : 22250 } },   { \"team\" : \"Therapeutic Hamsters\", \"score\" : 22250 },   { sort : { \"score\" : 1 }, projection: { \"_id\" : 0, \"team\" : 1 } })  The operation returns the original document with only the team field: { \"team\" : \"Cuddly Zebras\" }  \nREPLACE DOCUMENT WITH TIME LIMIT \nThe following operation sets a 5ms time limit to complete: try {   db.scores.findOneAndReplace(      { \"score\" : { $gt : 25000 } },      { \"team\" : \"Emphatic Rhinos\", \"score\" : 25010 },      { maxTimeMS: 5 }   );} catch(e){   print(e);}  If the operation exceeds the time limit, it returns: Error: findAndModifyFailed failed: { \"ok\" : 0, \"errmsg\" : \"operation exceeded time limit\", \"code\" : 50 }  \nREPLACE DOCUMENT WITH UPSERT \nThe following operation uses the upsert field to insert the replacement document if no document matches the filter: try {   db.scores.findOneAndReplace(      { \"team\" : \"Fortified Lobsters\" },      { \"_id\" : 6019, \"team\" : \"Fortified Lobsters\" , \"score\" : 32000},      { upsert : true, returnDocument: \"after\" }   );} catch (e){   print(e);}  The operation returns the following: {   \"_id\" : 6019,   \"team\" : \"Fortified Lobsters\",   \"score\" : 32000}  If returnDocument: \"before\" was set, the operation would return null because there is no original document to return. \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. Create a sample myColl collection with the following documents: db.myColl.insertMany([   { _id: 1, category: \"café\", status: \"A\" },   { _id: 2, category: \"cafe\", status: \"a\" },   { _id: 3, category: \"cafE\", status: \"a\" }]);  The following operation includes the collation option: db.myColl.findOneAndReplace(   { category: \"cafe\", status: \"a\" },   { category: \"cafÉ\", status: \"Replaced\" },   { collation: { locale: \"fr\", strength: 1 } });  The operation returns the following document: { \"_id\" : 1, \"category\" : \"café\", \"status\" : \"A\" }\n\n On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/cursor.maxAwaitTimeMS/": " Docs Home → MongoDB Manual \nCURSOR.MAXAWAITTIMEMS() \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \ncursor.maxAwaitTimeMS(<time limit>)  IMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for a language-specific driver, such as Node.js. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. Specifies the maximum time for the server to wait for new documents that match a tailable cursor query on a capped collection. For more information on iterating a cursor returned by a query, see: Iterate a Cursor in mongosh. The maxAwaitTimeMS() method has the following prototype form: db.collection.find(   { <query> },   { <projection> }).tailable( { awaitData: true } ).maxAwaitTimeMS( <milliseconds> )  The maxAwaitTimeMS() method has the following parameter: Parameter\nType\nDescription\nmilliseconds\ninteger\nSpecifies a maximum wait time for new documents. \nIMPORTANT This method, maxAwaitTimeMS(), sets a limit on how long a tailable cursor waits for the next response. maxTimeMS() sets a limit on total processing time. \nEXAMPLE \nQuery the capped sales collection to find agent Mary Kay's weekly sales totals: db.sales.find(   { agent: \"Mary Kay\" },   { _id: 0, agent: 1, weeklyTotal: 1  }).tailable( { awaitData: true } ).maxAwaitTimeMS( 1000 )  The highlighted line creates a tailable cursor on the sales collection. The maxAwaitTimeMS() sets a one second maximum wait time for the next cursor update. ←  cursor.max()cursor.maxTimeMS() → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/db.createUser/": " Docs Home → MongoDB Manual \nDB.CREATEUSER() \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Examples \nDEFINITION \ndb.createUser(user, writeConcern) \nCreates a new user for the database on which the method is run. db.createUser() returns a duplicate user error if the user already exists on the database. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the createUser command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The db.createUser() method has the following syntax: Field\nType\nDescription\nuser\ndocument\nThe document with authentication and access information about the user to create.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. The user document defines the user and has the following form: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. {  user: \"<name>\",  pwd: passwordPrompt(),      // Or  \"<cleartext password>\"  customData: { <any information> },  roles: [    { role: \"<role>\", db: \"<database>\" } | \"<role>\",    ...  ],  authenticationRestrictions: [     {       clientSource: [\"<IP>\" | \"<CIDR range>\", ...],       serverAddress: [\"<IP>\" | \"<CIDR range>\", ...]     },     ...  ],  mechanisms: [ \"<SCRAM-SHA-1|SCRAM-SHA-256>\", ... ],  passwordDigestor: \"<server|client>\"}  The user document has the following fields: Field\nType\nDescription\nuser\nstring\nThe name of the new user.\npwd\nstring The user's password. The pwd field is not required if you run db.createUser() on the $external database to create users who have credentials stored externally to MongoDB. The value can be either:  * the user's password in cleartext string, or  * passwordPrompt() to prompt for the user's password. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. customData\ndocument\nOptional. Any arbitrary information. This field can be used to store any data an admin wishes to associate with this particular user. For example, this could be the user's full name or employee id.\nroles\narray\nThe roles granted to the user. Can specify an empty array [] to create users without roles.\nauthenticationRestrictions\narray Optional. The authentication restrictions the server enforces on the created user. Specifies a list of IP addresses and CIDR ranges from which the user is allowed to connect to the server or from which the server can accept users. mechanisms\narray Optional. Specify the specific SCRAM mechanism or mechanisms for creating SCRAM user credentials. If authenticationMechanisms is specified, you can only specify a subset of the authenticationMechanisms. Valid values are:  * \"SCRAM-SHA-1\"\n   \n   * Uses the SHA-1 hashing function.  * \"SCRAM-SHA-256\"\n   \n   * Uses the SHA-256 hashing function.\n   \n   * Requires passwordDigestor to be server. The default is both SCRAM-SHA-1 and SCRAM-SHA-256. passwordDigestor\nstring Optional. Indicates whether the server or the client digests the password. Available values are:\n\n \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where db.createUser() runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field: Field Name\nValue\nDescription\nclientSource\nArray of IP addresses and/or CIDR ranges\nIf present, when authenticating a user, the server verifies that the client's IP address is either in the given list or belongs to a CIDR range in the list. If the client's IP address is not present, the server does not authenticate the user.\nserverAddress\nArray of IP addresses and/or CIDR ranges\nA list of IP addresses or CIDR ranges to which the client can connect. If present, the server will verify that the client's connection was accepted via an IP address in the given list. If the connection was accepted via an unrecognized IP address, the server does not authenticate the user. \nIMPORTANT If a user inherits multiple roles with incompatible authentication restrictions, that user becomes unusable. For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. The db.createUser() method wraps the createUser command. \nBEHAVIOR  USER ID \nStarting in version 4.0.9, MongoDB automatically assigns a unique userId to the user upon creation. \nREPLICA SET \nIf run on a replica set, db.createUser() is executed using \"majority\" write concern by default. \nENCRYPTION  WARNING By default, db.createUser() sends all specified data to the MongoDB instance in cleartext, even if using passwordPrompt(). Use TLS transport encryption to protect communications between clients and the server, including the password sent by db.createUser(). For instructions on enabling TLS transport encryption, see Configure mongod and mongos for TLS/SSL. MongoDB does not store the password in cleartext. The password is only vulnerable in transit between the client and the server, and only if TLS transport encryption is not enabled. \nEXTERNAL CREDENTIALS \nUsers created on the $external database should have credentials stored externally to MongoDB, as, for example, with MongoDB Enterprise installations that use Kerberos. To use Client Sessions and Causal Consistency Guarantees with $external authentication users (Kerberos, LDAP, or x.509 users), usernames cannot be greater than 10k bytes. \nLOCAL DATABASE \nYou cannot create users on the local database. \nREQUIRED ACCESS \n * To create a new user in a database, you must have the createUser action on that database resource.  * To grant roles to a user, you must have the grantRole action on the role's database. The userAdmin and userAdminAnyDatabase built-in roles provide createUser and grantRole actions on their respective resources. \nEXAMPLES \nThe following db.createUser() operation creates the accountAdmin01 user on the products database. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell.\n\n  The operation gives accountAdmin01 the following roles:  * the clusterAdmin and readAnyDatabase roles on the admin database  * the readWrite role on the products database \nCREATE USER WITH ROLES \nThe following operation creates accountUser in the products database and gives the user the readWrite and dbAdmin roles. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. use productsdb.createUser(   {     user: \"accountUser\",     pwd: passwordPrompt(),  // Or  \"<cleartext password>\"     roles: [ \"readWrite\", \"dbAdmin\" ]   })  \nCREATE USER WITHOUT ROLES \nThe following operation creates a user named reportsUser in the admin database but does not yet assign roles: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. use admindb.createUser(   {     user: \"reportsUser\",     pwd: passwordPrompt(),  // Or  \"<cleartext password>\"     roles: [ ]   })  \nCREATE ADMINISTRATIVE USER WITH ROLES \nThe following operation creates a user named appAdmin in the admin database and gives the user readWrite access to the config database, which lets the user change certain settings for sharded clusters, such as to the balancer setting. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. use admindb.createUser(   {     user: \"appAdmin\",     pwd: passwordPrompt(),   // Or  \"<cleartext password>\"     roles:       [         { role: \"readWrite\", db: \"config\" },         \"clusterAdmin\"       ]   })  \nCREATE USER WITH AUTHENTICATION RESTRICTIONS \nThe following operation creates a user named restricted in the admin database. This user may only authenticate if connecting from IP address 192.0.2.0 to IP address 198.51.100.0. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. use admindb.createUser(   {     user: \"restricted\",     pwd: passwordPrompt(),      // Or  \"<cleartext password>\"     roles: [ { role: \"readWrite\", db: \"reporting\" } ],     authenticationRestrictions: [ {        clientSource: [\"192.0.2.0\"],        serverAddress: [\"198.51.100.0\"]     } ]   })  \nCREATE USER WITH SCRAM-SHA-256 CREDENTIALS ONLY  NOTE To use SCRAM-SHA-256, the featureCompatibilityVersion must be set to 4.0. For more information on featureCompatibilityVersion, see Get FeatureCompatibilityVersion and setFeatureCompatibilityVersion. The following operation creates a user with only SCRAM-SHA-256 credentials. \nTIP\n\n use reportingdb.createUser(   {     user: \"reportUser256\",     pwd: passwordPrompt(),   // Or  \"<cleartext password>\"     roles: [ { role: \"readWrite\", db: \"reporting\" } ],     mechanisms: [ \"SCRAM-SHA-256\" ]   })  If the authenticationMechanisms parameter is set, the mechanisms field can only include values specified in the authenticationMechanisms parameter. ←  db.changeUserPassword()db.dropUser() → On this page  * Definition\n * Behavior\n * Required Access\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/PlanCache.clear/": " Docs Home → MongoDB Manual \nPLANCACHE.CLEAR() \nOn this page    \n * Definition\n   \n * Required Access \nDEFINITION \nPlanCache.clear() \nRemoves all cached query plans for a collection. \nIMPORTANT \nMONGOSH METHOD This page documents a mongosh method. This is not the documentation for database commands or language-specific drivers, such as Node.js. For the database command, see the planCacheClear command. For MongoDB API drivers, refer to the language-specific MongoDB driver documentation. For the legacy mongo shell documentation, refer to the documentation for the corresponding MongoDB Server release:      * mongo shell v4.4  * mongo shell v4.2 The method is only available from the plan cache object of a specific collection; i.e. db.collection.getPlanCache().clear()  For example, to clear the cache for the orders collection: db.orders.getPlanCache().clear()  \nREQUIRED ACCESS \nOn systems running with authorization, a user must have access that includes the planCacheWrite action. \nTIP \nSEE ALSO:  * db.collection.getPlanCache()  * PlanCache.clearPlansByQuery() ←  db.collection.getPlanCache()PlanCache.clearPlansByQuery() → On this page  * Definition\n * Required Access Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/KeyVault.getKeyByAltName/": " Docs Home → MongoDB Manual \nKEYVAULT.GETKEYBYALTNAME() \nOn this page    \n * Behavior\n   \n * Example New in version 4.2. KeyVault.getKeyByAltName(keyAltName) \nGets all data encryption keys with the specified keyAltName. getKeyByAltName() has the following syntax: keyVault = db.getMongo().getKeyVault()\nkeyVault.getKeyByAltName(\"keyAltName\")  Returns:Document representing a matching data encryption key. If no matching data encryption key is found, KeyVault.getKeyByAltName() returns an object containing a hint that no key was found. \nBEHAVIOR  REQUIRES CONFIGURING CLIENT-SIDE FIELD LEVEL ENCRYPTION ON DATABASE CONNECTION \nThe mongosh client-side field level encryption methods require a database connection with client-side field level encryption enabled. If the current database connection was not initiated with client-side field level encryption enabled, either:      * Use the Mongo() constructor from the mongosh to establish a connection with the required client-side field level encryption options. The Mongo() method supports the following Key Management Service (KMS) providers for Customer Master Key (CMK) management:\n   \n   * Amazon Web Services KMS\n   \n   * Azure Key Vault\n   \n   * Google Cloud Platform KMS\n   \n   * Locally Managed Key\n   \n   or  * Use the mongosh command line options to establish a connection with the required options. The command line options only support the Amazon Web Services KMS provider for CMK management. \nEXAMPLE \nThe following example uses a locally managed KMS for the client-side field level encryption configuration. To configure client-side field level encryption for a locally managed key:  * generate a base64-encoded 96-byte string with no line breaks  * use mongosh to load the key export TEST_LOCAL_KEY=$(echo \"$(head -c 96 /dev/urandom | base64 | tr -d '\\n')\")\nmongosh --nodb  Create the client-side field level encryption object using the generated local key string:  var autoEncryptionOpts = {   \"keyVaultNamespace\" : \"encryption.__dataKeys\",   \"kmsProviders\" : {     \"local\" : {       \"key\" : BinData(0, process.env[\"TEST_LOCAL_KEY\"])     }   } }  Use the Mongo() constructor with the client-side field level encryption options configured to create a database connection. Replace the mongodb://myMongo.example.net URI with the connection string URI of the target cluster. encryptedClient = Mongo(  \"mongodb://myMongo.example.net:27017/?replSetName=myMongo\",   autoEncryptionOpts)  Retrieve the KeyVault object and use the KeyVault.getKeyByAltName() method to retrieve the data encryption key whose keyAltNames array includes the specified key alternate name: keyVault.getKeyByAltName(\"data-encryption-key\")  getKeyByAltName() returns the following data encryption key:\n\n \n←  KeyVault.removeKeyAlternateName()KeyVault.rewrapManyDataKey() → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/method/sh.waitForPingChange/": " Docs Home → MongoDB Manual \nSH.WAITFORPINGCHANGE() \nOn this page    \n * Definition \nDEFINITION \nsh.waitForPingChange(activePings, timeout, interval) \nsh.waitForPingChange() waits for a change in ping state of one of the activepings, and only returns when the specified ping changes state. Parameter\nType\nDescription\nactivePings\narray\nAn array of active pings from the mongos collection.\ntimeout\ninteger\nNumber of milliseconds to wait for a change in ping state.\ninterval\ninteger\nNumber of milliseconds to sleep in each waiting cycle. ←  sh.waitForBalancerOff()sh.updateZoneKeyRange() → On this page  * Definition Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/program/mongos.exe/": " Docs Home → MongoDB Manual \nMONGOS.EXE \nOn this page    \n * Synopsis\n   \n * Options \nSYNOPSIS \nexe is the build of the MongoDB Shard (i.e. mongos) for the Windows platform. exe has all of the features of mongos on Unix-like platforms and is completely compatible with the other builds of mongos. In addition, exe provides several options for interacting with the Windows platform itself. This document only references options that are unique to exe. All mongos options are available. See the mongos and the Configuration File Options documents for more information regarding exe. To install and use exe, read the Install MongoDB Community Edition on Windows document. \nOPTIONS \n--install \nInstalls mongos.exe as a Windows Service and exits. If needed, you can install services for multiple instances of mongos.exe. Install each service with a unique --serviceName and --serviceDisplayName. Use multiple instances only when sufficient system resources exist and your system design requires it. --remove \nRemoves the mongos.exe Windows Service. If mongos.exe is running, this operation will stop and then remove the service. --remove requires the --serviceName if you configured a non-default --serviceName during the --install operation. --reinstall \nRemoves mongos.exe and reinstalls mongos.exe as a Windows Service. --serviceName name \nDefault: MongoS Sets the service name of mongos.exe when running as a Windows Service. Use this name with the net start <name> and net stop <name> operations. You must use --serviceName in conjunction with either the --install or --remove option. --serviceDisplayName <name> \nDefault: Mongo DB Router Sets the name listed for MongoDB on the Services administrative application. --serviceDescription <description> \nDefault: Mongo DB Sharding Router Sets the mongos.exe service description. You must use --serviceDescription in conjunction with the --install option. For descriptions that contain spaces, you must enclose the description in quotes. --serviceUser <user> \nRuns the mongos.exe service in the context of a certain user. This user must have \"Log on as a service\" privileges. You must use --serviceUser in conjunction with the --install option. --servicePassword <password> \nSets the password for <user> for mongos.exe when running with the --serviceUser option. You must use --servicePassword in conjunction with the --install option. ←  mongod.exemongoldap → On this page  * Synopsis\n * Options Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/program/mongokerberos/": " Docs Home → MongoDB Manual \nMONGOKERBEROS \nOn this page    \n * Synopsis\n   \n * Installation\n * Usage\n * Options New in version 4.4: MongoDB Enterprise \nSYNOPSIS \nStarting in version 4.4, MongoDB Enterprise provides mongokerberos for testing MongoDB's Kerberos and GSSAPI configuration options against a running Kerberos deployment. mongokerberos can be used in one of two modes: server and client. Mode\nDescription\nServer\nIn server mode, mongokerberos analyzes Kerberos-related configurations on the server, and returns a report which includes error messages for any configurations that are problematic. For usage, see Server Mode\nClient\nIn client mode, mongokerberos tests Kerberos authentication for a provided username, and returns a report which includes the success or failure of each step in the Kerberos authentication procedure. For usage, see Client Mode Error messages for both modes include information on specific errors encountered and potential advice for resolving the error. mongokerberos supports the following deployment types, in both server and client modes:      * Linux MongoDB clients authenticating to MIT Kerberos deployments on supported Linux platforms.  * Windows MongoDB clients authenticating to Windows Active Directory deployments on supported Windows platforms.  * Linux MongoDB clients authenticating to Windows Active Directory deployments. \nNOTE MongoDB Enterprise and mongokerberos only support the MIT implementation of Kerberos. Generally, when configuring options related to Kerberos authentication, it is good practice to verify your configuration with mongokerberos. mongokerberos is a testing and verification tool; it does not edit any files or configure any services. For configuring Kerberos on your platform please consult the MIT Kerberos documentation, or your platform's documentation. For configuring MongoDB to authenticate using Kerberos, please reference the following tutorials:  * Configure MongoDB with Kerberos Authentication on Linux  * Configure MongoDB with Kerberos Authentication on Windows. This document provides a complete overview of all command line options for mongokerberos. \nINSTALLATION \nThe mongokerberos tool is part of the MongoDB Database Tools Extra package, and can be installed with the MongoDB Server or as a standalone installation. \nINSTALL WITH SERVER \nTo install mongokerberos as part of a MongoDB Enterprise Server installation:  * Follow the instructions for your platform: Install MongoDB Enterprise Server  * After completing the installation, mongokerberos and the other included tools are available in the same location as the Server.\n   \n   \n   NOTE\n   \n   For the Windows .msi installer wizard, the Complete installation option includes mongokerberos. \nINSTALL AS STANDALONE \nTo install mongokerberos as a standalone installation:  * Follow the download link for MongoDB Enterprise Edition: MongoDB Enterprise Download Center  * Select your Platform (operating system) from the dropdown menu, then select the appropriate Package for your platform according to the following chart:\n   \n   OS\n   Package\n   Linux\n   tgz package\n   Windows\n   zip package\n   macOS\n   tgz package  * Once downloaded, unpack the archive and copy mongokerberos to a location on your hard drive.\n   \n   \n   TIP\n   \n   Linux and macOS users may wish to copy mongokerberos to a filesystem location that is defined in the $PATH environment variable, such as /usr/bin. Doing so allows referencing mongokerberos directly on the command line by name, without needing to specify its full path, or first navigating to its parent directory. See the installation guide for your platform for more information. \nUSAGE \nmongokerberos can be run in two modes: server and client. Run mongokerberos from the system command line, not in the mongosh. \nSERVER MODE \nRunning mongokerberos in server mode performs a series of verification steps against your system's Kerberos configuration, including checking for proper DNS resolution, validation of the Kerberos system keytab file, and testing against the MongoDB service principal for your mongod or mongos instance. Before you can use mongokerberos in server mode, you must:  1. Configure Kerberos on your platform according to your platform's documentation.\n\n Once you have completed these steps, you can run mongokerberos in server mode using the --server flag as follows: mongokerberos --server  If Kerberos has been configured properly on the server, and the service principal created successfully, the output might resemble the following: Resolving kerberos environment...[OK] Kerberos environment resolved without errors.\nVerifying DNS resolution works with Kerberos service at <hostname>...[OK] DNS test successful.\nGetting MIT Kerberos KRB5 environment variables...  * KRB5CCNAME: not set.  * KRB5_CLIENT_KTNAME: not set.  * KRB5_CONFIG: not set.  * KRB5_KTNAME: not set.  * KRB5_TRACE: not set.[OK]\nVerifying existence of KRB5 keytab FILE:/etc/krb5.keytab...[OK] KRB5 keytab exists and is populated.\nChecking principal(s) in KRB5 keytab...Found the following principals for MongoDB service mongodb:  * mongodb/server.example.com@SERVER.EXAMPLE.COMFound the following kvnos in keytab entries for service mongodb:  * 3[OK] KRB5 keytab is valid.\nFetching KRB5 Config...KRB5 config profile resolved as:   <Your Kerberos profile file will be output here>[OK] KRB5 config profile resolved without errors.\nAttempting to initiate security context with service credentials...[OK] Security context initiated successfully. The final message indicates that the system's Kerberos configuration is ready to be used with MongoDB. If any errors are encountered with the configuration, they will be presented as part of the above output. \nCLIENT MODE \nRunning mongokerberos in client mode tests authentication against your system's Kerberos environment, performing each step in the Kerberos authentication process, including checking for proper DNS resolution, verification of the Kerberos client keytab file, and testing whether a ticket can be successfully granted. Running mongokerberos in client mode simulates the client authentication procedure of mongosh. Before you can use mongokerberos in client mode, you must first have configured Kerberos on your platform according to your platform's documentation. Optionally, you may also choose to run mongokerberos in server mode first to verify that your platform's Kerberos configuration is valid before using client mode. Once you have completed these steps, you can run mongokerberos in client mode to test user authentication, using the --client flag as follows: mongokerberos --client --username <username>  You must provide a valid username, which is used to request a Kerberos ticket as part of the authentication procedure. Your platform's Kerberos infrastructure must be aware of this user. If the provided credentials are valid, and the Kerberos options in the configuration files are valid, the output might resemble the following:  Resolving kerberos environment... [OK] Kerberos environment resolved without errors.\n Verifying DNS resolution works with Kerberos service at <hostname>... [OK] DNS test successful.\n Getting MIT Kerberos KRB5 environment variables...   * KRB5CCNAME: not set.   * KRB5_CLIENT_KTNAME: not set.   * KRB5_CONFIG: not set.   * KRB5_KTNAME: not set.   * KRB5_TRACE: not set. [OK]\n Verifying existence of KRB5 client keytab FILE:/path/to/client.keytab... [OK] KRB5 client keytab exists and is populated.\n Checking principal(s) in KRB5 keytab... [OK] KRB5 keytab is valid.\n Fetching KRB5 Config... KRB5 config profile resolved as:    <Your Kerberos profile file will be output here> [OK] KRB5 config profile resolved without errors.\n Attempting client half of GSSAPI conversation... [OK] Client half of GSSAPI conversation completed successfully. The final message indicates that client authentication completed successfully for the user provided. If any errors are encountered during the authentication steps, they will be presented as part of the above output. \nOPTIONS \n--server \nRuns mongokerberos in server mode to test that your platform's Kerberos configuration is valid for use with MongoDB. See Server Mode for example usage and expected output. --client\n\n See Client Mode for example usage and expected output. --config <filename>, -f <filename> \nSpecifies a configuration file for runtime configuration options. The options are equivalent to the command-line configuration options. See Configuration File Options for more information. mongokerberos will read the values for saslHostName and saslServiceName from this file if present. These values can alteratively be specified with the --setParameter option instead. Ensure the configuration file uses ASCII encoding. The mongokerberos instance does not support configuration files with non-ASCII encoding, including UTF-8. Only valid in server mode. --setParameter <options> \nSets a configurable parameter. You can specify multiple setParameter fields. While you can use any supported parameters with setParameter, mongokerberos only checks for the value of the following:  * saslHostName  * saslServiceName If using the --config option with a configuration file that also contains these values, the setParameter values will override the values from the configuration file. Valid in both server mode and client mode. --host <hostname> \nSpecify the hostname of the MongoDB server to connect to when testing authentication. If --host is not specified, mongokerberos does not perform any DNS validation of the hostname (i.e. PTR record verification) Only valid in client mode. --username <username>, -u <username> \nUsername for mongokerberos to use when attempting Kerberos authentication. This value is required when running in client mode. Only valid in client mode. --gssapiServiceName <servicename> \ndefault: 'mongodb' Service principal name to use when authenticating using GSSAPI/Kerberos. Only valid in client mode. --gssapiHostName <hostname> \nRemote hostname to use for purpose of GSSAPI/Kerberos authentication. Only valid in client mode. ←  mongoldapinstall_compass → On this page  * Synopsis\n * Installation\n * Usage\n * Options Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/program/mongod.exe/": " Docs Home → MongoDB Manual \nMONGOD.EXE \nOn this page    \n * Synopsis\n   \n * Options \nSYNOPSIS \nexe is the build of the MongoDB daemon (i.e. mongod) for the Windows platform. exe has all of the features of mongod on Unix-like platforms and is completely compatible with the other builds of mongod. In addition, exe provides several options for interacting with the Windows platform itself. This document only references options that are unique to exe. exe supports all mongod options except those with documented Windows incompatibility. See the mongod and the Configuration File Options documents for more information on mongod options not listed here. To install and use exe, read the Install MongoDB Community Edition on Windows document. \nOPTIONS \n--install \nInstalls mongod.exe as a Windows Service and exits. If needed, you can install services for multiple instances of mongod.exe. Install each service with a unique --serviceName and --serviceDisplayName. Use multiple instances only when sufficient system resources exist and your system design requires it. --remove \nRemoves the mongod.exe Windows Service. If mongod.exe is running, this operation will stop and then remove the service. --remove requires the --serviceName if you configured a non-default --serviceName during the --install operation. --reinstall \nRemoves mongod.exe and reinstalls mongod.exe as a Windows Service. --serviceName name \nDefault: MongoDB Sets the service name of mongod.exe when running as a Windows Service. Use this name with the net start <name> and net stop <name> operations. You must use --serviceName in conjunction with either the --install or --remove option. --serviceDisplayName <name> \nDefault: MongoDB Sets the name listed for MongoDB on the Services administrative application. --serviceDescription <description> \nDefault: MongoDB Server Sets the mongod.exe service description. You must use --serviceDescription in conjunction with the --install option. For descriptions that contain spaces, you must enclose the description in quotes. --serviceUser <user> \nRuns the mongod.exe service in the context of a certain user. This user must have \"Log on as a service\" privileges. You must use --serviceUser in conjunction with the --install option. --servicePassword <password> \nSets the password for <user> for mongod.exe when running with the --serviceUser option. You must use --servicePassword in conjunction with the --install option. ←  mongosmongos.exe → On this page  * Synopsis\n * Options Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/commitReshardCollection/": " Docs Home → MongoDB Manual \nCOMMITRESHARDCOLLECTION \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \ncommitReshardCollection \nNew in version 5.0. During a resharding operation, MongoDB does not block writes until the estimated duration to complete the resharding operation is below two seconds. If the current estimate is above two seconds but the time frame is acceptable to you, you can finish resharding faster. The commitReshardCollection command blocks writes early and forces the resharding operation to complete. \nTIP In mongosh, this command can also be run through the sh.commitReshardCollection() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     commitReshardCollection: \"<database>.<collection>\"   })  The mongosh provides a wrapper method sh.commitReshardCollection(). \nEXAMPLE  COMMIT A RESHARDING OPERATION \nThe following command forces the resharding operation on the sales.orders to block writes and complete: db.adminCommand({  commitReshardCollection: \"sales.orders\"})  \nTIP \nSEE ALSO: Reshard a Collection ←  cleanupReshardCollectionconfigureCollectionBalancing → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getFreeMonitoringStatus/": " Docs Home → MongoDB Manual \nGETFREEMONITORINGSTATUS \nOn this page    \n * Definition\n   \n * Syntax\n * Access Control\n * Output \nDEFINITION \ngetFreeMonitoringStatus \nNew in version 4.0. Returns the status of whether free Cloud monitoring is enabled for standalone or replica sets (including shards). \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     getFreeMonitoringStatus: 1   })  mongosh provides the db.getFreeMonitoringStatus() helper for the command. \nACCESS CONTROL \nWhen running with access control, the user must have the checkFreeMonitoringStatus privilege actions on the cluster. That is, a user must have a role that grants the following privilege: { resource: { cluster : true }, actions: [ \"checkFreeMonitoringStatus\" ] }  The built-in role clusterMonitor role provides this privilege. \nOUTPUT \nThe command returns a document with the following fields: Field\nDescription\nstate\nThe free monitoring enablement state. Values are either: \"enabled\" or \"disabled\".\nmessage\nAny informational message related to your state.\nurl The unique URL at which your monitoring data can be accessed. \nNOTE Anyone with whom you share the URL can access your monitored data. Even when disabled, your unique URL is returned so that if you re-enable monitoring, you can access your previous metrics, if collected less than 24 hours ago. userReminder\nAny informational message related to your state.\nok operationTime $clusterTime\nFor details on these fields, see Response. \nTIP \nSEE ALSO: freeMonitoring field returned from db.serverStatus() ←  Free Monitoring CommandssetFreeMonitoring → On this page  * Definition\n * Syntax\n * Access Control\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/program/mongoldap/": " Docs Home → MongoDB Manual \nMONGOLDAP \nOn this page    \n * Synopsis\n   \n * Installation\n * Usage\n * Behavior\n * Options MongoDB Enterprise \nSYNOPSIS \nMongoDB Enterprise provides mongoldap for testing MongoDB's LDAP configuration options against a running LDAP server or set of servers. To validate the LDAP options in the configuration file, set the mongoldap --config option to the configuration file's path. To test the LDAP configuration options, you must specify a --user and --password. mongoldap simulates authentication to a MongoDB server running with the provided configuration options and credentials. mongoldap returns a report that includes the success or failure of any step in the LDAP authentication or authorization procedure. Error messages include information on specific errors encountered and potential advice for resolving the error. When configuring options related to LDAP authorization, mongoldap executes an LDAP query constructed using the provided configuration options and username, and returns a list of roles on the admin database which the user is authorized for. You can use this information when configuring LDAP authorization roles for user access control. For example, use mongoldap to ensure your configuration allows privileged users to gain the necessary roles to perform their expected tasks. Similarly, use mongoldap to ensure your configuration disallows non-privileged users from gaining roles for accessing the MongoDB server, or performing unauthorized actions. When configuring options related to LDAP authentication, use mongoldap to ensure that the authentication operation works as expected. Run mongoldap from the system command line, not in the mongosh. This document provides a complete overview of all command line options for mongoldap. \nINSTALLATION \nThe mongoldap tool is part of the MongoDB Database Tools Extra package, and can be installed with the MongoDB Server or as a standalone installation. \nINSTALL WITH SERVER \nTo install mongoldap as part of a MongoDB Enterprise Server installation:      * Follow the instructions for your platform: Install MongoDB Enterprise Server  * After completing the installation, mongoldap and the other included tools are available in the same location as the Server.\n   \n   \n   NOTE\n   \n   For the Windows .msi installer wizard, the Complete installation option includes mongoldap. \nINSTALL AS STANDALONE \nTo install mongoldap as a standalone installation:  * Follow the download link for MongoDB Enterprise Edition: MongoDB Enterprise Download Center  * Select your Platform (operating system) from the dropdown menu, then select the appropriate Package for your platform according to the following chart:\n   \n   OS\n   Package\n   Linux\n   tgz package\n   Windows\n   zip package\n   macOS\n   tgz package  * Once downloaded, unpack the archive and copy mongoldap to a location on your hard drive.\n   \n   \n   TIP\n   \n   Linux and macOS users may wish to copy mongoldap to a filesystem location that is defined in the $PATH environment variable, such as /usr/bin. Doing so allows referencing mongoldap directly on the command line by name, without needing to specify its full path, or first navigating to its parent directory. See the installation guide for your platform for more information. \nUSAGE  NOTE A full description of LDAP or Active Directory is beyond the scope of this documentation. Consider the following sample configuration file, designed to support LDAP authentication and authorization via Active Directory:\n\n  You can use mongoldap to validate the configuration file, which returns a report of the procedure. You must specify a username and password for mongoldap. mongoldap --config=<path-to-config> --user=\"bob@dba.example.com\" --password=\"secret123\"  If the provided credentials are valid, and the LDAP options in the configuration files are valid, the output might be as follows: Checking that an LDAP server has been specified...[OK] LDAP server found\nConnecting to LDAP server...[OK] Connected to LDAP server\nParsing MongoDB to LDAP DN mappings..[OK] MongoDB to LDAP DN mappings appear to be valid\nAttempting to authenticate against the LDAP server...[OK] Successful authentication performed\nChecking if LDAP authorization has been enabled by configuration...[OK] LDAP authorization enabled\nParsing LDAP query template..[OK] LDAP query configuration template appears valid\nExecuting query against LDAP server...[OK] Successfully acquired the following roles:...  \nBEHAVIOR \nStarting in MonogoDB 5.1, mongoldap supports prefixing LDAP server with srv: and srv_raw:. If your connection string specifies \"srv:<DNS_NAME>\", mongoldap verifies that \"_ldap._tcp.gc._msdcs.<DNS_NAME>\" exists for SRV to support Active Directory. If not found, it verifies \"_ldap._tcp.<DNS_NAME>\" exists for SRV. If an SRV record cannot be found, mongoldap warns you to use \"srv_raw:<DNS_NAME>\" instead. mongoldap does the reverse check for \"srv_raw:<DNS_NAME>\" by checking for \"_ldap._tcp.<DNS NAME>\". \nOPTIONS \n--config=<filename>, -f=<filename> \nSpecifies a configuration file for runtime configuration options. The options are equivalent to the command-line configuration options. See Configuration File Options for more information. mongoldap uses any configuration options related to LDAP Proxy Authentication or LDAP Authorization for testing LDAP authentication or authorization. Requires specifying --user. May accept --password for testing LDAP authentication. Ensure the configuration file uses ASCII encoding. The mongoldap instance does not support configuration files with non-ASCII encoding, including UTF-8. --user=<string> \nUsername for mongoldap to use when attempting LDAP authentication or authorization. --password=<string> \nPassword of the --user for mongoldap to use when attempting LDAP authentication. Not required for LDAP authorization. --ldapServers=<host1>:<port>,<host2>:<port>,...,<hostN>:<port> \nThe LDAP server against which the mongoldap authenticates users or determines what actions a user is authorized to perform on a given database. If the LDAP server specified has any replicated instances, you may specify the host and port of each replicated server in a comma-delimited list. If your LDAP infrastructure partitions the LDAP directory over multiple LDAP servers, specify one LDAP server or any of its replicated instances to --ldapServers. MongoDB supports following LDAP referrals as defined in RFC 4511 4.1.10. Do not use --ldapServers for listing every LDAP server in your infrastructure. This setting can be configured on a running mongoldap using setParameter. If unset, mongoldap cannot use LDAP authentication or authorization. --ldapQueryUser=<string> \nAvailable in MongoDB Enterprise only. The identity with which mongoldap binds as, when connecting to or performing queries on an LDAP server. Only required if any of the following are true:  * Using LDAP authorization.  * Using an LDAP query for username transformation.  * The LDAP server disallows anonymous binds You must use --ldapQueryUser with --ldapQueryPassword. If unset, mongoldap will not attempt to bind to the LDAP server. This setting can be configured on a running mongoldap using setParameter. \nNOTE Windows MongoDB deployments can use --ldapBindWithOSDefaults instead of --ldapQueryUser and --ldapQueryPassword. You cannot specify both --ldapQueryUser and --ldapBindWithOSDefaults at the same time. --ldapQueryPassword=<string | array> \nAvailable in MongoDB Enterprise only.\n\n If not set, mongoldap does not attempt to bind to the LDAP server. You can configure this setting on a running mongoldap using setParameter. Starting in MongoDB 4.4, the ldapQueryPassword setParameter command accepts either a string or an array of strings. If ldapQueryPassword is set to an array, MongoDB tries each password in order until one succeeds. Use a password array to roll over the LDAP account password without downtime. \nNOTE Windows MongoDB deployments can use --ldapBindWithOSDefaults instead of --ldapQueryUser and --ldapQueryPassword. You cannot specify both --ldapQueryPassword and --ldapBindWithOSDefaults at the same time. --ldapBindWithOSDefaults=<bool> \nDefault: false Available in MongoDB Enterprise for the Windows platform only. Allows mongoldap to authenticate, or bind, using your Windows login credentials when connecting to the LDAP server. Only required if:  * Using LDAP authorization.  * Using an LDAP query for username transformation.  * The LDAP server disallows anonymous binds Use --ldapBindWithOSDefaults to replace --ldapQueryUser and --ldapQueryPassword. --ldapBindMethod=<string> \nDefault: simple Available in MongoDB Enterprise only. The method mongoldap uses to authenticate to an LDAP server. Use with --ldapQueryUser and --ldapQueryPassword to connect to the LDAP server. --ldapBindMethod supports the following values: Value\nDescription\nsimple\nmongoldap uses simple authentication.\nsasl\nmongoldap uses SASL protocol for authentication. If you specify sasl, you can configure the available SASL mechanisms using --ldapBindSaslMechanisms. mongoldap defaults to using DIGEST-MD5 mechanism. --ldapBindSaslMechanisms=<string> \nDefault: DIGEST-MD5 Available in MongoDB Enterprise only. A comma-separated list of SASL mechanisms mongoldap can use when authenticating to the LDAP server. The mongoldap and the LDAP server must agree on at least one mechanism. The mongoldap dynamically loads any SASL mechanism libraries installed on the host machine at runtime. Install and configure the appropriate libraries for the selected SASL mechanism(s) on both the mongoldap host and the remote LDAP server host. Your operating system may include certain SASL libraries by default. Defer to the documentation associated with each SASL mechanism for guidance on installation and configuration. If using the GSSAPI SASL mechanism for use with Kerberos Authentication, verify the following for the mongoldap host machine: Linux  * The KRB5_CLIENT_KTNAME environment variable resolves to the name of the client Linux Keytab Files for the host machine. For more on Kerberos environment variables, please defer to the Kerberos documentation.  * The client keytab includes a User Principal for the mongoldap to use when connecting to the LDAP server and execute LDAP queries. WindowsIf connecting to an Active Directory server, the Windows Kerberos configuration automatically generates a Ticket-Granting-Ticket when the user logs onto the system. Set --ldapBindWithOSDefaults to true to allow mongoldap to use the generated credentials when connecting to the Active Directory server and execute queries. Set --ldapBindMethod to sasl to use this option. \nNOTE For a complete list of SASL mechanisms see the IANA listing. Defer to the documentation for your LDAP or Active Directory service for identifying the SASL mechanisms compatible with the service. MongoDB is not a source of SASL mechanism libraries, nor is the MongoDB documentation a definitive source for installing or configuring any given SASL mechanism. For documentation and support, defer to the SASL mechanism library vendor or owner. For more information on SASL, defer to the following resources:  * For Linux, please see the Cyrus SASL documentation.  * For Windows, please see the Windows SASL documentation. --ldapTransportSecurity=<string> \nDefault: tls Available in MongoDB Enterprise only. By default, mongoldap creates a TLS/SSL secured connection to the LDAP server. For Linux deployments, you must configure the appropriate TLS Options in /etc/openldap/ldap.conf file. Your operating system's package manager creates this file as part of the MongoDB Enterprise installation, via the libldap dependency. See the documentation for TLS Options in the ldap.conf OpenLDAP documentation for more complete instructions.\n\n Set --ldapTransportSecurity to none to disable TLS/SSL between mongoldap and the LDAP server. \nWARNING Setting --ldapTransportSecurity to none transmits plaintext information and possibly credentials between mongoldap and the LDAP server. --ldapTimeoutMS=<long> \nDefault: 10000 Available in MongoDB Enterprise only. The amount of time in milliseconds mongoldap should wait for an LDAP server to respond to a request. Increasing the value of --ldapTimeoutMS may prevent connection failure between the MongoDB server and the LDAP server, if the source of the failure is a connection timeout. Decreasing the value of --ldapTimeoutMS reduces the time MongoDB waits for a response from the LDAP server. This setting can be configured on a running mongoldap using setParameter. --ldapUserToDNMapping=<string> \nAvailable in MongoDB Enterprise only. Maps the username provided to mongoldap for authentication to a LDAP Distinguished Name (DN). You may need to use --ldapUserToDNMapping to transform a username into an LDAP DN in the following scenarios:  * Performing LDAP authentication with simple LDAP binding, where users authenticate to MongoDB with usernames that are not full LDAP DNs.  * Using an LDAP authorization query template that requires a DN.  * Transforming the usernames of clients authenticating to Mongo DB using different authentication mechanisms (e.g. x.509, kerberos) to a full LDAP DN for authorization. --ldapUserToDNMapping expects a quote-enclosed JSON-string representing an ordered array of documents. Each document contains a regular expression match and either a substitution or ldapQuery template used for transforming the incoming username. Each document in the array has the following form: {  match: \"<regex>\"  substitution: \"<LDAP DN>\" | ldapQuery: \"<LDAP Query>\"} \nField\nDescription\nExample\nmatch\nAn ECMAScript-formatted regular expression (regex) to match against a provided username. Each parenthesis-enclosed section represents a regex capture group used by substitution or ldapQuery.\n\"(.+)ENGINEERING\" \"(.+)DBA\"\nsubstitution An LDAP distinguished name (DN) formatting template that converts the authentication name matched by the match regex into a LDAP DN. Each curly bracket-enclosed numeric value is replaced by the corresponding regex capture group extracted from the authentication username via the match regex. The result of the substitution must be an RFC4514 escaped string. \"cn={0},ou=engineering, dc=example,dc=com\"\nldapQuery\nA LDAP query formatting template that inserts the authentication name matched by the match regex into an LDAP query URI encoded respecting RFC4515 and RFC4516. Each curly bracket-enclosed numeric value is replaced by the corresponding regex capture group extracted from the authentication username via the match expression. mongoldap executes the query against the LDAP server to retrieve the LDAP DN for the authenticated user. mongoldap requires exactly one returned result for the transformation to be successful, or mongoldap skips this transformation.\n\"ou=engineering,dc=example, dc=com??one?(user={0})\" \nNOTE An explanation of RFC4514, RFC4515, RFC4516, or LDAP queries is out of scope for the MongoDB Documentation. Please review the RFC directly or use your preferred LDAP resource. For each document in the array, you must use either substitution or ldapQuery. You cannot specify both in the same document. When performing authentication or authorization, mongoldap steps through each document in the array in the given order, checking the authentication username against the match filter. If a match is found, mongoldap applies the transformation and uses the output for authenticating the user. mongoldap does not check the remaining documents in the array. If the given document does not match the provided authentication name, mongoldap continues through the list of documents to find additional matches. If no matches are found in any document, or the transformation the document describes fails, mongoldap returns an error. Starting in MongoDB 4.4, mongoldap also returns an error if one of the transformations cannot be evaluated due to networking or authentication failures to the LDAP server. mongoldap rejects the connection request and does not check the remaining documents in the array.\n\n \nEXAMPLE The following shows two transformation documents. The first document matches against any string ending in @ENGINEERING, placing anything preceeding the suffix into a regex capture group. The second document matches against any string ending in @DBA, placing anything preceeding the suffix into a regex capture group. \nIMPORTANT \nYOU MUST PASS THE ARRAY TO --LDAPUSERTODNMAPPING AS A STRING.  \"[   {      match: \"(.+)@ENGINEERING.EXAMPLE.COM\",      substitution: \"cn={0},ou=engineering,dc=example,dc=com\"   },   {      match: \"(.+)@DBA.EXAMPLE.COM\",      ldapQuery: \"ou=dba,dc=example,dc=com??one?(user={0})\"\n   }\n]\"  A user with username alice@ENGINEERING.EXAMPLE.COM matches the first document. The regex capture group {0} corresponds to the string alice. The resulting output is the DN \"cn=alice,ou=engineering,dc=example,dc=com\". A user with username bob@DBA.EXAMPLE.COM matches the second document. The regex capture group {0} corresponds to the string bob. The resulting output is the LDAP query \"ou=dba,dc=example,dc=com??one?(user=bob)\". mongoldap executes this query against the LDAP server, returning the result \"cn=bob,ou=dba,dc=example,dc=com\". If --ldapUserToDNMapping is unset, mongoldap applies no transformations to the username when attempting to authenticate or authorize a user against the LDAP server. This setting can be configured on a running mongoldap using the setParameter database command. --ldapAuthzQueryTemplate=<string> \nAvailable in MongoDB Enterprise only. A relative LDAP query URL formatted conforming to RFC4515 and RFC4516 that mongoldap executes to obtain the LDAP groups to which the authenticated user belongs to. The query is relative to the host or hosts specified in --ldapServers. In the URL, you can use the following substituion tokens: Substitution Token\nDescription\n{USER}\nSubstitutes the authenticated username, or the transformed username if a username mapping is specified.\n{PROVIDED_USER} Substitutes the supplied username, i.e. before either authentication or LDAP transformation. New in version 4.2. When constructing the query URL, ensure that the order of LDAP parameters respects RFC4516: [ dn  [ ? [attributes] [ ? [scope] [ ? [filter] [ ? [Extensions] ] ] ] ] ]  If your query includes an attribute, mongoldap assumes that the query retrieves a the DNs which this entity is member of. If your query does not include an attribute, mongoldap assumes the query retrieves all entities which the user is member of. For each LDAP DN returned by the query, mongoldap assigns the authorized user a corresponding role on the admin database. If a role on the on the admin database exactly matches the DN, mongoldap grants the user the roles and privileges assigned to that role. See the db.createRole() method for more information on creating roles. \nEXAMPLE This LDAP query returns any groups listed in the LDAP user object's memberOf attribute. \"{USER}?memberOf?base\"  Your LDAP configuration may not include the memberOf attribute as part of the user schema, may possess a different attribute for reporting group membership, or may not track group membership through attributes. Configure your query with respect to your own unique LDAP configuration. If unset, mongoldap cannot authorize users using LDAP. This setting can be configured on a running mongoldap using the setParameter database command. \nNOTE An explanation of RFC4515, RFC4516 or LDAP queries is out of scope for the MongoDB Documentation. Please review the RFC directly or use your preferred LDAP resource. ←  mongos.exemongokerberos → On this page  * Synopsis\n * Installation\n * Usage\n * Behavior\n * Options Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/isdbgrid/": " Docs Home → MongoDB Manual \nISDBGRID \nOn this page    \n * Definition\n   \n * Syntax\n * Output \nDEFINITION \nisdbgrid \nThis command verifies that a process is a mongos. If you issue the isdbgrid command when connected to a mongos, the response document includes the isdbgrid field set to 1. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     isdbgrid: 1   })  \nOUTPUT \nThe returned document is similar to the following: { \"isdbgrid\" : 1, \"hostname\" : \"app.example.net\", \"ok\" : 1 }  If you issue the isdbgrid command when connected to a mongod, MongoDB returns an error document. The isdbgrid command is not available to mongod. The error document, however, also includes a line that reads \"isdbgrid\" : 1, just as in the document returned for a mongos. The error document is similar to the following: {   \"errmsg\" : \"no such cmd: isdbgrid\",   \"bad cmd\" : {         \"isdbgrid\" : 1   },   \"ok\" : 0}  You can instead use the hello command to determine connection to a mongos. When connected to a mongos, the hello command returns a document that contains the string isdbgrid in the msg field. ←  getShardVersionlistShards → On this page  * Definition\n * Syntax\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/endSessions/": " Docs Home → MongoDB Manual \nENDSESSIONS \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Access Control \nDEFINITION \nendSessions \nThe endSessions command expires the specified sessions. The command overrides the timeout period that sessions wait before expiring. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     endSessions: [ { id : <UUID> }, ... ]   })  To run endSessions, use the db.runCommand( { <command> } ) method. db.runCommand(   {     endSessions: [ { id : <UUID> }, ... ]   } )  \nBEHAVIOR  SESSION IDENTIFICATION \nMongoDB concatenates each of the specified UUIDs with the hash of the authenticated user credentials to identify the user's sessions to end. If the user has no session that match, the endSessions has no effect. \nACCESS CONTROL \nIf the deployment enforces authentication/authorization, you must be authenticated to run the endSessions command. A user can only end sessions belonging to the user. \nTIP \nSEE ALSO:      * startSession  * $listLocalSessions  * $listSessions ←  commitTransactionkillAllSessions → On this page  * Definition\n * Syntax\n * Behavior\n * Access Control Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/connectionStatus/": " Docs Home → MongoDB Manual \nCONNECTIONSTATUS \nOn this page    \n * Definition\n   \n * Syntax\n * Example\n * Output \nDEFINITION \nconnectionStatus \nReturns information about the current connection, specifically the state of authenticated users and their available permissions. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     connectionStatus: 1,     showPrivileges: <boolean>   })  \nCOMMAND FIELDS \nconnectionStatus supports the following optional field: Field\nType\nDescription\nshowPrivileges\nboolean Optional. Set showPrivileges to true to instruct connectionStatus to return the full set of privileges that currently-authenticated users possess. By default, this field is false. \nEXAMPLE \nTo run connectionStatus use the db.runCommand() method, as in the following: db.runCommand( { connectionStatus: 1, showPrivileges: true } )  \nOUTPUT \nconnectionStatus.authInfo \nA document with data about the authentication state of the current connection, including users and available permissions. connectionStatus.authinfo.authenticatedUsers \nAn array with documents for each authenticated user. connectionStatus.authInfo.authenticatedUsers[n].user \nThe user's name. connectionStatus.authInfo.authenticatedUsers[n].db \nThe database associated with the user's credentials. connectionStatus.authinfo.authenticatedUserRoles \nAn array with documents for each role granted to the current connection: connectionStatus.authinfo.authenticatedUserRoles[n].role \nThe definition of the current roles associated with the current authenticated users. See Built-In Roles and Privilege Actions for more information. connectionStatus.authinfo.authenticatedUserRoles[n].db \nThe database to which role applies. connectionStatus.authInfo.authenticatedUserPrivileges \nAn array with documents describing the actions granted to the current connection, grouped by resource. connectionStatus.authInfo.authenticatedUserPrivileges[n].resource \nA document describing the database and, if applicable, collection to which connectionStatus.authInfo.authenticatedUserPrivileges[n].actions applies. connectionStatus.authInfo.authenticatedUserPrivileges[n].actions \nAn array listing the privilege actions that the connection has access to for the specified resource. connectionStatus.ok \nThe return value for the command. A value of 1 indicates success. ←  connPoolStatsdataSize → On this page  * Definition\n * Syntax\n * Example\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/collMod/": " Docs Home → MongoDB Manual \nCOLLMOD \nOn this page    \n * Definition\n   \n * Syntax\n * Options\n * Index Options\n * Document Validation\n * Views\n * Time Series Collections\n * Resize a Capped Collection\n * Change Streams with Document Pre- and Post-Images\n * Write Concern\n * Access Control\n * Behavior\n * Resource Locking\n * Examples\n * Change Expiration Value for Indexes\n * Hide an Index from the Query Planner\n * Convert an Existing Index to a Unique Index \nDEFINITION \ncollMod \ncollMod makes it possible to add options to a collection or to modify view definitions. \nTIP In mongosh, this command can also be run through the hideIndex() and unhideIndex() helper methods. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nNOTE The view modified by this command does not refer to materialized views. For discussion of on-demand materialized views, see $merge instead. \nSYNTAX \nThe command has the following syntax: \nNOTE \nSTARTING IN MONGODB 4.2      * The noPadding and usePowerOf2Sizes MMAPv1 options for the collMod command are removed. Do not use those options because upgrading from MongoDB 4.0 to 4.2 causes the 4.2 secondary members to immediately halt.  * A view definition pipeline cannot include the $out or the $merge stage. This restriction also applies to embedded pipelines, such as pipelines used in $lookup or $facet stages. db.runCommand(   {     collMod: <collection or view>,     <option1>: <value1>,     <option2>: <value2>,     ...   })  For the <collection or view>, specify the name of a collection or view in the current database. \nOPTIONS  INDEX OPTIONS \nindex \nThe index option can change the following properties of an existing index: Index Property\nDescription\nexpireAfterSeconds The number of seconds that determines the expiration threshold of a TTL Collection. If successful, the command returns a document that contains:  * expireAfterSeconds_new, the new value for expireAfterSeconds  * expireAfterSeconds_old, the old value for expireAfterSeconds, if the index had a value for expireAfterSeconds before. Modifying the index option expireAfterSeconds resets the $indexStats for the index. If you use TTL indexes created before MongoDB 5.0, or if you want to sync data created in MongDB 5.0 with a pre-5.0 installation, see Indexes Configured Using NaN to avoid misconfiguration issues. hidden A boolean that determines whether the index is hidden or not from the query planner. If the hidden value changes, the command returns a document that contains both the old and new values for the changed property: hidden_old and hidden_new. However, if the hidden value has not changed (i.e. hiding an already hidden index or unhiding an already unhidden index), the command omits the hidden_old and hidden_new fields from the output. To hide an index, you must have featureCompatibilityVersion set to 4.4 or greater. Modifying the index option hidden resets the $indexStats for the index if the value changes. New in version 4.4. prepareUnique A boolean that determines whether the index will accept new duplicate entries. New duplicate entries fail with DuplicateKey errors when prepareUnique is true. The resulting index can be converted to a unique index. To convert the index, use collMod with the unique option. If an existing index is updated so that prepareUnique is true, the index is not checked for pre-existing, duplicate index entries. New in version 6.0. unique A boolean that determines whether or not the index is unique. When unique is true, collMod scans the keyPattern index for duplicates and then converts it to a unique index if there are no duplicate index entries. If duplicates are detected during the initial scan, collMod returns CannotConvertIndexToUnique and a list of conflicting documents. To convert an index with duplicate entries to a unique index, correct any reported conflicts and rerun collMod. To end a conversion, set prepareUnique to false. New in version 6.0. To change index options, specify either the key pattern or name of the existing index and the index option or options you wish to change:\n\n  If the index does not exist, the command errors with the message \"cannot find index <name|keyPattern> for ns <db.collection>\". \nTIP \nSEE ALSO:  * Hidden Indexes \nDOCUMENT VALIDATION \nvalidator \nvalidator allows users to specify validation rules or expressions for a collection. For more information, see Schema Validation. The validator option takes a document that specifies the validation rules or expressions. You can specify the expressions using the same operators as the query operators with the exception of $near, $nearSphere, $text, and $where. \nNOTE  * Validation occurs during updates and inserts. Existing documents do not undergo validation checks until modification.  * You cannot specify a validator for collections in the admin, local, and config databases.  * You cannot specify a validator for system.* collections. validationLevel \nThe validationLevel determines how strictly MongoDB applies the validation rules to existing documents during an update. validationLevel\nDescription\n\"off\"\nNo validation for inserts or updates.\n\"strict\"\nDefault Apply validation rules to all inserts and all updates.\n\"moderate\"\nApply validation rules to inserts and to updates on existing valid documents. Do not apply rules to updates on existing invalid documents. To see an example that uses validationLevel, see Specify Validation Level for Existing Documents. validationAction \nThe validationAction option determines whether to error on invalid documents or just warn about the violations but allow invalid documents. \nIMPORTANT Validation of documents only applies to those documents as determined by the validationLevel. To see an example that uses validationAction, see Choose How to Handle Invalid Documents. \nVIEWS  NOTE The view modified by this command does not refer to materialized views. For discussion of on-demand materialized views, see $merge instead. viewOn \nThe underlying source collection or view for the view. The view definition is determined by applying the specified pipeline to this source. Required if modifying a view on a MongoDB deployment that is running with access control. pipeline \nThe aggregation pipeline that defines the view. \nNOTE A view definition pipeline cannot include the $out or the $merge stage. This restriction also applies to embedded pipelines, such as pipelines used in $lookup or $facet stages. Required if modifying a view on a MongoDB deployment that is running with access control. The view definition is public; i.e. db.getCollectionInfos() and explain operations on the view will include the pipeline that defines the view. As such, avoid referring directly to sensitive fields and values in view definitions. db.runCommand( {   collMod: \"myView\",   viewOn: \"activities\",   pipeline: [     { $match: { status: \"Q\" } },     { $project: { user: 1, date: 1, description: 1} } ]} )  \nTIME SERIES COLLECTIONS \nTo enable automatic removal of documents or change the expireAfterSeconds parameter value for an existing time series collection, issue the following collMod command: db.runCommand( {   collMod: <collection>,   expireAfterSeconds: <Number> || \"off\"} )  The expireAfterSeconds field must be either:  * A non-negative decimal number (>=0)  * The string \"off\". A number specifies the number of seconds after which documents expire. The string \"off\" removes the expireAfterSeconds parameter and disables automatic removal. \nTIP \nSEE ALSO: Set up Automatic Removal for Time Series Collections (TTL) \nRESIZE A CAPPED COLLECTION \nNew in version 6.0. Starting in MongoDB 6.0, you can resize a capped collection. To change a capped collection's maximum size in bytes, use the cappedSize option. To change the maximum number of documents in an existing capped collection, use the cappedMax option. \nNOTE You can't use these commands to resize the oplog. Use replSetResizeOplog instead. cappedSize \nSpecifies a new maximum size, in bytes, for a capped collection. cappedSize must be greater than 0 and less than 1e+15 (1 PB). cappedMax \nSpecifies a new maximum number of documents in a capped collection. Setting cappedMax less than or equal to 0 implies no limit. For example, the following command sets the maximum size of a capped collection to 100000 bytes and sets the maximum number of documents in the collection to 500: db.runCommand( {   collMod: <collection>,   cappedSize: 100000,   cappedMax: 500} )  \nCHANGE STREAMS WITH DOCUMENT PRE- AND POST-IMAGES\n\n  * The pre-image is the document before it was replaced, updated, or deleted. There is no pre-image for an inserted document.  * The post-image is the document after it was inserted, replaced, or updated. There is no post-image for a deleted document.  * Enable changeStreamPreAndPostImages for a collection using db.createCollection(), create, or collMod. To use collMod to enable change stream pre- and post-images for a collection, use the changeStreamPreAndPostImages field: db.runCommand( {   collMod: <collection>,   changeStreamPreAndPostImages: { enabled: <boolean> }} ) To enable change stream pre- and post-images for a collection, set changeStreamPreAndPostImages to true. For example: db.runCommand( {   collMod: \"orders\",   changeStreamPreAndPostImages: { enabled: true }} )  To disable change stream pre- and post-images for a collection, set changeStreamPreAndPostImages to false. For example: db.runCommand( {   collMod: \"orders\",   changeStreamPreAndPostImages: { enabled: false }} )  Pre- and post-images are not available for a change stream event if the images were:  * Not enabled on the collection at the time of a document update or delete operation.  * Removed after the pre- and post-image retention time set in expireAfterSeconds.\n   \n   * The following example sets expireAfterSeconds to 100 seconds:\n     \n     use admindb.runCommand( {   setClusterParameter:      { changeStreamOptions: { preAndPostImages: { expireAfterSeconds: 100 } } }} )\n     \n     \n   \n   * The following example returns the current changeStreamOptions settings, including expireAfterSeconds:\n     \n     db.adminCommand( { getClusterParameter: \"changeStreamOptions\" } )\n     \n     \n   \n   * Setting expireAfterSeconds to off uses the default retention policy: pre- and post-images are retained until the corresponding change stream events are removed from the oplog.\n   \n   * If a change stream event is removed from the oplog, then the corresponding pre- and post-images are also deleted regardless of the expireAfterSeconds pre- and post-image retention time. Additional considerations:  * Enabling pre- and post-images consumes storage space and adds processing time. Only enable pre- and post-images if you need them.  * Limit the change stream event size to less than 16 megabytes. To limit the event size, you can:\n   \n   * Limit the document size to 8 megabytes. You can request pre- and post-images simultaneously in the change stream output if other change stream event fields like updateDescription are not large.\n   \n   * Request only post-images in the change stream output for documents up to 16 megabytes if other change stream event fields like updateDescription are not large.\n   \n   * Request only pre-images in the change stream output for documents up to 16 megabytes if:\n     \n     * document updates affect only a small fraction of the document structure or content, and\n     \n     * do not cause a replace change event. A replace event always includes the post-image.  * To request a pre-image, you set fullDocumentBeforeChange to required or whenAvailable in db.collection.watch(). To request a post-image, you set fullDocument using the same method.  * Pre-images are written to the config.system.preimages collection.\n   \n   * The config.system.preimages collection may become large. To limit the collection size, you can set expireAfterSeconds time for the pre-images as shown earlier.\n   \n   * Pre-images are removed asynchronously by a background process. \nIMPORTANT \nBACKWARD-INCOMPATIBLE FEATURE Starting in MongoDB 6.0, if you are using document pre- and post-images for change streams, you must disable changeStreamPreAndPostImages for each collection using the collMod command before you can downgrade to an earlier MongoDB version. \nTIP \nSEE ALSO:  * For change stream events and output, see Change Events.  * To watch a collection for changes, see db.collection.watch().\n\n \nWRITE CONCERN \nOptional. A document expressing the write concern of the collMod command. Omit to use the default write concern. \nACCESS CONTROL \nIf the deployment enforces authentication/authorization, you must have the following privilege to run the collMod command: Task\nRequired Privileges\nModify a non-capped collection\ncollMod in the database\nModify a view collMod in the database and either:  * no find on the view to modify, or  * both find on the view to modify and find on the source collection/view. The built-in role dbAdmin provides the required privileges. \nBEHAVIOR  RESOURCE LOCKING \nThe collMod command obtains an exclusive lock on the parent database of the specified collection for the duration of the operation. All subsequent operations on the database and all its collections must wait until collMod releases the lock. \nEXAMPLES  CHANGE EXPIRATION VALUE FOR INDEXES \nThe following example updates the expireAfterSeconds property of an existing TTL index { lastAccess: 1 } on a collection named user_log. The current expireAfterSeconds property for the index is set to 1800 seconds (or 30 minutes) and the example changes the value to 3600 seconds (or 60 minutes). db.runCommand({   collMod: \"user_log\",   index: {      keyPattern: { lastAccess: 1 },      expireAfterSeconds: 3600   }})  If successful, the operation returns a document that includes both the old and new value for the changed property: { \"expireAfterSeconds_old\" : 1800, \"expireAfterSeconds_new\" : 3600, \"ok\" : 1 }  \nHIDE AN INDEX FROM THE QUERY PLANNER  NOTE To hide an index, you must have featureCompatibilityVersion set to 4.4 or greater. However, once hidden, the index remains hidden even with featureCompatibilityVersion set to 4.2 on MongoDB 4.4 binaries. The following example hides an existing index on the orders collection. Specifically, the operation hides the index with the specification { shippedDate: 1 } from the query planner. db.runCommand({   collMod: \"orders\",   index: {      keyPattern: { shippedDate: 1 },      hidden: true   }})  If successful, the operation returns a document that includes both the old and new value for the changed property: { \"hidden_old\" : false, \"hidden_new\" : true, \"ok\" : 1 }  \nNOTE If the operation is successful but the hidden value has not changed (i.e. hiding an already hidden index or unhiding an already unhidden index), the command omits the hidden_old and hidden_new fields from the output. To hide a text index, you must specify the index by name and not by keyPattern. \nTIP \nSEE ALSO:  * Hidden Indexes  * db.collection.hideIndex()  * db.collection.unhideIndex() \nCONVERT AN EXISTING INDEX TO A UNIQUE INDEX \nCreate the apples collection: db.apples.insertMany( [   { type: \"Delicious\", quantity: 12 },   { type: \"Macintosh\", quantity: 13 },   { type: \"Delicious\", quantity: 13 },   { type: \"Fuji\", quantity: 15 },   { type: \"Washington\", quantity: 10 },] )  Add a single field index on type: db.apples.createIndex( { type: 1 } )  Prepare the index on the type field for conversion: db.runCommand( {    collMod: \"apples\",    index: {       keyPattern: { type: 1 },       prepareUnique: true    } } )  The existing index may contain duplicate entries, but it will not accept new documents that duplicate an index entry when prepareUnique is true. Try to insert a document with a duplicate index value: db.apples.insertOne( { type: \"Delicious\", quantity: 200 } )  The operation returns an error. The index will not accept new duplicate entries. Use the unique``option to convert the index to a unique index. ``collMod checks the collection for duplicate index entries before converting the index: db.runCommand( {    collMod: \"apples\",    index: {       keyPattern: { type: 1 },       unique: true    } } ) \n\n \"errmsg\" : \"Cannot convert the index to unique. Please resolve            conflicting documents before running collMod again.\" Some drivers also return a list of ObjectIds for the duplicate entries: {     \"ok\" : 0,     \"errmsg\" : \"Cannot convert the index to unique. Please resolve \\                 conflicting documents before running collMod again.\",     \"code\" : 359,     \"codeName\" : \"CannotConvertIndexToUnique\",     \"violations\" : [             {                     \"ids\" : [                             ObjectId(\"62a2015777e2d47c4da33146\"),                             ObjectId(\"62a2015777e2d47c4da33148\")                     ]             }     ]} To complete the conversion, modify the duplicate entries to remove any conflicts and re-run collMod() with the unique option. ←  cloneCollectionAsCappedcompact → On this page  * Definition\n * Syntax\n * Options\n * Index Options\n * Document Validation\n * Views\n * Time Series Collections\n * Resize a Capped Collection\n * Change Streams with Document Pre- and Post-Images\n * Write Concern\n * Access Control\n * Behavior\n * Resource Locking\n * Examples\n * Change Expiration Value for Indexes\n * Hide an Index from the Query Planner\n * Convert an Existing Index to a Unique Index Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/rotateCertificates/": " Docs Home → MongoDB Manual \nROTATECERTIFICATES \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Output\n * Behavior\n * Logging\n * Required Access\n * Example \nDEFINITION \nNew in version 5.0. rotateCertificates \nRotates the currently used TLS certificates for a mongod or mongos to use the updated values for these certificates defined in the configuration file. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     rotateCertificates: 1,     message: \"<optional log message>\"   })  \nCOMMAND FIELDS \nThe command takes the following optional field: Parameter\nType\nDescription\nmessage\nstring\noptional A message logged by the server to the log file and audit file. \nOUTPUT \nThe rotateCertificates command returns a document with the following field: Field\nType\nDescription\nok\nbool\nContains the command's execution status. true on success, or false if an error occurred. If false, an errmsg field is additionally provided with a detailed error message. \nBEHAVIOR \nRotation includes the following certificates:      * TLS Certificates  * CRL (Certificate Revocation List) files (on Linux and Windows platforms)  * CA (Certificate Authority) files To rotate one or more of these certificates:  1. Replace the certificate or certificates you wish to rotate on the filesystem, noting the following constraints:\n    \n    * Each new certificate must have the same filename and same filepath as the certificate it is replacing.\n    \n    * If rotating an encrypted TLS Certificate, its password must be the same as the password for the old certificate (as specified to the certificateKeyFilePassword configuration file setting). Certificate rotation does not support the interactive password prompt.  2. Connect mongosh to the mongod or mongos instance that you wish to perform certificate rotation on.  3. Run the rotateCertificates command to rotate the certificates used by the the mongod or mongos instance. When certificate rotation takes place:  * Existing connections to the mongod or mongos instance are not terminated, and will continue to use the old certificates.  * Any new connections will use the new certificates. If you have configured OCSP for your deployment, the rotateCertificates command will also fetch stapled OCSP responses during rotation. The rotateCertificates command may be run on a running mongod or mongos regardless of replication status. Only one instance of db.rotateCertificates() or rotateCertificates may run on each mongod or mongos process at a time. Attempting to initiate a second instance while one is already running will result in an error. Incorrect, expired, revoked, or missing certificate files will cause the certificate rotation to fail, but will not invalidate the existing TLS configuration or terminate the running mongod or mongos process. If the mongod or mongos is running with --tlsCertificateSelector set to thumbprint, rotateCertificates will fail and write a warning message to the log file. \nLOGGING \nOn successful rotation, the subject names, thumbprints, and the validity period of the server and cluster certificate thumbprints are logged to the configured log destination. If auditing is enabled, this information is also written to the audit log. On Linux and Windows platforms, if a CRL file is present, its thumbprint and validity period are also logged to these locations. \nREQUIRED ACCESS \nYou must have the rotateCertificates action in order to use the rotateCertificates command. The rotateCertificates action is part of the hostManager role. \nEXAMPLE \nThe following operation rotates the certificates on a running mongod instance, after having made the appropriate updates to the configuration file to specify the updated certificate information: db.adminCommand( { rotateCertificates: 1 } )  The following performs the same as above, but also writes a custom log message at rotation time to the log file and audit file: db.adminCommand( { rotateCertificates: 1, message: \"Rotating certificates\" } ) \n←  renameCollectionsetAuditConfig → On this page  * Definition\n * Syntax\n * Command Fields\n * Output\n * Behavior\n * Logging\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/unsetSharding/": " Docs Home → MongoDB Manual \nUNSETSHARDING \nunsetSharding \nRemoved in MongoDB 5.0. unsetSharding is an internal command that supports sharding functionality. ←  splitVectorupdateZoneKeyRange → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/createRole/": " Docs Home → MongoDB Manual \nCREATEROLE \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Required Access\n * Example \nDEFINITION \ncreateRole \nCreates a role and specifies its privileges. The role applies to the database on which you run the command. The createRole command returns a duplicate role error if the role already exists in the database. \nTIP In mongosh, this command can also be run through the db.createRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     createRole: \"<new role>\",     privileges: [       { resource: { <resource> }, actions: [ \"<action>\", ... ] },       ...     ],     roles: [       { role: \"<role>\", db: \"<database>\" } | \"<role>\",       ...     ],     authenticationRestrictions: [       {         clientSource: [\"<IP>\" | \"<CIDR range>\", ...],         serverAddress: [\"<IP>\" | \"<CIDR range>\", ...]       },       ...     ],     writeConcern: <write concern document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe createRole command has the following fields: Field\nType\nDescription\ncreateRole\nstring\nThe name of the new role.\nprivileges\narray The privileges to grant the role. A privilege consists of a resource and permitted actions. For the syntax of a privilege, see the privileges array. You must include the privileges field. Use an empty array to specify no privileges. roles\narray An array of roles from which this role inherits privileges. You must include the roles field. Use an empty array to specify no roles to inherit from. authenticationRestrictions\narray Optional. The authentication restrictions the server enforces on the role. Specifies a list of IP addresses and CIDR ranges users granted this role are allowed to connect to and/or which they can connect from. writeConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where createRole runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field: Field Name\nValue\nDescription\nclientSource\nArray of IP addresses and/or CIDR ranges\nIf present, when authenticating a user, the server verifies that the client's IP address is either in the given list or belongs to a CIDR range in the list. If the client's IP address is not present, the server does not authenticate the user.\nserverAddress\nArray of IP addresses and/or CIDR ranges\nA list of IP addresses or CIDR ranges to which the client can connect. If present, the server will verify that the client's connection was accepted via an IP address in the given list. If the connection was accepted via an unrecognized IP address, the server does not authenticate the user. \nIMPORTANT\n\n For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. \nBEHAVIOR \nA role's privileges apply to the database where the role is created. The role can inherit privileges from other roles in its database. A role created on the admin database can include privileges that apply to all databases or to the cluster and can inherit privileges from roles in other databases. \nREQUIRED ACCESS \nTo create a role in a database, you must have:  * the createRole action on that database resource.  * the grantRole action on that database to specify privileges for the new role as well as to specify roles to inherit from. Built-in roles userAdmin and userAdminAnyDatabase provide createRole and grantRole actions on their respective resources. To create a role with authenticationRestrictions specified, you must have the setAuthenticationRestriction action on the database resource which the role is created. \nEXAMPLE \nThe following createRole command creates the myClusterwideAdmin role on the admin database: db.adminCommand({ createRole: \"myClusterwideAdmin\",  privileges: [    { resource: { cluster: true }, actions: [ \"addShard\" ] },    { resource: { db: \"config\", collection: \"\" }, actions: [ \"find\", \"update\", \"insert\", \"remove\" ] },    { resource: { db: \"users\", collection: \"usersCollection\" }, actions: [ \"update\", \"insert\", \"remove\" ] },    { resource: { db: \"\", collection: \"\" }, actions: [ \"find\" ] }  ],  roles: [    { role: \"read\", db: \"admin\" }  ],  writeConcern: { w: \"majority\" , wtimeout: 5000 }}) \n←  Role Management CommandsdropRole → On this page  * Definition\n * Syntax\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setUserWriteBlockMode/": " Docs Home → MongoDB Manual \nSETUSERWRITEBLOCKMODE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \nsetUserWriteBlockMode \nNew in version 6.0. The setUserWriteBlockMode command blocks and unblocks writes to the entire cluster. During cluster-to-cluster sync, mongosync, the cluster-to-cluster synchronization tool, uses the setUserWriteBlockMode command to block writes on the destination cluster. For more information, see the HTTP API start command. \nNOTE Users and applications with the bypassWriteBlockingMode privilege can bypass the block and continue to perform writes. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     setUserWriteBlockMode: 1,     global: <boolean>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nsetUserWriteBlockMode\ninteger\nSet this field to 1.\nglobal\nboolean\nBlocks writes on a cluster when set to true. To enable writes on a cluster, set global: false. \nREQUIRED ACCESS \nTo execute the setUserWriteBlockMode command, the user must have the setUserWriteBlockMode privilege. \nEXAMPLE \n      1. Enable user write block mode:\n    \n    db.adminCommand( {   setUserWriteBlockMode: 1,   global: true} )\n    \n      2. Add a record to the collection:\n    \n    db.names.insertOne( { name: \"George Washington Cable\" } )\n    \n    \n    \n    The server blocks the write because the user write block is enabled.\n    \n    Example Output:\n    \n    MongoServerError: User writes blocked\n    \n      3. Disable user write block mode:\n    \n    db.adminCommand( {   setUserWriteBlockMode: 1,   global: false} )\n    \n      4. Add a record to the collection:\n    \n    db.names.insertOne( { name: \"George Washington Cable\" } )\n    \n    \n    \n    The insertOne() method writes to a collection. The server allows the write because the user write block is disabled. ←  setDefaultRWConcernshutdown → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/invalidateUserCache/": " Docs Home → MongoDB Manual \nINVALIDATEUSERCACHE \nOn this page    \n * Definition\n   \n * Syntax\n * Required Access \nDEFINITION \ninvalidateUserCache \nFlushes user information from in-memory cache, including removal of each user's credentials and roles. This allows you to purge the cache at any given moment, regardless of the interval set in the userCacheInvalidationIntervalSecs parameter. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     invalidateUserCache: 1   })  \nREQUIRED ACCESS \nYou must have privileges that include the invalidateUserCache action on the cluster resource in order to use this command. ←  grantRolesToRolerevokePrivilegesFromRole → On this page  * Definition\n * Syntax\n * Required Access Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/count/": " Docs Home → MongoDB Manual \nCOUNT \nOn this page    \n * Definition\n   \n * Syntax\n * Stable API Support\n * Behavior\n * Examples \nDEFINITION \ncount \nCounts the number of documents in a collection or a view. Returns a document that contains this count and as well as the command status. \nTIP In mongosh, this command can also be run through the count() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nNOTE MongoDB drivers compatible with the 4.0 features deprecate their respective cursor and collection count() APIs (which runs the count command) in favor of new APIs that corresponds to countDocuments() and estimatedDocumentCount(). For the specific API names for a given driver, see the driver API documentation. \nSYNTAX \nThe command has the following syntax: \nNOTE Starting in version 4.2, MongoDB implements a stricter validation of the option names for the count command. The command now errors if you specify an unknown option name. db.runCommand(   {     count: <collection or view>,     query: <document>,     limit: <integer>,     skip: <integer>,     hint: <hint>,     readConcern: <document>,     collation: <document>,     comment: <any>   })  \nCOMMAND FIELDS \ncount has the following fields: Field\nType\nDescription\ncount\nstring\nThe name of the collection or view to count.\nquery\ndocument\nOptional. A query that selects which documents to count in the collection or view.\nlimit\ninteger\nOptional. The maximum number of matching documents to return.\nskip\ninteger\nOptional. The number of matching documents to skip before returning results.\nhint\nstring or document\nOptional. The index to use. Specify either the index name as a string or the index specification document.\nreadConcern\ndocument Optional. Specifies the read concern. The option has the following syntax: readConcern: { level: <value> }  Possible read concern levels are:      * \"local\". This is the default read concern level for read operations against the primary and secondaries.  * \"available\". Available for read operations against the primary and secondaries. \"available\" behaves the same as \"local\" against the primary and non-sharded secondaries. The query returns the instance's most recent data.  * \"majority\". Available for replica sets that use WiredTiger storage engine.  * \"linearizable\". Available for read operations on the primary only. For more formation on the read concern levels, see Read Concern Levels. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nSTABLE API SUPPORT\n\n \nBEHAVIOR  INACCURATE COUNTS WITHOUT QUERY PREDICATE \nWhen you call count without a query predicate, you may receive inaccurate document counts. Without a query predicate, count commands return results based on the collection's metadata, which may result in an approximate count. In particular,  * On a sharded cluster, the resulting count will not correctly filter out orphaned documents.  * After an unclean shutdown or file copy based initial sync, the count may be incorrect. For counts based on collection metadata, see also collStats pipeline stage with the count option. \nCOUNT AND TRANSACTIONS \nWhen you use count in a transaction, the resulting count will not filter out any uncommitted multi-document transactions. For details, see Transactions and Count Operations. \nACCURACY AND SHARDED CLUSTERS \nOn a sharded cluster, the count command when run without a query predicate can result in an inaccurate count if orphaned documents exist or if a chunk migration is in progress. To avoid these situations, on a sharded cluster, use the db.collection.aggregate() method: You can use the $count stage to count the documents. For example, the following operation counts the documents in a collection: db.collection.aggregate( [   { $count: \"myCount\" }])  The $count stage is equivalent to the following $group + $project sequence: db.collection.aggregate( [   { $group: { _id: null, count: { $sum: 1 } } }   { $project: { _id: 0 } }] )  \nTIP \nSEE ALSO: $collStats to return an approximate count based on the collection's metadata. \nACCURACY AFTER UNEXPECTED SHUTDOWN \nAfter an unclean shutdown of a mongod using the Wired Tiger storage engine, count statistics reported by count may be inaccurate. The amount of drift depends on the number of insert, update, or delete operations performed between the last checkpoint and the unclean shutdown. Checkpoints usually occur every 60 seconds. However, mongod instances running with non-default --syncdelay settings may have more or less frequent checkpoints. Run validate on each collection on the mongod to restore statistics after an unclean shutdown. After an unclean shutdown:  * validate updates the count statistic in the collStats output with the latest value.  * Other statistics like the number of documents inserted or removed in the collStats output are estimates. \nNOTE This loss of accuracy only applies to count operations that do not include a query document. \nCLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued count disconnects before the operation completes, MongoDB marks count for termination using killOp. \nEXAMPLES \nThe following sections provide examples of the count command. \nCOUNT ALL DOCUMENTS \nThe following operation counts the number of all documents in the orders collection: db.runCommand( { count: 'orders' } )  In the result, the n, which represents the count, is 26, and the command status ok is 1: { \"n\" : 26, \"ok\" : 1 }  \nCOUNT DOCUMENTS THAT MATCH A QUERY \nThe following operation returns a count of the documents in the orders collection where the value of the ord_dt field is greater than Date('01/01/2012'): db.runCommand( { count:'orders',                 query: { ord_dt: { $gt: new Date('01/01/2012') } }               } )  In the result, the n, which represents the count, is 13 and the command status ok is 1: { \"n\" : 13, \"ok\" : 1 }  \nSKIP DOCUMENTS IN COUNT \nThe following operation returns a count of the documents in the orders collection where the value of the ord_dt field is greater than Date('01/01/2012') and skip the first 10 matching documents: db.runCommand( { count:'orders',                 query: { ord_dt: { $gt: new Date('01/01/2012') } },                 skip: 10 }  )  In the result, the n, which represents the count, is 3 and the command status ok is 1: { \"n\" : 3, \"ok\" : 1 }  \nSPECIFY THE INDEX TO USE\n\n db.runCommand(   {     count:'orders',     query: {              ord_dt: { $gt: new Date('01/01/2012') },              status: \"D\"            },     hint: { status: 1 }   })  In the result, the n, which represents the count, is 1 and the command status ok is 1: { \"n\" : 1, \"ok\" : 1 }  \nOVERRIDE DEFAULT READ CONCERN \nTo override the default read concern level of \"local\", use the readConcern option. The following operation on a replica set specifies a Read Concern of \"majority\" to read the most recent copy of the data confirmed as having been written to a majority of the nodes. \nIMPORTANT  * To use the readConcern level of \"majority\", you must specify a nonempty query condition.  * Regardless of the read concern level, the most recent data on a node may not reflect the most recent version of the data in the system. db.runCommand(   {     count: \"restaurants\",     query: { rating: { $gte: 4 } },     readConcern: { level: \"majority\" }   })  To ensure that a single thread can read its own writes, use \"majority\" read concern and \"majority\" write concern against the primary of the replica set. ←  aggregatedistinct → On this page  * Definition\n * Syntax\n * Stable API Support\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-administration/": " Docs Home → MongoDB Manual \nADMINISTRATION COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\ncloneCollectionAsCapped\nCopies a non-capped collection as a new capped collection.\ncollMod\nAdd options to a collection or modify a view definition.\ncompact\nDefragments a collection and rebuilds the indexes.\nconvertToCapped\nConverts a non-capped collection to a capped collection.\ncreate\nCreates a collection or a view.\ncreateIndexes\nBuilds one or more indexes for a collection.\ncurrentOp\nReturns a document that contains information on in-progress operations for the database instance.\ndrop\nRemoves the specified collection from the database.\ndropDatabase\nRemoves the current database.\ndropConnections\nDrops outgoing connections to the specified list of hosts.\ndropIndexes\nRemoves indexes from a collection.\nfilemd5\nReturns the md5 hash for files stored using GridFS.\nfsync\nFlushes pending writes to the storage layer and locks the database to allow backups.\nfsyncUnlock\nUnlocks one fsync lock.\ngetDefaultRWConcern Retrieves the global default read and write concern options for the deployment. New in version 4.4. getAuditConfig Retrieves details on audit configuration and filters. New in version 5.0. getClusterParameter Retrieves cluster parameter values from all nodes in a cluster. New in version 6.0. getParameter\nRetrieves configuration options.\nkillCursors\nKills the specified cursors for a collection.\nkillOp\nTerminates an operation as specified by the operation ID.\nlistCollections\nReturns a list of collections in the current database.\nlistDatabases\nReturns a document that lists all databases and returns basic database statistics.\nlistIndexes\nLists all indexes for a collection.\nlogRotate\nRotates the MongoDB logs to prevent a single file from taking too much space.\nreIndex\nRebuilds all indexes on a collection.\nrenameCollection\nChanges the name of an existing collection.\nrotateCertificates\nPerforms online TLS certificate rotation.\nsetFeatureCompatibilityVersion\nEnables or disables features that persist data that are backwards-incompatible.\nsetIndexCommitQuorum\nChanges the minimum number of data-bearing members (i.e commit quorum), including the primary, that must vote to commit an in-progress index build before the primary marks those indexes as ready.\nsetClusterParameter Modifies configuration options for all nodes in a cluster. New in version 6.0. setParameter\nModifies configuration options.\nsetDefaultRWConcern Sets the global default read and write concern options for the deployment. New in version 4.4. shutdown\nShuts down the mongod or mongos process. ←  startSessioncloneCollectionAsCapped → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getShardVersion/": " Docs Home → MongoDB Manual \nGETSHARDVERSION \ngetShardVersion \ngetShardVersion is a command that supports sharding functionality and is not part of the stable client facing API. ←  getShardMapisdbgrid → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getClusterParameter/": " Docs Home → MongoDB Manual \nGETCLUSTERPARAMETER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Examples \nDEFINITION \ngetClusterParameter \ngetClusterParameter is an administrative command for retrieving the values of cluster parameters. Cluster parameters are configurable parameters which affect all nodes in a sharded cluster. To run getClusterParameter, use the db.adminCommand( { command } ) method. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     getClusterParameter: <parameter> | [<parameter>, <parameter>] | \"'*'\"   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\n<parameter>\nString or array of strings You can specify one of the following:      * The name of the parameter you want to retrieve.  * An array with the names of multiple parameters.  * '*', to return values for all parameters available to getClusterParameter. \nBEHAVIOR \n * You can only run getClusterParameter on the admin database. If you run the command on any other database, MongoDB returns an error.  * You can run getClusterParameter on any node in a replica set or sharded cluster.  * When you run getClusterParameter on mongod, getClusterParameter returns cached parameter values.  * When you run getClusterParameter on mongos, getClusterParameter returns the durable value of the cluster parameter from the config server \nACCESSS CONTROL \nWhen authentication is enabled, getClusterParameter only works when authenticated as a user with a role that has access to the getClusterParameter action. \nEXAMPLES  NOTE The output of the following examples may vary depending on the specific configuration of the running MongoDB deployment. \nRETRIEVE SINGLE CLUSTER PARAMETER \nThe following operation runs getClusterParameter on the admin database using a value of hostName to retrieve the value for a hypothetical cluster parameter named hostName: use admindb.adminCommand( { getClusterParameter : \"hostName\" } )  \nRETRIEVE MULTIPLE CLUSTER PARAMETERS \nThe following operation runs getClusterParameter on the admin database using the values hostName and testParameter to retrieve the values for hypothetical cluster parameters named hostName and testParameter: use admindb.adminCommand( { getClusterParameter: [ \"hostName\", \"testParameter\" ] } )  \nRETRIEVE ALL CLUSTER PARAMETERS \nThe following operation runs getClusterParameter with a value of '*' to retrieve the values from all cluster parameters: use admindb.adminCommand( { getClusterParameter : '*' } )  \nTIP \nSEE ALSO: setClusterParameter for more about these parameters. ←  getAuditConfiggetDefaultRWConcern → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/balancerStatus/": " Docs Home → MongoDB Manual \nBALANCERSTATUS \nOn this page    \n * Definition\n   \n * Syntax\n * Output Document\n * Example \nDEFINITION \nbalancerStatus \nReturns a document that contains information about the status of the balancer. You can only issue the balancerStatus against the admin database. \nTIP In mongosh, this command can also be run through the sh.isBalancerRunning() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     balancerStatus: 1   })  \nOUTPUT DOCUMENT \nThe following is an example of a document returned by the command: {   \"mode\" : \"full\",   \"inBalancerRound\" : false,   \"numBalancerRounds\" : NumberLong(86),   \"ok\" : 1} \nField\nDescription\n\"mode\" A string that specifies whether the balancer thread is running or stopped. Possible values are:    \n * \"full\"Balancer thread is running but not necessarily in a balancing round.\n * \"off\"Balancer thread is stopped. No chunk balancing can occur in this mode. \"inBalancerRound\"\nA boolean that specifies if the balancer is currently in a balancing round; i.e. distributing chunks.\n\"numBalancerRounds\"\nThe number of balancer rounds which have occurred since the config servers were started. This value is not persisted and is reset to 0 upon restart of the config servers. \nEXAMPLE \nConnect to a mongos instance and issue the following command: db.adminCommand( { balancerStatus: 1 } ) \n←  balancerStartbalancerStop → On this page  * Definition\n * Syntax\n * Output Document\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dbHash/": " Docs Home → MongoDB Manual \nDBHASH \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Return Document\n * Examples \nDEFINITION \ndbHash \nReturns the hash values of the collections in a database and an MD5 value for these collections. dbHash is useful to compare databases across mongod instances, such as across members of replica sets. \nWARNING The dbHash command obtains a shared (S) lock on the database, which prevents writes until the command completes. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dbHash: 1,     collections: [ <collection1>, ... ]   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ndbHash\nAny type\nThe command to run. Specify any value.\ncollections\narray Optional. An array of collection names. Either specify the collections for which to return the hash values, or omit or specify an empty array to return the hash values for all collections in the database. \nBEHAVIOR  NON-EXISTENT COLLECTION \nIf a collection in the collections array is non-existent, dbHash does not return a hash value for that collection. \nRESTRICTIONS \nMongoDB drivers automatically set afterClusterTime for operations associated with causally consistent sessions. Starting in MongoDB 4.2, the dbHash command no longer support afterClusterTime. As such, dbHash cannot be associated with causally consistent sessions. \nRETURN DOCUMENT \nThe command returns a document with the following fields: Field\nDescription\nhost\nThe host and port of the mongod instance on which the command is run.\ncollections A document with the collections and their corresponding hash values. {   <collection1>: <hash1>,   <collection2>: <hash2>,   ...} \ncapped\nAn array that lists the capped collections.\nuuids A document with the collections and their corresponding UUID values. {   <collection1>: <UUID1>,   <collection2>: <UUID2>,   ...} \nmd5\nThe aggregate hash value for these collections.\ntimeMillis\nNumber of milliseconds to generate the hash.\nok\noperationTime\n$clusterTime\nReturned with every command. See also Response for details. \nEXAMPLES  RETURN HASH VALUES FOR ALL COLLECTIONS IN A DATABASE \nThe following example returns the hash value for all collections in the database test: use testdb.runCommand( { dbHash: 1 } )  The operation returns the following document:\n\n  \nRETURN HASH VALUES FOR SPECIFIED COLLECTIONS IN A DATABASE \nThe following example returns the hash value for the collections inventory and orders in the database test: use testdb.runCommand( { dbHash: 1, collections: [ \"inventory\", \"orders\" ] } )  The operation returns the following document: {   \"host\" : \"myHostName.local:27017\",   \"collections\" : {      \"inventory\" : \"ec3d821581ea1bd3aa8196c94b946874\",      \"orders\" : \"0242c0a128c284ea9576a34db2306c12\"   },   \"capped\" : [ ],   \"uuids\" : {      \"inventory\" : UUID(\"0830e0ad-cc24-4fc7-80d0-8e22fe45e382\"),      \"orders\" : UUID(\"755be489-745f-400c-ac3b-f27525ad0108\")   },   \"md5\" : \"cb4676f316ff2ff29c701a5edd18afe3\",   \"timeMillis\" : 0,   \"ok\" : 1,   \"operationTime\" : Timestamp(1529208801, 1),   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1529208801, 1),      \"signature\" : {         \"hash\" : BinData(0,\"I4z4a4Mgs+tcx0MP5xIU8DYAMB0=\"),         \"keyId\" : NumberLong(\"6567898567824900097\")      }   }} \n←  dataSizedbStats → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Return Document\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/removeShardFromZone/": " Docs Home → MongoDB Manual \nREMOVESHARDFROMZONE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Example \nDEFINITION \nremoveShardFromZone \nThe removeShardFromZone administrative command removes the association between a shard and a zone. \nTIP In mongosh, this command can also be run through the sh.removeShardFromZone() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nYou can only run removeShardFromZone on the admin database from a mongos instance. The command has the following syntax: db.adminCommand(   {     removeShardFromZone: <string>,     zone: <string>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nremoveShardFromZone\nstring The name of the shard from which to remove the zone association. zone\nstring The name of the zone whose association with the shard you want to remove. mongosh provides the helper method sh.removeShardFromZone(). \nBEHAVIOR \nremoveShardFromZone does not remove ranges associated with the zone. To completely remove a zone from the cluster, you must run removeShardFromZone on each shard associated with the zone. If the shard specified is the last shard associated with the zone, you must ensure there are no remaining shard key ranges associated with the zone. Use updateZoneKeyRange to remove any existing ranges associated with the zone before running removeShardFromZone. See the zone manual page for more information on zones in sharded clusters. \nSECURITY \nFor sharded clusters that enforce access control, you must authenticate either as:      * a user whose privileges include:\n   \n   * update on the shards collection in the config database, and\n   \n   * find on the tags collection in the config database;\n   \n   or, alternatively  * a user whose privileges include enableSharding on the cluster resource (available starting in version 4.2.2, 4.0.14, 3.6.16). The clusterAdmin or clusterManager built-in roles have the appropriate permissions for issuing removeShardFromZone. See the Role-Based Access Control manual page for more information. \nEXAMPLE \nThe following example removes the association between shard0000 and zone NYC: db.adminCommand( { removeShardFromZone : \"shard0000\" , zone : \"NYC\" } ) \n←  removeShardreshardCollection → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/createUser/": " Docs Home → MongoDB Manual \nCREATEUSER \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Required Access\n * Example \nDEFINITION \ncreateUser \nCreates a new user on the database where you run the command. The createUser command returns a duplicate user error if the user exists. \nTIP In mongosh, this command can also be run through the db.createUser() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. db.runCommand(   {     createUser: \"<name>\",     pwd: passwordPrompt(),      // Or  \"<cleartext password>\"     customData: { <any information> },     roles: [       { role: \"<role>\", db: \"<database>\" } | \"<role>\",       ...     ],     writeConcern: { <write concern> },     authenticationRestrictions: [        { clientSource: [ \"<IP|CIDR range>\", ... ], serverAddress: [ \"<IP|CIDR range>\", ... ] },        ...     ],     mechanisms: [ \"<scram-mechanism>\", ... ],  //Available starting in MongoDB 4.0     digestPassword: <boolean>,     comment: <any>   })  \nCOMMAND FIELDS \ncreateUser has the following fields: Field\nType\nDescription\ncreateUser\nstring\nThe name of the new user.\npwd\nstring The user's password. The pwd field is not required if you run createUser on the $external database to create users who have credentials stored externally to MongoDB. The value can be either:      * the user's password in cleartext string, or  * passwordPrompt() to prompt for the user's password. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. customData\ndocument\nOptional. Any arbitrary information. This field can be used to store any data an admin wishes to associate with this particular user. For example, this could be the user's full name or employee id.\nroles\narray\nThe roles granted to the user. Can specify an empty array [] to create users without roles.\ndigestPassword\nboolean Optional. Indicates whether the server or the client digests the password. If true, the server receives undigested password from the client and digests the password. If false, the client digests the password and passes the digested password to the server. Not compatible with SCRAM-SHA-256 Changed in version 4.0: The default value is true. In earlier versions, the default value is false. writeConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. authenticationRestrictions\narray\nOptional. The authentication restrictions the server enforces on the created user. Specifies a list of IP addresses and CIDR ranges from which the user is allowed to connect to the server or from which the server can accept users.\nmechanisms\narray Optional. Specify the specific SCRAM mechanism or mechanisms for creating SCRAM user credentials. If authenticationMechanisms is specified, you can only specify a subset of the authenticationMechanisms. Valid values are:  * \"SCRAM-SHA-1\"  * Uses the SHA-1 hashing function.  * \"SCRAM-SHA-256\"  * Uses the SHA-256 hashing function.  * Requires featureCompatibilityVersion set to 4.0.  * Requires digestPassword to be true.\n\n The default for featureCompatibilityVersion is 3.6 is SCRAM-SHA-1. New in version 4.0. digestPassword\nboolean Optional. Indicates whether the server or the client digests the password. If true, the server receives undigested password from the client and digests the password. If false, the client digests the password and passes the digested password to the server. Not compatible with SCRAM-SHA-256 Changed in version 4.0: The default value is true. In earlier versions, the default value is false. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where createUser runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field: Field Name\nValue\nDescription\nclientSource\nArray of IP addresses and/or CIDR ranges\nIf present, when authenticating a user, the server verifies that the client's IP address is either in the given list or belongs to a CIDR range in the list. If the client's IP address is not present, the server does not authenticate the user.\nserverAddress\nArray of IP addresses and/or CIDR ranges\nA list of IP addresses or CIDR ranges to which the client can connect. If present, the server will verify that the client's connection was accepted via an IP address in the given list. If the connection was accepted via an unrecognized IP address, the server does not authenticate the user. \nIMPORTANT If a user inherits multiple roles with incompatible authentication restrictions, that user becomes unusable. For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. \nBEHAVIOR  USER ID \nStarting in version 4.0.9, MongoDB automatically assigns a unique userId to the user upon creation. \nENCRYPTION  WARNING By default, createUser sends all specified data to the MongoDB instance in cleartext, even if using passwordPrompt(). Use TLS transport encryption to protect communications between clients and the server, including the password sent by createUser. For instructions on enabling TLS transport encryption, see Configure mongod and mongos for TLS/SSL. MongoDB does not store the password in cleartext. The password is only vulnerable in transit between the client and the server, and only if TLS transport encryption is not enabled. \nEXTERNAL CREDENTIALS \nUsers created on the $external database should have credentials stored externally to MongoDB, as, for example, with MongoDB Enterprise installations that use Kerberos. To use Client Sessions and Causal Consistency Guarantees with $external authentication users (Kerberos, LDAP, or x.509 users), usernames cannot be greater than 10k bytes. \nLOCAL DATABASE \nYou cannot create users on the local database. \nUSERNAME LIMITS \nUsernames must consist of at least one character and cannot be larger than 7MB. \nREQUIRED ACCESS \n * To create a new user in a database, you must have the createUser action on that database resource.  * To grant roles to a user, you must have the grantRole action on the role's database. The userAdmin and userAdminAnyDatabase built-in roles provide createUser and grantRole actions on their respective resources. \nEXAMPLE\n\n \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell.  db.getSiblingDB(\"products\").runCommand( {       createUser: \"accountAdmin01\",       pwd: passwordPrompt(),       customData: { employeeId: 12345 },       roles: [                { role: \"clusterAdmin\", db: \"admin\" },                { role: \"readAnyDatabase\", db: \"admin\" },                \"readWrite\"              ],       writeConcern: { w: \"majority\" , wtimeout: 5000 }} ) \n←  User Management CommandsdropAllUsersFromDatabase → On this page  * Definition\n * Syntax\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-sharding/": " Docs Home → MongoDB Manual \nSHARDING COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\nabortReshardCollection Aborts a resharding operation. New in version 5.0. addShard\nAdds a shard to a sharded cluster.\naddShardToZone\nAssociates a shard with a zone. Supports configuring zones in sharded clusters.\nbalancerCollectionStatus Returns information on whether the chunks of a sharded collection are balanced. New in version 4.4. balancerStart\nStarts a balancer thread.\nbalancerStatus\nReturns information on the balancer status.\nbalancerStop\nStops the balancer thread.\ncheckShardingIndex\nInternal command that validates index on shard key.\nclearJumboFlag\nClears the jumbo flag for a chunk.\ncleanupOrphaned\nRemoves orphaned data with shard key values outside of the ranges of the chunks owned by a shard.\ncleanupReshardCollection Cleans up a failed resharding operation. New in version 5.0. commitReshardCollection Forces a resharding operation to block writes and complete. New in version 5.0. configureCollectionBalancing Configures balancer settings on a sharded collection. New in version 5.3. enableSharding\nCreates a database.\nflushRouterConfig\nForces a mongod / mongos instance to update its cached routing metadata.\ngetShardMap\nInternal command that reports on the state of a sharded cluster.\ngetShardVersion\nInternal command that returns the config server version.\nisdbgrid\nVerifies that a process is a mongos.\nlistShards\nReturns a list of configured shards.\nmedianKey\nDeprecated internal command. See splitVector.\nmoveChunk\nInternal command that migrates chunks between shards.\nmovePrimary\nReassigns the primary shard when removing a shard from a sharded cluster.\nmoveRange\nMigrates ranges between shards.\nmergeChunks\nProvides the ability to combine chunks on a single shard.\nrefineCollectionShardKey Refines a collection's shard key by adding a suffix to the existing key. New in version 4.4. removeShard\nStarts the process of removing a shard from a sharded cluster.\nremoveShardFromZone\nRemoves the association between a shard and a zone. Supports configuring zones in sharded clusters.\nreshardCollection Initiates a resharding operation to change the shard key for a collection, changing the distribution of your data. New in version 5.0. setAllowMigrations\nPrevents the start of new automatic migrations on a collection, prevents in-flight manual migrations from committing, and excludes the collection from new balancer rounds.\nsetShardVersion\nInternal command to sets the config server version.\nshardCollection\nEnables the sharding functionality for a collection, allowing the collection to be sharded.\nshardingState\nReports whether the mongod is a member of a sharded cluster.\nsplit\nCreates a new chunk.\nsplitVector\nInternal command that determines split points.\nunsetSharding\nRemoved in MongoDB 5.0. Internal command that affects connections between instances in a MongoDB deployment.\nupdateZoneKeyRange\nAdds or removes the association between a range of sharded data and a zone. Supports configuring zones in sharded clusters. ←  replSetSyncFromabortReshardCollection → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetReconfig/": " Docs Home → MongoDB Manual \nREPLSETRECONFIG \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Additional Information \nDEFINITION \nreplSetReconfig \nThe replSetReconfig administrative command modifies the configuration of an existing replica set. You can use this command to add and remove members, and to alter the options set on existing members. You must run this command on the admin database of the primary replica set member. \nTIP In mongosh, this command can also be run through the rs.reconfig() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     replSetReconfig: <new_config_document>,     force: <boolean>,     maxTimeMS: <int>    } )  \nCOMMAND FIELDS \nThe command takes the following optional field: Field\nDescription force Defaults to false. Specify true to force the available replica set members to accept the new configuration. Force reconfiguration can result in unexpected or undesired behavior, including rollback of \"majority\" committed writes. maxTimeMS Optional. Specifies a cumulative time limit in milliseconds for processing the replSetReconfig. By default, replSetReconfig waits indefinitely for the replica configuration to propagate to a majority of replica set members. Setting maxTimeMS may result in the operation failing before it can apply the new configuration. See Reconfiguration Waits Until a Majority of Members Install the Replica Configuration for more information. New in version 4.4. You may also run replSetReconfig with the shell's rs.reconfig() method. \nBEHAVIOR  GLOBAL WRITE CONCERN \nStarting in MongoDB 5.0, you must explicitly set the global default write concern before attempting to reconfigure a replica set with a configuration that would change the implicit default write concern. To set the global default write concern, use the setDefaultRWConcern command. \nTERM REPLICA CONFIGURATION FIELD \nMongoDB 4.4 adds the term field to the replica set configuration document. The term field is set by the primary replica set member. The primary ignores the term field if set explicitly in the replSetReconfig operation. \nRECONFIGURATION CAN ADD OR REMOVE NO MORE THAN ONE VOTING MEMBER AT A TIME \nStarting in MongoDB 4.4, replSetReconfig by default allows adding or removing no more than 1 voting member at a time. For example, a new configuration can make at most one of the following changes to the cluster membership:      * Adding a new voting replica set member.  * Removing an existing voting replica set member.  * Modifying the votes for an existing replica set member. To add or remove multiple voting members, issue a series of replSetReconfig operations to add or remove one member at a time. Issuing a force reconfiguration immediately installs the new configuration even if it adds or removes multiple voting members. Force reconfiguration can cause unexpected behavior, such as the rollback of \"majority\" committed write operations. \nRECONFIGURATION WAITS UNTIL A MAJORITY OF MEMBERS INSTALL THE REPLICA CONFIGURATION \nStarting in MongoDB 4.4, replSetReconfig waits until a majority of voting replica set members install the new replica configuration before returning success. A voting member is any replica set member where members[n].votes is 1, including arbiters. Replica set members propagate their replica configuration via heartbeats. Whenever a member learns of a configuration with a higher version and term, it installs the new configuration. The reconfiguration process has two distinct 'waiting' phases: 1) Wait for the current configuration to be committed before installing the new configuration. The \"current\" configuration refers to the replica configuration in use by the primary at the time replSetReconfig is issued. A configuration is committed when:  * A majority of voting replica set members have installed the current configuration, and  * All writes which were \"majority\" committed in the previous configuration have also replicated to a majority in the current configuration. Typically, the current configuration has already been installed on a majority of voting replica set members. However, the majority committed writes in the previous configuration may not all be committed in the current configuration. Delayed members or members that are lagging behind the primary can increase the time spent in this phase.\n\n 2) Wait for a majority of voting members in the new configuration to install the new configuration. The \"new\" configuration refers to the replica configuration specified to replSetReconfig. The primary installs and begins using the new replica configuration before propagating the configuration to the remaining replica set members. The operation only waits for a majority of voting members to install the new configuration, and does not require waiting for the new configuration to be committed. If the operation was issued with a maxTimeMS limit and the operation exceeds the limit while waiting, the operation returns an error but continues using and propagating the new configuration. Issuing a force reconfiguration immediately installs the new configuration regardless of the commitment status of the previous configuration. Force reconfiguration can cause unexpected behavior, such as the rollback of \"majority\" committed write operations. To check the commitment status of the current replica configuration, issue replSetGetConfig with the commitmentStatus parameter on the replica set primary. \nAUTOMATIC RECONFIGURATION FOR NEW VOTING REPLICA SET MEMBERS \nStarting in MongoDB 5.0, a newly added secondary does not count as a voting member and cannot be elected until it has reached the SECONDARY state. When a new voting node is added to a replica set, replSetReconfig will internally add a newlyAdded field to the node's configuration. Nodes with the newlyAdded field do not count towards the current number of voting nodes. When initial sync completes and the node reaches SECONDARY state, the newlyAdded field is automatically removed. \nNOTE  * Configurations that attempt to add a field named newlyAdded will error even if run with { force: true }.  * If an existing node has a newlyAdded field, using rs.reconfig() to change the configuration will not remove the newlyAdded field. The newlyAdded field will be appended to the user provided configuration.  * replSetGetConfig will remove any newlyAdded fields from its output. If you would like to see any newlyAdded fields, you can query the local.system.replset collection directly. \nACCESS CONTROL \nTo run the command on deployments that enforce access control, the user must have replSetConfigure privilege action on the cluster resource. The clusterManager built-in role, available in the admin database, provides the required privileges for this command. \nLOCKING BEHAVIOR \nreplSetReconfig obtains a special mutually exclusive lock to prevent more than one replSetReconfig operation from occurring at the same time. \nMIXED VERSION REPLICA SET  WARNING Avoid reconfiguring replica sets that contain members of different MongoDB versions as validation rules may differ across MongoDB versions. \nAVAILABILITY \nA majority of the set's members must be operational for the changes to propagate properly. replSetReconfig can trigger the current primary to step down in some situations. Primary step-down triggers an election to select a new primary:  * Starting in MongoDB 4.4, when the new primary steps up, it increments the term field to distinguish configuration changes made on the new primary from changes made on the previous primary.  * Starting in MongoDB 4.2, when the primary steps down, it no longer closes all client connections; however, writes that were in progress are killed. For details, see Behavior.  * In MongoDB 4.0 and earlier, when the primary steps down, it closes all client connections. The median time before a cluster elects a new primary should not typically exceed 12 seconds, assuming default replica configuration settings. This includes time required to mark the primary as unavailable and call and complete an election. You can tune this time period by modifying the settings.electionTimeoutMillis replication configuration option. Factors such as network latency may extend the time required for replica set elections to complete, which in turn affects the amount of time your cluster may operate without a primary. These factors are dependent on your particular cluster architecture. During the election process, the cluster cannot accept write operations until it elects the new primary. Your application connection logic should include tolerance for automatic failovers and the subsequent elections. MongoDB drivers can detect the loss of the primary and automatically retry certain write operations a single time, providing additional built-in handling of automatic failovers and elections: Compatible drivers enable retryable writes by default To further reduce potential impact to a production cluster, reconfigure only during scheduled maintenance periods. \n{ FORCE: TRUE }  WARNING Forcing the replSetReconfig command can lead to a rollback situation. Use with caution. \nDROP OUTGOING CONNECTIONS AFTER REMOVING A MEMBER \nUsing replSetReconfig to remove a replica set member does not automatically drop open outgoing connections from other replica set members to the removed member.\n\n New in version 4.2: To immediately drop all outgoing connections from the replica set to the removed member, run the dropConnections administrative command on each remaining member on the replica set: db.adminCommand(  {    \"dropConnections\" : 1,    \"hostAndPort\" : [      \"<hostname>:<port>\"    ]  })  Replace <hostname> and <port> with those of the removed member. \nWARNING Starting in MongDB 5.0, split horizon DNS nodes that are only configured with an IP address fail startup validation and report an error. See disableSplitHorizonIPCheck. \nMEMBER PRIORITY AND VOTES \n * Members with priority greater than 0 cannot have 0 votes.  * Non-voting (i.e. votes is 0) members must have priority of 0. \nADDITIONAL INFORMATION \nReplica Set Configuration Fields, Replica Set Configuration, rs.reconfig(), and rs.conf(). ←  replSetMaintenancereplSetResizeOplog → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/balancerStop/": " Docs Home → MongoDB Manual \nBALANCERSTOP \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nbalancerStop \nDisables the balancer in a sharded cluster. If a balancing round is in progress, the operation waits for balancing to complete. You can only issue the balancerStop against the admin database on a mongos instance. \nTIP In mongosh, this command can also be run through the sh.stopBalancer() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. Starting in MongoDB 6.0.3, automatic chunk splitting is not performed. This is because of balancing policy improvements. Auto-splitting commands still exist, but do not perform an operation. For details, see Balancing Policy Changes. In MongoDB versions earlier than 6.0, the command disables auto-splitting for the sharded cluster. To enable auto-splitting when the balancer is disabled, you can use sh.enableAutoSplit(). \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     balancerStop: 1,     maxTimeMS: <number>   })  \nCOMMAND FIELDS \nField\nType\nDescription\nbalancerStop\nany\nAny value.\nmaxTimeMS\ninteger Time limit for disabling the balancer. Defaults to 60000 milliseconds. \nEXAMPLE \nTo stop the balancer thread, connect to a mongos instance and issue the following command: db.adminCommand( { balancerStop: 1 } )  \nTIP \nSEE ALSO:      * sh.stopBalancer()  * balancerStart ←  balancerStatuscheckShardingIndex → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-diagnostic/": " Docs Home → MongoDB Manual \nDIAGNOSTIC COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\nbuildInfo\nDisplays statistics about the MongoDB build.\ncollStats\nReports storage utilization statics for a specified collection.\nconnPoolStats\nReports statistics on the outgoing connections from this MongoDB instance to other MongoDB instances in the deployment.\nconnectionStatus\nReports the authentication state for the current connection.\ndataSize\nReturns the data size for a range of data. For internal use.\ndbHash\nReturns hash value a database and its collections.\ndbStats\nReports storage utilization statistics for the specified database.\ndriverOIDTest\nInternal command that converts an ObjectId to a string to support tests.\nexplain\nReturns information on the execution of various operations.\nfeatures\nReports on features available in the current MongoDB instance.\ngetCmdLineOpts\nReturns a document with the run-time arguments to the MongoDB instance and their parsed options.\ngetLog\nReturns recent log messages.\nhostInfo\nReturns data that reflects the underlying host system.\n_isSelf\nInternal command to support testing.\nlistCommands\nLists all database commands provided by the current mongod instance.\nlockInfo\nInternal command that returns information on locks that are currently being held or pending. Only available for mongod instances.\nnetstat\nInternal command that reports on intra-deployment connectivity. Only available for mongos instances.\nping\nInternal command that tests intra-deployment connectivity.\nprofile\nInterface for the database profiler.\nserverStatus\nReturns a collection metrics on instance-wide resource utilization and status.\nshardConnPoolStats Deprecated in 4.4 Use :dbcommand:`connPoolStats` instead. Reports statistics on a mongos's connection pool for client operations against shards. top\nReturns raw usage statistics for each database in the mongod instance.\nvalidate\nInternal command that scans for a collection's data and indexes for correctness.\nvalidateDBMetadata\nChecks that the stored metadata of a database/collection is valid within a particular API version.\nwhatsmyuri\nInternal command that returns information on the current client. ←  shutdownbuildInfo → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/driverOIDTest/": " Docs Home → MongoDB Manual \nDRIVEROIDTEST \ndriverOIDTest \ndriverOIDTest is an internal command. ←  dbStatsexplain → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/listCollections/": " Docs Home → MongoDB Manual \nLISTCOLLECTIONS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Output\n * Example \nDEFINITION \nlistCollections \nRetrieve information, i.e. the name and options, about the collections and views in a database. Specifically, the command returns a document that contains information with which to create a cursor to the collection information. mongosh provides the db.getCollectionInfos() and the db.getCollectionNames() helper methods. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     listCollections: 1,     filter: <document>,     nameOnly: <boolean>,     authorizedCollections: <boolean>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command can take the following optional fields: Field\nType\nDescription\nfilter\ndocument Optional. A query expression to filter the list of collections. You can specify a query expression on any of the fields returned by listCollections. nameOnly\nboolean Optional. A flag to indicate whether the command should return just the collection/view names and type or return both the name and other information. Returning just the name and type (view or collection) does not take collection-level locks whereas returning full collection information locks each collection in the database. The default value is false. \nNOTE When nameOnly is true, your filter expression can only filter based on a collection's name and type. No other fields are available. authorizedCollections\nboolean Optional. A flag, when set to true and used with nameOnly: true, that allows a user without the required privilege (i.e. listCollections action on the database) to run the command when access control is enforced. When both authorizedCollections and nameOnly options are set to true, the command returns only those collections for which the user has privileges. For example, if a user has find action on specific collections, the command returns only those collections; or, if a user has find or any other action, on the database resource, the command lists all collections in the database. The default value is false. That is, the user must have listCollections action on the database to run the command. For a user who has listCollections action on the database, this option has no effect since the user has privileges to list the collections in the database. When used without nameOnly: true, this option has no effect. That is, the user must have the required privileges to run the command when access control is enforced. Otherwise, the user is unauthorized to run the command. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR  FILTER \nUse a filter to limit the results of listCollections. You can specify a filter on any of the fields returned in the listCollections result set. \nLOCKS \nThe listCollections command takes Intent Shared lock on the database. In previous versions, the command takes Shared lock on the database. Unless the nameOnly option is specified, the command also takes an Intent Shared lock on each of the collections in turn while holding the Intent Shared lock on the database. \nCLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued listCollections disconnects before the operation completes, MongoDB marks listCollections for termination using killOp. \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, listCollections operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nREQUIRED ACCESS \nTo run listCollections when access control is enforced, users must, in general, have privileges that grant listCollections action on the database. For example, the following privilege grants users to run db.getCollectionInfos() against the test database:\n\n  The built-in role read provides the privilege to run listCollections for a specific database. Users without the required privilege can run the command with both authorizedCollections and nameOnly options set to true. In this case, the command returns just the name and type of the collection(s) to which the user has privileges. For example, consider a user with a role that grants just the following privilege: { resource: { db: \"test\", collection: \"foo\" }, actions: [ \"find\" ] }  The user can run the command if the command includes both authorizedCollections and nameOnly options set to true (with or without the filter option): db.runCommand( { listCollections: 1.0, authorizedCollections: true, nameOnly: true } )  The operation returns the name and type of the foo collection. However, the following operations (with or without the filter option) error for the user without the required access: db.runCommand( { listCollections: 1.0, authorizedCollections: true } )db.runCommand( { listCollections: 1.0, nameOnly: true } )  \nSHOW COLLECTIONS \nThe mongo shell, show collections is equivalent to: db.runCommand( { listCollections: 1.0, authorizedCollections: true, nameOnly: true } )   * For users with the required access, show collections lists the non-system collections for the database.  * For users without the required access, show collections lists only the collections for which the users has privileges. \nOUTPUT \nlistCollections.cursor \nA document that contains information with which to create a cursor to documents that contain collection names and options. The cursor information includes the cursor id, the full namespace for the command, as well as the first batch of results. Each document in the batch output contains the following fields: Field\nType\nDescription\nname\nString\nName of the collection.\ntype\nString\nType of data store. Returns collection for collections, view for views, and timeseries for time series collection.\noptions\nDocument Collection options. These options correspond directly to the options available in db.createCollection(). For the descriptions on the options, see db.createCollection(). info\nDocument Lists the following fields related to the collection: readOnlyboolean. If true the data store is read only.uuidUUID. Once established, the collection UUID does not change. The collection UUID remains the same across replica set members and shards in a sharded cluster.\nidIndex\nDocument\nProvides information on the _id index for the collection. listCollections.ok \nThe return value for the command. A value of 1 indicates success. \nEXAMPLE  LIST ALL COLLECTIONS \nThe following example uses the db.getCollectionInfos() helper to return information for all collections in the records database: use recordsdb.getCollectionInfos();  \nTIP \nSEE ALSO: db.getCollectionInfos() ←  killOplistDatabases → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Output\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/program/mongod/": " Docs Home → MongoDB Manual \nMONGOD \nOn this page    \n * Synopsis\n   \n * Options\n * Core Options\n * Free Monitoring\n * LDAP Authentication or Authorization Options\n * Storage Options\n * WiredTiger Options\n * Replication Options\n * Sharded Cluster Options\n * TLS Options\n * SSL Options (Deprecated)\n * Profiler Options\n * Audit Options\n * SNMP Options\n * inMemory Options\n * Encryption Key Management Options \nSYNOPSIS \nmongod is the primary daemon process for the MongoDB system. It handles data requests, manages data access, and performs background management operations. This document provides a complete overview of all command line options for mongod. These command line options are primarily useful for testing: In common operation, use the configuration file options to control the behavior of your database. \nTIP \nSEE ALSO: Configuration File Settings and Command-Line Options Mapping \nNOTE MongoDB disables support for TLS 1.0 encryption on systems where TLS 1.1+ is available. For more details, see Disable TLS 1.0. \nOPTIONS \nChanged in version 5.2:      * MongoDB removes the --cpu command-line option.  Changed in version 5.0:  * MongoDB removes the --serviceExecutor command-line option and the corresponding net.serviceExecutor configuration option.  Changed in version 4.4:  * MongoDB removes the --noIndexBuildRetry command-line option and the corresponding storage.indexBuildRetry option.  Changed in version 4.2:  * MongoDB deprecates the SSL options and instead adds new corresponding TLS options.  * MongoDB adds --tlsClusterCAFile/net.tls.clusterCAFile.  \nCORE OPTIONS \n--help, -h \nReturns information on the options and use of mongod. --version \nReturns the mongod release number. --config <filename>, -f <filename> \nSpecifies a configuration file for runtime configuration options. The configuration file is the preferred method for runtime configuration of mongod. The options are equivalent to the command-line configuration options. See Configuration File Options for more information. Ensure the configuration file uses ASCII encoding. The mongod instance does not support configuration files with non-ASCII encoding, including UTF-8. --configExpand <none|rest|exec> \nDefault: none New in version 4.2. Enables using Expansion Directives in configuration files. Expansion directives allow you to set externally sourced values for configuration file options. --configExpand supports the following expansion directives: Value\nDescription\nnone\nDefault. mongod does not expand expansion directives. mongod fails to start if any configuration file settings use expansion directives.\nrest\nmongod expands __rest expansion directives when parsing the configuration file.\nexec\nmongod expands __exec expansion directives when parsing the configuration file. You can specify multiple expansion directives as a comma-separated list, e.g. rest, exec. If the configuration file contains expansion directives not specified to --configExpand, the mongod returns an error and terminates. See Externally Sourced Configuration File Values for configuration files for more information on expansion directives. --verbose, -v \nIncreases the amount of internal reporting returned on standard output or in log files. Increase the verbosity with the -v form by including the option multiple times, (e.g. -vvvvv.) \nNOTE Starting in version 4.2, MongoDB includes the Debug verbosity level (1-5) in the log messages. For example, if the verbosity level is 2, MongoDB logs D2. In previous versions, MongoDB log messages only specified D for Debug level. --quiet \nRuns mongod in a quiet mode that attempts to limit the amount of output. This option suppresses:  * output from database commands  * replication activity  * connection accepted events  * connection closed events --port <port> \nDefault:  * 27017 if mongod is not a shard member or a config server member  * 27018 if mongod is a shard member  * 27019 if mongod is a config server member The TCP port on which the MongoDB instance listens for client connections. --bind_ip <hostnames|ipaddresses|Unix domain socket paths> \nDefault: localhost The hostnames and/or IP addresses and/or full Unix domain socket paths on which mongod should listen for client connections. You may attach mongod to any interface. To bind to multiple addresses, enter a list of comma-separated values. \nEXAMPLE \nLOCALHOST,/TMP/MONGOD.SOCK \n\n \nEXAMPLE \nLOCALHOST, 2001:0DB8:E132:BA26:0D5C:2774:E7F9:D513  \nNOTE If specifying an IPv6 address or a hostname that resolves to an IPv6 address to --bind_ip, you must start mongod with --ipv6 to enable IPv6 support. Specifying an IPv6 address to --bind_ip does not enable IPv6 support. If specifying a link-local IPv6 address (fe80::/10), you must append the zone index to that address (i.e. fe80::<address>%<adapter-name>). \nEXAMPLE \nLOCALHOST,FE80::A00:27FF:FEE0:1FCF%ENP0S3  \nIMPORTANT To avoid configuration updates due to IP address changes, use DNS hostnames instead of IP addresses. It is particularly important to use a DNS hostname instead of an IP address when configuring replica set members or sharded cluster members. Use hostnames instead of IP addresses to configure clusters across a split network horizon. Starting in MongoDB 5.0, nodes that are only configured with an IP address will fail startup validation and will not start. \nWARNING Before binding to a non-localhost (e.g. publicly accessible) IP address, ensure you have secured your cluster from unauthorized access. For a complete list of security recommendations, see Security Checklist. At minimum, consider enabling authentication and hardening network infrastructure. For more information about IP Binding, refer to the IP Binding documentation. To bind to all IPv4 addresses, enter 0.0.0.0. To bind to all IPv4 and IPv6 addresses, enter ::,0.0.0.0 or starting in MongoDB 4.2, an asterisk \"*\" (enclose the asterisk in quotes to avoid filename pattern expansion). Alternatively, use the net.bindIpAll setting. \nNOTE  * --bind_ip and --bind_ip_all are mutually exclusive. Specifying both options causes mongod to throw an error and terminate.  * The command-line option --bind overrides the configuration file setting net.bindIp. --bind_ip_all \nIf specified, the mongod instance binds to all IPv4 addresses (i.e. 0.0.0.0). If mongod starts with --ipv6, --bind_ip_all also binds to all IPv6 addresses (i.e. ::). mongod only supports IPv6 if started with --ipv6. Specifying --bind_ip_all alone does not enable IPv6 support. \nWARNING Before binding to a non-localhost (e.g. publicly accessible) IP address, ensure you have secured your cluster from unauthorized access. For a complete list of security recommendations, see Security Checklist. At minimum, consider enabling authentication and hardening network infrastructure. For more information about IP Binding, refer to the IP Binding documentation. Alternatively, you can set the --bind_ip option to ::,0.0.0.0 or, starting in MongoDB 4.2, to an asterisk \"*\" (enclose the asterisk in quotes to avoid filename pattern expansion). \nNOTE --bind_ip and --bind_ip_all are mutually exclusive. That is, you can specify one or the other, but not both. --clusterIpSourceAllowlist <string> \nNew in version 5.0. A list of IP addresses/CIDR (Classless Inter-Domain Routing) ranges against which the mongod validates authentication requests from other members of the replica set and, if part of a sharded cluster, the mongos instances. The mongod verifies that the originating IP is either explicitly in the list or belongs to a CIDR range in the list. If the IP address is not present, the server does not authenticate the mongod or mongos. --clusterIpSourceAllowlist has no effect on a mongod started without authentication. --clusterIpSourceAllowlist accepts multiple comma-separated IPv4/6 addresses or Classless Inter-Domain Routing (CIDR) ranges: mongod --clusterIpSourceAllowlist 192.0.2.0/24,127.0.0.1,::1  \nIMPORTANT Ensure --clusterIpSourceAllowlist includes the IP address or CIDR ranges that include the IP address of each replica set member or mongos in the deployment to ensure healthy communication between cluster components. --clusterIpSourceWhitelist <string> \nDeprecated in version 5.0: Use --clusterIpSourceAllowlist instead.\n\n --clusterIpSourceWhitelist has no effect on a mongod started without authentication. --clusterIpSourceWhitelist accepts multiple comma-separated IPv4/6 addresses or Classless Inter-Domain Routing (CIDR) ranges: mongod --clusterIpSourceWhitelist 192.0.2.0/24,127.0.0.1,::1  \nIMPORTANT Ensure --clusterIpSourceWhitelist includes the IP address or CIDR ranges that include the IP address of each replica set member or mongos in the deployment to ensure healthy communication between cluster components. --ipv6 \nEnables IPv6 support. mongod disables IPv6 support by default. Setting --ipv6 does not direct the mongod to listen on any local IPv6 addresses or interfaces. To configure the mongod to listen on an IPv6 interface, you must either:  * Configure --bind_ip with one or more IPv6 addresses or hostnames that resolve to IPv6 addresses, or  * Set --bind_ip_all to true. --listenBacklog <number> \nDefault: Target system SOMAXCONN constant The maximum number of connections that can exist in the listen queue. \nWARNING Consult your local system's documentation to understand the limitations and configuration requirements before using this parameter. \nIMPORTANT To prevent undefined behavior, specify a value for this parameter between 1 and the local system SOMAXCONN constant. The default value for the listenBacklog parameter is set at compile time to the target system SOMAXCONN constant. SOMAXCONN is the maximum valid value that is documented for the backlog parameter to the listen system call. Some systems may interpret SOMAXCONN symbolically, and others numerically. The actual listen backlog applied in practice may differ from any numeric interpretation of the SOMAXCONN constant or argument to --listenBacklog, and may also be constrained by system settings like net.core.somaxconn on Linux. Passing a value for the listenBacklog parameter that exceeds the SOMAXCONN constant for the local system is, by the letter of the standards, undefined behavior. Higher values may be silently integer truncated, may be ignored, may cause unexpected resource consumption, or have other adverse consequences. On systems with workloads that exhibit connection spikes, for which it is empirically known that the local system can honor higher values for the backlog parameter than the SOMAXCONN constant, setting the listenBacklog parameter to a higher value may reduce operation latency as observed by the client by reducing the number of connections which are forced into a backoff state. --maxConns <number> \nThe maximum number of simultaneous connections that mongod will accept. This setting has no effect if it is higher than your operating system's configured maximum connection tracking threshold. Do not assign too low of a value to this option, or you will encounter errors during normal application operation. --logpath <path> \nSends all diagnostic logging information to a log file instead of to standard output or to the host's syslog system. MongoDB creates the log file at the path you specify. By default, MongoDB will move any existing log file rather than overwrite it. To instead append to the log file, set the --logappend option. --syslog \nSends all logging output to the host's syslog system rather than to standard output or to a log file (--logpath). The --syslog option is not supported on Windows. \nWARNING The syslog daemon generates timestamps when it logs a message, not when MongoDB issues the message. This can lead to misleading timestamps for log entries, especially when the system is under heavy load. We recommend using the --logpath option for production systems to ensure accurate timestamps. Starting in version 4.2, MongoDB includes the component in its log messages to syslog. ...  ACCESS   [repl writer worker 5] Unsupported modification to roles collection ... \n--syslogFacility <string> \nDefault: user Specifies the facility level used when logging messages to syslog. The value you specify must be supported by your operating system's implementation of syslog. To use this option, you must enable the --syslog option. --logappend \nAppends new entries to the end of the existing log file when the mongod instance restarts. Without this option, mongod will back up the existing log and create a new file. --logRotate <string> \nDefault: rename Determines the behavior for the logRotate command when rotating the server log and/or the audit log. Specify either rename or reopen:  * rename renames the log file.\n\n --timeStampFormat <string> \nDefault: iso8601-local The time format for timestamps in log messages. Specify one of the following values: Value\nDescription\niso8601-utc\nDisplays timestamps in Coordinated Universal Time (UTC) in the ISO-8601 format. For example, for New York at the start of the Epoch: 1970-01-01T00:00:00.000Z\niso8601-local\nDisplays timestamps in local time in the ISO-8601 format. For example, for New York at the start of the Epoch: 1969-12-31T19:00:00.000-05:00 \nNOTE Starting in MongoDB 4.4, --timeStampFormat no longer supports ctime. An example of ctime formatted date is: Wed Dec 31 18:17:54.811. --traceExceptions \nFor internal diagnostic use only. --pidfilepath <path> \nSpecifies a file location to store the process ID (PID) of the mongod process. The user running the mongod or mongos process must be able to write to this path. If the --pidfilepath option is not specified, the process does not create a PID file. This option is generally only useful in combination with the --fork option. \nNOTE \nLINUX On Linux, PID file management is generally the responsibility of your distro's init system: usually a service file in the /etc/init.d directory, or a systemd unit file registered with systemctl. Only use the --pidfilepath option if you are not using one of these init systems. For more information, please see the respective Installation Guide for your operating system. \nNOTE \nMACOS On macOS, PID file management is generally handled by brew. Only use the --pidfilepath option if you are not using brew on your macOS system. For more information, please see the respective Installation Guide for your operating system. --keyFile <file> \nSpecifies the path to a key file that stores the shared secret that MongoDB instances use to authenticate to each other in a sharded cluster or replica set. --keyFile implies --auth. See Internal/Membership Authentication for more information. Starting in MongoDB 4.2, keyfiles for internal membership authentication use YAML format to allow for multiple keys in a keyfile. The YAML format accepts content of:  * a single key string (same as in earlier versions),  * multiple key strings (each string must be enclosed in quotes), or  * sequence of key strings. The YAML format is compatible with the existing single-key keyfiles that use the text file format. --setParameter <options> \nSpecifies one of the MongoDB parameters described in MongoDB Server Parameters. You can specify multiple setParameter fields. --nounixsocket \nDisables listening on the UNIX domain socket. --nounixsocket applies only to Unix-based systems. The mongod process always listens on the UNIX socket unless one of the following is true:  * --nounixsocket is set  * net.bindIp is not set  * net.bindIp does not specify localhost or its associated IP address mongod installed from official .deb and .rpm packages have the bind_ip configuration set to 127.0.0.1 by default. --unixSocketPrefix <path> \nDefault: /tmp The path for the UNIX socket. --unixSocketPrefix applies only to Unix-based systems. If this option has no value, the mongod process creates a socket with /tmp as a prefix. MongoDB creates and listens on a UNIX socket unless one of the following is true:  * net.unixDomainSocket.enabled is false  * --nounixsocket is set  * net.bindIp is not set  * net.bindIp does not specify localhost or its associated IP address --filePermissions <path> \nDefault: 0700 Sets the permission for the UNIX domain socket file. --filePermissions applies only to Unix-based systems. --fork \nEnables a daemon mode that runs the mongod process in the background. By default mongod does not run as a daemon: typically you will run mongod as a daemon, either by using --fork or by using a controlling process that handles the daemonization process (e.g. as with upstart and systemd). Using the --fork option requires that you configure log output for the mongod with one of the following:  * --logpath  * --syslog The --fork option is not supported on Windows. --auth\n\n To configure users, use the mongosh client. If no users exist, the localhost interface will continue to have access to the database until you create the first user. See Security for more information. --noauth \nDisables authentication. Currently the default. Exists for future compatibility and clarity. --transitionToAuth \nAllows the mongod to accept and create authenticated and non-authenticated connections to and from other mongod and mongos instances in the deployment. Used for performing rolling transition of replica sets or sharded clusters from a no-auth configuration to internal authentication. Requires specifying a internal authentication mechanism such as --keyFile. For example, if using keyfiles for internal authentication, the mongod creates an authenticated connection with any mongod or mongos in the deployment using a matching keyfile. If the security mechanisms do not match, the mongod utilizes a non-authenticated connection instead. A mongod running with --transitionToAuth does not enforce user access controls. Users may connect to your deployment without any access control checks and perform read, write, and administrative operations. \nNOTE A mongod running with internal authentication and without --transitionToAuth requires clients to connect using user access controls. Update clients to connect to the mongod using the appropriate user prior to restarting mongod without --transitionToAuth. --sysinfo \nReturns diagnostic system information and then exits. The information provides the page size, the number of physical pages, and the number of available physical pages. --noscripting \nDisables the scripting engine. --notablescan \nForbids operations that require a collection scan. See notablescan for additional information. --shutdown \nThe --shutdown option cleanly and safely terminates the mongod process. When invoking mongod with this option you must set the --dbpath option either directly or by way of the configuration file and the --config option. The --shutdown option is available only on Linux systems. For additional ways to shut down, see also Stop mongod Processes. --redactClientLogData \nAvailable in MongoDB Enterprise only. A mongod running with --redactClientLogData redacts any message accompanying a given log event before logging. This prevents the mongod from writing potentially sensitive data stored on the database to the diagnostic log. Metadata such as error or operation codes, line numbers, and source file names are still visible in the logs. Use --redactClientLogData in conjunction with Encryption at Rest and TLS/SSL (Transport Encryption) to assist compliance with regulatory requirements. For example, a MongoDB deployment might store Personally Identifiable Information (PII) in one or more collections. The mongod logs events such as those related to CRUD operations, sharding metadata, etc. It is possible that the mongod may expose PII as a part of these logging operations. A mongod running with --redactClientLogData removes any message accompanying these events before being output to the log, effectively removing the PII. Diagnostics on a mongod running with --redactClientLogData may be more difficult due to the lack of data related to a log event. See the process logging manual page for an example of the effect of --redactClientLogData on log output. On a running mongod, use setParameter with the redactClientLogData parameter to configure this setting. --networkMessageCompressors <string> \nDefault: snappy,zstd,zlib Specifies the default compressor(s) to use for communication between this mongod instance and:  * other members of the deployment if the instance is part of a replica set or a sharded cluster  * mongosh  * drivers that support the OP_COMPRESSED message format. MongoDB supports the following compressors:  * snappy  * zlib  * zstd New in version 4.2: Both mongod and mongos instances default to snappy,zstd,zlib compressors, in that order. To disable network compression, set the value to disabled. \nIMPORTANT Messages are compressed when both parties enable network compression. Otherwise, messages between the parties are uncompressed. If you specify multiple compressors, then the order in which you list the compressors matter as well as the communication initiator. For example, if mongosh specifies the following network compressors zlib,snappy and the mongod specifies snappy,zlib, messages between mongosh and mongod uses zlib.\n\n --timeZoneInfo <path> \nThe full path from which to load the time zone database. If this option is not provided, then MongoDB will use its built-in time zone database. The configuration file included with Linux and macOS packages sets the time zone database path to /usr/share/zoneinfo by default. The built-in time zone database is a copy of the Olson/IANA time zone database. It is updated along with MongoDB releases, but the time zone database release cycle differs from the MongoDB release cycle. The most recent release of the time zone database is available on our download site. wget https://downloads.mongodb.org/olson_tz_db/timezonedb-latest.zipunzip timezonedb-latest.zipmongod --timeZoneInfo timezonedb-2017b/  \nWARNING MongoDB uses the third party timelib library to provide accurate conversions between timezones. Due to a recent update, timelib could create inaccurate time zone conversions in older versions of MongoDB. To explicitly link to the time zone database in versions of MongoDB prior to 5.0, 4.4.7, and 4.2.14, download the time zone database. and use the timeZoneInfo parameter. \nTIP \nSEE ALSO: processManagement.timeZoneInfo. --outputConfig \nNew in version 4.2. Outputs the mongod instance's configuration options, formatted in YAML, to stdout and exits the mongod instance. For configuration options that uses Externally Sourced Configuration File Values, --outputConfig returns the resolved value for those options. \nWARNING This may include any configured passwords or secrets previously obfuscated through the external source. For usage examples, see:  * Output the Configuration File with Resolved Expansion Directive Values  * Convert Command-Line Options to YAML \nFREE MONITORING \n--enableFreeMonitoring <runtime|on|off> \nEnables or disables free MongoDB Cloud monitoring. --enableFreeMonitoring accepts the following values: Value\nDescription\nruntime Default. You can enable or disable free monitoring during runtime. To enable or disable free monitoring during runtime, see db.enableFreeMonitoring() and db.disableFreeMonitoring(). To enable or disable free monitoring during runtime when running with access control, users must have required privileges. See db.enableFreeMonitoring() and db.disableFreeMonitoring() for details. on\nEnables free monitoring at startup; i.e. registers for free monitoring. When enabled at startup, you cannot disable free monitoring during runtime.\noff\nDisables free monitoring at startup, regardless of whether you have previously registered for free monitoring. When disabled at startup, you cannot enable free monitoring during runtime. Once enabled, the free monitoring state remains enabled until explicitly disabled. That is, you do not need to re-enable each time you start the server. For the corresponding configuration file setting, see cloud.monitoring.free.state. --freeMonitoringTag <string> \nOptional tag to describe environment context. The tag can be sent as part of the free MongoDB Cloud monitoring registration at start up. For the corresponding configuration file setting, see cloud.monitoring.free.tags. \nLDAP AUTHENTICATION OR AUTHORIZATION OPTIONS \n--ldapServers <host1>:<port>,<host2>:<port>,...,<hostN>:<port> \nAvailable in MongoDB Enterprise only. The LDAP server against which the mongod authenticates users or determines what actions a user is authorized to perform on a given database. If the LDAP server specified has any replicated instances, you may specify the host and port of each replicated server in a comma-delimited list. If your LDAP infrastructure partitions the LDAP directory over multiple LDAP servers, specify one LDAP server or any of its replicated instances to --ldapServers. MongoDB supports following LDAP referrals as defined in RFC 4511 4.1.10. Do not use --ldapServers for listing every LDAP server in your infrastructure. This setting can be configured on a running mongod using setParameter. If unset, mongod cannot use LDAP authentication or authorization. --ldapValidateLDAPServerConfig <boolean> \nAvailable in MongoDB Enterprise A flag that determines if the mongod instance checks the availability of the LDAP server(s) as part of its startup:  * If true, the mongod instance performs the availability check and only continues to start up if the LDAP server is available.  * If false, the mongod instance skips the availability check; i.e. the instance starts up even if the LDAP server is unavailable. --ldapQueryUser <string>\n\n The identity with which mongod binds as, when connecting to or performing queries on an LDAP server. Only required if any of the following are true:  * Using LDAP authorization.  * Using an LDAP query for username transformation.  * The LDAP server disallows anonymous binds You must use --ldapQueryUser with --ldapQueryPassword. If unset, mongod will not attempt to bind to the LDAP server. This setting can be configured on a running mongod using setParameter. \nNOTE Windows MongoDB deployments can use --ldapBindWithOSDefaults instead of --ldapQueryUser and --ldapQueryPassword. You cannot specify both --ldapQueryUser and --ldapBindWithOSDefaults at the same time. --ldapQueryPassword <string | array> \nAvailable in MongoDB Enterprise only. The password used to bind to an LDAP server when using --ldapQueryUser. You must use --ldapQueryPassword with --ldapQueryUser. If not set, mongod does not attempt to bind to the LDAP server. You can configure this setting on a running mongod using setParameter. Starting in MongoDB 4.4, the ldapQueryPassword setParameter command accepts either a string or an array of strings. If ldapQueryPassword is set to an array, MongoDB tries each password in order until one succeeds. Use a password array to roll over the LDAP account password without downtime. \nNOTE Windows MongoDB deployments can use --ldapBindWithOSDefaults instead of --ldapQueryUser and --ldapQueryPassword. You cannot specify both --ldapQueryPassword and --ldapBindWithOSDefaults at the same time. --ldapBindWithOSDefaults <bool> \nDefault: false Available in MongoDB Enterprise for the Windows platform only. Allows mongod to authenticate, or bind, using your Windows login credentials when connecting to the LDAP server. Only required if:  * Using LDAP authorization.  * Using an LDAP query for username transformation.  * The LDAP server disallows anonymous binds Use --ldapBindWithOSDefaults to replace --ldapQueryUser and --ldapQueryPassword. --ldapBindMethod <string> \nDefault: simple Available in MongoDB Enterprise only. The method mongod uses to authenticate to an LDAP server. Use with --ldapQueryUser and --ldapQueryPassword to connect to the LDAP server. --ldapBindMethod supports the following values:  * simple - mongod uses simple authentication.  * sasl - mongod uses SASL protocol for authentication If you specify sasl, you can configure the available SASL mechanisms using --ldapBindSaslMechanisms. mongod defaults to using DIGEST-MD5 mechanism. --ldapBindSaslMechanisms <string> \nDefault: DIGEST-MD5 Available in MongoDB Enterprise only. A comma-separated list of SASL mechanisms mongod can use when authenticating to the LDAP server. The mongod and the LDAP server must agree on at least one mechanism. The mongod dynamically loads any SASL mechanism libraries installed on the host machine at runtime. Install and configure the appropriate libraries for the selected SASL mechanism(s) on both the mongod host and the remote LDAP server host. Your operating system may include certain SASL libraries by default. Defer to the documentation associated with each SASL mechanism for guidance on installation and configuration. If using the GSSAPI SASL mechanism for use with Kerberos Authentication, verify the following for the mongod host machine: Linux  * The KRB5_CLIENT_KTNAME environment variable resolves to the name of the client Linux Keytab Files for the host machine. For more on Kerberos environment variables, please defer to the Kerberos documentation.  * The client keytab includes a User Principal for the mongod to use when connecting to the LDAP server and execute LDAP queries. WindowsIf connecting to an Active Directory server, the Windows Kerberos configuration automatically generates a Ticket-Granting-Ticket when the user logs onto the system. Set --ldapBindWithOSDefaults to true to allow mongod to use the generated credentials when connecting to the Active Directory server and execute queries. Set --ldapBindMethod to sasl to use this option. \nNOTE For a complete list of SASL mechanisms see the IANA listing. Defer to the documentation for your LDAP or Active Directory service for identifying the SASL mechanisms compatible with the service.\n\n For more information on SASL, defer to the following resources:  * For Linux, please see the Cyrus SASL documentation.  * For Windows, please see the Windows SASL documentation. --ldapTransportSecurity <string> \nDefault: tls Available in MongoDB Enterprise only. By default, mongod creates a TLS/SSL secured connection to the LDAP server. For Linux deployments, you must configure the appropriate TLS Options in /etc/openldap/ldap.conf file. Your operating system's package manager creates this file as part of the MongoDB Enterprise installation, via the libldap dependency. See the documentation for TLS Options in the ldap.conf OpenLDAP documentation for more complete instructions. For Windows deployment, you must add the LDAP server CA certificates to the Windows certificate management tool. The exact name and functionality of the tool may vary depending on operating system version. Please see the documentation for your version of Windows for more information on certificate management. Set --ldapTransportSecurity to none to disable TLS/SSL between mongod and the LDAP server. \nWARNING Setting --ldapTransportSecurity to none transmits plaintext information and possibly credentials between mongod and the LDAP server. --ldapTimeoutMS <long> \nDefault: 10000 Available in MongoDB Enterprise only. The amount of time in milliseconds mongod should wait for an LDAP server to respond to a request. Increasing the value of --ldapTimeoutMS may prevent connection failure between the MongoDB server and the LDAP server, if the source of the failure is a connection timeout. Decreasing the value of --ldapTimeoutMS reduces the time MongoDB waits for a response from the LDAP server. This setting can be configured on a running mongod using setParameter. --ldapUserToDNMapping <string> \nAvailable in MongoDB Enterprise only. Maps the username provided to mongod for authentication to a LDAP Distinguished Name (DN). You may need to use --ldapUserToDNMapping to transform a username into an LDAP DN in the following scenarios:  * Performing LDAP authentication with simple LDAP binding, where users authenticate to MongoDB with usernames that are not full LDAP DNs.  * Using an LDAP authorization query template that requires a DN.  * Transforming the usernames of clients authenticating to Mongo DB using different authentication mechanisms (e.g. x.509, kerberos) to a full LDAP DN for authorization. --ldapUserToDNMapping expects a quote-enclosed JSON-string representing an ordered array of documents. Each document contains a regular expression match and either a substitution or ldapQuery template used for transforming the incoming username. Each document in the array has the following form: {  match: \"<regex>\"  substitution: \"<LDAP DN>\" | ldapQuery: \"<LDAP Query>\"} \nField\nDescription\nExample\nmatch\nAn ECMAScript-formatted regular expression (regex) to match against a provided username. Each parenthesis-enclosed section represents a regex capture group used by substitution or ldapQuery.\n\"(.+)ENGINEERING\" \"(.+)DBA\"\nsubstitution An LDAP distinguished name (DN) formatting template that converts the authentication name matched by the match regex into a LDAP DN. Each curly bracket-enclosed numeric value is replaced by the corresponding regex capture group extracted from the authentication username via the match regex. The result of the substitution must be an RFC4514 escaped string. \"cn={0},ou=engineering, dc=example,dc=com\"\nldapQuery\nA LDAP query formatting template that inserts the authentication name matched by the match regex into an LDAP query URI encoded respecting RFC4515 and RFC4516. Each curly bracket-enclosed numeric value is replaced by the corresponding regex capture group extracted from the authentication username via the match expression. mongod executes the query against the LDAP server to retrieve the LDAP DN for the authenticated user. mongod requires exactly one returned result for the transformation to be successful, or mongod skips this transformation.\n\"ou=engineering,dc=example, dc=com??one?(user={0})\" \nNOTE An explanation of RFC4514, RFC4515, RFC4516, or LDAP queries is out of scope for the MongoDB Documentation. Please review the RFC directly or use your preferred LDAP resource. For each document in the array, you must use either substitution or ldapQuery. You cannot specify both in the same document.\n\n If the given document does not match the provided authentication name, mongod continues through the list of documents to find additional matches. If no matches are found in any document, or the transformation the document describes fails, mongod returns an error. Starting in MongoDB 4.4, mongod also returns an error if one of the transformations cannot be evaluated due to networking or authentication failures to the LDAP server. mongod rejects the connection request and does not check the remaining documents in the array. Starting in MongoDB 5.0, --ldapUserToDNMapping accepts an empty string \"\" or empty array [ ] in place of a mapping documnent. If providing an empty string or empty array to --ldapUserToDNMapping, MongoDB will map the authenticated username as the LDAP DN. Previously, providing an empty mapping document would cause mapping to fail. \nEXAMPLE The following shows two transformation documents. The first document matches against any string ending in @ENGINEERING, placing anything preceeding the suffix into a regex capture group. The second document matches against any string ending in @DBA, placing anything preceeding the suffix into a regex capture group. \nIMPORTANT \nYOU MUST PASS THE ARRAY TO --LDAPUSERTODNMAPPING AS A STRING.  \"[   {      match: \"(.+)@ENGINEERING.EXAMPLE.COM\",      substitution: \"cn={0},ou=engineering,dc=example,dc=com\"   },   {      match: \"(.+)@DBA.EXAMPLE.COM\",      ldapQuery: \"ou=dba,dc=example,dc=com??one?(user={0})\"\n   }\n]\"  A user with username alice@ENGINEERING.EXAMPLE.COM matches the first document. The regex capture group {0} corresponds to the string alice. The resulting output is the DN \"cn=alice,ou=engineering,dc=example,dc=com\". A user with username bob@DBA.EXAMPLE.COM matches the second document. The regex capture group {0} corresponds to the string bob. The resulting output is the LDAP query \"ou=dba,dc=example,dc=com??one?(user=bob)\". mongod executes this query against the LDAP server, returning the result \"cn=bob,ou=dba,dc=example,dc=com\". If --ldapUserToDNMapping is unset, mongod applies no transformations to the username when attempting to authenticate or authorize a user against the LDAP server. This setting can be configured on a running mongod using the setParameter database command. --ldapAuthzQueryTemplate <string> \nAvailable in MongoDB Enterprise only. A relative LDAP query URL formatted conforming to RFC4515 and RFC4516 that mongod executes to obtain the LDAP groups to which the authenticated user belongs to. The query is relative to the host or hosts specified in --ldapServers. In the URL, you can use the following substituion tokens: Substitution Token\nDescription\n{USER}\nSubstitutes the authenticated username, or the transformed username if a username mapping is specified.\n{PROVIDED_USER} Substitutes the supplied username, i.e. before either authentication or LDAP transformation. New in version 4.2. When constructing the query URL, ensure that the order of LDAP parameters respects RFC4516: [ dn  [ ? [attributes] [ ? [scope] [ ? [filter] [ ? [Extensions] ] ] ] ] ]  If your query includes an attribute, mongod assumes that the query retrieves a the DNs which this entity is member of. If your query does not include an attribute, mongod assumes the query retrieves all entities which the user is member of. For each LDAP DN returned by the query, mongod assigns the authorized user a corresponding role on the admin database. If a role on the on the admin database exactly matches the DN, mongod grants the user the roles and privileges assigned to that role. See the db.createRole() method for more information on creating roles. \nEXAMPLE This LDAP query returns any groups listed in the LDAP user object's memberOf attribute. \"{USER}?memberOf?base\" \n\n If unset, mongod cannot authorize users using LDAP. This setting can be configured on a running mongod using the setParameter database command. \nNOTE An explanation of RFC4515, RFC4516 or LDAP queries is out of scope for the MongoDB Documentation. Please review the RFC directly or use your preferred LDAP resource. \nSTORAGE OPTIONS \n--storageEngine string \nDefault: wiredTiger \nNOTE Starting in version 4.2, MongoDB removes the deprecated MMAPv1 storage engine. Specifies the storage engine for the mongod database. Available values include: Value\nDescription\nwiredTiger\nTo specify the WiredTiger Storage Engine.\ninMemory To specify the In-Memory Storage Engine. Available in MongoDB Enterprise only. If you attempt to start a mongod with a --dbpath that contains data files produced by a storage engine other than the one specified by --storageEngine, mongod will refuse to start. --dbpath <path> \nDefault: /data/db on Linux and macOS, \\data\\db on Windows The directory where the mongod instance stores its data. If using the default configuration file included with a package manager installation of MongoDB, the corresponding storage.dbPath setting uses a different default. The files in --dbpath must correspond to the storage engine specified in --storageEngine. If the data files do not correspond to --storageEngine, mongod will refuse to start. --directoryperdb \nUses a separate directory to store data for each database. The directories are under the --dbpath directory, and each subdirectory name corresponds to the database name. Not available for mongod instances that use the in-memory storage engine. Starting in MongoDB 5.0, dropping the final collection in a database (or dropping the database itself) when --directoryperdb is enabled deletes the newly empty subdirectory for that database. To change the --directoryperdb option for existing deployments:  * For standalone instances:\n   \n   1. Use mongodump on the existing mongod instance to generate a backup.\n   \n   2. Stop the mongod instance.\n   \n   3. Add the --directoryperdb value and configure a new data directory\n   \n   4. Restart the mongod instance.\n   \n   5. Use mongorestore to populate the new data directory.  * For replica sets:\n   \n   1. Stop a secondary member.\n   \n   2. Add the --directoryperdb value and configure a new data directory to that secondary member.\n   \n   3. Restart that secondary.\n   \n   4. Use initial sync to populate the new data directory.\n   \n   5. Update remaining secondaries in the same fashion.\n   \n   6. Step down the primary, and update the stepped-down member in the same fashion. --syncdelay <value> \nDefault: 60 Controls how much time can pass before MongoDB flushes data to the data files via an fsync operation. Do not set this value on production systems. In almost every situation, you should use the default setting. \nWARNING If you set --syncdelay to 0, MongoDB will not sync the memory mapped files to disk. The mongod process writes data very quickly to the journal and lazily to the data files. --syncdelay has no effect on the journal files or journaling, but if --syncdelay is set to 0 the journal will eventually consume all available disk space. If you set --syncdelay to 0 for testing purposes, you should also set --nojournal to true. Not available for mongod instances that use the in-memory storage engine. --upgrade \nUpgrades the on-disk data format of the files specified by the --dbpath to the latest version, if needed. This option only affects the operation of the mongod if the data files are in an old format. In most cases you should not set this value, so you can exercise the most control over your upgrade process. See the MongoDB release notes for more information about the upgrade process. --repair \nRuns a repair routine on all databases for a mongod instance. Starting in MongoDB 5.0:  * The repair operation validates the collections to find any inconsistencies and fixes them if possible, which avoids rebuilding the indexes.  * If a collection's data file is salvaged or if the collection has inconsistencies that the validate step is unable to fix, then all indexes are rebuilt.\n\n  * Salvage corrupt data. The operation discards any corrupt data that cannot be salvaged.  * Rebuild indexes. The operation validates collections and rebuilds all indexes for collections with inconsistencies between the collection data and one or more indexes. The operation also rebuilds indexes for all salvaged and modified collections. (Changed in version 4.4.) \nTIP If you are running with journaling enabled, there is almost never any need to run repair since the server can use the journal files to restore the data files to a clean state automatically. However, you may need to run repair in cases where you need to recover from a disk-level data corruption. \nWARNING  * Only use mongod --repair if you have no other options. The operation removes and does not save any corrupt data during the repair process.  * Avoid running --repair against a replica set member:\n   \n   * To repair a replica set member, if you have an intact copy of your data available (e.g. a recent backup or an intact member of the replica set), restore from that intact copy instead(see Resync a Member of a Replica Set).\n   \n   * If you do choose to run mongod --repair against a replica set member and the operation modifies the data or the metadata, you must still perform a full resync in order for the member to rejoin the replica set.  * Before using --repair, make a backup copy of the dbpath directory.  * If repair fails to complete for any reason, you must restart the instance using the --repair option. --journal \nEnables the durability journal to ensure data files remain valid and recoverable. This option applies only when you specify the --dbpath option. mongod enables journaling by default. Not available for mongod instances that use the in-memory storage engine. If any voting member of a replica set uses the in-memory storage engine, you must set writeConcernMajorityJournalDefault to false. Starting in version 4.2 (and 4.0.13 and 3.6.14 ), if a replica set member uses the in-memory storage engine (voting or non-voting) but the replica set has writeConcernMajorityJournalDefault set to true, the replica set member logs a startup warning. --nojournal \nDisables journaling. mongod enables journaling by default. Not available for mongod instances that use the in-memory storage engine. Starting in MongoDB 4.0, you cannot specify --nojournal option or storage.journal.enabled: false for replica set members that use the WiredTiger storage engine. --journalCommitInterval <value> \nDefault: 100 The maximum amount of time in milliseconds that the mongod process allows between journal operations. Values can range from 1 to 500 milliseconds. Lower values increase the durability of the journal, at the expense of disk performance. On WiredTiger, the default journal commit interval is 100 milliseconds. Additionally, a write that includes or implies j:true will cause an immediate sync of the journal. For details or additional conditions that affect the frequency of the sync, see Journaling Process. Not available for mongod instances that use the in-memory storage engine. \nNOTE Known Issue in 4.2.0: The --journalCommitInterval is missing in 4.2.0. \nWIREDTIGER OPTIONS \n--wiredTigerCacheSizeGB <float> \nDefines the maximum size of the internal cache that WiredTiger will use for all data. The memory consumed by an index build (see maxIndexBuildMemoryUsageMegabytes) is separate from the WiredTiger cache memory. Values can range from 0.25 GB to 10000 GB. Starting in MongoDB 3.4, the default WiredTiger internal cache size is the larger of either:  * 50% of (RAM - 1 GB), or  * 256 MB. For example, on a system with a total of 4GB of RAM the WiredTiger cache will use 1.5GB of RAM (0.5 * (4 GB - 1 GB) = 1.5 GB). Conversely, a system with a total of 1.25 GB of RAM will allocate 256 MB to the WiredTiger cache because that is more than half of the total RAM minus one gigabyte (0.5 * (1.25 GB - 1 GB) = 128 MB < 256 MB). \nNOTE In some instances, such as when running in a container, the database can have memory constraints that are lower than the total system memory. In such instances, this memory limit, rather than the total system memory, is used as the maximum RAM available. To see the memory limit, see hostInfo.system.memLimitMB.\n\n With WiredTiger, MongoDB utilizes both the WiredTiger internal cache and the filesystem cache. Via the filesystem cache, MongoDB automatically uses all free memory that is not used by the WiredTiger cache or by other processes. \nNOTE The --wiredTigerCacheSizeGB limits the size of the WiredTiger internal cache. The operating system will use the available free memory for filesystem cache, which allows the compressed MongoDB data files to stay in memory. In addition, the operating system will use any free RAM to buffer file system blocks and file system cache. To accommodate the additional consumers of RAM, you may have to decrease WiredTiger internal cache size. The default WiredTiger internal cache size value assumes that there is a single mongod instance per machine. If a single machine contains multiple MongoDB instances, then you should decrease the setting to accommodate the other mongod instances. If you run mongod in a container (e.g. lxc, cgroups, Docker, etc.) that does not have access to all of the RAM available in a system, you must set --wiredTigerCacheSizeGB to a value less than the amount of RAM available in the container. The exact amount depends on the other processes running in the container. See memLimitMB. --wiredTigerMaxCacheOverflowFileSizeGB <float>  NOTE \nDEPRECATED IN MONGODB 4.4 MongoDB deprecates the --wiredTigerMaxCacheOverflowFileSizeGB option. The option has no effect starting in MongoDB 4.4. Specifies the maximum size (in GB) for the \"lookaside (or cache overflow) table\" file WiredTigerLAS.wt for MongoDB 4.2.1-4.2.x. The file no longer exists starting in version 4.4. The setting can accept the following values: Value\nDescription\n0\nThe default value. If set to 0, the file size is unbounded.\nnumber >= 0.1\nThe maximum size (in GB). If the WiredTigerLAS.wt file exceeds this size, mongod exits with a fatal assertion. You can clear the WiredTigerLAS.wt file and restart mongod. To change the maximum size during runtime, use the wiredTigerMaxCacheOverflowSizeGB parameter. New in version 4.2.1. --wiredTigerJournalCompressor <compressor> \nDefault: snappy Specifies the type of compression to use to compress WiredTiger journal data. Available compressors are:  * none  * snappy  * zlib  * zstd (Available starting in MongoDB 4.2) --wiredTigerDirectoryForIndexes \nWhen you start mongod with --wiredTigerDirectoryForIndexes, mongod stores indexes and collections in separate subdirectories under the data (i.e. --dbpath) directory. Specifically, mongod stores the indexes in a subdirectory named index and the collection data in a subdirectory named collection. By using a symbolic link, you can specify a different location for the indexes. Specifically, when mongod instance is not running, move the index subdirectory to the destination and create a symbolic link named index under the data directory to the new destination. --wiredTigerCollectionBlockCompressor <compressor> \nDefault: snappy Specifies the default compression for collection data. You can override this on a per-collection basis when creating collections. Available compressors are:  * none  * snappy  * zlib  * zstd (Available starting MongoDB 4.2) --wiredTigerCollectionBlockCompressor affects all collections created. If you change the value of --wiredTigerCollectionBlockCompressor on an existing MongoDB deployment, all new collections will use the specified compressor. Existing collections will continue to use the compressor specified when they were created, or the default compressor at that time. --wiredTigerIndexPrefixCompression <boolean> \nDefault: true Enables or disables prefix compression for index data. Specify true for --wiredTigerIndexPrefixCompression to enable prefix compression for index data, or false to disable prefix compression for index data. The --wiredTigerIndexPrefixCompression setting affects all indexes created. If you change the value of --wiredTigerIndexPrefixCompression on an existing MongoDB deployment, all new indexes will use prefix compression. Existing indexes are not affected. \nREPLICATION OPTIONS \n--replSet <setname> \nConfigures replication. Specify a replica set name as an argument to this set. All hosts in the replica set must have the same set name. Starting in MongoDB 4.0,  * For the WiredTiger storage engine, --replSet cannot be used in conjunction with --nojournal.\n\n --oplogSize <value> \nSpecifies a maximum size in megabytes for the replication operation log (i.e., the oplog). \nNOTE The oplog can grow past its configured size limit to avoid deleting the majority commit point. By default, the mongod process creates an oplog based on the maximum amount of space available. For 64-bit systems, the oplog is typically 5% of available disk space. Once the mongod has created the oplog for the first time, changing the --oplogSize option will not affect the size of the oplog. To change the minimum oplog retention period after starting the mongod, use replSetResizeOplog. replSetResizeOplog enables you to resize the oplog dynamically without restarting the mongod process. To persist the changes made using replSetResizeOplog through a restart, update the value of --oplogSize. See Oplog Size for more information. --oplogMinRetentionHours <value> \nNew in version 4.4: Specifies the minimum number of hours to preserve an oplog entry, where the decimal values represent the fractions of an hour. For example, a value of 1.5 represents one hour and thirty minutes. The value must be greater than or equal to 0. A value of 0 indicates that the mongod should truncate the oplog starting with the oldest entries to maintain the configured maximum oplog size. Defaults to 0. A mongod started with --oplogMinRetentionHours only removes an oplog entry if:  * The oplog has reached the maximum configured oplog size and  * The oplog entry is older than the configured number of hours based on the host system clock. The mongod has the following behavior when configured with a minimum oplog retention period:  * The oplog can grow without constraint so as to retain oplog entries for the configured number of hours. This may result in reduction or exhaustion of system disk space due to a combination of high write volume and large retention period.  * If the oplog grows beyond its maximum size, the mongod may continue to hold that disk space even if the oplog returns to its maximum size or is configured for a smaller maximum size. See Reducing Oplog Size Does Not Immediately Return Disk Space.  * The mongod compares the system wall clock to an oplog entries creation wall clock time when enforcing oplog entry retention. Clock drift between cluster components may result in unexpected oplog retention behavior. See Clock Synchronization for more information on clock synchronization across cluster members. To change the minimum oplog retention period after starting the mongod, use replSetResizeOplog. replSetResizeOplog enables you to resize the oplog dynamically without restarting the mongod process. To persist the changes made using replSetResizeOplog through a restart, update the value of --oplogMinRetentionHours. --enableMajorityReadConcern \nDefault: true Configures support for \"majority\" read concern. Starting in MongoDB 5.0, --enableMajorityReadConcern cannot be changed and is always set to true. In earlier versions of MongoDB, --enableMajorityReadConcern was configurable. \nWARNING If you are using a three-member primary-secondary-arbiter (PSA) architecture, consider the following:  * The write concern \"majority\" can cause performance issues if a secondary is unavailable or lagging. For advice on how to mitigate these issues, see Mitigate Performance Issues with PSA Replica Set.  * If you are using a global default \"majority\" and the write concern is less than the size of the majority, your queries may return stale (not fully replicated) data. \nSHARDED CLUSTER OPTIONS \n--configsvr \nRequired if starting a config server. Declares that this mongod instance serves as the config server of a sharded cluster. When running with this option, clients (i.e. other cluster components) cannot write data to any database other than config and admin. The default port for a mongod with this option is 27019 and the default --dbpath directory is /data/configdb, unless specified. \nIMPORTANT When starting a MongoDB server with --configsvr, you must also specify a --replSet. The use of the deprecated mirrored mongod instances as config servers (SCCC) is no longer supported. The replica set config servers (CSRS) must run the WiredTiger storage engine. The --configsvr option creates a local oplog.\n\n Do not use the --configsvr with the skipShardingConfigurationChecks parameter. That is, if you are temporarily starting the mongod as a standalone for maintenance operations, include the parameter skipShardingConfigurationChecks and exclude --configsvr. Once maintenance has completed, remove the skipShardingConfigurationChecks parameter and restart with --configsvr. --shardsvr \nRequired if starting a shard server. Configures this mongod instance as a shard in a sharded cluster. The default port for these instances is 27018. \nIMPORTANT When starting a MongoDB server with --shardsvr, you must also specify a --replSet. Do not use the --shardsvr with the skipShardingConfigurationChecks parameter. That is, if you are temporarily starting the mongod as a standalone for maintenance operations, include the parameter skipShardingConfigurationChecks and exclude --shardsvr. Once maintenance has completed, remove the skipShardingConfigurationChecks parameter and restart with --shardsvr. --moveParanoia \nIf specified, during chunk migration, a shard saves, to the moveChunk directory of the --dbpath, all documents migrated from that shard. MongoDB does not automatically delete the data saved in the moveChunk directory. --noMoveParanoia \nDuring chunk migration, a shard does not save documents migrated from the shard. This is the default behavior. \nTLS OPTIONS  TIP \nSEE: Configure mongod and mongos for TLS/SSL for full documentation of MongoDB's support. --tlsMode <mode> \nNew in version 4.2. Enables TLS used for all network connections. The argument to the --tlsMode option can be one of the following: Value\nDescription\ndisabled\nThe server does not use TLS.\nallowTLS\nConnections between servers do not use TLS. For incoming connections, the server accepts both TLS and non-TLS.\npreferTLS\nConnections between servers use TLS. For incoming connections, the server accepts both TLS and non-TLS.\nrequireTLS\nThe server uses and accepts only TLS encrypted connections. If --tlsCAFile or tls.CAFile is not specified and you are not using x.509 authentication, the system-wide CA certificate store will be used when connecting to an TLS-enabled server. If using x.509 authentication, --tlsCAFile or tls.CAFile must be specified unless using --tlsCertificateSelector. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsCertificateKeyFile <filename> \nNew in version 4.2: Specifies the .pem file that contains both the TLS certificate and key. On macOS or Windows, you can use the --tlsCertificateSelector option to specify a certificate from the operating system's secure certificate store instead of a PEM key file. --tlsCertificateKeyFile and --tlsCertificateSelector options are mutually exclusive. You can only specify one.  * On Linux/BSD, you must specify --tlsCertificateKeyFile when TLS/SSL is enabled.  * On Windows or macOS, you must specify either --tlsCertificateKeyFile or --tlsCertificateSelector when TLS/SSL is enabled.\n   \n   \n   IMPORTANT\n   \n   For Windows only, MongoDB does not support encrypted PEM files. The mongod fails to start if it encounters an encrypted PEM file. To securely store and access a certificate for use with TLS on Windows, use --tlsCertificateSelector. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsCertificateKeyFilePassword <value> \nNew in version 4.2. Specifies the password to decrypt the certificate-key file (i.e. --tlsCertificateKeyFile). Use the --tlsCertificateKeyFilePassword option only if the certificate-key file is encrypted. In all cases, the mongod will redact the password from all logging and reporting output.  * On Linux/BSD, if the private key in the PEM file is encrypted and you do not specify the --tlsCertificateKeyFilePassword option, MongoDB will prompt for a passphrase. See TLS/SSL Certificate Passphrase.\n\n  * On Windows, MongoDB does not support encrypted certificates. The mongod fails if it encounters an encrypted PEM file. Use --tlsCertificateSelector instead. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --clusterAuthMode <option> \nDefault: keyFile The authentication mode used for cluster authentication. If you use internal x.509 authentication, specify so here. This option can have one of the following values: Value\nDescription\nkeyFile\nUse a keyfile for authentication. Accept only keyfiles.\nsendKeyFile\nFor rolling upgrade purposes. Send a keyfile for authentication but can accept both keyfiles and x.509 certificates.\nsendX509\nFor rolling upgrade purposes. Send the x.509 certificate for authentication but can accept both keyfiles and x.509 certificates.\nx509\nRecommended. Send the x.509 certificate for authentication and accept only x.509 certificates. If --tlsCAFile or tls.CAFile is not specified and you are not using x.509 authentication, the system-wide CA certificate store will be used when connecting to an TLS-enabled server. If using x.509 authentication, --tlsCAFile or tls.CAFile must be specified unless using --tlsCertificateSelector. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsClusterFile <filename> \nNew in version 4.2: Specifies the .pem file that contains the x.509 certificate-key file for membership authentication for the cluster or replica set. On macOS or Windows, you can use the --tlsClusterCertificateSelector option to specify a certificate from the operating system's secure certificate store instead of a PEM key file. --tlsClusterFile and --tlsClusterCertificateSelector options are mutually exclusive. You can only specify one. If --tlsClusterFile does not specify the .pem file for internal cluster authentication or the alternative --tlsClusterCertificateSelector, the cluster uses the .pem file specified in the --tlsCertificateKeyFile option or the certificate returned by the --tlsCertificateSelector. If using x.509 authentication, --tlsCAFile or tls.CAFile must be specified unless using --tlsCertificateSelector. Changed in version 4.4: mongod / mongos logs a warning on connection if the presented x.509 certificate expires within 30 days of the mongod/mongos host system time. See x.509 Certificates Nearing Expiry Trigger Warnings for more information. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . \nIMPORTANT For Windows only, MongoDB does not support encrypted PEM files. The mongod fails to start if it encounters an encrypted PEM file. To securely store and access a certificate for use with membership authentication on Windows, use --tlsClusterCertificateSelector. --tlsCertificateSelector <parameter>=<value> \nNew in version 4.2: Available on Windows and macOS as an alternative to --tlsCertificateKeyFile. Specifies a certificate property in order to select a matching certificate from the operating system's certificate store to use for TLS. The --tlsCertificateKeyFile and --tlsCertificateSelector options are mutually exclusive. You can only specify one. --tlsCertificateSelector accepts an argument of the format <property>=<value> where the property can be one of the following: Property\nValue type\nDescription\nsubject\nASCII string\nSubject name or common name on certificate\nthumbprint\nhex string A sequence of bytes, expressed as hexadecimal, used to identify a public key by its SHA-1 digest. The thumbprint is sometimes referred to as a fingerprint. When using the system SSL certificate store, OCSP (Online Certificate Status Protocol) is used to validate the revocation status of certificates. The mongod searches the operating system's secure certificate store for the CA certificates required to validate the full certificate chain of the specified TLS certificate. Specifically, the secure certificate store must contain the root CA and any intermediate CA certificates required to build the full certificate chain to the TLS certificate. Do not use --tlsCAFile or --tlsClusterCAFile to specify the root and intermediate CA certificate\n\n \nNOTE You cannot use the rotateCertificates command or the db.rotateCertificates() shell method when using net.tls.certificateSelector or --tlsCertificateSelector set to thumbprint --tlsClusterCertificateSelector <parameter>=<value> \nNew in version 4.2: Available on Windows and macOS as an alternative to --tlsClusterFile. Specifies a certificate property in order to select a matching certificate from the operating system's certificate store to use for internal x.509 membership authentication. --tlsClusterFile and --tlsClusterCertificateSelector options are mutually exclusive. You can only specify one. --tlsClusterCertificateSelector accepts an argument of the format <property>=<value> where the property can be one of the following: Property\nValue type\nDescription\nsubject\nASCII string\nSubject name or common name on certificate\nthumbprint\nhex string A sequence of bytes, expressed as hexadecimal, used to identify a public key by its SHA-1 digest. The thumbprint is sometimes referred to as a fingerprint. The mongod searches the operating system's secure certificate store for the CA certificates required to validate the full certificate chain of the specified cluster certificate. Specifically, the secure certificate store must contain the root CA and any intermediate CA certificates required to build the full certificate chain to the cluster certificate. Do not use --tlsCAFile or --tlsClusterCAFile to specify the root and intermediate CA certificate. For example, if the cluster certificate was signed with a single root CA certificate, the secure certificate store must contain that root CA certificate. If the cluster certificate was signed with an intermediate CA certificate, the secure certificate store must contain the intermedia CA certificate and the root CA certificate. Changed in version 4.4: mongod / mongos logs a warning on connection if the presented x.509 certificate expires within 30 days of the mongod/mongos host system time. See x.509 Certificates Nearing Expiry Trigger Warnings for more information. --tlsClusterPassword <value> \nNew in version 4.2. Specifies the password to decrypt the x.509 certificate-key file specified with --tlsClusterFile. Use the --tlsClusterPassword option only if the certificate-key file is encrypted. In all cases, the mongod will redact the password from all logging and reporting output.  * On Linux/BSD, if the private key in the x.509 file is encrypted and you do not specify the --tlsClusterPassword option, MongoDB will prompt for a passphrase. See TLS/SSL Certificate Passphrase.  * On macOS, if the private key in the x.509 file is encrypted, you must explicitly specify the --tlsClusterPassword option. Alternatively, you can either use a certificate from the secure system store (see --tlsClusterCertificateSelector) instead of a cluster PEM file or use an unencrypted PEM file.  * On Windows, MongoDB does not support encrypted certificates. The mongod fails if it encounters an encrypted PEM file. Use --tlsClusterCertificateSelector instead. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsCAFile <filename> \nNew in version 4.2. Specifies the .pem file that contains the root certificate chain from the Certificate Authority. Specify the file name of the .pem file using relative or absolute paths. Windows/macOS OnlyIf using --tlsCertificateSelector and/or --tlsClusterCertificateSelector, do not use --tlsCAFile to specify the root and intermediate CA certificates. Store all CA certificates required to validate the full trust chain of the --tlsCertificateSelector and/or --tlsClusterCertificateSelector certificates in the secure certificate store. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsClusterCAFile <filename> \nNew in version 4.2. Specifies the .pem file that contains the root certificate chain from the Certificate Authority used to validate the certificate presented by a client establishing a connection. Specify the file name of the .pem file using relative or absolute paths. --tlsClusterCAFile requires that --tlsCAFile is set.\n\n --tlsClusterCAFile lets you use separate Certificate Authorities to verify the client to server and server to client portions of the TLS handshake. Windows/macOS OnlyIf using --tlsCertificateSelector and/or --tlsClusterCertificateSelector, do not use --tlsClusterCAFile to specify the root and intermediate CA certificates. Store all CA certificates required to validate the full trust chain of the --tlsCertificateSelector and/or --tlsClusterCertificateSelector certificates in the secure certificate store. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsCRLFile <filename> \nNew in version 4.2. Specifies the .pem file that contains the Certificate Revocation List. Specify the file name of the .pem file using relative or absolute paths. \nNOTE  * You cannot specify a CRL file on macOS. Instead, you can use the system SSL certificate store, which uses OCSP (Online Certificate Status Protocol) to validate the revocation status of certificates. See --tlsCertificateSelector to use the system SSL certificate store.  * Starting in version 4.4, to check for certificate revocation, MongoDB enables the use of OCSP (Online Certificate Status Protocol) by default as an alternative to specifying a CRL file or using the system SSL certificate store. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsAllowInvalidCertificates \nNew in version 4.2. Bypasses the validation checks for TLS certificates on other servers in the cluster and allows the use of invalid certificates to connect. \nNOTE If you specify --tlsAllowInvalidCertificates or tls.allowInvalidCertificates: true when using x.509 authentication, an invalid certificate is only sufficient to establish a TLS connection but is insufficient for authentication. When using the --tlsAllowInvalidCertificates setting, MongoDB logs a warning regarding the use of the invalid certificate. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsAllowInvalidHostnames \nNew in version 4.2. Disables the validation of the hostnames in TLS certificates, when connecting to other members of the replica set or sharded cluster for inter-process authentication. This allows mongod to connect to other members if the hostnames in their certificates do not match their configured hostname. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsAllowConnectionsWithoutCertificates \nNew in version 4.2. For clients that don't provide certificates, mongod or mongos encrypts the TLS/SSL connection, assuming the connection is successfully made. For clients that present a certificate, however, mongod performs certificate validation using the root certificate chain specified by --tlsCAFile and reject clients with invalid certificates. Use the --tlsAllowConnectionsWithoutCertificates option if you have a mixed deployment that includes clients that do not or cannot present certificates to the mongod. For more information about TLS and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --tlsDisabledProtocols <protocol(s)> \nNew in version 4.2. Prevents a MongoDB server running with TLS from accepting incoming connections that use a specific protocol or protocols. To specify multiple protocols, use a comma separated list of protocols. --tlsDisabledProtocols recognizes the following protocols: TLS1_0, TLS1_1, TLS1_2, and TLS1_3.  * On macOS, you cannot disable TLS1_1 and leave both TLS1_0 and TLS1_2 enabled. You must disable at least one of the other two, for example, TLS1_0,TLS1_1.  * To list multiple protocols, specify as a comma separated list of protocols. For example TLS1_0,TLS1_1.  * Specifying an unrecognized protocol will prevent the server from starting.  * The specified disabled protocols overrides any default disabled protocols. MongoDB disables the use of TLS 1.0 if TLS 1.1+ is available on the system. To enable the disabled TLS 1.0, specify none to --tlsDisabledProtocols. See Disable TLS 1.0. Members of replica sets and sharded clusters must speak at least one protocol in common. \nTIP\n\n Disallow Protocols --tlsFIPSMode \nNew in version 4.2. Directs the mongod to use the FIPS mode of the TLS library. Your system must have a FIPS compliant library to use the --tlsFIPSMode option. \nNOTE FIPS-compatible TLS/SSL is available only in MongoDB Enterprise. See Configure MongoDB for FIPS for more information. \nSSL OPTIONS (DEPRECATED)  IMPORTANT All SSL options are deprecated since 4.2. Use the TLS counterparts instead, as they have identical functionality to the SSL options. The SSL protocol is deprecated and MongoDB supports TLS 1.0 and later. \nTIP \nSEE: Configure mongod and mongos for TLS/SSL for full documentation of MongoDB's support. --sslOnNormalPorts \nDeprecated since version 2.6: Use --tlsMode requireTLS instead. Enables TLS/SSL for mongod. With --sslOnNormalPorts, a mongod requires TLS/SSL encryption for all connections on the default MongoDB port, or the port specified by --port. By default, --sslOnNormalPorts is disabled. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslMode <mode> \nDeprecated since version 4.2: Use --tlsMode instead. Enables TLS/SSL or mixed TLS/SSL used for all network connections. The argument to the --sslMode option can be one of the following: Value\nDescription\ndisabled\nThe server does not use TLS/SSL.\nallowSSL\nConnections between servers do not use TLS/SSL. For incoming connections, the server accepts both TLS/SSL and non-TLS/non-SSL.\npreferSSL\nConnections between servers use TLS/SSL. For incoming connections, the server accepts both TLS/SSL and non-TLS/non-SSL.\nrequireSSL\nThe server uses and accepts only TLS/SSL encrypted connections. If --tlsCAFile/net.tls.CAFile (or their aliases --sslCAFile/net.ssl.CAFile) is not specified and you are not using x.509 authentication, the system-wide CA certificate store will be used when connecting to an TLS/SSL-enabled server. To use x.509 authentication, --tlsCAFile or net.tls.CAFile must be specified unless you are using --tlsCertificateSelector or --net.tls.certificateSelector. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslPEMKeyFile <filename> \nDeprecated since version 4.2: Use --tlsCertificateKeyFile instead. Specifies the .pem file that contains both the TLS/SSL certificate and key. On macOS or Windows, you can use the --sslCertificateSelector option to specify a certificate from the operating system's secure certificate store instead of a PEM key file. --sslPEMKeyFile and --sslCertificateSelector options are mutually exclusive. You can only specify one.  * On Linux/BSD, you must specify --sslPEMKeyFile when TLS/SSL is enabled.  * On Windows or macOS, you must specify either --sslPEMKeyFile or --sslCertificateSelector when TLS/SSL is enabled.\n   \n   \n   IMPORTANT\n   \n   For Windows only, MongoDB does not support encrypted PEM files. The mongod fails to start if it encounters an encrypted PEM file. To securely store and access a certificate for use with TLS/SSL on Windows, use --sslCertificateSelector. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslPEMKeyPassword <value> \nDeprecated since version 4.2: Use --tlsCertificateKeyFilePassword instead. Specifies the password to decrypt the certificate-key file (i.e. --sslPEMKeyFile). Use the --sslPEMKeyPassword option only if the certificate-key file is encrypted. In all cases, the mongod will redact the password from all logging and reporting output.  * On Linux/BSD, if the private key in the PEM file is encrypted and you do not specify the --sslPEMKeyPassword option, MongoDB will prompt for a passphrase. See TLS/SSL Certificate Passphrase.\n\n  * On Windows, MongoDB does not support encrypted certificates. The mongod fails if it encounters an encrypted PEM file. Use --sslCertificateSelector instead. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslClusterFile <filename> \nDeprecated since version 4.2: Use --tlsClusterFile instead. Specifies the .pem file that contains the x.509 certificate-key file for membership authentication for the cluster or replica set. On macOS or Windows, you can use the --sslClusterCertificateSelector option to specify a certificate from the operating system's secure certificate store instead of a PEM key file. --sslClusterFile and --sslClusterCertificateSelector options are mutually exclusive. You can only specify one. If --sslClusterFile does not specify the .pem file for internal cluster authentication or the alternative --sslClusterCertificateSelector, the cluster uses the .pem file specified in the --sslPEMKeyFile option or the certificate returned by the --sslCertificateSelector. To use x.509 authentication, --tlsCAFile or net.tls.CAFile must be specified unless you are using --tlsCertificateSelector or --net.tls.certificateSelector. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . \nIMPORTANT For Windows only, MongoDB does not support encrypted PEM files. The mongod fails to start if it encounters an encrypted PEM file. To securely store and access a certificate for use with membership authentication on Windows, use --sslClusterCertificateSelector. --sslCertificateSelector <parameter>=<value> \nDeprecated since version 4.2: Use --tlsCertificateSelector instead. Available on Windows and macOS as an alternative to --tlsCertificateKeyFile. Specifies a certificate property to select a matching certificate from the operating system's secure certificate store to use for TLS/SSL. --sslPEMKeyFile and --sslCertificateSelector options are mutually exclusive. You can only specify one. --sslCertificateSelector accepts an argument of the format <property>=<value> where the property can be one of the following: Property\nValue type\nDescription\nsubject\nASCII string\nSubject name or common name on certificate\nthumbprint\nhex string A sequence of bytes, expressed as hexadecimal, used to identify a public key by its SHA-1 digest. The thumbprint is sometimes referred to as a fingerprint. When using the system SSL certificate store, OCSP (Online Certificate Status Protocol) is used to validate the revocation status of certificates. The mongod searches the operating system's secure certificate store for the CA certificates required to validate the full certificate chain of the specified TLS/SSL certificate. Specifically, the secure certificate store must contain the root CA and any intermediate CA certificates required to build the full certificate chain to the TLS/SSL certificate. Do not use --sslCAFile or --sslClusterCAFile to specify the root and intermediate CA certificate For example, if the TLS/SSL certificate was signed with a single root CA certificate, the secure certificate store must contain that root CA certificate. If the TLS/SSL certificate was signed with an intermediate CA certificate, the secure certificate store must contain the intermedia CA certificate and the root CA certificate. --sslClusterCertificateSelector <parameter>=<value> \nDeprecated since version 4.2: Use --tlsClusterCertificateSelector instead. Available on Windows and macOS as an alternative to --sslClusterFile. Specifies a certificate property to select a matching certificate from the operating system's secure certificate store to use for internal x.509 membership authentication. --sslClusterFile and --sslClusterCertificateSelector options are mutually exclusive. You can only specify one. --sslClusterCertificateSelector accepts an argument of the format <property>=<value> where the property can be one of the following: Property\nValue type\nDescription\nsubject\nASCII string\nSubject name or common name on certificate\nthumbprint\nhex string A sequence of bytes, expressed as hexadecimal, used to identify a public key by its SHA-1 digest. The thumbprint is sometimes referred to as a fingerprint.\n\n For example, if the cluster certificate was signed with a single root CA certificate, the secure certificate store must contain that root CA certificate. If the cluster certificate was signed with an intermediate CA certificate, the secure certificate store must contain the intermedia CA certificate and the root CA certificate. --sslClusterPassword <value> \nDeprecated since version 4.2: Use --tlsClusterPassword instead. Specifies the password to decrypt the x.509 certificate-key file specified with --sslClusterFile. Use the --sslClusterPassword option only if the certificate-key file is encrypted. In all cases, the mongod will redact the password from all logging and reporting output.  * On Linux/BSD, if the private key in the x.509 file is encrypted and you do not specify the --sslClusterPassword option, MongoDB will prompt for a passphrase. See TLS/SSL Certificate Passphrase.  * On macOS, if the private key in the x.509 file is encrypted, you must explicitly specify the --sslClusterPassword option. Alternatively, you can either use a certificate from the secure system store (see --sslClusterCertificateSelector) instead of a cluster PEM file or use an unencrypted PEM file.  * On Windows, MongoDB does not support encrypted certificates. The mongod fails if it encounters an encrypted PEM file. Use --sslClusterCertificateSelector instead. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslCAFile <filename> \nDeprecated since version 4.2: Use --tlsCAFile instead. Specifies the .pem file that contains the root certificate chain from the Certificate Authority. Specify the file name of the .pem file using relative or absolute paths. Windows/macOS OnlyIf using --sslCertificateSelector and/or --sslClusterCertificateSelector, do not use --sslCAFile to specify the root and intermediate CA certificates. Store all CA certificates required to validate the full trust chain of the --sslCertificateSelector and/or --sslClusterCertificateSelector certificates in the secure certificate store. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslClusterCAFile <filename> \nDeprecated since version 4.2: Use --tlsClusterCAFile instead. Specifies the .pem file that contains the root certificate chain from the Certificate Authority used to validate the certificate presented by a client establishing a connection. Specify the file name of the .pem file using relative or absolute paths. --sslClusterCAFile requires that --sslCAFile is set. If --sslClusterCAFile does not specify the .pem file for validating the certificate from a client establishing a connection, the cluster uses the .pem file specified in the --sslCAFile option. --sslClusterCAFile lets you use separate Certificate Authorities to verify the client to server and server to client portions of the TLS handshake. Windows/macOS OnlyIf using --sslCertificateSelector and/or --sslClusterCertificateSelector, do not use --sslClusterCAFile to specify the root and intermediate CA certificates. Store all CA certificates required to validate the full trust chain of the --sslCertificateSelector and/or --sslClusterCertificateSelector certificates in the secure certificate store. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslCRLFile <filename> \nDeprecated since version 4.2: Use --tlsCRLFile instead. Specifies the .pem file that contains the Certificate Revocation List. Specify the file name of the .pem file using relative or absolute paths. \nNOTE  * You cannot specify a CRL file on macOS. Instead, you can use the system SSL certificate store, which uses OCSP (Online Certificate Status Protocol) to validate the revocation status of certificates. See --tlsCertificateSelector in MongoDB 4.2+ to use the system SSL certificate store.  * Starting in version 4.4, to check for certificate revocation, MongoDB enables the use of OCSP (Online Certificate Status Protocol) by default as an alternative to specifying a CRL file or using the system SSL certificate store. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslAllowInvalidCertificates\n\n Bypasses the validation checks for TLS/SSL certificates on other servers in the cluster and allows the use of invalid certificates to connect. \nNOTE Starting in MongoDB 4.2, if you specify --tlsAllowInvalidateCertificates or net.tls.allowInvalidCertificates: true when using x.509 authentication, an invalid certificate is only sufficient to establish a TLS connection but it is insufficient for authentication. When using the --sslAllowInvalidCertificates setting, MongoDB logs a warning regarding the use of the invalid certificate. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslAllowInvalidHostnames \nDeprecated since version 4.2: Use --tlsAllowInvalidHostnames instead. Disables the validation of the hostnames in TLS/SSL certificates, when connecting to other members of the replica set or sharded cluster for inter-process authentication. This allows mongod to connect to other members if the hostnames in their certificates do not match their configured hostname. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslAllowConnectionsWithoutCertificates \nDeprecated since version 4.2: Use --tlsAllowConnectionsWithoutCertificates instead. For clients that don't provide certificates, mongod or mongos encrypts the TLS/SSL connection, assuming the connection is successfully made. For clients that present a certificate, however, mongod performs certificate validation using the root certificate chain specified by --sslCAFile and reject clients with invalid certificates. Use the --sslAllowConnectionsWithoutCertificates option if you have a mixed deployment that includes clients that do not or cannot present certificates to the mongod. For more information about TLS/SSL and MongoDB, see Configure mongod and mongos for TLS/SSL and TLS/SSL Configuration for Clients . --sslDisabledProtocols <protocol(s)> \nDeprecated since version 4.2: Use --tlsDisabledProtocols instead. Prevents a MongoDB server running with TLS/SSL from accepting incoming connections that use a specific protocol or protocols. To specify multiple protocols, use a comma separated list of protocols. --sslDisabledProtocols recognizes the following protocols: TLS1_0, TLS1_1, TLS1_2, and TLS1_3.  * On macOS, you cannot disable TLS1_1 and leave both TLS1_0 and TLS1_2 enabled. You must disable at least one of the other two, for example, TLS1_0,TLS1_1.  * To list multiple protocols, specify as a comma separated list of protocols. For example TLS1_0,TLS1_1.  * Specifying an unrecognized protocol will prevent the server from starting.  * The specified disabled protocols overrides any default disabled protocols. MongoDB disables the use of TLS 1.0 if TLS 1.1+ is available on the system. To enable the disabled TLS 1.0, specify none to --sslDisabledProtocols. See Disable TLS 1.0. Members of replica sets and sharded clusters must speak at least one protocol in common. \nTIP \nSEE ALSO: Disallow Protocols --sslFIPSMode \nDeprecated since version 4.2: Use --tlsFIPSMode instead. Directs the mongod to use the FIPS mode of the TLS/SSL library. Your system must have a FIPS compliant library to use the --sslFIPSMode option. \nNOTE FIPS-compatible TLS/SSL is available only in MongoDB Enterprise. See Configure MongoDB for FIPS for more information. \nPROFILER OPTIONS \n--profile <level> \nDefault: 0 Configures the database profiler level. The following profiler levels are available: Level\nDescription\n0\nThe profiler is off and does not collect any data. This is the default profiler level.\n1 The profiler collects data for operations that take longer than the value of slowms or that match a filter. When a filter is set:  * The slowms and sampleRate options are not used for profiling.  * The profiler only captures operations that match the filter. 2\nThe profiler collects data for all operations. \nIMPORTANT Profiling can impact performance and shares settings with the system log. Carefully consider any performance and security implications before configuring and enabling the profiler on a production deployment. See Profiler Overhead for more information on potential performance degradation. --slowms <integer> \nDefault: 100\n\n When logLevel is set to 0, MongoDB records slow operations to the diagnostic log at a rate determined by slowOpSampleRate. At higher logLevel settings, all operations appear in the diagnostic log regardless of their latency with the following exception: the logging of slow oplog entry messages by the secondaries. The secondaries log only the slow oplog entries; increasing the logLevel does not log all oplog entries. For mongod instances, --slowms affects the diagnostic log and, if enabled, the profiler. \nTIP \nSEE ALSO: Database Profiler --slowOpSampleRate <double> \nDefault: 1.0 The fraction of slow operations that should be profiled or logged. --slowOpSampleRate accepts values between 0 and 1, inclusive. --slowOpSampleRate does not affect the slow oplog entry logging by the secondary members of a replica set. Secondary members log all oplog entries that take longer than the slow operation threshold regardless of the --slowOpSampleRate. For mongod instances, --slowOpSampleRate affects the diagnostic log and, if enabled, the profiler. \nAUDIT OPTIONS \n--auditCompressionMode \nNew in version 5.3. Specifies the compression mode for audit log encryption. You must also enable audit log encryption using either --auditEncryptionKeyUID or --auditLocalKeyFile. --auditCompressionMode can be set to one of these values: Value\nDescription\nzstd\nUse the zstd algorithm to compress the audit log.\nnone (default)\nDo not compress the audit log. \nNOTE Available only in MongoDB Enterprise. MongoDB Enterprise and Atlas have different configuration requirements. --auditDestination \nEnables auditing and specifies where mongod sends all audit events. --auditDestination can have one of the following values: Value\nDescription\nsyslog Output the audit events to syslog in JSON format. Not available on Windows. Audit messages have a syslog severity level of info and a facility level of user. The syslog message limit can result in the truncation of audit messages. The auditing system will neither detect the truncation nor error upon its occurrence. console\nOutput the audit events to stdout in JSON format.\nfile\nOutput the audit events to the file specified in --auditPath in the format specified in --auditFormat. \nNOTE Available only in MongoDB Enterprise and MongoDB Atlas. --auditEncryptionKeyUID \nNew in version 6.0. Specifies the unique identifier of the Key Management Interoperability Protocol (KMIP) key for audit log encryption. You cannot use --auditEncryptionKeyUID and --auditLocalKeyFile together. \nNOTE Available only in MongoDB Enterprise. MongoDB Enterprise and Atlas have different configuration requirements. --auditFormat \nSpecifies the format of the output file for auditing if --auditDestination is file. The --auditFormat option can have one of the following values: Value\nDescription\nJSON\nOutput the audit events in JSON format to the file specified in --auditPath.\nBSON\nOutput the audit events in BSON binary format to the file specified in --auditPath. Printing audit events to a file in JSON format degrades server performance more than printing to a file in BSON format. \nNOTE Available only in MongoDB Enterprise and MongoDB Atlas. --auditLocalKeyFile \nNew in version 5.3. Specifies the path and file name for a local audit key file for audit log encryption. \nNOTE Only use --auditLocalKeyFile for testing because the key is not secured. To secure the key, use --auditEncryptionKeyUID and an external Key Management Interoperability Protocol (KMIP) server. You cannot use --auditLocalKeyFile and --auditEncryptionKeyUID together. \nNOTE Available only in MongoDB Enterprise. MongoDB Enterprise and Atlas have different configuration requirements. --auditPath \nSpecifies the output file for auditing if --auditDestination has value of file. The --auditPath option can take either a full path name or a relative path name. \nNOTE Available only in MongoDB Enterprise and MongoDB Atlas. --auditFilter \nSpecifies the filter to limit the types of operations the audit system records. The option takes a string representation of a query document of the form: { <field1>: <expression1>, ... }  The <field> can be any field in the audit message, including fields returned in the param document. The <expression> is a query condition expression. To specify an audit filter, enclose the filter document in single quotes to pass the document as a string.\n\n \nNOTE Available only in MongoDB Enterprise and MongoDB Atlas. \nSNMP OPTIONS  NOTE MongoDB Enterprise on macOS does not include support for SNMP due to SERVER-29352. --snmp-disabled \nDisables SNMP access to mongod. The option is incompatible with --snmp-subagent and --snmp-master. --snmp-subagent \nRuns SNMP as a subagent. The option is incompatible with --snmp-disabled. --snmp-master \nRuns SNMP as a master. The option is incompatible with --snmp-disabled. \nTIP \nSEE ALSO:  * Monitor MongoDB With SNMP on Linux  * Monitor MongoDB Windows with SNMP  * Troubleshoot SNMP \nINMEMORY OPTIONS \n--inMemorySizeGB <float> \nDefault: 50% of physical RAM minus 1 GB. Maximum amount of memory to allocate for the in-memory storage engine data, including indexes, the oplog (if the mongod is part of a replica set), sharded cluster metadata, etc. Values can range from 256MB to 10TB and can be a float. By default, the in-memory storage engine uses 50% of physical RAM minus 1 GB. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. \nENCRYPTION KEY MANAGEMENT OPTIONS \n--enableEncryption \nDefault: false Enables encryption for the WiredTiger storage engine. This option must be enabled in order to pass in encryption keys and configurations. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --encryptionCipherMode <string> \nDefault: AES256-CBC The cipher mode to use for encryption at rest: Mode\nDescription\nAES256-CBC\n256-bit Advanced Encryption Standard in Cipher Block Chaining Mode\nAES256-GCM 256-bit Advanced Encryption Standard in Galois/Counter Mode MongoDB Enterprise on Windows no longer supports AES256-GCM. This cipher is now available only on Linux. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --encryptionKeyFile <string> \nThe path to the local keyfile when managing keys via process other than KMIP. Only set when managing keys via process other than KMIP. If data is already encrypted using KMIP, MongoDB will throw an error. The keyfile can contain only a single key. The key is either a 16 or 32 character string. Requires --enableEncryption. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipKeyIdentifier <string> \nUnique KMIP identifier for an existing key within the KMIP server. Include to use the key associated with the identifier as the system key. You can only use the setting the first time you enable encryption for the mongod instance. Requires --enableEncryption. If unspecified, MongoDB will request that the KMIP server create a new key to utilize as the system key. If the KMIP server cannot locate a key with the specified identifier or the data is already encrypted with a key, MongoDB will throw an error \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipRotateMasterKey <boolean> \nDefault: false If true, rotate the master key and re-encrypt the internal keystore. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. \nTIP \nSEE ALSO: KMIP Master Key Rotation --kmipServerName <string> \nHostname or IP address of the KMIP server to connect to. Requires --enableEncryption. Starting in MongoDB 4.2.1, you can specify multiple KMIP servers as a comma-separated list, e.g. server1.example.com,server2.example.com. On startup, the mongod will attempt to establish a connection to each server in the order listed, and will select the first server to which it can successfully establish a connection. KMIP server selection occurs only at startup. When connecting to a KMIP server, the mongod verifies that the specified --kmipServerName matches the Subject Alternative Name SAN (or, if SAN is not present, the Common Name CN) in the certificate presented by the KMIP server. If SAN is present, mongod does not match against the CN. If the hostname does not match the SAN (or CN), the mongod will fail to connect. Starting in MongoDB 4.2, when performing comparison of SAN, MongoDB supports comparison of DNS names or IP addresses. In previous versions, MongoDB only supports comparisons of DNS names. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only.\n\n \nDefault: 5696 Port number to use to communicate with the KMIP server. Requires --kmipServerName. Requires --enableEncryption. If specifying multiple KMIP servers with --kmipServerName, the mongod will use the port specified with --kmipPort for all provided KMIP servers. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipConnectRetries <number> \nDefault: 0 New in version 4.4. How many times to retry the initial connection to the KMIP server. Use together with --kmipConnectTimeoutMS to control how long the mongod waits for a response between each retry. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipConnectTimeoutMS <number> \nDefault: 5000 New in version 4.4. Timeout in milliseconds to wait for a response from the KMIP server. If the --kmipConnectRetries setting is specified, the mongod will wait up to the value specified with --kmipConnectTimeoutMS for each retry. Value must be 1000 or greater. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipClientCertificateSelector <string> \nNew in version 4.2.15,: 4.4.7, and 5.0 Available on Windows and macOS as an alternative to --kmipClientCertificateFile. --kmipClientCertificateFile and --kmipClientCertificateSelector options are mutually exclusive. You can only specify one. Specifies a certificate property in order to select a matching certificate from the operating system's certificate store to authenticate MongoDB to the KMIP server. --kmipClientCertificateSelector accepts an argument of the format <property>=<value> where the property can be one of the following: Property\nValue type\nDescription\nsubject\nASCII string\nSubject name or common name on certificate\nthumbprint\nhex string A sequence of bytes, expressed as hexadecimal, used to identify a public key by its SHA-1 digest. The thumbprint is sometimes referred to as a fingerprint. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipClientCertificateFile <string> \nPath to the .pem file used to authenticate MongoDB to the KMIP server. The specified .pem file must contain both the TLS/SSL certificate and key. To use this option, you must also specify the --kmipServerName option. \nNOTE On macOS or Windows, you can use a certificate from the operating system's secure store instead of a PEM key file. See --kmipClientCertificateSelector. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipClientCertificatePassword <string> \nThe password (if one exists) for the client certificate passed into --kmipClientCertificateFile. Is used for authenticating MongoDB to the KMIP server. Requires that a --kmipClientCertificateFile be provided. \nNOTE \nENTERPRISE FEATURE Available in MongoDB Enterprise only. --kmipServerCAFile <string> \nPath to CA File. Used for validating secure client connection to KMIP server. \nNOTE On macOS or Windows, you can use a certificate from the operating system's secure store instead of a PEM key file. See --kmipClientCertificateSelector. When using the secure store, you do not need to, but can, also specify the --kmipServerCAFile. --kmipActivateKeys <boolean> \nDefault: true New in version 5.3. Activates all newly created KMIP keys upon creation and then periodically checks those keys are in an active state. When --kmipActivateKeys is true and you have existing keys on a KMIP server, the key must be activated first or the mongod node will fail to start. If the key being used by the mongod transitions into a non-active state, the mongod node will shut down unless kmipActivateKeys is false. To ensure you have an active key, rotate the KMIP master key by using --kmipRotateMasterKey. --kmipKeyStatePollingSeconds <integer> \nDefault: 900 seconds New in version 5.3. Frequency in seconds at which mongod polls the KMIP server for active keys. To disable disable polling, set the value to -1. --eseDatabaseKeyRollover \nNew in version 4.2. Roll over the encrypted storage engine database keys configured with AES256-GCM cipher. When mongod instance is started with this option, the instance rotates the keys and exits. \nNOTE \nENTERPRISE FEATURE\n\n ←  MongoDB Package Componentsmongos → On this page  * Synopsis\n * Options\n * Core Options\n * Free Monitoring\n * LDAP Authentication or Authorization Options\n * Storage Options\n * WiredTiger Options\n * Replication Options\n * Sharded Cluster Options\n * TLS Options\n * SSL Options (Deprecated)\n * Profiler Options\n * Audit Options\n * SNMP Options\n * inMemory Options\n * Encryption Key Management Options Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/grantRolesToUser/": " Docs Home → MongoDB Manual \nGRANTROLESTOUSER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \ngrantRolesToUser \nGrants additional roles to a user. \nTIP In mongosh, this command can also be run through the db.grantRolesToUser() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe grantRolesToUser command uses the following syntax: db.runCommand(   {     grantRolesToUser: \"<user>\",     roles: [ <roles> ],     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ngrantRolesToUser\nstring\nThe name of the user to give additional roles.\nroles\narray\nAn array of additional roles to grant to the user.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. In the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where grantRolesToUser runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nREQUIRED ACCESS \nYou must have the grantRole action on a database to grant a role on that database. \nEXAMPLE \nGiven a user accountUser01 in the products database with the following roles: \"roles\" : [    { \"role\" : \"assetsReader\",      \"db\" : \"assets\"    }]  The following grantRolesToUser operation gives accountUser01 the read role on the stock database and the readWrite role on the products database. use productsdb.runCommand( { grantRolesToUser: \"accountUser01\",                 roles: [                    { role: \"read\", db: \"stock\"},                    \"readWrite\"                 ],                 writeConcern: { w: \"majority\" , wtimeout: 2000 }             } )  The user accountUser01 in the products database now has the following roles: \"roles\" : [    { \"role\" : \"assetsReader\",      \"db\" : \"assets\"    },    { \"role\" : \"read\",      \"db\" : \"stock\"    },    { \"role\" : \"readWrite\",      \"db\" : \"products\"    }] \n←  dropUserrevokeRolesFromUser → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/geoSearch/": " Docs Home → MongoDB Manual \nGEOSEARCH \nOn this page    \n * Behavior\n   \n * Examples geoSearch  IMPORTANT \nREMOVED IN MONGODB 5.0 MongoDB 5.0 removes the deprecated geoHaystack index and geoSearch command. Use a 2d index with $geoNear or one of the supported geospatial query operators instead. Upgrading your MongoDB instance to 5.0 and setting featureCompatibilityVersion to 5.0 will delete any pre-existing geoHaystack indexes. The geoSearch command provides an interface to MongoDB's haystack index functionality. These indexes are useful for returning results based on location coordinates after collecting results based on some other query (i.e. a \"haystack.\") The geoSearch command accepts a document that contains the following fields. Field\nType\nDescription\ngeoSearch\nstring\nThe collection to query.\nsearch\ndocument\nQuery to filter documents.\nnear\narray\nCoordinates of a point.\nmaxDistance\nnumber\nOptional. Maximum distance from the specified point.\nlimit\nnumber\nOptional. Maximum number of documents to return.\nreadConcern\ndocument Optional. Specifies the read concern. Starting in MongoDB 3.6, the readConcern option has the following syntax: readConcern: { level: <value> } Possible read concern levels are:      * \"local\". This is the default read concern level for read operations against the primary and secondaries.  * \"available\". Available for read operations against the primary and secondaries. \"available\" behaves the same as \"local\" against the primary and non-sharded secondaries. The query returns the instance's most recent data.  * \"majority\". Available for replica sets that use WiredTiger storage engine.  * \"linearizable\". Available for read operations on the primary only. For more formation on the read concern levels, see Read Concern Levels. For more information on the read concern levels, see Read Concern Levels. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR  LIMIT \nUnless specified otherwise, the geoSearch command limits results to 50 documents. \nSHARDED CLUSTERS \ngeoSearch is not supported for sharded clusters. \nTRANSACTIONS \ngeoSearch can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. \nEXAMPLES \nConsider the following example: db.runCommand({   geoSearch : \"places\",   near: [ -73.9667, 40.78 ],   maxDistance : 6,   search : { type : \"restaurant\" },   limit : 30})  The above command returns all documents with a type of restaurant having a maximum distance of 6 units from the coordinates [ -73.9667, 40.78 ] in the collection places up to a maximum of 30 results. \nOVERRIDE DEFAULT READ CONCERN \nTo override the default read concern level of \"local\", use the readConcern option. The following operation on a replica set specifies a Read Concern of \"majority\" to read the most recent copy of the data confirmed as having been written to a majority of the nodes. \nNOTE Regardless of the read concern level, the most recent data on a node may not reflect the most recent version of the data in the system. db.runCommand(   {      geoSearch: \"places\",      near: [ -73.9667, 40.78 ],      search : { type : \"restaurant\" },      readConcern: { level: \"majority\" }    }) \n\n ←  Geospatial CommandsQuery and Write Operation Commands → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getShardMap/": " Docs Home → MongoDB Manual \nGETSHARDMAP \ngetShardMap \ngetShardMap is an internal command that supports the sharding functionality. ←  flushRouterConfiggetShardVersion → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/createIndexes/": " Docs Home → MongoDB Manual \nCREATEINDEXES \nOn this page    \n * Definition\n   \n * Syntax\n * Considerations\n * Behavior\n * Example\n * Output \nDEFINITION \ncreateIndexes \nBuilds one or more indexes on a collection. \nTIP In mongosh, this command can also be run through the db.collection.createIndex() and db.collection.createIndexes() helper methods.. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe createIndexes command takes the following form: db.runCommand(   {     createIndexes: <collection>,     indexes: [         {             key: {                 <key-value_pair>,                 <key-value_pair>,                 ...             },             name: <index_name>,             <option1>,             <option2>,             ...         },         { ... },         { ... }     ],     writeConcern: { <write concern> },     commitQuorum: <int|string>,     comment: <any>   } )  \nCOMMAND FIELDS \nThe createIndexes command takes the following fields: Field\nType\nDescription\ncreateIndexes\nstring\nThe collection for which to create indexes.\nindexes\narray\nSpecifies the indexes to create. Each document in the array specifies a separate index.\nwriteConcern\ndocument\nOptional. A document expressing the write concern. Omit to use the default write concern.\ncommitQuorum\ninteger or string Optional. The minimum number of data-bearing replica set members (i.e. commit quorum), including the primary, that must report a successful index build before the primary marks the indexes as ready. Starting in MongoDB v5.0, you can resume some interupted index builds when the commit quorum is set to \"votingMembers\". Replica set nodes in a commit quorum must have members[n].buildIndexes set to true. If any voting nodes have members[n].buildIndexes set to false, you can't use the default \"votingMembers\" commit quorum. Either configure all nodes with members[n].buildIndexes set to true, or select a different commit quorum. Supports the following values:      * \"votingMembers\" - all data-bearing voting replica set members (Default). A \"voting\" member is any replica set member where members[n].votes is greater than 0.  * \"majority\" - a simple majority of data-bearing replica set members.  * <int> - a specific number of data-bearing replica set members.  * 0 - Disables quorum-voting behavior. Members start the index build simultaneously but do not vote or wait for quorum before completing the index build. If you start an index build with a commit quorum of 0, you cannot later modify the commit quorum using setIndexCommitQuorum.  * A replica set tag name. New in version 4.4. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. Each document in the indexes array can take the following fields: Field\nType\nDescription\nkey\ndocument\n\n MongoDB supports several different index types including text, geospatial, and hashed indexes. See index types for more information. Changed in version 4.2: MongoDB 4.2 wildcard indexes support workloads where users query against custom fields or a large variety of fields in a collection:  * To create a wildcard index on all fields and subfields in a document, specify { \"$**\" : 1 } as the index key. You cannot specify a descending index key when creating a wildcard index.\n   \n   You can also either include or exclude specific fields and their subfields from the index using the optional wildcardProjection parameter.\n   \n   Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document:\n   \n   {  \"wildcardProjection\" : {    \"_id\" : 1,    \"<field>\" : 0|1  }}\n   \n   \n   \n   With the exception of explicitly including _id field, you cannot combine inclusion and exclusion statements in the wildcardProjection document.  * You can create a wildcard index on a specific field and its subpaths by specifying the full path to that field as the index key and append \"$**\" to the path:\n   \n   { \"path.to.field.$**\" : 1 }\n   \n   You cannot specify a descending index key when creating a wildcard index.\n   \n   The path-specific wildcard index syntax is incompatible with the wildcardProjection option. You cannot specify additional inclusions or exclusions on the specified path. The wildcard index key must use one of the syntaxes listed above. For example, you cannot specify a compound index key. For more complete documentation on wildcard indexes, including restrictions on their creation, see Wildcard Index Restrictions. The mongod featureCompatibilityVersion must be 4.2 to create wildcard indexes. For instructions on setting the fCV, see Set Feature Compatibility Version on MongoDB 6.0 Deployments. For examples of wildcard index creation, see Create a Wildcard Index. name\nstring\nA name that uniquely identifies the index.\nbackground\nboolean Optional. Deprecated in MongoDB 4.2.  * For feature compatibility version (fcv) \"4.0\", specifying background: true directs MongoDB to build the index in the background. Background builds do not block operations on the collection. The default value is false.  * Changed in version 4.2.\n   \n   For feature compatibility version (fcv) \"4.2\", all index builds use an optimized build process that holds the exclusive lock only at the beginning and end of the build process. The rest of the build process yields to interleaving read and write operations. MongoDB ignores the background option if specified. unique\nboolean Optional. Creates a unique index so that the collection will not accept insertion or update of documents where the index key value matches an existing value in the index. Specify true to create a unique index. The default value is false. The option is unavailable for hashed indexes. partialFilterExpression\ndocument Optional. If specified, the index only references documents that match the filter expression. See Partial Indexes for more information. A filter expression can include:  * equality expressions (i.e. field: value or using the $eq operator),  * $exists: true expression,  * $gt, $gte, $lt, $lte expressions,  * $type expressions,  * $and operator,  * $or operator,  * $in operator You can specify a partialFilterExpression option for all MongoDB index types. sparse\nboolean Optional. If true, the index only references documents with the specified field. These indexes use less space but behave differently in some situations (particularly sorts). The default value is false. See Sparse Indexes for more information. The following index types are sparse by default and ignore this option:  * 2dsphere  * 2d  * geoHaystack  * text For a compound index that includes 2dsphere index key(s) along with keys of other types, only the 2dsphere index fields determine whether the index references a document. MongoDB provides the option to create partial indexes. These offer a superset of the functionality of sparse indexes and are preferred instead. expireAfterSeconds\ninteger\n\n If you use TTL indexes created before MongoDB 5.0, or if you want to sync data created in MongDB 5.0 with a pre-5.0 installation, see Indexes Configured Using NaN to avoid misconfiguration issues. hidden\nboolean Optional. A flag that determines whether the index is hidden from the query planner. A hidden index is not evaluated as part of query plan selection. Default is false. To use the hidden option, you must have featureCompatibilityVersion set to 4.4 or greater. However, once hidden, the index remains hidden even with featureCompatibilityVersion set to 4.2 on MongoDB 4.4 binaries. New in version 4.4. storageEngine\ndocument Optional. Allows users to configure the storage engine on a per-index basis when creating an index. The storageEngine option should take the following form: storageEngine: { <storage-engine-name>: <options> }  Storage engine configuration options specified when creating indexes are validated and logged to the oplog during replication to support replica sets with members that use different storage engines. weights\ndocument\nOptional. For text indexes, a document that contains field and weight pairs. The weight is an integer ranging from 1 to 99,999 and denotes the significance of the field relative to the other indexed fields in terms of the score. You can specify weights for some or all the indexed fields. See Control Search Results with Weights to adjust the scores. The default value is 1.\ndefault_language\nstring\nOptional. For text indexes, the language that determines the list of stop words and the rules for the stemmer and tokenizer. See Text Search Languages for the available languages and Specify a Language for Text Index for more information and examples. The default value is english.\nlanguage_override\nstring\nOptional. For text indexes, the name of the field, in the collection's documents, that contains the override language for the document. The default value is language. See Use any Field to Specify the Language for a Document for an example.\ntextIndexVersion\ninteger Optional. The text index version number. Users can use this option to override the default version number. For available versions, see Versions. 2dsphereIndexVersion\ninteger Optional. The 2dsphere index version number. Users can use this option to override the default version number. For the available versions, see Versions. bits\ninteger Optional. For 2d indexes, the number of precision of the stored geohash value of the location data. The bits value ranges from 1 to 32 inclusive. The default value is 26. min\nnumber\nOptional. For 2d indexes, the lower inclusive boundary for the longitude and latitude values. The default value is -180.0.\nmax\nnumber\nOptional. For 2d indexes, the upper inclusive boundary for the longitude and latitude values. The default value is 180.0.\nbucketSize\nnumber For geoHaystack indexes, specify the number of units within which to group the location values; i.e. group in the same bucket those location values that are within the specified number of units to each other. The value must be greater than 0. collation\ndocument Optional. Specifies the collation for the index. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. If you have specified a collation at the collection level, then:  * If you do not specify a collation when creating the index, MongoDB creates the index with the collection's default collation.  * If you do specify a collation when creating the index, MongoDB creates the index with the specified collation. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. wildcardProjection\ndocument Optional. Allows users to include or exclude specific field paths from a wildcard index using the { \"$**\" : 1} key pattern. This option is only valid if creating a wildcard index on all document fields. You cannot specify this option if creating a wildcard index on a specific field path and its subfields, e.g. { \"path.to.field.$**\" : 1 } The wildcardProjection option takes the following form:\n\n  The <value> can be either of the following:  * 1 or true to include the field in the wildcard index.  * 0 or false to exclude the field from the wildcard index. Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document: {  \"wildcardProjection\" : {    \"_id\" : 1,    \"<field>\" : 0|1  }}  With the exception of explicitly including _id field, you cannot combine inclusion and exclusion statements in the wildcardProjection document. mongosh provides the methods db.collection.createIndex() and db.collection.createIndexes() as wrappers for the createIndexes command. \nCONSIDERATIONS \nMongoDB disallows the creation of version 0 indexes. To upgrade existing version 0 indexes, see Version 0 Indexes. \nINDEX NAMES  NOTE \nCHANGED IN MONGODB 4.2 Starting in version 4.2, for featureCompatibilityVersion set to \"4.2\" or greater, MongoDB removes the Index Name Length limit of 127 byte maximum. In previous versions or MongoDB versions with featureCompatibilityVersion (fCV) set to \"4.0\", index names must fall within the limit. Starting in version 4.2, the createIndexes command and mongosh helpers db.collection.createIndex() and db.collection.createIndexes() report an error if you create an index with one name, and then try to create the same index again but with another name. {   \"ok\" : 0,   \"errmsg\" : \"Index with name: x_1 already exists with a different name\",   \"code\" : 85,   \"codeName\" : \"IndexOptionsConflict\"}  In previous versions, MongoDB did not create the index again, but would return a response object with ok value of 1 and a note that implied that the index was not recreated. For example: {   \"numIndexesBefore\" : 2,   \"numIndexesAfter\" : 2,   \"note\" : \"all indexes already exist\",   \"ok\" : 1}  \nREPLICA SETS AND SHARDED CLUSTERS  NOTE \nREQUIRES FEATURECOMPATIBILITYVERSION 4.4+ Each mongod in the replica set or sharded cluster must have featureCompatibilityVersion set to at least 4.4 to start index builds simultaneously across replica set members. MongoDB 4.4 running featureCompatibilityVersion: \"4.2\" builds indexes on the primary before replicating the index build to secondaries. Starting with MongoDB 4.4, index builds on a replica set or sharded cluster build simultaneously across all data-bearing replica set members. For sharded clusters, the index build occurs only on shards containing data for the collection being indexed. The primary requires a minimum number of data-bearing voting members (i.e commit quorum), including itself, that must complete the build before marking the index as ready for use. See Index Builds in Replicated Environments for more information. To start an index build with a non-default commit quorum, specify the commitQuorum. MongoDB 4.4 adds the setIndexCommitQuorum command for modifying the commit quorum of an in-progress index build. In MongoDB 4.2 and earlier, index builds on a replica set or sharded cluster build on the primary first before replicating to the secondaries. See Index Builds In Replicated Environments (4.2) for the MongoDB 4.2 index build behavior. To minimize the impact of building an index on replica sets and sharded clusters, use a rolling index build procedure as described on Rolling Index Builds on Replica Sets. \nCOLLATION AND INDEX TYPES \nThe following indexes only support simple binary comparison and do not support collation:  * text indexes,  * 2d indexes, and  * geoHaystack indexes. \nTIP To create a text, a 2d, or a geoHaystack index on a collection that has a non-simple collation, you must explicitly specify {collation: {locale: \"simple\"} } when creating the index. \nSTABLE API \nWhen using Stable API V1:  * You cannot specify any of the following fields in the indexes array:\n   \n   * background\n   \n   * bucketSize\n   \n   * sparse\n   \n   * storageEngine  * You cannot create geoHaystack or text indexes. \nBEHAVIOR  CONCURRENCY \nChanged in version 4.2.\n\n For featureCompatibilityVersion \"4.0\", createIndexes uses the pre-4.2 index build process which by default obtains an exclusive lock on the parent database for the entire duration of the build process. The pre-4.2 build process blocks all operations on the database and all its collections until the operation completed. background indexes do not take an exclusive lock. For more information on the locking behavior of createIndexes, see Index Builds on Populated Collections. \nMEMORY USAGE LIMIT \ncreateIndexes supports building one or more indexes on a collection. createIndexes uses a combination of memory and temporary files on disk to complete index builds. The default limit on memory usage for createIndexes is 200 megabytes (for versions 4.2.3 and later) and 500 (for versions 4.2.2 and earlier), shared between all indexes built using a single createIndexes command. Once the memory limit is reached, createIndexes uses temporary disk files in a subdirectory named _tmp within the --dbpath directory to complete the build. You can override the memory limit by setting the maxIndexBuildMemoryUsageMegabytes server parameter. Setting a higher memory limit may result in faster completion of index builds. However, setting this limit too high relative to the unused RAM on your system can result in memory exhaustion and server shutdown. \nINDEX OPTIONS \nNON-HIDDEN OPTION \nThe hidden option can be changed without dropping and recreating the index. See Hidden Option. CHANGING INDEX OPTIONS \nCollation options on an existing index can be updated. To change other index options, drop the existing index with db.collection.dropIndex() then run createIndexes with the new options. COLLATION OPTION \nYou can create multiple indexes on the same key(s) with different collations. To create indexes with the same key pattern but different collations, you must supply unique index names. If you have specified a collation at the collection level, then:  * If you do not specify a collation when creating the index, MongoDB creates the index with the collection's default collation.  * If you do specify a collation when creating the index, MongoDB creates the index with the specified collation. \nTIP By specifying a collation strength of 1 or 2, you can create a case-insensitive index. Index with a collation strength of 1 is both diacritic- and case-insensitive. To use an index for string comparisons, an operation must also specify the same collation. That is, an index with a collation cannot support an operation that performs string comparisons on the indexed fields if the operation specifies a different collation. For example, the collection myColl has an index on a string field category with the collation locale \"fr\". db.myColl.createIndex( { category: 1 }, { collation: { locale: \"fr\" } } )  The following query operation, which specifies the same collation as the index, can use the index: db.myColl.find( { category: \"cafe\" } ).collation( { locale: \"fr\" } )  However, the following query operation, which by default uses the \"simple\" binary collator, cannot use the index: db.myColl.find( { category: \"cafe\" } )  For a compound index where the index prefix keys are not strings, arrays, and embedded documents, an operation that specifies a different collation can still use the index to support comparisons on the index prefix keys. For example, the collection myColl has a compound index on the numeric fields score and price and the string field category; the index is created with the collation locale \"fr\" for string comparisons: db.myColl.createIndex(   { score: 1, price: 1, category: 1 },   { collation: { locale: \"fr\" } } )  The following operations, which use \"simple\" binary collation for string comparisons, can use the index: db.myColl.find( { score: 5 } ).sort( { price: 1 } )db.myColl.find( { score: 5, price: { $gt: NumberDecimal( \"10\" ) } } ).sort( { price: 1 } )  The following operation, which uses \"simple\" binary collation for string comparisons on the indexed category field, can use the index to fulfill only the score: 5 portion of the query: db.myColl.find( { score: 5, category: \"cafe\" } )  HIDDEN OPTION \nNew in version 4.4. \nNOTE\n\n To change the hidden option for existing indexes, you can use the following mongosh methods:  * db.collection.hideIndex()  * db.collection.unhideIndex() For example,  * To change the hidden option for an index to true, use the db.collection.hideIndex() method:\n   \n   db.restaurants.hideIndex( { borough: 1, ratings: 1 } );\n   \n     * To change the hidden option for an index to false, use the db.collection.unhideIndex() method:\n   \n   db.restaurants.unhideIndex( { borough: 1, city: 1 } );\n   \n    \nTIP \nSEE ALSO: Hidden Indexes \nWILDCARD INDEXES \nNew in version 4.2.  * Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document:\n   \n   {  \"wildcardProjection\" : {    \"_id\" : 1,    \"<field>\" : 0|1  }}\n   \n   \n   \n   With the exception of explicitly including _id field, you cannot combine inclusion and exclusion statements in the wildcardProjection document.  * The mongod featureCompatibilityVersion must be 4.2 to create wildcard indexes. For instructions on setting the fCV, see Set Feature Compatibility Version on MongoDB 6.0 Deployments.  * Wildcard indexes do not support the following index types or properties:\n   \n   * Compound\n   \n   * TTL\n   \n   * Text\n   \n   * 2d (Geospatial)\n   \n   * 2dsphere (Geospatial)\n   \n   * Hashed\n   \n   * Unique\n   \n   \n   NOTE\n   \n   Wildcard Indexes are distinct from and incompatible with Wildcard Text Indexes. Wildcard indexes cannot support queries using the $text operator.\n   \n   For complete documentation on wildcard index restrictions, see Wildcard Index Restrictions. For examples of wildcard index creation, see Create a Wildcard Index. For complete documentation on Wildcard Indexes, see Wildcard Indexes. \nTRANSACTIONS \nChanged in version 4.4. Starting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. To use createIndexes in a transaction, the transaction must use read concern \"local\". If you specify a read concern level other than \"local\", the transaction fails. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction \nEXAMPLE \nThe following command builds two indexes on the inventory collection of the products database: db.getSiblingDB(\"products\").runCommand(  {    createIndexes: \"inventory\",    indexes: [        {            key: {                item: 1,                manufacturer: 1,                model: 1            },            name: \"item_manufacturer_model\",            unique: true        },        {            key: {                item: 1,                supplier: 1,                model: 1            },            name: \"item_supplier_model\",            unique: true        }    ],    writeConcern: { w: \"majority\" }  })  When the indexes successfully finish building, MongoDB returns a results document that includes a status of \"ok\" : 1. \nCREATE A WILDCARD INDEX\n\n For complete documentation on Wildcard Indexes, see Wildcard Indexes. The following lists examples of wildcard index creation:  * Create a Wildcard Index on a Single Field Path  * Create a Wildcard Index on All Field Paths  * Create a Wildcard Index on Multiple Specific Field Paths  * Create a Wildcard Index that Excludes Multiple Specific Field Paths CREATE A WILDCARD INDEX ON A SINGLE FIELD PATH \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index on the product_attributes field: use inventorydb.runCommand(  {    createIndexes: \"products_catalog\",    indexes: [      {        key: { \"product_attributes.$**\" : 1 },        name: \"wildcardIndex\"      }    ]  })  With this wildcard index, MongoDB indexes all scalar values of product_attributes. If the field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The wildcard index can support arbitrary single-field queries on product_attributes or one of its nested fields: db.products_catalog.find( { \"product_attributes.superFlight\" : true } )db.products_catalog.find( { \"product_attributes.maxSpeed\" : { $gt : 20 } } )db.products_catalog.find( { \"product_attributes.elements\" : { $eq: \"water\" } } )  \nNOTE The path-specific wildcard index syntax is incompatible with the wildcardProjection option. See the parameter documentation for more information. CREATE A WILDCARD INDEX ON ALL FIELD PATHS \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index on all scalar fields (excluding the _id field): use inventorydb.runCommand(  {    createIndexes: \"products_catalog\",    indexes: [      {        key: { \"$**\" : 1 },        name: \"wildcardIndex\"      }    ]  })  With this wildcard index, MongoDB indexes all scalar fields for each document in the collection. If a given field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The created index can support queries on any arbitrary field within documents in the collection:\n\n  \nNOTE Wildcard indexes omit the _id field by default. To include the _id field in the wildcard index, you must explicitly include it in the wildcardProjection document. See parameter documentation for more information. CREATE A WILDCARD INDEX ON MULTIPLE SPECIFIC FIELD PATHS \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index and uses the wildcardProjection option to include only scalar values of the product_attributes.elements and product_attributes.resistance fields in the index. use inventorydb.runCommand(  {    createIndexes: \"products_catalog\",    indexes: [      {        key: { \"$**\" : 1 },        \"wildcardProjection\" : {          \"product_attributes.elements\" : 1,          \"product_attributes.resistance\" : 1        },        name: \"wildcardIndex\"      }    ]  })  While the key pattern \"$**\" covers all fields in the document, the wildcardProjection field limits the index to only the included fields and their nested fields. If a field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The created index can support queries on any scalar field included in the wildcardProjection: db.products_catalog.find( { \"product_attributes.elements\" : { $eq: \"Water\" } } )db.products_catalog.find( { \"product_attributes.resistance\" : \"Bludgeoning\" } )  \nNOTE Wildcard indexes do not support mixing inclusion and exclusion statements in the wildcardProjection document except when explicitly including the _id field. For more information on wildcardProjection, see the parameter documentation. CREATE A WILDCARD INDEX THAT EXCLUDES MULTIPLE SPECIFIC FIELD PATHS \nConsider a collection products_catalog where documents may contain a product_attributes field. The product_attributes field can contain arbitrary nested fields, including embedded documents and arrays: {  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0b\"),  \"product_name\" : \"Blaster Gauntlet\",  \"product_attributes\" : {     \"price\" : {       \"cost\" : 299.99       \"currency\" : USD     }     ...  }},{  \"_id\" : ObjectId(\"5c1d358bf383fbee028aea0c\"),  \"product_name\" : \"Super Suit\",  \"product_attributes\" : {     \"superFlight\" : true,     \"resistance\" : [ \"Bludgeoning\", \"Piercing\", \"Slashing\" ]     ...  },} The following operation creates a wildcard index and uses the wildcardProjection document to index all scalar fields for each document in the collection, excluding the product_attributes.elements and product_attributes.resistance fields:\n\n  While the key pattern \"$**\" covers all fields in the document, the wildcardProjection field excludes the specified fields from the index. If a field is a nested document or array, the wildcard index recurses into the document/array and indexes all scalar fields in the document/array. The created index can support queries on any scalar field except those excluded by wildcardProjection: db.products_catalog.find( { \"product_attributes.maxSpeed\" : { $gt: 25 } } )db.products_catalog.find( { \"product_attributes.superStrength\" : true } )  \nNOTE Wildcard indexes do not support mixing inclusion and exclusion statements in the wildcardProjection document except when explicitly including the _id field. For more information on wildcardProjection, see the parameter documentation. \nCREATE INDEX WITH COMMIT QUORUM  NOTE \nREQUIRES FEATURECOMPATIBILITYVERSION 4.4+ Each mongod in the replica set or sharded cluster must have featureCompatibilityVersion set to at least 4.4 to start index builds simultaneously across replica set members. MongoDB 4.4 running featureCompatibilityVersion: \"4.2\" builds indexes on the primary before replicating the index build to secondaries. Starting with MongoDB 4.4, index builds on a replica set or sharded cluster build simultaneously across all data-bearing replica set members. For sharded clusters, the index build occurs only on shards containing data for the collection being indexed. The primary requires a minimum number of data-bearing voting members (i.e commit quorum), including itself, that must complete the build before marking the index as ready for use. See Index Builds in Replicated Environments for more information. Specify the commitQuorum option to the createIndexes operation to set the minimum number of data-bearing voting members (i.e commit quorum), including the primary, which must complete the index build before the primary marks the indexes as ready. The default commit quorum is votingMembers, or all data-bearing replica set members. The following operation creates an index with a commit quorum of \"majority\", or a simple majority of data-bearing members: db.getSiblingDB(\"examples\").runCommand(  {    createIndexes: \"invoices\",    indexes: [      {        key: { \"invoices\" : 1 },        \"name\" : \"invoiceIndex\"      }    ],    \"commitQuorum\" : \"majority\"  })  The primary marks index build as ready only after a simple majority of data-bearing voting members \"vote\" to commit the index build. For more information on index builds and the voting process, see Index Builds in Replicated Environments. \nOUTPUT \nThe createIndexes command returns a document that indicates the success of the operation. The document contains some but not all of the following fields, depending on outcome: createIndexes.createdCollectionAutomatically \nIf true, then the collection didn't exist and was created in the process of creating the index. createIndexes.numIndexesBefore \nThe number of indexes at the start of the command. createIndexes.numIndexesAfter \nThe number of indexes at the end of the command. createIndexes.ok \nA value of 1 indicates the indexes are in place. A value of 0 indicates an error. createIndexes.note \nThis note is returned if an existing index or indexes already exist. This indicates that the index was not created or changed. createIndexes.errmsg \nReturns information about any errors. createIndexes.code \nThe error code representing the type of error. ←  createcurrentOp → On this page  * Definition\n * Syntax\n * Considerations\n * Behavior\n * Example\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-free-monitoring/": " Docs Home → MongoDB Manual \nFREE MONITORING COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\ngetFreeMonitoringStatus\nReturns the free monitoring status.\nsetFreeMonitoring\nEnables/disables free monitoring during runtime. ←  whatsmyurigetFreeMonitoringStatus → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/shardingState/": " Docs Home → MongoDB Manual \nSHARDINGSTATE \nOn this page    \n * Syntax\n   \n * Behavior shardingState \nshardingState is an admin command that reports if mongod is a member of a sharded cluster. To run the command, you must run against the admin database. See db.adminCommand(). \nSYNTAX \nThe command has the following syntax: db.adminCommand(  {    shardingState: 1  })  \nBEHAVIOR \nFor shardingState to detect that a mongod is a member of a sharded cluster, the mongod must satisfy the following conditions:       1. the mongod is a primary member of a replica set, and  2. the mongod instance is a member of a sharded cluster. If shardingState detects that a mongod is a member of a sharded cluster, shardingState returns a document that resembles the following prototype: {  \"enabled\" : true,  \"configServer\" : \"<configdb-string>\",  \"shardName\" : \"<string>\",  \"shardHost\" : \"string:\",  \"versions\" : {       \"<database>.<collection>\" : Timestamp(<...>),       \"<database>.<collection>\" : Timestamp(<...>)  },  \"ok\" : 1,  \"$clusterTime\" : {     \"clusterTime\" : Timestamp(1510716515, 1),     \"signature\" : {        \"hash\" : BinData(0,\"B2ViX7XLzFLS5Fl9XEuFXbwKIM4=\"),        \"keyId\" : NumberLong(\"6488045157173166092\")     }  },  \"operationTime\" : Timestamp(1510716515, 1)}  Otherwise, shardingState will return the following document: {  \"enabled\" : false,  \"ok\" : 1,  \"$clusterTime\" : {     \"clusterTime\" : Timestamp(1510716515, 1),     \"signature\" : {        \"hash\" : BinData(0,\"B2ViX7XLzFLS5Fl9XEuFXbwKIM4=\"),        \"keyId\" : NumberLong(\"6488045157173166092\")     }  },  \"operationTime\" : Timestamp(1510716515, 1)}  The response from shardingState when used with a config server is: {   \"enabled\" : false,   \"ok\" : 1,   \"operationTime\" : Timestamp(1510767613, 1),   \"$gleStats\" : {      \"lastOpTime\" : Timestamp(0, 0),      \"electionId\" : ObjectId(\"7fffffff0000000000000001\")   },   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1510767613, 1),      \"signature\" : {         \"hash\" : BinData(0,\"IwBZ4SZjIMI5NdM62NObV/R31GM=\"),         \"keyId\" : NumberLong(\"6488693018630029321\")      }   }}  \nNOTE mongos instances do not provide the shardingState. \nWARNING This command obtains a write lock on the affected database and will block other operations until it has completed; however, the operation is typically short lived. ←  shardCollectionsplit → On this page  * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/killAllSessionsByPattern/": " Docs Home → MongoDB Manual \nKILLALLSESSIONSBYPATTERN \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Access Control\n * Examples \nDEFINITION \nkillAllSessionsByPattern \nThe killAllSessionsByPattern command kills all sessions that match any of the specified patterns. [1] \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     killAllSessionsByPattern: [ <pattern>, ... ]   })  \nCOMMAND FIELDS \nThe command takes an array of documents that specify the patterns to match: Pattern\nDescription\n{ lsid: { id : <UUID> } }\nSpecify the UUID portion of the session ID to kill.\n{ uid: <BinData> }\nSpecifies the hash of the owner of the sessions to kill.\n{ users: [ { user: <user>, db: <dbname> }, ... ] }\nSpecifies the owners of the sessions to kill. Requires additional privileges. See Access Control.\n{ roles: [ { role: <role>, db: <dbname> }, ... ] }\nSpecifies the roles assigned to the owners of the sessions to kill. Requires additional privileges. See Access Control. Specify an empty array to kill all sessions. [1] To view existing sessions, see $listSessions operation or $listLocalSessions. \nTIP \nSEE ALSO: killAllSessions [1](1, 2) The killAllSessionsByPattern operation ignores sessions that have transactions in prepared state. See Behavior for details. \nBEHAVIOR  IN-PROGRESS OPERATIONS \nKilling a session kills any in-progress operations in the session and closes any open cursors associated with these operations. \nKILLED SESSION AVAILABILITY \nThe killed session may still be listed as a current session, and future operations may use the killed session. To view existing sessions, see $listSessions operation or $listLocalSessions. \nSESSIONS WITH TRANSACTIONS IN PREPARED STATE \nThe killAllSessionsByPattern operation ignores sessions that have transactions in prepared state. Transactions in prepared state refer to transactions with write operations that span multiple shards whose commit coordinator has completed the \"sendingPrepare\" action. \nACCESS CONTROL \nIf the deployment enforces authentication/authorization, you must have the killAnySession privilege action to run the killAllSessionsByPattern command. For patterns that include users or roles, you must also have privileges that grant impersonate action on the cluster resource. \nNOTE Users can kill their own sessions even without the killAnySession privilege action. \nEXAMPLES  KILL ALL SESSIONS \nThe following operation kills all sessions: db.runCommand( { killAllSessionsByPattern: [ ] } )  \nKILL ALL SESSIONS FOR SPECIFIC USERS \nThe following operation kills all sessions that have the specified uid and whose owner has the specified role: db.runCommand( { killAllSessionsByPattern: [   { \"uid\" : BinData(0,\"oBRA45vMY78p1tv6kChjQPTdYsnCHi/kA/fFMZTIV1o=\") },   { roles: [ { role: \"readWrite\", db: \"test\" } ] }] } ) \n←  killAllSessionskillSessions → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Access Control\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/killOp/": " Docs Home → MongoDB Manual \nKILLOP \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Example \nDEFINITION \nkillOp \nTerminates an operation as specified by the operation ID. mongosh provides the db.killOp() helper. To find operations and their corresponding IDs, see $currentOp or db.currentOp(). The killOp command must be run against the admin database. To run killOp, use the db.runCommand( { <command> } ) method. \nSYNTAX \nThe command has the following form: db.adminCommand(   {     killOp: 1,     op: <opid>,     comment: <any>   })  \nCOMMAND FIELDS \nParameter\nType\nDescription\nop\nnumber\nAn operation ID.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nWARNING Terminate running operations with extreme caution. Only use killOp to terminate operations initiated by clients and do not terminate internal database operations. \nBEHAVIOR \nDo not use killOp to terminate an in-progress index builds in replica sets or sharded clusters. Use dropIndexes on the primary to drop the index. See Stop In-Progress Index Builds. \nACCESS CONTROL \nOn systems running with authorization, to kill operations not owned by the user, the user must have access that includes the killop privilege action. On mongod instances, users can kill their own operations even without the killop privilege action. \nSHARDED CLUSTER \nThe killOp command can be run on a mongos and can kill queries (i.e. read operations) that span shards in a cluster. The killOp command from the mongos does not propagate to the shards when the operation to be killed is a write operation. For information on how to list sharding operations that are active on a mongos, see the localOps parameter in $currentOp. For more information and examples on killing operations on a sharded cluster, see:  * Kill Read Operations  * Kill Write Operations \nEXAMPLE \nThe following example uses killOp to target the running operation with opid 3478. db.adminCommand( { \"killOp\": 1, \"op\": 3478 } )  The operation returns the following result: { \"info\" : \"attempting to kill op\", \"ok\" : 1 }  killOp reports success if it succeeded in marking the specified operation for termination. Operations may not actually be terminated until they reach an appropriate interruption point. Use $currentOp or db.currentOp() to confirm the target operation was terminated. ←  killCursorslistCollections → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/killCursors/": " Docs Home → MongoDB Manual \nKILLCURSORS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * killCursors and Transactions\n * Example \nDEFINITION \nkillCursors \nKills the specified cursor or cursors for a collection. MongoDB drivers use the killCursors command as part of the client-side cursor implementation. \nNOTE In general, applications should not use the killCursors command directly. The killCursors command must be run against the database of the collection whose cursors you wish to kill. To run killCursors, use the db.runCommand( { <command> } ) method. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     killCursors: <collection>,     cursors: [ <cursor id1>, ... ], comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nkillCursors\nstring\nThe name of the collection.\ncursors\narray\nThe ids of the cursors to kill.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nREQUIRED ACCESS  KILL OWN CURSORS \n * In MongoDB 4.2 and later, users can always kill their own cursors, regardless of whether the users have the privilege to killCursors. Cursors are associated with the users at the time of cursor creation.  * In MongoDB 3.6.3 through MongoDB 4.0.x, users require killCursors privilege to kill their own cursors. Cursors are associated with the users at the time of cursor creation. \nKILL ANY CURSOR \nIf a user possesses the killAnyCursor privilege, that user may kill any cursor, even cursors created by other users. \nKILLCURSORS AND TRANSACTIONS \nStarting in MongoDB 4.2, you cannot specify killCursors as the first operation in a transaction. \nEXAMPLE \nConsider the following find operation on the test.restaurants collection: use testdb.runCommand(   { find: \"restaurants\",     filter: { stars: 5 },     projection: { name: 1, rating: 1, address: 1 },     sort: { name: 1 },     batchSize: 5   })  which returns the following:\n\n  To kill this cursor, use the killCursors command. use test\ndb.runCommand( { killCursors: \"restaurants\", cursors: [ NumberLong(\"18314637080\") ] } )  killCursors returns the following operation details: {   \"cursorsKilled\" : [      NumberLong(\"18314637080\")   ],   \"cursorsNotFound\" : [ ],   \"cursorsAlive\" : [ ],   \"cursorsUnknown\" : [ ],   \"ok\" : 1} \n←  getParameterkillOp → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * killCursors and Transactions\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/logout/": " Docs Home → MongoDB Manual \nLOGOUT \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nlogout \nDeprecated since version 5.0. Attempting to use the logout command will write an error message to the log once per logout attempt. This command will be removed in a future release. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     logout: 1   })  \nEXAMPLE  NOTE If you're not logged in and using authentication, logout has no effect. Because MongoDB allows users defined in one database to have privileges on another database, you must call logout while using the same database context that you authenticated to. If you authenticated to a database such as users or $external, you must issue logout against this database in order to successfully log out. \nEXAMPLE Use the use <database-name> helper in mongosh, or the following db.getSiblingDB() method in an interactive mongosh session or in mongosh shell scripts to change the db object: db = db.getSiblingDB('<database-name>')  When you have set the database context and db object, you can use the logout to log out of database as in the following operation: db.runCommand( { logout: 1 } ) \n←  getnonceUser Management Commands → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/listShards/": " Docs Home → MongoDB Manual \nLISTSHARDS \nOn this page    \n * Definition\n   \n * Syntax\n * Example\n * Output \nDEFINITION \nlistShards \nThe listShards command returns a list of the configured shards in a sharded cluster. listShards is only available on mongos instances and must be issued against the admin database. \nSYNTAX \nThe command has the following syntax: db.runCommand(  {    listShards: 1  })  \nEXAMPLE \nThe following operation runs listShards against the mongos admin database: db.adminCommand({ listShards: 1 })  The following document is an example of the output from a listShards command: {  \"shards\": [    {      \"_id\": \"shard01\",      \"host\": \"shard01/host1:27018,host2:27018,host3:27018\",      \"state\": 1    },    {      \"_id\": \"shard02\",      \"host\": \"shard02/host4:27018,host5:27018,host6:27018\",      \"tags\": [ \"NYC\" ],      \"state\": 1    },    {      \"_id\": \"shard03\",      \"host\": \"shard03/host7:27018,host8:27018,host9:27018\",      \"maxSize\": NumberLong(\"1024\"),      \"state\": 1    }  ],  \"ok\": 1,  \"$clusterTime\" : {     \"clusterTime\" : Timestamp(1510716515, 1),     \"signature\" : {        \"hash\" : BinData(0,\"B2ViX7XLzFLS5Fl9XEuFXbwKIM4=\"),        \"keyId\" : NumberLong(\"6488045157173166092\")     }  },  \"operationTime\" : Timestamp(1510716515, 1)}  \nOUTPUT \nlistShards returns a document that includes:      * A shards field which contains an array of documents, each describing one shard. Each document may contain the following fields:\n   \n   Field\n   Description\n   _id\n   The name of the shard.\n   host\n   The hostname of the shard. If the shard is a replica set, host lists the hostname of each mongod instance in the replica set.\n   draining\n   If true, indicates that removeShard has been called for this shard and it is in the process of being drained.\n   tags\n   A list of zones to which the shard belongs.\n   maxSize\n   The configured maximum storage size of the shard in megabytes. This includes all data files on the shard including the local and admin databases. If exceeded, this shard will not receive any more chunks.\n   state\n   An internal field used during the addShard sequence to ensure that all steps and configuration necessary to add the shard to the cluster are completed.  * The ok status field, the operationTime field, and the $clusterTime field for the operation. For details on these fields, see Response. ←  isdbgridmedianKey → On this page  * Definition\n * Syntax\n * Example\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/planCacheClear/": " Docs Home → MongoDB Manual \nPLANCACHECLEAR \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Examples \nDEFINITION \nplanCacheClear \nRemoves cached query plans for a collection. Specify a query shape to remove cached query plans for that shape. Omit the query shape to clear all cached query plans. \nTIP In mongosh, this command can also be run through the PlanCache.clear() and PlanCache.clearPlansByQuery() helper methods. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {      planCacheClear: <collection>,      query: <query>,      sort: <sort>,      projection: <projection>,      comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following optional fields: Field\nType\nDescription\nquery\ndocument\nOptional. The query predicate of the query shape. Only the structure of the predicate, including the field names, are significant to the shape; the values in the query predicate are insignificant.\nprojection\ndocument\nOptional. The projection associated with the query shape.\nsort\ndocument\nOptional. The sort associated with the query shape.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. To see the query shapes for which cached query plans exist, see List Query Shapes. \nREQUIRED ACCESS \nOn systems running with authorization, a user must have access that includes the planCacheWrite action. \nEXAMPLES  CLEAR CACHED PLANS FOR A QUERY SHAPE \nIf a collection orders has the following query shape: {  \"query\" : { \"qty\" : { \"$gt\" : 10 } },  \"sort\" : { \"ord_date\" : 1 },  \"projection\" : { },  \"queryHash\" : \"9AAD95BE\"  // Available starting in MongoDB 4.2}  The following operation clears the query plan cached for the shape: db.runCommand(   {      planCacheClear: \"orders\",      query: { \"qty\" : { \"$gt\" : 10 } },      sort: { \"ord_date\" : 1 }   })  \nCLEAR ALL CACHED PLANS FOR A COLLECTION \nThe following example clears all the cached query plans for the orders collection: db.runCommand(   {      planCacheClear: \"orders\"   })  \nTIP \nSEE ALSO:  * PlanCache.clearPlansByQuery()  * PlanCache.clear() ←  Query Plan Cache CommandsplanCacheClearFilters → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/planCacheSetFilter/": " Docs Home → MongoDB Manual \nPLANCACHESETFILTER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Examples \nDEFINITION \nplanCacheSetFilter \nSet an index filter for a collection. If an index filter already exists for the query shape, the command overrides the previous index filter. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {      planCacheSetFilter: <collection>,      query: <query>,      sort: <sort>,      projection: <projection>,      collation: { <collation> },      indexes: [ <index1>, <index2>, ...],      comment: <any>   })  The query shape for the index filter is the combination of:      * query  * sort  * projection  * collation \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\nplanCacheSetFilter\nstring\nThe name of the collection for the index filter.\nquery\ndocument The query predicate for the index filter. Only the predicate structure, including the field names, is used in the index filter. The field values in the query predicate are not used. Therefore, the query predicate in an index filter is used by similar queries that differ only in the field values. sort\ndocument\nOptional. The sort for the index filter.\nprojection\ndocument\nOptional. The projection for the index filter.\ncollation\ndocument Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. Starting in MongoDB 6.0, an index filter uses the collation previously set using the planCacheSetFilter command. indexes\narray An array of index filters for the specified query shape. Specify the index filters as one of these arrays:  * Index specification documents. For example, [ { x : 1 }, ... ].  * Index names. For example, [ \"x_1\", ... ]. The query optimizer uses either a collection scan or the index arrays for the query plan. If the specified indexes do not exist or are hidden, the optimizer uses a collection scan. For multiple indexes with the same key pattern, you must specify the index as an array of names. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. Index filters only exist for the duration of the server process and do not persist after shutdown. To clear the index filters, use the planCacheClearFilters command. \nREQUIRED ACCESS \nA user must have access that includes the planCacheIndexFilter action. \nEXAMPLES  SET FILTER ON QUERY SHAPE CONSISTING OF PREDICATE ONLY \nThe following example creates an index filter on the orders collection such that for queries that consist only of an equality match on the status field without any projection and sort, the query optimizer evaluates only the two specified indexes and the collection scan for the winning plan:\n\n  In the query predicate, only the structure of the predicate, including the field names, are significant; the values are insignificant. As such, the created filter applies to the following operations: db.orders.find( { status: \"D\" } )db.orders.find( { status: \"P\" } )  To see whether MongoDB will apply an index filter for a query shape, check the indexFilterSet field of either the db.collection.explain() or the cursor.explain() method. \nSET FILTER ON QUERY SHAPE CONSISTING OF PREDICATE, PROJECTION, AND SORT \nThe following example creates an index filter for the orders collection. The filter applies to queries whose predicate is an equality match on the item field, where only the quantity field is projected and an ascending sort by order_date is specified. db.runCommand(   {      planCacheSetFilter: \"orders\",      query: { item: \"ABC\" },      projection: { quantity: 1, _id: 0 },      sort: { order_date: 1 },      indexes: [         { item: 1, order_date: 1 , quantity: 1 }      ]   })  For the query shape, the query optimizer will only consider indexed plans which use the index { item: 1, order_date: 1, quantity: 1 }. \nSET FILTER ON QUERY SHAPE CONSISTING OF PREDICATE AND COLLATION \nThe following example creates an index filter for the orders collection. The filter applies to queries whose predicate is an equality match on the item field and the collation en_US (English United States). db.runCommand(   {      planCacheSetFilter: \"orders\",      query: { item: \"Movie\" },      collation: { locale: \"en_US\" },      indexes: [         { item: 1, order_date: 1 , quantity: 1 }      ]   })  For the query shape, the query optimizer only uses indexed plans that use the index { item: 1, order_date: 1, quantity: 1 }. Starting in MongoDB 6.0, an index filter uses the collation previously set using the planCacheSetFilter command. \nTIP \nSEE ALSO:  * planCacheClearFilters  * planCacheListFilters  * $planCacheStats ←  planCacheListFiltersAuthentication Commands → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/drop/": " Docs Home → MongoDB Manual \nDROP \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior \nDEFINITION \ndrop \nThe drop command removes an entire collection from a database. \nTIP In mongosh, this command can also be run through the drop() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has following syntax: db.runCommand(   {     drop: <collection_name>,     writeConcern: <document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nDescription\ndrop\nThe name of the collection to drop.\nwriteConcern Optional. A document expressing the write concern of the drop command. Omit to use the default write concern. When issued on a sharded cluster, mongos converts the write concern of the drop command and its helper db.collection.drop() to \"majority\". comment Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. mongosh provides the equivalent helper method db.collection.drop(). \nBEHAVIOR \n * Starting in MongoDB 5.0, the drop command and the db.collection.drop() method will raise an error if passed an unrecognized parameter. In MongoDB 4.4 and earlier, unrecognized parameters are silently ignored.  * This command also removes any indexes associated with the dropped collection.  * Starting in MongoDB 4.4, the db.collection.drop() method and drop command abort any in-progress index builds on the target collection before dropping the collection. Prior to MongoDB 4.4, attempting to drop a collection with in-progress index builds results in an error, and the collection is not dropped.\n   \n   For replica sets or shard replica sets, aborting an index on the primary does not simultaneously abort secondary index builds. MongoDB attempts to abort the in-progress builds for the specified indexes on the primary and if successful creates an associated abort oplog entry. Secondary members with replicated in-progress builds wait for a commit or abort oplog entry from the primary before either committing or aborting the index build.  * The drop command and its helper db.collection.drop() create an invalidate for any Change Streams opened on the dropped collection.  * Dropping a collection deletes its associated zone/tag ranges.  * Starting in MongoDB 5.0, the drop command and the db.collection.drop() method return an error if you try to drop a collection in the admin database or the config database from a mongos. To drop these collections, connect to the config server and run the command there.\n   \n   \n   WARNING\n   \n   Dropping collections in the admin database or the config database can leave your cluster in an unusable state. \nRESOURCE LOCKING \nChanged in version 4.2. drop obtains an exclusive lock on the specified collection for the duration of the operation. All subsequent operations on the collection must wait until drop releases the lock. Prior to MongoDB 4.2, drop obtained an exclusive lock on the parent database, blocking all operations on the database and all its collections until the operation completed. ←  currentOpdropDatabase → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/fsyncUnlock/": " Docs Home → MongoDB Manual \nFSYNCUNLOCK \nOn this page    \n * Definition\n   \n * Syntax\n * Results\n * Examples \nDEFINITION \nfsyncUnlock \nReduces the lock taken by fsync (with the lock option) on a mongod instance by 1. \nIMPORTANT The fsync lock and fsyncUnlock operations maintain a lock count. Each fsync lock operation increments the lock count, and fsyncUnlock decrements the lock count. To unlock a mongod instance for writes, the lock count must be zero. That is, for a given number of fsync lock operations, you must issue a corresponding number of fsyncUnlock operations to unlock the instance for writes. fsyncUnlock is an administrative operation. Typically you will use fsyncUnlock following a database backup operation. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     fsyncUnlock: 1,     comment: <any>   })  The comment field is optional and may contain a comment of any data type. \nRESULTS \nThe operation returns a document with the following fields: Field\nDescription\ninfo\nInformation on the status of the operation\nlockCount (New in version 3.4)\nThe number of locks remaining on the instance after the operation.\nok\nThe status code. \nTIP mongosh provides the helper method db.fsyncUnlock(). \nEXAMPLES \nConsider a situation where db.fsyncLock() has been issued two times. The following fsyncUnlock operation reduces the locks taken by db.fsyncLock() by 1: db.adminCommand( { fsyncUnlock: 1 } )  The operation returns the following document: { \"info\" : \"fsyncUnlock completed\", \"lockCount\" : NumberLong(1), \"ok\" : 1 }  As the lockCount is greater than 0, the mongod instance is locked against writes. To unlock the instance for writes, run the unlock operation again: db.adminCommand( { fsyncUnlock: 1 } )  The operation returns the following document: { \"info\" : \"fsyncUnlock completed\", \"lockCount\" : NumberLong(0), \"ok\" : 1 }  The mongod instance is unlocked for writes. ←  fsyncgetAuditConfig → On this page  * Definition\n * Syntax\n * Results\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/addShard/": " Docs Home → MongoDB Manual \nADDSHARD \nOn this page    \n * Definition\n   \n * Syntax\n * Considerations\n * Examples \nDEFINITION \naddShard \nAdds a shard replica set to a sharded cluster. \nTIP In mongosh, this command can also be run through the sh.addShard() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     addShard: \"<replica_set>/<hostname><:port>\",     maxSize: <size>,     name: \"<shard_name>\"   })  \nCOMMAND FIELDS \nThe command contains the following fields: Field\nType\nDescription\naddShard\nstring The replica set name, hostname, and port of at least one member of the shard's replica set. Any additional replica set member hostnames must be comma separated. For example: <replica_set>/<hostname><:port>,<hostname><:port>, ... \nmaxSize\ninteger\nOptional. The maximum size in megabytes of the shard. If you set maxSize to 0, MongoDB does not limit the size of the shard.\nname\nstring\nOptional. A name for the shard. If this is not specified, MongoDB automatically provides a unique name. The addShard command stores shard configuration information in the config database. Always run addShard when using the admin database. Specify a maxSize when you have machines with different disk capacities, or if you want to limit the amount of data on some shards. The maxSize constraint prevents the balancer from migrating chunks to the shard when the totalSize returned from running listDatabases on the shard exceeds the value of maxSize. \nCONSIDERATIONS  BALANCING \nWhen you add a shard to a sharded cluster, you affect the balance of chunks among the shards of a cluster for all existing sharded collections. The balancer will begin migrating chunks so that the cluster will achieve balance. See Balancer Internals for more information. Chunk migrations can have an impact on disk space, as the source shard automatically archives the migrated documents by default. For details, see moveChunk directory. \nHIDDEN MEMBERS  IMPORTANT You cannot include a hidden member in the seed list provided to addShard. \nEXAMPLES \nThe following command adds a replica set as a shard: use admindb.runCommand( { addShard: \"repl0/mongodb3.example.net:27327\"} )  \nWARNING Do not use localhost for the hostname unless your config server is also running on localhost. ←  abortReshardCollectionaddShardToZone → On this page  * Definition\n * Syntax\n * Considerations\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/splitVector/": " Docs Home → MongoDB Manual \nSPLITVECTOR \nsplitVector \nIs an internal command that supports meta-data operations in sharded clusters. ←  splitunsetSharding → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/collStats/": " Docs Home → MongoDB Manual \nCOLLSTATS \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example\n * Output \nDEFINITION \ncollStats \nThe collStats command returns a variety of storage statistics for a given collection. \nTIP In mongosh, this command can also be run through the stats() helper method. Specific fields in the collStats output can be accessed using the dataSize(), estimatedDocumentCount(), isCapped(), latencyStats(), storageSize(), totalIndexSize(), and totalSize() helper methods. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. To run collStats, use the db.runCommand( { <command> } ) method. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     collStats: <string>,     scale: <int>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ncollStats\nstring\nThe name of the target collection.\nscale\nint Optional. The scale factor for the various size data (with the exception of those sizes that specify the unit of measurement in the field name). The value defaults to 1 to return size data in bytes. To display kilobytes rather than bytes, specify a scale value of 1024. If you specify a non-integer scale factor, MongoDB uses the integer part of the specified factor. For example, if you specify a scale factor of 1023.999, MongoDB uses 1023 as the scale factor. The scale factor rounds the affected size values to whole numbers. Starting in version 4.2, the output includes the scaleFactor used to scale the size values. \nNOTE Starting in version 4.2, MongoDB removes the MMAPv1 storage engine. In conjunction with this change, MongoDB removes the MMAPv1 specific collStats option verbose. \nBEHAVIOR  SCALED SIZES \nUnless otherwise specified by the metric name (such as \"bytes currently in the cache\"), values related to size are displayed in bytes and can be overridden by scale. The scale factor rounds the affected size values to whole numbers. \nACCURACY AFTER UNEXPECTED SHUTDOWN \nAfter an unclean shutdown of a mongod using the Wired Tiger storage engine, size statistics reported by collStats may be inaccurate. The amount of drift depends on the number of insert, update, or delete operations performed between the last checkpoint and the unclean shutdown. Checkpoints usually occur every 60 seconds. However, mongod instances running with non-default --syncdelay settings may have more or less frequent checkpoints. Run validate on each collection on the mongod to restore statistics after an unclean shutdown. After an unclean shutdown:      * validate updates the count statistic in the collStats output with the latest value.  * Other statistics like the number of documents inserted or removed in the collStats output are estimates. \nIN-PROGRESS INDEXES \nStarting in MongoDB 4.2, the collStats includes information on indexes currently being built. For details, see:  * collStats.nindexes  * collStats.indexDetails  * collStats.indexBuilds  * collStats.totalIndexSize  * collStats.indexSizes \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, collStats operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nNON-EXISTENT COLLECTIONS \nIf you run collStats for a non-existent collection, then depending on your database implementation, collStats might return 0 values in the output fields instead of returning an error. For example: db.runCommand( { collStats : \"nonExistentCollection\" } )  Example output with 0 values in the fields: {   ns: 'test.nonExistentCollection',   size: 0,   count: 0,   ...} \nEXAMPLE \nThe following operation runs the collStats command on the restaurants collection, specifying a scale of 1024 bytes: db.runCommand( { collStats : \"restaurants\", scale: 1024 } ) \n\n\n\n  \nOUTPUT \ncollStats.ns \nThe namespace of the current collection, which follows the format [database].[collection]. collStats.size \nThe total uncompressed size in memory of all records in a collection. The size does not include the size of any indexes associated with the collection, which the totalIndexSize field reports. The scale argument affects this value. Data compression does not affect this value. collStats.timeseries \ntimeseries appears when you run the collStats command on a time series collection. This document contains data for internal diagnostic use. collStats.count \nThe number of objects or documents in this collection. collStats.avgObjSize \nThe average size of an object in the collection. The scale argument does not affect this value. collStats.numOrphanDocs \nThe number of orphaned documents in the collection. New in version 6.0. collStats.storageSize \nThe total amount of storage allocated to this collection for document storage. The scale argument affects this value. If collection data is compressed (which is the default for WiredTiger), the storage size reflects the compressed size and may be smaller than the value for collStats.size. storageSize does not include index size. See totalIndexSize for index sizing. collStats.freeStorageSize \nUnavailable for the In-Memory Storage Engine The amount of storage available for reuse. The scale argument affects this value. The field is only available if storage is available for reuse (i.e. greater than zero). New in version 4.4. collStats.nindexes \nThe number of indexes on the collection. All collections have at least one index on the _id field. Starting in MongoDB 4.2, nindexes includes in its count those indexes currently being built. collStats.indexDetails \nA document that reports data from the WiredTiger storage engine for each index in the collection. Other storage engines will return an empty document. The fields in this document are the names of the indexes, while the values themselves are documents that contain statistics for the index provided by the storage engine. These statistics are for internal diagnostic use. Starting in MongoDB 4.2, indexDetails includes details on indexes currently being built. collStats.indexBuilds \nAn array that contains the names of the indexes that are currently being built on the collection. Once an index build completes, the index does not appear in the indexBuilds. New in version 4.2. collStats.totalIndexSize \nThe total size of all indexes. The scale argument affects this value. If an index uses prefix compression (which is the default for WiredTiger), the returned size reflects the compressed size for any such indexes when calculating the total. Starting in MongoDB 4.2, totalIndexSize includes in its total the size of those indexes currently being built. collStats.totalSize \nThe sum of the storageSize and totalIndexSize. The scale argument affects this value. New in version 4.4. collStats.indexSizes \nThis field specifies the key and size of every existing index on the collection. The scale argument affects this value. If an index uses prefix compression (which is the default for WiredTiger), the returned size reflects the compressed size. Starting in MongoDB 4.2, indexSizes includes sizes of indexes currently being built. collStats.scaleFactor \nThe scale value used by the command. If you had specified a non-integer scale factor, MongoDB uses the integer part of the specified factor. For example, if you specify a scale factor of 1023.999, MongoDB uses 1023 as the scale factor. New in version 4.2. collStats.capped \nThis field will be \"true\" if the collection is capped. collStats.max \nShows the maximum number of documents that may be present in a capped collection. collStats.maxSize \nShows the maximum size of a capped collection. collStats.wiredTiger \nwiredTiger only appears when using the WiredTiger storage engine. This document contains data reported directly by the WiredTiger engine and other data for internal diagnostic use. collStats.inMemory \ninMemory only appears when using the in-memory storage engine. This document contains data reported directly by the storage engine and other data for internal diagnostic use. ←  buildInfoconnPoolStats → On this page  * Definition\n * Syntax\n * Behavior\n * Example\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-crud/": " Docs Home → MongoDB Manual \nQUERY AND WRITE OPERATION COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\ndelete\nDeletes one or more documents.\nfind\nSelects documents in a collection or a view.\nfindAndModify\nReturns and modifies a single document.\ngetLastError\nRemoved in MongoDB 5.1.\ngetMore\nReturns batches of documents currently pointed to by the cursor.\ninsert\nInserts one or more documents.\nresetError\nRemoved in MongoDB 5.0. Resets the last error status.\nupdate\nUpdates one or more documents. ←  geoSearchdelete → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetSyncFrom/": " Docs Home → MongoDB Manual \nREPLSETSYNCFROM \nOn this page    \n * Description\n   \n * Syntax\n * Command Fields\n * Behavior \nDESCRIPTION \nreplSetSyncFrom \nTemporarily overrides the default sync target for the current mongod. This operation is useful for testing different patterns and in situations where a set member is not replicating from the desired host. \nTIP In mongosh, this command can also be run through the rs.syncFrom() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. Run replSetSyncFrom in the admin database. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     replSetSyncFrom: \"hostname<:port>\"   } )  \nCOMMAND FIELDS \nThe command takes the following field: Field\nType\nDescription\nreplSetSyncFrom\nstring The name and port number of the replica set member that this member should replicate from. Use the [hostname]:[port] form. Replica set members with 1 vote cannot sync from members with 0 votes. \nBEHAVIOR  SYNC LOGIC \nIf an initial sync operation is in progress when you run replSetSyncFrom, replSetSyncFrom stops the in-progress initial sync and restarts the sync process with the new target. Only modify the default sync logic as needed, and always exercise caution. \nTARGET \nThe member to sync from must be a valid source for data in the set. To sync from a member, the member must:      * Have data. It cannot be an arbiter, in startup or recovering mode, and must be able to answer data queries.  * Be accessible.  * Be a member of the same set in the replica set configuration.  * Build indexes with the members[n].buildIndexes setting.  * A different member of the set, to prevent syncing from itself. If you attempt to replicate from a member that is more than 10 seconds behind the current member, mongod will log a warning but will still replicate from the lagging member. See also Replication Lag and Flow Control. \nPERSISTENCE \nreplSetSyncFrom provide a temporary override of default behavior. mongod will revert to the default sync behavior in the following situations:  * The mongod instance restarts.  * The connection between the mongod and the sync target closes.  * If the sync target falls more than 30 seconds behind another member of the replica set. For more information the use of replSetSyncFrom, see Configure a Secondary's Sync Target. ←  replSetStepDownSharding Commands → On this page  * Description\n * Syntax\n * Command Fields\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/cloneCollectionAsCapped/": " Docs Home → MongoDB Manual \nCLONECOLLECTIONASCAPPED \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior \nDEFINITION \ncloneCollectionAsCapped \nThe cloneCollectionAsCapped command creates a new capped collection from an existing, non-capped collection within the same database. The operation does not affect the original non-capped collection. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     cloneCollectionAsCapped: <existing collection>,     toCollection: <capped collection>,     size: <capped size>,     writeConcern: <document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nDescription\ncloneCollectionAsCapped\nThe name of the existing collection to copy.\ntoCollection The name of the new capped collection to create. The name of the new capped collection must be distinct and cannot be the same as that of the original existing collection. size\nThe maximum size,in bytes, for the capped collection.\nwriteConcern\nOptional. A document expressing the write concern of the drop command. Omit to use the default write concern.\ncomment Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. The command copies an existing collection and creates a new capped collection with a maximum size specified by the capped size in bytes. To replace the original non-capped collection with a capped collection, use the convertToCapped command. \nBEHAVIOR \nIf the capped size is less than the size of the source collection, then not all documents in the source collection will exist in the destination capped collection. This holds a database exclusive lock for the duration of the operation. Other operations which lock the same database will be blocked until the operation completes. See What locks are taken by some common client operations? for operations that lock the database. ←  Administration CommandscollMod → On this page  * Definition\n * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dropConnections/": " Docs Home → MongoDB Manual \nDROPCONNECTIONS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Access Control\n * Behavior\n * Example \nDEFINITION \ndropConnections \nNew in version 4.2. The dropConnections command drops the mongod / mongos instance's outgoing connections to the specified hosts. The dropConnections must be run against the admin database. \nSYNTAX \nThe command has following syntax: db.adminCommand(   {     dropConnections: 1,     hostAndPort : [ \"host1:port1\", \"host2:port2\", ... ],     comment: <any>   })  \nCOMMAND FIELDS \nThe command requires the following field: Field\nType\nDescription\nhostAndPort\narray\nEach array element represents the hostname and port of a remote machine.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nACCESS CONTROL \nIf the deployment enforces authentication/authorization, the dropConnections command requires the dropConnections action on the cluster resource. Create a user-defined role in the admin database where the privilege array includes the following document: { \"resource\" : { \"cluster\" : true } }, \"actions\" : [ \"dropConnections\" ] }   * Use db.createUser() to create a user on the admin database with the custom role.\n   \n   or  * Use db.grantRolesToUser() to grant the role to an existing user on the admin database. For example, the following operation creates a user-defined role on the admin database with the privileges to support dropConnections: db.getSiblingDB(\"admin\").createRole(  {    \"role\" : \"dropConnectionsRole\",    \"privileges\" : [      {        \"resource\" : { \"cluster\" : true },        \"actions\" : [ \"dropConnections\" ]      }    ],    \"roles\" : []  })  Assign the custom role to a user on the admin database: db.getSiblingDB(\"admin\").createUser(  {    \"user\" : \"dropConnectionsUser\",    \"pwd\" : \"replaceThisWithASecurePassword\",    \"roles\" : [ \"dropConnectionsRole\" ]  })  The created user can execute dropConnections. For more examples of user creation, see Create a User. For a tutorial on adding privileges to an existing database user, see Modify Access for an Existing User. \nBEHAVIOR \ndropConnections silently ignores hostAndPort elements that do not include both the hostname and port of the remote machine. \nEXAMPLE \nConsider a replica set with a recently removed member at oldhost.example.com:27017. Running the following dropConnections command against each active replica set member ensures there are no remaining outgoing connections to oldhost.example.com:27017: db.adminCommand(  {    \"dropConnections\" : 1,    \"hostAndPort\" : [      \"oldhost.example.com:27017\"    ]  })  The command returns output similar to the following: { \"ok\" : 1, \"$clusterTime\" : {   \"clusterTime\" : Timestamp(1551375968, 1),   \"signature\" : {     \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"),     \"keyId\" : NumberLong(0)   } }, \"operationTime\" : Timestamp(1551375968, 1)}  You can confirm the status of the connection pool for the mongod or mongos using the connPoolStats command. ←  dropDatabasedropIndexes → On this page  * Definition\n * Syntax\n * Command Fields\n * Access Control\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dropRole/": " Docs Home → MongoDB Manual \nDROPROLE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \ndropRole \nDeletes a user-defined role from the database on which you run the command. \nTIP In mongosh, this command can also be run through the db.dropRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dropRole: \"<role>\",     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\ndropRole\nstring\nThe name of the user-defined role to remove from the database.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nREQUIRED ACCESS \nYou must have the dropRole action on a database to drop a role from that database. \nEXAMPLE \nThe following operations remove the readPrices role from the products database: use productsdb.runCommand(   {     dropRole: \"readPrices\",     writeConcern: { w: \"majority\" }   }) \n←  createRoledropAllRolesFromDatabase → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dataSize/": " Docs Home → MongoDB Manual \nDATASIZE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Example \nDEFINITION \ndataSize \nThe dataSize command returns the size in bytes for the specified data. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dataSize: <string>,     keyPattern: <document>,     min: <document>,     max: <document>,     estimate: <boolean>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ndataSize\nstring\nThe name of the target collection.\nkeyPattern\ndocument Optional. The collection's key pattern to examine. The collection must have an index with the corresponding pattern. Otherwise dataSize returns an error message. min\ndocument\nOptional. The lower bound of the key range to be examined.\nmax\ndocument\nOptional. The upper bound of the key range to be examined.\nestimate\nboolean Optional. When true, dataSize estimates the data size by assuming that all documents in the specified range are uniformly sized as per the collection's average object size. The collection's average object size is obtained from the avgObjSize field in the output of the collStats command. Defaults to false. \nEXAMPLE \nThe following operation runs the dataSize command on the database.collection collection, specifying a key pattern of {field: 1} with the lower bound of the range of keys to be examined being {field: 10} and the upper bound of the key to be examined being {field: 100}. db.runCommand({ dataSize: \"database.collection\", keyPattern: { field: 1 }, min: { field: 10 }, max: { field: 100 } })  This returns a document with the size in bytes for all matching documents. Replace database.collection with the database and collection from your deployment. The amount of time required to return dataSize depends on the amount of data in the collection. ←  connectionStatusdbHash → On this page  * Definition\n * Syntax\n * Command Fields\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/resetError/": " Docs Home → MongoDB Manual \nRESETERROR \nresetError \nRemoved in MongoDB 5.0. The resetError command resets the last error status. \nTIP \nSEE ALSO: db.resetError() (also removed in MongoDB 5.0) ←  insertupdate → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/logApplicationMessage/": " Docs Home → MongoDB Manual \nLOGAPPLICATIONMESSAGE \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior \nDEFINITION \nlogApplicationMessage  NOTE Available only in MongoDB Enterprise and MongoDB Atlas. The logApplicationMessage command allows users to post a custom message to the audit log. If running with authorization, users must have clusterAdmin role, or roles that inherit from clusterAdmin, to run the command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     logApplicationMessage: <string>   })  \nBEHAVIOR \nMongoDB associates these custom messages with the audit operation applicationMessage, and the messages are subject to any filtering. ←  System Events Auditing CommandsDefault MongoDB Port → On this page  * Definition\n * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/planCacheClearFilters/": " Docs Home → MongoDB Manual \nPLANCACHECLEARFILTERS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Examples \nDEFINITION \nplanCacheClearFilters \nRemoves index filters on a collection. Although index filters only exist for the duration of the server process and do not persist after shutdown, you can also clear existing index filters with the planCacheClearFilters command. Specify the query shape to remove a specific index filter. Omit the query shape to clear all index filters on a collection. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {      planCacheClearFilters: <collection>,      query: <query pattern>,      sort: <sort specification>,      projection: <projection specification>,      collation: { <collation> },      comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\nplanCacheClearFilters\nstring\nThe name of the collection to remove the index filters from.\nquery\ndocument Optional. The query predicate for the index filter to remove. If omitted, the command clears all index filters from the collection. The values in the query predicate do not:      * Determine the query shape.  * Need to match the values returned by planCacheListFilters. sort\ndocument\nOptional. The sort for the index filter to remove, if any.\nprojection\ndocument\nOptional. The projection for the index filter to remove, if any.\ncollation\ndocument Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. Starting in MongoDB 6.0, an index filter uses the collation previously set using the planCacheSetFilter command. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nREQUIRED ACCESS \nA user must have access that includes the planCacheIndexFilter action. \nEXAMPLES  CLEAR SPECIFIC INDEX FILTER ON COLLECTION \nThe orders collection contains the following index filters: {  \"query\" : { \"status\" : \"A\" },  \"sort\" : { \"ord_date\" : -1 },  \"projection\" : { },  \"indexes\" : [ { \"status\" : 1, \"cust_id\" : 1 } ]}\n{  \"query\" : { \"status\" : \"A\" },  \"sort\" : { },  \"projection\" : { },  \"indexes\" : [ { \"status\" : 1, \"cust_id\" : 1 } ]}\n{  \"query\": { \"item\": \"Movie\" },  \"collation\": { locale: \"en_US\" },  \"indexes\": [ { \"item\": 1, \"order_date\": 1 , \"quantity\": 1 } ]}  \nNOTE Starting in MongoDB 6.0, an index filter uses the collation previously set using the planCacheSetFilter command.\n\n db.runCommand(   {      planCacheClearFilters: \"orders\",      query: { \"status\" : \"A\" }   })  Because the values in the query predicate are insignificant in determining the query shape, the following command would also remove the second index filter: db.runCommand(   {      planCacheClearFilters: \"orders\",      query: { \"status\" : \"P\" }   })  \nCLEAR ALL INDEX FILTERS ON A COLLECTION \nThe following example clears all index filters on the orders collection: db.runCommand(   {      planCacheClearFilters: \"orders\"   })  \nCLEAR INDEX FILTER CONTAINING A QUERY AND A COLLATION \nThe following example clears the index filter containing the query on Movie and the collation en_US for the orders collection: db.runCommand(   {      planCacheClearFilters: \"orders\",      query: { item: \"Movie\" },      collation: { locale: \"en_US\" }   })  \nTIP \nSEE ALSO:  * planCacheListFilters  * planCacheSetFilter ←  planCacheClearplanCacheListFilters → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetMaintenance/": " Docs Home → MongoDB Manual \nREPLSETMAINTENANCE \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior \nDEFINITION \nreplSetMaintenance \nThe replSetMaintenance admin command enables or disables the maintenance mode for a secondary member of a replica set. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     replSetMaintenance: <boolean>   })  \nBEHAVIOR \nConsider the following behavior when running the replSetMaintenance command:      * You cannot run the command on the Primary.  * You must run the command against the admin database.  * When enabled replSetMaintenance: true, the member enters the RECOVERING state. While the secondary is RECOVERING:\n   \n   * The member is not accessible for read operations.\n   \n   * The member continues to sync its oplog from the Primary.  * When a node receives a replSetMaintenance: true request, it adds a maintenance mode task to a queue of tasks. If the queue of tasks was empty and now is not, the node will transition to RECOVERING state and begin to reject read requests. When a node receives a replSetMaintenance: false request, it removes a maintenance mode task from the queue (even if that task was initiated by a different client). If the request empties the maintenance mode task queue, the node will return to SECONDARY state.  * If you want to prevent a node from servicing reads, consider using Hidden Replica Set Members instead. ←  replSetInitiatereplSetReconfig → On this page  * Definition\n * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dropUser/": " Docs Home → MongoDB Manual \nDROPUSER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \ndropUser \nRemoves the user from the database on which you run the command. \nTIP In mongosh, this command can also be run through the db.dropUser() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dropUser: \"<user>\",     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\ndropUser\nstring\nThe name of the user to delete. You must issue the dropUser command while using the database where the user exists.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. Before dropping a user who has the userAdminAnyDatabase role, ensure you have at least another user with user administration privileges. \nREQUIRED ACCESS \nYou must have the dropUser action on a database to drop a user from that database. \nEXAMPLE \nThe following sequence of operations in mongosh removes reportUser1 from the products database: use productsdb.runCommand( {   dropUser: \"reportUser1\",   writeConcern: { w: \"majority\", wtimeout: 5000 }} ) \n←  dropAllUsersFromDatabasegrantRolesToUser → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/shardConnPoolStats/": " Docs Home → MongoDB Manual \nSHARDCONNPOOLSTATS \nOn this page    \n * Definition\n   \n * Output \nDEFINITION \nshardConnPoolStats \nRemoved in MongoDB 5.0. Use connPoolStats instead. Returns information on the pooled and cached connections in the sharded connection pool. The command also returns information on the per-thread connection cache in the connection pool. The shardConnPoolStats command uses the following syntax: { shardConnPoolStats: 1 }  The sharded connection pool is specific to connections between members in a sharded cluster. The mongos instances in a cluster use the connection pool to execute client reads and writes. The mongod instances in a cluster use the pool when issuing mapReduce to query temporary collections on other shards. When the cluster requires a connection, MongoDB pulls a connection from the sharded connection pool into the per-thread connection cache. MongoDB returns the connection to the connection pool after every operation. \nOUTPUT \nshardConnPoolStats.hosts \nDisplays connection status for each config server, replica set, and standalone instance in the cluster. shardConnPoolStats.hosts.<host>.available \nThe number of connections available for this host to connect to the mongos. shardConnPoolStats.hosts.<host>.created \nThe number of connections the host has ever created to connect to the mongos. shardConnPoolStats.replicaSets \nDisplays information specific to replica sets. shardConnPoolStats.replicaSets.<name>.host \nHolds an array of documents that report on each replica set member. These values derive from the replica set status values. shardConnPoolStats.replicaSets.<name>.host[n].addr \nThe host address in the format [hostname]:[port]. shardConnPoolStats.replicaSets.<name>.host[n].ok \nThis field is for internal use. Reports false when the mongos either cannot connect to instance or received a connection exception or error. shardConnPoolStats.replicaSets.<name>.host[n].ismaster \nThe host is the replica set's primary if this is true. shardConnPoolStats.replicaSets.<name>.host[n].hidden \nThe host is a hidden member of the replica set if this is true. shardConnPoolStats.replicaSets.<name>.host[n].secondary \nThe host is a hidden member of the replica set if this is true. The host is a secondary member of the replica set if this is true. shardConnPoolStats.replicaSets.<name>.host[n].pingTimeMillis \nThe latency, in milliseconds, from the mongos to this member. shardConnPoolStats.replicaSets.<name>.host[n].tags \nA tags document contains user-defined tag field and value pairs for the replica set member. { \"<tag1>\": \"<string1>\", \"<tag2>\": \"<string2>\",... }      * For read operations, you can specify a tag set in the read preference to direct the operations to replica set member(s) with the specified tag(s).  * For write operations, you can create a customize write concern using settings.getLastErrorModes and settings.getLastErrorDefaults. For more information, see Configure Replica Set Tag Sets. \nTIP \nSEE ALSO: members[n].tags shardConnPoolStats.createdByType \nThe number connections in the cluster's connection pool. shardConnPoolStats.createdByType.master \nThe number of connections to a shard. shardConnPoolStats.createdByType.set \nThe number of connections to a replica set. shardConnPoolStats.createdByType.sync \nThe number of connections to the config database. shardConnPoolStats.totalAvailable \nThe number of connections available from the mongos to the config servers, replica sets, and standalone mongod instances in the cluster. shardConnPoolStats.totalCreated \nThe number of connections the mongos has ever created to other members of the cluster. shardConnPoolStats.threads \nDisplays information on the per-thread connection cache. shardConnPoolStats.threads.hosts \nDisplays each incoming client connection. For a mongos, this array field displays one document per incoming client thread. For a mongod, the array displays one entry per incoming sharded mapReduce client thread. shardConnPoolStats.threads.hosts.host\n\n shardConnPoolStats.threads.hosts.created \nThe number of times the host pulled a connection from the pool. shardConnPoolStats.threads.hosts.avail \nThe thread's availability. shardConnPoolStats.threads.seenNS \nThe namespaces used on this connection thus far. ←  serverStatustop → On this page  * Definition\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/compact/": " Docs Home → MongoDB Manual \nCOMPACT \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * compact Required Privileges\n * Behavior \nDEFINITION \ncompact \nRewrites and defragments all data and indexes in a collection. On WiredTiger databases, this command will release unneeded disk space to the operating system. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     compact: <collection name>   })  \nCOMMAND FIELDS \nThe command takes the following fields: \nNOTE \nSTARTING IN MONGODB 4.2 MongoDB removes the MMAPv1 storage engine and the MMAPv1 specific options paddingFactor, paddingBytes, preservePadding for compact. Field\nType\nDescription\ncompact\nstring\nThe name of the collection.\nforce\nflag Changed in version 4.4. Optional. Starting in v4.4, if specified, forces compact to run on the primary in a replica set. Before v4.4, this boolean field enabled compact to run on the primary in a replica set if the value was true and returned an error when run on a primary if the value was false, because the command blocked all other operations. Starting in v4.4, compact does not block MongoDB CRUD Operations on the database it is compacting. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nWARNING Always have an up-to-date backup before performing server maintenance such as the compact operation. \nCOMPACT REQUIRED PRIVILEGES \nFor clusters enforcing authentication, you must authenticate as a user with the compact privilege action on the target collection. The dbAdmin role provides the required privileges for running compact against non-system collections. For more information on configuring the resource document, see Resource Document. To add the dbAdmin or the custom role to an existing user, use db.grantRolesToUser() or db.updateUser(). The following operation grants the custom compact role to the myCompactUser on the admin database: use admindb.grantRolesToUser(\"myCompactUser\", [ \"dbAdmin\" | \"myCustomCompactRole\" ] )  To add the dbAdmin or the custom role to a new user, specify the role to the roles array of the db.createUser() method when creating the user. use admindb.createUser(  {     user: \"myCompactUser\",     pwd: \"myCompactUserPassword\",     roles: [       { role: \"dbAdmin\", db: \"<database>\" } | \"myCustomCompactRole\"     ]   })  \nBEHAVIOR  BLOCKING \nBlocking behavior is version specific. Version\nBlocking Behavior\nPre 4.4\ncompact blocks all read and write activity.\n4.4  * compact blocks these operations:\n   \n   * db.collection.drop()\n   \n   * db.collection.createIndex()\n   \n   * db.collection.createIndexes()\n   \n   * db.collection.dropIndex()\n   \n   * db.collection.dropIndexes()\n   \n   * collMod  * All other operations are permitted. Post 4.4.17, 5.0.12, 6.0.2  * compact blocks these operations:\n   \n   * db.collection.drop()\n   \n   * db.collection.createIndex()\n   \n   * db.collection.createIndexes()\n   \n   * db.collection.dropIndex()\n   \n   * db.collection.dropIndexes()\n   \n   * collMod  * All other operations are permitted.  * The locking order changes. To run compact in a replica set, see Replica Sets for additional considerations. \nMONITORING PROGRESS\n\n \nOPERATION TERMINATION \nIf you terminate the operation with the db.killOp() method or restart the server before the compact operation has finished, be aware of the following:  * If you have journaling enabled, the data remains valid and usable, regardless of the state of the compact operation. You may have to manually rebuild the indexes.  * If you do not have journaling enabled and the mongod or compact terminates during the operation, it is impossible to guarantee that the data is in a valid state.  * In either case, much of the existing free space in the collection may become un-reusable. In this scenario, you should rerun the compaction to completion to restore the use of this free space. \nDISK SPACE \nTo see how the storage space changes for the collection, run the collStats command before and after compaction. On WiredTiger, compact attempts to reduce the required storage space for data and indexes in a collection, releasing unneeded disk space to the operating system. The effectiveness of this operation is workload dependent and no disk space may be recovered. This command is useful if you have removed a large amount of data from the collection, and do not plan to replace it. compact may require additional disk space to run on WiredTiger databases. \nREPLICA SETS \nYou can use compact on collections and indexes that are stored in a replica set, however there are some important considerations:  * The primary node does not replicate the compact command to the secondaries.  * The compact command blocks writes while it runs.  * You should run compact on secondary nodes whenever possible. If you cannot run compact on secondaries, see the force option.  * Starting in MongoDB 6.0.2 (and 5.0.12, and 4.4.17):\n   \n   * A secondary node can replicate while compact is running.\n   \n   * Reads are permitted. To run compact on a cluster 1 COMPACT THE SECONDARY NODES. \nRun compact on one of the secondary nodes. When compact finishes, repeat the operation on each of the remaining secondaries in turn. 2 REASSIGN THE PRIMARY NODE. \nTo step down the current primary and trigger an election, use the rs.stepDown() method. To nominate a particular secondary node, adjust the member priority. 3 COMPACT THE OLD PRIMARY. \nAfter stepping down, the old primary node becomes a secondary node. Run compact on the old primary node. VERSION SPECIFIC CONSIDERATIONS FOR SECONDARY NODES \nBlocking behavior on secondary nodes is version specific. Version\nBlocking Behavior\nPre 4.4  * compact blocks all read and write activity.  * No replication possible.  * Secondary node is in RECOVERING state. 4.4  * compact blocks all write activity.  * No replication possible.  * Reads not permitted. Post 4.4.17, 5.0.12, 6.0.2  * compact blocks all write activity.  * No replication possible.  * Reads permitted. When compact completes, the secondary returns to the SECONDARY state. For more information about replica set member states, see See Replica Set Member States. For replica set maintenance and availability, see Perform Maintenance on Replica Set Members. \nSHARDED CLUSTERS \ncompact only applies to mongod instances. In a sharded environment, run compact on each shard separately as a maintenance operation. You cannot issue compact against a mongos instance. \nCAPPED COLLECTIONS \nOn WiredTiger, the compact command will attempt to compact the collection. \nINDEX BUILDING \nmongod rebuilds all indexes in parallel following the compact operation. ←  collModconvertToCapped → On this page  * Definition\n * Syntax\n * Command Fields\n * compact Required Privileges\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dropIndexes/": " Docs Home → MongoDB Manual \nDROPINDEXES \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Examples \nDEFINITION \ndropIndexes \nChanged in version 6.0. The dropIndexes command drops one or more indexes (except the index on the _id field and the last remaining shard key index, if one exists) from the specified collection. \nTIP In mongosh, this command can also be run through the db.collection.dropIndex() and db.collection.dropIndexes() helper methods.. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dropIndexes: <string>,     index: <string|document|arrayofstrings>,     writeConcern: <document>, comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ndropIndexes\nString\nThe name of the collection whose indexes to drop.\nindex\nstring or document or array of strings The index or indexes to drop.      * To drop all indexes except the _id index and the last remaining shard key index from the collection if one exists, specify \"*\".  * To drop a single index, specify either the index name, the index specification document (unless the index is a text index), or an array of the index name. To drop a text index, specify the index names instead of the index specification document. If this index is the last remaining shard key index, dropIndexes raises an error.  * To drop multiple indexes (Available starting in MongoDB 4.2), specify an array of the index names. writeConcern\ndocument\nOptional. A document expressing the write concern of the drop command. Omit to use the default write concern.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR \nStarting in MongoDB 6.0, dropIndexes raises an error if you attempt to use it to remove the last remaining shard key compatible index. Passing \"*\" to dropIndexes drops all indexes except the _id index and the last remaining shard key compatible index, if one exists. Starting in MongoDB 5.2, you can use dropIndexes to drop existing indexes on the same collection even if there is a build in progress on another index. In earlier versions, attempting to drop a different index during an in-progress index build results in a BackgroundOperationInProgressForNamespace error. \nKILL RELATED QUERIES ONLY \nStarting in MongoDB 4.2, the dropIndexes operation only kills queries that are using the index being dropped. This may include queries considering the index as part of query planning. Prior to MongoDB 4.2, dropping an index on a collection would kill all open queries on the collection. \nRESOURCE LOCKING \nChanged in version 4.2. dropIndexes obtains an exclusive lock on the specified collection for the duration of the operation. All subsequent operations on the collection must wait until dropIndexes releases the lock. Prior to MongoDB 4.2, dropIndexes obtained an exclusive lock on the parent database, blocking all operations on the database and all its collections until the operation completed. \nINDEX NAMES \nIf the method is passed an array of index names that includes a non-existent index, the method errors without dropping any of the specified indexes. \n_ID INDEX \nYou cannot drop the default index on the _id field. \nTEXT INDEXES \nTo drop a text index, specify the index name instead of the index specification document. \nSTOP IN-PROGRESS INDEX BUILDS\n\n For replica sets, run dropIndexes on the primary. The primary stops the index build and creates an associated \"abortIndexBuild\" oplog entry. Secondaries which replicate the \"abortIndexBuild\" oplog entry stop the in-progress index build and discard the build job. See Index Build Process for detailed documentation on the index build process. Use currentOp to identify the index builds associated with a createIndexes or db.collection.createIndexes() operation. See Active Indexing Operations for an example. \nHIDDEN INDEXES \nStarting in version 4.4, MongoDB adds the ability to hide or unhide indexes from the query planner. By hiding an index from the planner, users can evaluate the potential impact of dropping an index without actually dropping the index. If after the evaluation, the user decides to drop the index, the user can drop the hidden index; i.e. you do not need to unhide it first to drop it. If, however, the impact is negative, the user can unhide the index instead of having to recreate a dropped index. And because indexes are fully maintained while hidden, the indexes are immediately available for use once unhidden. For more information on hidden indexes, see Hidden Indexes. \nEXAMPLES \n * To drop all non-_id indexes , specify \"*\" for the index (See Indexes Named *).\n   \n   db.runCommand( { dropIndexes: \"collection\", index: \"*\" } )\n   \n     * To drop a single index, issue the command by specifying the name of the index you want to drop. For example, to drop the index named age_1, use the following command:\n   \n   db.runCommand( { dropIndexes: \"collection\", index: \"age_1\" })\n   \n   \n   \n   mongosh provides the helper methods db.collection.dropIndex() and db.collection.dropIndexes():\n   \n   db.collection.dropIndex(\"age_1\");\n   \n     * To drop multiple indexes, issue the command by specifying an array of the index names:\n   \n   db.runCommand( { dropIndexes: \"collection\", index: [ \"age_1\", \"age_1_status_1\" ] } )\n   \n    \nTIP \nSEE ALSO:  * db.collection.dropIndexes()  * db.collection.dropIndex() ←  dropConnectionsfilemd5 → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/currentOp/": " Docs Home → MongoDB Manual \nCURRENTOP \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Access Control\n * Examples\n * Output Example\n * Output Fields \nDEFINITION \ncurrentOp \nReturns a document that contains information on in-progress operations for the mongod instance. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     currentOp: 1   })  \nNOTE Starting in MongoDB 5.0, the $currentOp aggregation stage is used when running the helper method db.currentOp() with mongosh. Given this, in the 5.0 version of the shell and with mongosh, db.currentOp() result sets are not subject to the 16MB BSON document return size document return size limit for documents of the previous legacy mongo versions. \nBEHAVIOR \ncurrentOp must run against the admin database, and it can accept several optional fields. Field\nDescription\n\"$ownOps\" Boolean. If set to true, returns information on the current user's operations only. On mongod instances, users are always authorized to run currentOp with \"$ownOps\": true to view their own operations. See access control. \"$all\" Boolean. If set to true, returns information on all operations, including operations on idle connections and system operations. \"$all\": true overrides any output field filters. <filter>\nSpecify filter conditions on the Output Fields. See Examples.\ncomment Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. currentOp and the database profiler report the same basic diagnostic information for all CRUD operations, including the following:  * aggregate  * count  * delete  * distinct  * find (OP_QUERY and command)  * findAndModify  * getMore (OP_GET_MORE and command)  * insert  * mapReduce  * update These operations are also included in the logging of slow queries (see slowOpThresholdMs for more information about slow query logging). \nACCESS CONTROL \nOn systems running with authorization, the user must have access that includes the inprog privilege action. Users can use $ownOps on mongod instances to view their own operations without the inprog privilege action. db.adminCommand( { currentOp: 1, \"$ownOps\": 1 } )  \nTIP \nSEE ALSO: Create a Role to Manage Current Operations \nEXAMPLES \nThe following examples use the currentOp command with various query documents to filter the output. \nDISPLAY ALL CURRENT OPERATIONS \ndb.adminCommand(   {     currentOp: true,     \"$all\": true   })  \nWRITE OPERATIONS WAITING FOR A LOCK \nThe following example returns information on all write operations that are waiting for a lock: db.adminCommand(   {     currentOp: true,     \"waitingForLock\" : true,     $or: [        { \"op\" : { \"$in\" : [ \"insert\", \"update\", \"remove\" ] } },        { \"command.findandmodify\": { $exists: true } }    ]   })  \nACTIVE OPERATIONS WITH NO YIELDS \nThe following example returns information on all active running operations that have never yielded: db.adminCommand(   {     currentOp: true,     \"active\" : true,     \"numYields\" : 0,     \"waitingForLock\" : false   })  \nACTIVE OPERATIONS ON A SPECIFIC DATABASE \nThe following example returns information on all active operations for database db1 that have been running longer than 3 seconds: db.adminCommand(   {     currentOp: true,     \"active\" : true,     \"secs_running\" : { \"$gt\" : 3 },     \"ns\" : /^db1\\./   })  \nACTIVE INDEXING OPERATIONS \nThe following example returns information on index creation operations:\n\n  \nOUTPUT EXAMPLE   \nOUTPUT FIELDS \ncurrentOp.type \nNew in version 4.2. The type of operation. Values are either:  * op  * idleSession  * idleCursor If the currentOp.type is op, currentOp.op provides details on the specific operation. currentOp.host \nThe name of the host against which the operation is run. currentOp.desc \nA description of the client. This string includes the connectionId. currentOp.connectionId \nAn identifier for the connection where the operation originated. currentOp.client \nA string with information about where the operation originated. For multi-document transactions, client stores information about the most recent client to run an operation inside the transaction. currentOp.appName \nThe identifier of the client application which ran the operation. Use the appName connection string option to set a custom value for the appName field. currentOp.clientMetadata \nAdditional information on the client. For multi-document transactions, client stores information about the most recent client to run an operation inside the transaction. currentOp.currentOpTime \nThe start time of the operation. currentOp.effectiveUsers \nAn array that contains a document for each user associated with the operation. Each user document contains the user name and the authentication db. \nTIP \nSEE ALSO: currentOp.runBy New in version 4.2. currentOp.runBy \nAn array that contains a document for each user who is impersonating the effectiveUser(s) for the operation. The runBy document contains the user name and the authentication db. In general, the runBy user is the __system user; e.g. \"runBy\" : [   {      \"user\" : \"__system\",      \"db\" : \"local\"   }]  Only available on sharded clusters New in version 4.2. currentOp.lsid \nThe session identifier. Only present if the operation is associated with a session. currentOp.transaction \nA document that contains multi-document transaction information. Only present if the operation is part of a multi-document transaction. currentOp.transaction.parameters \nA document that contains information on multi-document transaction. Only present if the operation is part of a multi-document transaction. currentOp.transaction.parameters.txnNumber \nThe transaction number. Only present if the operation is part of a multi-document transaction. currentOp.transaction.parameters.autocommit \nA boolean flag that indicates if autocommit is on for the transaction. Only present if the operation is part of a multi-document transaction. currentOp.transaction.parameters.readConcern \nThe read concern for the transaction. Multi-document transactions support read concern \"snapshot\", \"local\", and \"majority\". Only present if the operation is part of a multi-document transaction. currentOp.transaction.readTimestamp \nThe timestamp of the snapshot being read by the operations in the transaction. Only present if the operation is part of a multi-document transaction. currentOp.transaction.startWallClockTime \nThe date and time (with time zone) of the transaction start. Only present if the operation is part of a multi-document transaction. currentOp.transaction.timeOpenMicros \nThe duration of the transaction in microseconds. The timeActiveMicros value added to the timeInactiveMicros should equal the timeOpenMicros. Only present if the operation is part of a multi-document transaction. currentOp.transaction.timeActiveMicros \nThe total amount of time that the transaction has been active; i.e. when the transaction had operations running. The timeActiveMicros value added to the timeInactiveMicros should equal the timeOpenMicros. Only present if the operation is part of a multi-document transaction. currentOp.transaction.timeInactiveMicros \nThe total amount of time that the transaction has been inactive; i.e. when the transaction had no operations running. The timeInactiveMicros value added to the timeActiveMicros should equal the timeOpenMicros. Only present if the operation is part of a multi-document transaction. currentOp.transaction.expiryTime \nThe date and time (with time zone) when the transaction will time out and abort. The currentOp.transaction.expiryTime equals the currentOp.transaction.startWallClockTime + the transactionLifetimeLimitSeconds. For more information, seee Runtime Limit for transactions. Only present if the operation is part of a multi-document transaction. currentOp.twoPhaseCommitCoordinator \nInformation on either:\n\n  * A specific commit coordination operation (i.e. currentOp.type is op and currentOp.desc is \"TransactionCoordinator\") spawned by the transaction coordinator. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.lsid \nThe session identifier for the multi-shard transaction. The combination of the lsid and txnNumber identifies the transaction. Available for both the commit coordination metrics and for specific coordination operation. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.txnNumber \nThe transaction number for the multi-shard transaction. The combination of the txnNumber and lsid identifies the transaction. Available for both the commit coordination metrics and for specific coordination operation. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.action \nThe specific commit coordination operation spawned by the transaction coordinator:  * \"sendingPrepare\"  * \"sendingCommit\"  * \"sendingAbort\"  * \"writingParticipantList\"  * \"writingDecision\"  * \"deletingCoordinatorDoc\" Only available for specific coordination operation. currentOp.twoPhaseCommitCoordinator.startTime \nThe start date and time of the action. Only available for specific coordination operation. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.numParticipants \nNumber of shards participating in this commit. Only available for the commit coordination metrics. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.state \nThe current step/state of the commit coordination process. Step/stage\nDescription\ninactive\nNot actively part of a commit.\nwritingParticipantList\nWriting a local record of the list of shards that are part of this multi-shard transaction.\nwaitingForVotes\nWaiting for the participants to respond with vote to commit or abort.\nwritingDecision\nWriting a local record of the coordinator's decision to commit or abort based on votes.\nwaitingForDecisionAck\nWaiting for participants to acknowledge the coordinator's decision to commit or abort.\ndeletingCoordinatorDoc\nDeleting the local record of commit decision. Only available for the commit coordination metrics. See also currentOp.twoPhaseCommitCoordinator.stepDurations. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.commitStartTime \nThe date and time when the commit started. Only available for the commit coordination metrics. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.hasRecoveredFromFailover \nA boolean that indicates whether the commit coordination was restarted due to failover on the shard that is coordinating the commit. If hasRecoveredFromFailover is true, then the times specified in currentOp.twoPhaseCommitCoordinator.stepDurations may not be accurate for all steps. Only available for the commit coordination metrics. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.stepDurations \nA document that contains the duration, in microseconds, of the commit coordination steps/state completed or in progress: \"stepDurations\" : {   \"writingParticipantListMicros\" : NumberLong(17801),   \"totalCommitDurationMicros\" : NumberLong(42488463),   \"waitingForVotesMicros\" : NumberLong(30378502),   \"writingDecisionMicros\" : NumberLong(15015),   \"waitingForDecisionAcksMicros\" : NumberLong(12077145),   \"deletingCoordinatorDocMicros\" : NumberLong(6009)},  If currentOp.twoPhaseCommitCoordinator.hasRecoveredFromFailover is true, then the times specified in stepDurations may not be accurate for all steps. For a coordinator in an inactive state, the document is empty: \"stepDurations\" : {}  Only available for the commit coordination metrics. See currentOp.twoPhaseCommitCoordinator.state. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.decision \nA document that contains the commit/abort decision, for example:  * For a commmit decision:\n   \n   \"decision\" : {   \"decision\" : \"commit\",   \"commitTimestamp\" : Timestamp(1572034669, 3)}\n   \n   \n\n Only available for the commit coordination metrics. New in version 4.2.1. currentOp.twoPhaseCommitCoordinator.deadline \nThe date and time by which the commit must finish. Only available for the commit coordination metrics. New in version 4.2.1. currentOp.opid \nThe identifier for the operation. You can pass this value to db.killOp() in mongosh to terminate the operation. \nWARNING Terminate running operations with extreme caution. Only use db.killOp() to terminate operations initiated by clients and do not terminate internal database operations. currentOp.active \nA boolean value specifying whether the operation has started. Value is true if the operation has started or false if the operation is idle, such as an idle connection or an internal thread that is currently idle. An operation can be active even if the operation has yielded to another operation. For some inactive background threads, such as an inactive signalProcessingThread, MongoDB suppresses various empty fields. currentOp.secs_running \nThe duration of the operation in seconds. MongoDB calculates this value by subtracting the current time from the start time of the operation. Only appears if the operation is running; i.e. if active is true. currentOp.microsecs_running \nThe duration of the operation in microseconds. MongoDB calculates this value by subtracting the current time from the start time of the operation. Only appears if the operation is running; i.e. if active is true. currentOp.op \nA string that identifies the specific operation type. Only present if currentOp.type is op. The possible values are:  * \"none\"  * \"update\"  * \"insert\"  * \"query\"  * \"command\"  * \"getmore\"  * \"remove\"  * \"killcursors\" \"query\" operations include read operations. \"command\" operations include most commands such as the createIndexes and findAndModify. currentOp.ns \nThe namespace the operation targets. A namespace consists of the database name and the collection name concatenated with a dot (.); that is, \"<database>.<collection>\". currentOp.command \nA document containing the full command object associated with this operation. For example, the following output contains the command object for a find operation on a collection named items in a database named test: \"command\" : {  \"find\" : \"items\",  \"filter\" : {    \"sku\" : 1403978  },  ...  \"$db\" : \"test\"}  The following example output contains the command object for a getMore operation generated by a command with cursor ID 19234103609 on a collection named items in a database named test: \"command\" : {    \"getMore\" : NumberLong(\"19234103609\"),    \"collection\" : \"items\",    \"batchSize\" : 10,    ...    \"$db\" : \"test\"},  If the command document exceeds 1 kilobyte, the document has the following form: \"command\" : {  \"$truncated\": <string>,  \"comment\": <string>}  The $truncated field contains a string summary of the document excluding the document's comment field if present. If the summary still exceeds 1 kilobyte then it is further truncated, denoted by an ellipsis (...) at the end of the string. The comment field is present if a comment was passed to the operation. Starting in MongoDB 4.4, a comment may be attached to any database command. currentOp.planSummary \nSpecifies whether the cursor uses a collection scan (COLLSCAN) or an index scan (IXSCAN { ... }). The IXSCAN also includes the specification document of the index used. currentOp.prepareReadConflicts \nThe number of times the current operation had to wait for a prepared transaction with a write to commit or abort. While waiting, the current operation continues to hold any necessary locks and storage engine resources. New in version 4.2. currentOp.writeConflicts \nThe number of times the current operation conflicted with another write operation on the same document. New in version 4.2. currentOp.cursor \nNew in version 4.2. A document that contains the cursor information for getmore operations; i.e. where op is getmore. If reporting on a getmore operation before the getmore has accessed its cursor information, the cursor field is not available. currentOp.cursor.cursorId \nNew in version 4.2. The ID of the cursor. currentOp.cursor.createdDate \nNew in version 4.2. The date and time when the cursor was created. currentOp.cursor.lastAccessDate\n\n The date and time when the cursor was last used. currentOp.cursor.nDocsReturned \nNew in version 4.2. The cumulative number of documents returned by the cursor. currentOp.cursor.nBatchesReturned \nNew in version 4.2. The curmulative number of batches returned by the cursor. currentOp.cursor.noCursorTimeout \nNew in version 4.2. The flag that indicates that the cursor will not timeout when idle; i.e. if the cursor has the noTimeout option set.  * If true, the cursor does not time out when idle.  * If false, the cursor will time out when idle. \nTIP \nSEE ALSO: cursor.addOption() currentOp.cursor.tailable \nNew in version 4.2. The flag that indicates if the cursor is a tailable cursor for a capped collection. Tailable cursors remain open after the client exhausts the results in the initial cursor. \nTIP \nSEE ALSO:  * find  * cursor.tailable()  * cursor.addOption() currentOp.cursor.awaitData \nNew in version 4.2. The flag that indicates whether the tailable cursor should temporarily block a getMore command on the cursor while waiting for new data rather than returning no data. For non-tailable cursors, the value is always false. \nTIP \nSEE ALSO:  * find  * cursor.tailable()  * cursor.addOption() currentOp.cursor.originatingCommand \nNew in version 4.2. The originatingCommand field contains the full command object (e.g. find or aggregate) which originally created the cursor. \nNOTE Starting in version 4.2, MongoDB now returns originatingCommand field as a nested field in the new cursor field. In previous versions, the originatingCommand was a top-level field for the associated \"getmore\" document. currentOp.cursor.planSummary \nNew in version 4.2. Specifies whether the cursor uses a collection scan (COLLSCAN) or an index scan (IXSCAN { ... }). The IXSCAN also includes the specification document of the index used. currentOp.cursor.operationUsingCursorId \nNew in version 4.2. The opid of the operation using the cursor. Only present if the cursor is not idle. currentOp.waitingForLatch \nThe waitingForLatch document is only available if the operation is waiting to acquire an internal locking primitive (a.k.a. a latch) or for an internal condition to be met. For example, \"waitingForLatch\" : {   \"timestamp\" : ISODate(\"2020-03-19T23:25:58.412Z\"),   \"captureName\" : \"FutureResolution\",}, \nOutput Field\nDescription\ntimestamp\nThe date and time at which the operation started to wait.\ncaptureName\nThe internal name of the section where the operation is currently blocked. New in version 4.2.2. currentOp.locks \nThe locks document reports the type and mode of locks the operation currently holds. The possible lock types are as follows: Lock Type\nDescription\nParallelBatchWriterMode Represents a lock for parallel batch writer mode. In earlier versions, PBWM information was reported as part of the Global lock information. New in version 4.2. ReplicationStateTransition Represents lock taken for replica set member state transitions. New in version 4.2. Global\nRepresents global lock.\nDatabase\nRepresents database lock.\nCollection\nRepresents collection lock.\nMutex\nRepresents mutex.\nMetadata\nRepresents metadata lock.\noplog\nRepresents lock on the oplog. The possible modes are as follows: Lock Mode\nDescription\nR\nRepresents Shared (S) lock.\nW\nRepresents Exclusive (X) lock.\nr\nRepresents Intent Shared (IS) lock.\nw\nRepresents Intent Exclusive (IX) lock. currentOp.waitingForLock \nReturns a boolean value. waitingForLock is true if the operation is waiting for a lock and false if the operation has the required lock. currentOp.msg \nThe msg provides a message that describes the status and progress of the operation. In the case of indexing or mapReduce operations, the field reports the completion percentage. currentOp.progress \nReports on the progress of mapReduce or indexing operations. The progress fields corresponds to the completion percentage in the msg field. The progress specifies the following information: currentOp.progress.done \nReports the number completed. currentOp.progress.total \nReports the total number. currentOp.killPending\n\n currentOp.numYields \nnumYields is a counter that reports the number of times the operation has yielded to allow other operations to complete. Typically, operations yield when they need access to data that MongoDB has not yet fully read into memory. This allows other operations that have data in memory to complete quickly while MongoDB reads in data for the yielding operation. currentOp.dataThroughputLastSecond \nAmount of data (in MiB) processed by the validate operation in the last second. Only available for a validate operation that is currently scanning documents. For example: \"msg\" : \"Validate: scanning documents Validate: scanning documents: 7258/24000 30%\",\"progress\" : {   \"done\" : 7258,   \"total\" : 24000},\"numYields\" : 0,\"dataThroughputLastSecond\" : 15.576952934265137,\"dataThroughputAverage\" : 15.375944137573242,  New in version 4.4. currentOp.dataThroughputAverage \nThe average amount of data (in MiB) processed by the validate operation. Only available for a validate operation that is currently scanning documents. For example: \"msg\" : \"Validate: scanning documents Validate: scanning documents: 7258/24000 30%\",\"progress\" : {   \"done\" : 7258,   \"total\" : 24000},\"numYields\" : 0,\"dataThroughputLastSecond\" : 15.576952934265137,\"dataThroughputAverage\" : 15.375944137573242,  New in version 4.4. currentOp.fsyncLock \nSpecifies if database is currently locked for fsync write/snapshot. Only appears if locked; i.e. if fsyncLock is true. currentOp.info \nInformation regarding how to unlock database from db.fsyncLock(). Only appears if fsyncLock is true. currentOp.lockStats \nFor each lock type and mode (see currentOp.locks for descriptions of lock types and modes), returns the following information: currentOp.lockStats.acquireCount \nNumber of times the operation acquired the lock in the specified mode. currentOp.lockStats.acquireWaitCount \nNumber of times the operation had to wait for the acquireCount lock acquisitions because the locks were held in a conflicting mode. acquireWaitCount is less than or equal to acquireCount. currentOp.lockStats.timeAcquiringMicros \nCumulative time in microseconds that the operation had to wait to acquire the locks. timeAcquiringMicros divided by acquireWaitCount gives an approximate average wait time for the particular lock mode. currentOp.lockStats.deadlockCount \nNumber of times the operation encountered deadlocks while waiting for lock acquisitions. currentOp.waitingForFlowControl \nA boolean that indicates if the operation is in the process of waiting for flow control. New in version 4.2. currentOp.flowControlStats \nThe flow control statistics for this operation. New in version 4.2. currentOp.flowControlStats.acquireCount \nThe number of times this operation acquired a ticket. New in version 4.2. currentOp.flowControlStats.acquireWaitCount \nThe number of times this operation waited to aqcuire a ticket. New in version 4.2. currentOp.flowControlStats.timeAcquiringMicros \nThe total time this operation has waited to acquire a ticket. New in version 4.2. currentOp.totalOperationTimeElapsed \nThe total time elapsed, in seconds, for the current resharding operation. The time is set to 0 when a new resharding operation starts. Only present if a resharding operation is taking place. New in version 5.0. currentOp.remainingOperationTimeEstimated \nThe estimated time remaining in seconds for the current resharding operation. The time is set to -1 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.approxDocumentsToCopy \nThe approximate number of documents to be copied from the donor shards to the recipient shards during the resharding operation. This number is an estimate that is set at the beginning of the resharding operation and does not change after it has been set. The number is set to 0 when a new resharding operation starts. It is possible for $currentOp.documentsCopied and $currentOp.bytesCopied to end up exceeding $currentOp.approxDocumentsToCopy and $currentOp.approxBytesToCopy, respectively, if the post-resharding data distribution is not perfectly uniform.\n\n New in version 5.0. currentOp.documentsCopied \nThe number of documents copied form donor shards to recipient shards during the resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.approxBytesToCopy \nThe approximate number of bytes to be copied from the donor shards to the recipient shards during the resharding operation. This number is an estimate that is set at the beginning of the resharding operation and does not change after it has been set. The number is set to 0 when a new resharding operation starts. It is possible for $currentOp.documentsCopied and $currentOp.bytesCopied to end up exceeding $currentOp.approxDocumentsToCopy and $currentOp.approxBytesToCopy, respectively, if the post-resharding data distribution is not perfectly uniform. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.bytesCopied \nThe number of bytes copied from donor shards to recipient shards during the resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.totalCopyTimeElapsed \nThe total elapsed time, in seconds, for ongoing data copy tasks from donor shards to recipient shards for the current resharding operation. The time is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.oplogEntriesFetched \nThe number of entries fetched from the oplog for the current resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.oplogEntriesApplied \nThe number of entries applied to the oplog for the current resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.totalApplyTimeElapsed \nThe total elapsed time, in seconds, for the apply step of the current resharding operation. In the apply step, recipient shards apply oplog entries to modify their data based on new incoming writes from donor shards. The time is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. currentOp.countWritesDuringCriticalSection \nThe number of writes perfomed in the critical section for the current resharding operation. The critical section prevents new incoming writes to the collection currently being resharded. The number is set to 0 when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place. New in version 5.0. currentOp.totalCriticalSectionTimeElapsed \nThe total elapsed time, in seconds, for the critical section of the current resharding operation. The critical section prevents new incoming writes to the collection currently being resharded. The time is set to 0 when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place. New in version 5.0. currentOp.donorState \nThe current state of a donor shard for the resharding operation. The state is set to unused when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place.\n\n New in version 5.0. currentOp.recipientState \nThe current state of a recipient shard for a resharding operation. The state is set to unused when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place. State\nDescription\nunused\nThe resharding operation is about to start or recovering from a primary failover.\nawaiting-fetch-timestamp\nThe recipient shard is waiting for the donor shards to be prepared to donate their data.\ncreating-collection\nThe recipient shard is creating the new sharded collection.\ncloning\nThe recipient shard is receiving data from the donor shards.\napplying\nThe recipient shard is applying oplog entries to modify its copy of the data based on the new incoming writes from donor shards.\nerror\nAn error occurred during the resharding operation.\nstrict-consistency\nThe recipient shard has all data changes stored in a temporary collection.\ndone\nThe resharding operation is complete. New in version 5.0. currentOp.coordinatorState \nThe state of the resharding coordinator for the current resharding operation. The resharding coordinator is an operation that runs on the config server primary. The state is set to unused when a new resharding operation starts. Only present on the coordinating config server. State\nDescription\nunused\nThe resharding operation is about to start or recovering from a primary failover.\ninitializing\nThe resharding coordinator has inserted the coordinator document into config.reshardingOperations and has added the reshardingFields to the config.collections entry for the original collection.\npreparing-to-donate The resharding coordinator  * has created a config.collections entry for the temporary resharding collection.  * has inserted entries into config.chunks for ranges based on the new shard key.  * has inserted entries into config.tags for any zones associated with the new shard key. The coordinator informs participant shards to begin the resharding operation. The coordinator then waits until all donor shards have picked a minFetchTimestamp and are ready to donate. cloning\nThe resharding coordinator informs donor shards to donate data to recipient shards. The coordinator waits for all recipients to finish cloning the data from the donor.\napplying\nThe resharding coordinator informs recipient shards to modify their copies of data based on new incoming writes from donor shards. The coordinator waits for all recipients to finish applying oplog entries.\nblocking-writes\nThe resharding coordinator informs donor shards to prevent new incoming write operations to the collection being resharded. The coordinator then waits for all recipients to have all data changes.\naborting\nAn unrecoverable error occurred during the resharding operation or the abortReshardCollection command (or the sh.abortReshardCollection() method) was run.\ncommitting\nThe resharding coordinator removes the config.collections entry for the temporary resharding collection. The coordinator then adds the recipientFields to the source collection's entry. New in version 5.0. currentOp.opStatus \nThe current state of a resharding operation. Only present if a resharding operation is taking place. Once the operation has completed, the operation is removed from currentOp output. State\nDescription\nactively running\nThe resharding operation is actively running.\nsuccess\nThe resharding operation has succeeded.\nfailure\nThe resharding operation has failed.\ncanceled\nThe resharding operation was canceled. New in version 5.0. ←  createIndexesdrop → On this page  * Definition\n * Syntax\n * Behavior\n * Access Control\n * Examples\n * Output Example\n * Output Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/updateZoneKeyRange/": " Docs Home → MongoDB Manual \nUPDATEZONEKEYRANGE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Security\n * Example \nDEFINITION \nupdateZoneKeyRange \nThe updateZoneKeyRange administrative command can either create or remove the association between a range of shard key values and a zone. Starting in MongoDB 4.0.2, you can run updateZoneKeyRange database command and its helpers sh.updateZoneKeyRange() and sh.addTagRange() on an unsharded collection or a non-existing collection. \nTIP In mongosh, this command can also be run through the sh.updateZoneKeyRange() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. To run updateZoneKeyRange, use the db.runCommand( { <command> } ) method. You must run addShardToZone on the admin database. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     updateZoneKeyRange: <string>,     min: <document>,     max: <document>,     zone: <string> | <null>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Parameter\nType\nDescription\nupdateZoneKeyRange\nstring The namespace of the collection to associate with the range. The collection must be sharded for the command to succeed. min\ndocument The inclusive lower bound of the range of shard key values. Specify each field of the shard key in the form of <fieldname> : <value>. The value must be of the same BSON type or types as the shard key. \nNOTE To use hashed sharding, the field value needs to be of type NumberLong. max\ndocument The exclusive upper bound of the range of shard key values. Specify each field of the shard key in the form of <fieldname> : <value>. The value must be of the same BSON type or types as the shard key. \nNOTE To use hashed sharding, the field value needs to be of type NumberLong. zone\nstring The name of the zone to associate with the range bounded by the min and max. If the value does not match an existing zone, the command fails. Specify null to remove the association between the range with lower bounds of min and upper bound of max and the updateZoneKeyRange collection. The values of min and max must match exactly the target range. \nBEHAVIOR \nIf no zone range matches the minimum and maximum bounds passed to updateZoneKeyRange, nothing is removed. Only issue updateZoneKeyRange when connected to a mongos instance. mongosh provides two helper methods:      * sh.updateZoneKeyRange() for adding a range of shard key values to a zone.  * sh.removeRangeFromZone() for removing a range of shard key values from a zone. You cannot create a range of shard key values whose lower and upper boundaries overlap with an existing range for the sharded collection. For example, given an existing range of 1 to 10, you cannot create a new range of 5 to 20, as the new range would overlap with the existing range. A zone can have multiple ranges of data associated with it, but a range can at most be associated with a single zone. When removing the association between a range and a zone, updateZoneKeyRange does not remove the zone. Use the removeShardFromZone command to remove the association between a zone and a shard. See the zone manual page for more information on zones in sharded clusters. \nINITIAL CHUNK DISTRIBUTION FOR EMPTY OR NON-EXISTING COLLECTIONS \nIf you are considering performing zone sharding on an empty or non-existent collection, use updateZoneKeyRange to create the zones and zone ranges before sharding the collection (since 4.0.2). Starting in version 4.0.3, creating zones and zone ranges on empty or non-existing collections allows MongoDB to optimize the initial chunk creation and distribution process when sharding the collection. This optimized process supports faster setup of zoned sharding with less balancer overhead than creating zones after sharding. The balancer performs all chunk management after the optimized initial chunk creation and distribution. For an example of defining zones and zone ranges for initial chunk distribution, see Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection.\n\n \nStarting in version 4.4, MongoDB supports sharding collections on compound hashed indexes. MongoDB can perform optimized initial chunk creation and distribution when sharding the empty or non-existing collection on a compound hashed shard key.  \nFor a more complete example of defining zones and zone ranges for initial chunk distribution on a compound hashed shard key, see Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection. \nBALANCER \nAfter successfully running updateZoneKeyRange, there may be chunk migrations during the next balancer round. After adding a range to a zone, the balancer must first run in order to migrate any chunks whose ranges are covered by the zone to shards inside of that zone. Until balancing completes, some chunks may reside on the wrong shard given the configured zones for the sharded cluster. Removing the association between a range and a zone removes the constraints keeping chunks covered by the range on the shards inside that zone. During the next balancer round, the balancer may migrate chunks that were previously covered by the zone. See the documentation for the sharded cluster balancer for more information on how migrations work in a sharded cluster. \nBOUNDS \nZone ranges are always inclusive of the lower boundary and exclusive of the upper boundary. \nDROPPED COLLECTIONS \nDropping a collection deletes its associated zone/tag ranges. In earlier versions, MongoDB does not remove the tag associations for a dropped collection, and if you later create a new collection with the same name, the old tag associations will apply to the new collection. \nSECURITY \nFor sharded clusters running with authentication, you must authenticate as either:  * a user whose privileges include the specified actions on various collections in the config database:\n   \n   * find on the config.shards collection\n   \n   * find, update, and remove on the config.tags collection;\n   \n   or, alternatively,  * a user whose privileges include enableSharding on the cluster resource (available starting in version 4.2.2, 4.0.14, 3.6.16). The clusterAdmin or clusterManager built-in roles have the appropriate permissions for issuing updateZoneKeyRange. See the documentation page for Role-Based Access Control for more information. \nEXAMPLE \nGiven a sharded collection exampledb.collection with a shard key of { a : 1 }, the following operation creates a range with a lower bound of 1 and an upper bound of 10 on the alpha zone: admin = db.getSiblingDB(\"admin\")admin.runCommand(   {      updateZoneKeyRange : \"exampledb.collection\",      min : { a : 1 },      max : { a : 10 },      zone : \"alpha\"   })  The following operation removes the previously created range by passing null to the zone field. admin = db.getSiblingDB(\"admin\")admin.runCommand(   {      updateZoneKeyRange : \"exampledb.collection\",      min : { a : 1 },      max : { a : 10 },      zone : null   })  The min and max must match exactly the bounds of the target range. The following operation attempts to remove the previously created range, but specifies { a : 0 } as the min bound: admin = db.getSiblingDB(\"admin\")admin.runCommand(   {      updateZoneKeyRange : \"exampledb.collection\",      min : { a : 0 },      max : { a : 10 },      zone : null   })  While the range of { a : 0 } and { a : 10 } encompasses the existing range, it is not an exact match and therefore updateZoneKeyRange does not remove anything. \nCOMPOUND SHARD KEY \nGiven a sharded collection exampledb.collection with a shard key of { a : 1, b : 1 }, the following operation creates a range covering the lower bound of { a: 1, b : 1 } and an upper bound of { a : 10, b : 10} and associates it with the alpha zone:\n\n \n←  unsetShardingSessions Commands → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Security\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/refineCollectionShardKey/": " Docs Home → MongoDB Manual \nREFINECOLLECTIONSHARDKEY \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Access Control\n * Considerations\n * Examples \nDEFINITION \nrefineCollectionShardKey \nNew in version 4.4. Modifies the collection's shard key by adding new field(s) as a suffix to the existing key. Refining a collection's shard key can address situations where the existing key has led to jumbo (i.e. indivisible) chunks due to insufficient cardinality. \nNOTE \nDATA DISTRIBUTION As part of refining the shard key, the refineCollectionShardKey command updates the chunk ranges and zone ranges to incorporate the new fields without modifying the range values of the existing key fields. That is, the refinement of the shard key does not immediately affect the distribution of chunks across shards or zones. Any future chunk splits or migration occur as part of the routine sharding operations. \nSYNTAX  NOTE To use the refineCollectionShardKey command, the sharded cluster must have feature compatibility version (fcv) of 4.4. The command has the following syntax: db.adminCommand(   {     refineCollectionShardKey: \"<database>.<collection>\",     key: { <existing key specification>, <suffix1>: <1|\"hashed\">, ... }   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nrefineCollectionShardKey\nstring The namespace of the sharded collection in the form \"<database>.<collection>\". key\ndocument The document that specifies the field or fields to use as the new shard key for the collection. { <existing key specification>, <suffix1>: <1|\"hashed\">, ... } \nIMPORTANT      * The new key must start with the existing shard key.  * The new key must be supported by an index that must exist before you run the refineCollectionShardKey command. See Index Considerations for more information. For the suffix fields, set the field values to either:  * 1 for ranged based sharding  * \"hashed\" to specify a hashed shard key if no other field in the shard key has a \"hashed\" specification and the current shard key is not supported by a unique index since hashed indexes do not support uniqueness constraint. \nTIP \nSEE ALSO: Shard Keys \nACCESS CONTROL \nWhen running with access control, the user must have the refineCollectionShardKey privilege actions on database and/or collection to run the command. That is, a user must have a role that grants the following privilege: { resource: { db: <database>, collection: <collection> }, actions: [ \"refineCollectionShardKey\" ] }  The built-in clusterManager role provides the appropriate privileges. \nCONSIDERATIONS  INDEX CONSIDERATIONS \n * Index Existence\n   \n   An index that supports the command's specified key must exist prior to running the command.\n   \n   A supporting index is an index that starts with the new shard key specification; i.e. the index prefix matches the new shard key specification. That is, to change the shard key to { x: 1, y: 1 } from { x: 1 }, and index that starts with { x: 1, y: 1 } must exist; e.g.\n   \n    * { x: 1, y: 1 }\n   \n    * { x: 1, y: 1, a: 1, b: 1}\n   \n   \n   NOTE\n   \n    * The supporting index cannot be a partial index.\n   \n    * The supporting index cannot be a sparse index.\n   \n    * If the collection uses a non-simple collation, the supporting index must specify { locale: \"simple\" } collation.\n\n \nWARNING Do not modify the range or hashed type for any of the current shard key fields. It causes data inconsistencies. For example, do not modify a shard key from { customer_id: 1 } to { customer_id: \"hashed\", order_id: 1 }. \nEXAMPLES \nTo set up the example in the test database:  1. Use following shardCollection operation to shard the orders collection in the test database. The operation uses the customer_id field as the initial shard key:\n    \n    db.adminCommand( { shardCollection: \"test.orders\", key: { customer_id: 1 } } )\n    \n     To modify the shard key to be the customer_id field and the order_id field { customer_id: 1, order_id: 1 },  1. Create the index to support the new shard key if the index does not already exist.\n    \n    db.getSiblingDB(\"test\").orders.createIndex( { customer_id: 1, order_id: 1 } )\n    \n      2. Run refineCollectionShardKey command to add the order_id field as a suffix:\n    \n    db.adminCommand( {   refineCollectionShardKey: \"test.orders\",   key: { customer_id: 1, order_id: 1 }} )\n    \n     Upon successful completion of the command, the shard key for the collection has changed to { customer_id: 1, order_id: 1 }. To verify, you can run sh.status(). \nTIP After you refine the shard key, it may be that not all documents in the collection have the suffix field(s). To populate the missing shard key field(s), see Missing Shard Key Fields. Before refining the shard key, ensure that all or most documents in the collection have the suffix fields, if possible, to avoid having to populate the field afterwards. \nCOLLECTION WITH NON-SIMPLE COLLATION \nTo set up the example in the test database:  1. Create the cafés collection in the test database, specifying French fr as the default collation.\n    \n    db.getSiblingDB(\"test\").createCollection( \"cafés\", { collation: { locale: \"fr\" } } );\n    \n      2. Shard the collection using customer_id field as the initial shard key. Because the collection has a default fr collation and not a simple collation, the shardCollection command must include a collation: { locale: \"simple\" } option:\n    \n    db.adminCommand( {   shardCollection: \"test.cafés\",   key: { customer_id: 1 },   collation: { locale: \"simple\" }} )\n    \n     To modify the shard key to be both the customer_id field and the order_id field { customer_id: 1, order_id: 1 },  1. Create the index to support the new shard key if the index does not already exist. Because the collection uses a non-simple collation, the index must include the collation: { locale: \"simple\" } option.\n    \n    db.getSiblingDB(\"test\").cafés.createIndex(   { customer_id: 1, order_id: 1 },   { collation: { locale: \"simple\" } })\n    \n      2. Run refineCollectionShardKey command to add the order_id field as a suffix:\n    \n    db.adminCommand( {   refineCollectionShardKey: \"test.cafés\",   key: { customer_id: 1, order_id: 1 }} )\n    \n     Upon successful completion of the command, the shard key for the collection has changed to { customer_id: 1, order_id: 1 }. To verify, you can run sh.status(). \nTIP After you refine the shard key, it may be that not all documents in the collection have the suffix field(s). To populate the missing shard key field(s), see Missing Shard Key Fields.\n\n \nTIP \nSEE ALSO: Shard Keys ←  mergeChunksremoveShard → On this page  * Definition\n * Syntax\n * Command Fields\n * Access Control\n * Considerations\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/update/": " Docs Home → MongoDB Manual \nUPDATE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Access Control\n * Behavior\n * Examples\n * Output \nDEFINITION \nupdate \nThe update command modifies documents in a collection. A single update command can contain multiple update statements. \nTIP In mongosh, this command can also be run through the updateOne(), updateMany(), replaceOne(), findOneAndReplace(), and findOneAndUpdate() helper methods. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nChanged in version 5.0. The command has the following syntax: db.runCommand(   {      update: <collection>,      updates: [         {           q: <query>,           u: <document or pipeline>,           c: <document>, // Added in MongoDB 5.0           upsert: <boolean>,           multi: <boolean>,           collation: <document>,           arrayFilters: <array>,           hint: <document|string>         },         ...      ],      ordered: <boolean>,      writeConcern: { <write concern> },      bypassDocumentValidation: <boolean>,      comment: <any>,      let: <document> // Added in MongoDB 5.0   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nupdate\nstring\nThe name of the target collection.\nupdates\narray\nAn array of one or more update statements to perform on the named collection. For details of the update statements, see Update Statements.\nordered\nboolean\nOptional. If true, then when an update statement fails, return without performing the remaining update statements. If false, then when an update fails, continue with the remaining update statements, if any. Defaults to true.\nwriteConcern\ndocument Optional. A document expressing the write concern of the update command. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. bypassDocumentValidation\nboolean\nOptional. Enables update to bypass document validation during the operation. This lets you update documents that do not meet the validation requirements.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. let\ndocument Optional. Specifies a document with a list of variables. This allows you to improve command readability by separating the variables from the query text. The document syntax is: { <variable_name_1>: <expression_1>,  ...,  <variable_name_n>: <expression_n> } The variable is set to the value returned by the expression, and cannot be changed afterwards. To access the value of a variable in the command, use the double dollar sign prefix ($$) together with your variable name in the form $$<variable_name>. For example: $$targetTotal. For a complete example, see Use Variables in let Option or c Field. New in version 5.0. \nUPDATE STATEMENTS \nEach element of the updates array is an update statement document. Each document contains the following fields: Field\nType\nDescription\nq\ndocument The query that matches documents to update. Use the same query selectors as used in the find() method. u\ndocument or pipeline The modifications to apply. The value can be either:  * A document that contains update operator expressions,\n\n  * Starting in MongoDB 4.2, an aggregation pipeline.\n   \n   * $addFields and its alias $set\n   \n   * $project and its alias $unset\n   \n   * $replaceRoot and its alias $replaceWith. For details, see Behavior. c\ndocument Optional. Specifies a document with a list of variables. This allows you to improve command readability by separating the variables from the query text. The document syntax is: { <variable_name_1>: <expression_1>,  ...,  <variable_name_n>: <expression_n> } The variable is set to the value returned by the expression, and cannot be changed afterwards. To access the value of a variable in the command, use the double dollar sign prefix ($$) together with your variable name in the form $$<variable_name>. For example: $$targetTotal. \nNOTE To use a variable to filter results, you must access the variable within the $expr operator. For a complete example using let and variables, see Use Variables in let Option or c Field. New in version 5.0. upsert\nboolean Optional. When true, update either:  * Creates a new document if no documents match the query. For more details see upsert behavior.  * Updates a single document that matches the query. If both upsert and multi are true and no documents match the query, the update operation inserts only a single document. To avoid multiple upserts, ensure that the query field(s) are uniquely indexed. See Upsert with Unique Index for an example. Defaults to false, which does not insert a new document when no match is found. multi\nboolean Optional. If true, updates all documents that meet the query criteria. If false, limit the update to one document that meet the query criteria. Defaults to false. When updating multiple documents, if a single document fails to update, further documents are not updated. See multi-update failures for more details on this behavior. collation\ndocument Optional. Specifies the collation to use for the operation. Collation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. The collation option has the following syntax: collation: {   locale: <string>,   caseLevel: <boolean>,   caseFirst: <string>,   strength: <int>,   numericOrdering: <boolean>,   alternate: <string>,   maxVariable: <string>,   backwards: <boolean>}  When specifying collation, the locale field is mandatory; all other collation fields are optional. For descriptions of the fields, see Collation Document. If the collation is unspecified but the collection has a default collation (see db.createCollection()), the operation uses the collation specified for the collection. If no collation is specified for the collection or for the operations, MongoDB uses the simple binary comparison used in prior versions for string comparisons. You cannot specify multiple collations for an operation. For example, you cannot specify different collations per field, or if performing a find with a sort, you cannot use one collation for the find and another for the sort. arrayFilters\narray Optional. An array of filter documents that determines which array elements to modify for an update operation on an array field. In the update document, use the $[<identifier>] filtered positional operator to define an identifier, which you then reference in the array filter documents. You cannot have an array filter document for an identifier if the identifier is not included in the update document. \nNOTE The <identifier> must begin with a lowercase letter and contain only alphanumeric characters. You can include the same identifier multiple times in the update document; however, for each distinct identifier ($[identifier]) in the update document, you must specify exactly one corresponding array filter document. That is, you cannot specify multiple array filter documents for the same identifier. For example, if the update statement includes the identifier x (possibly multiple times), you cannot specify the following for arrayFilters that includes 2 separate filter documents for x: // INVALID\n[  { \"x.a\": { $gt: 85 } },  { \"x.b\": { $gt: 80 } }] However, you can specify compound conditions on the same identifier in a single filter document, such as in the following examples:\n\n  For examples, see Specify arrayFilters for Array Update Operations. hint\nDocument or string Optional. A document or string that specifies the index to use to support the query predicate. The option can take an index specification document or the index name string. If you specify an index that does not exist, the operation errors. For an example, see Specify hint for Update Operations. New in version 4.2. \nRETURNS \nThe command returns a document that contains the status of the operation. For example: {   \"ok\" : 1,   \"nModified\" : 0,   \"n\" : 1,   \"upserted\" : [      {         \"index\" : 0,         \"_id\" : ObjectId(\"52ccb2118908ccd753d65882\")      }   ]} For details of the output fields, see Output. \nACCESS CONTROL \nOn deployments running with authorization, the user must have access that includes the following privileges:  * update action on the specified collection(s).  * find action on the specified collection(s).  * insert action on the specified collection(s). The built-in role readWrite provides the required privileges. \nBEHAVIOR  UPDATE WITH AN UPDATE OPERATOR EXPRESSIONS DOCUMENT \nThe update statement field u can accept a document that only contains update operator expressions. For example: updates: [   {     q: <query>,     u: { $set: { status: \"D\" }, $inc: { quantity: 2 } },      ...   },   ...] Then, the update command updates only the corresponding fields in the document. \nUPDATE WITH A REPLACEMENT DOCUMENT \nThe update statement field u field can accept a replacement document, i.e. the document contains only field:value expressions. For example: updates: [   {      q: <query>,      u: { status: \"D\", quantity: 4 },      ...   },   ...] Then the update command replaces the matching document with the update document. The update command can only replace a single matching document; i.e. the multi field cannot be true. The update command does not replace the _id value. \nMULTI-UPDATE FAILURES \nIf a single document fails to update in an update command with the multi parameter set to true, no further documents update as part of that command. For example, create a members collection with the following documents: db.members.insertMany( [   { \"_id\" : 1, \"member\" : \"Taylor\", \"status\" : \"pending\", \"points\" : 1},   { \"_id\" : 2, \"member\" : \"Alexis\", \"status\" : \"enrolled\", \"points\" : 59},   { \"_id\" : 3, \"member\" : \"Elizabeth\", \"status\" : \"enrolled\", \"points\" : 34}] )  The following operation creates a document validator on the members collection with a rule that the points value can not equal 60. db.runCommand( {   collMod: \"members\",   validator: { points: { $ne: 60 } }} )  This update command increases the points field of every document by 1. db.runCommand( {    update: \"members\",    updates: [     {         q: {},         u: { $inc: { points: 1 } },         multi: true     }    ] })  After running the command, the collection contains the following documents: { _id: 1, member: 'Taylor', status: 'A', points: 2 }{ _id: 2, member: 'Alexis', status: 'D', points: 59 }{ _id: 3, member: 'Elizabeth', status: 'C', points: 34 } The update command updated the points value of the first document but failed to update the second document because of the validator rule that the points value can not equal 60. The third document did not update because no further documents update following a write error. \nTIP \nSEE ALSO: Schema Validation \nUPDATE WITH AN AGGREGATION PIPELINE\n\n  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. Using the aggregation pipeline allows for a more expressive update statement, such as expressing conditional updates based on current field values or updating one field using the value of another field(s). For example: updates: [   {      q: <query>,      u: [        { $set: { status: \"Modified\", comments: [ \"$misc1\", \"$misc2\" ] } },        { $unset: [ \"misc1\", \"misc2\" ] }      ],      ...   },   ...] \nNOTE The $set and $unset used in the pipeline refers to the aggregation stages $set and $unset respectively, and not the update operators $set and $unset. For examples, see Update with Aggregation Pipeline. \nUPSERT WITH UNIQUE INDEX \nWhen using the upsert: true option with the update command, and not using a unique index on the query field(s), multiple instances of an update operation with similar query field(s) could result in duplicate documents being inserted in certain circumstances. Consider an example where no document with the name Andy exists and multiple clients issue the following command at roughly the same time: db.runCommand(   {     update: \"people\",     updates: [       { q: { name: \"Andy\" }, u: { $inc: { score: 1 } }, multi: true, upsert: true }     ]   })  If all update operations finish the query phase before any client successfully inserts data, and there is no unique index on the name field, each update operation may result in an insert, creating multiple documents with name: Andy. To ensure that only one such document is created, and the other update operations update this new document instead, create a unique index on the name field. This guarantees that only one document with name: Andy is permitted in the collection. With this unique index in place, the multiple update operations now exhibit the following behavior:  * Exactly one update operation will successfully insert a new document.  * All other update operations will update the newly-inserted document, incrementing the score value. \nLIMITS \nFor each update element in the updates array, the sum of the query and the update sizes (i.e. q and u ) must be less than or equal to the maximum BSON document size. The total number of update statements in the updates array must be less than or equal to the maximum bulk size. \nDOCUMENT VALIDATION \nThe update command adds support for the bypassDocumentValidation option, which lets you bypass document validation when inserting or updating documents in a collection with validation rules. \nSHARDED COLLECTIONS \nUPSERT ON A SHARDED COLLECTION \nTo use update with multi: false on a sharded collection,  * If you do not specify upsert: true, the filter q must either include an equality match on the _id field or target a single shard (such as by including the shard key).  * If you specify upsert: true, the filter q must include an equality match on the shard key.\n   \n   However, starting in version 4.4, documents in a sharded collection can be missing the shard key fields. To target a document that is missing the shard key, you can use the null equality match in conjunction with another filter condition (such as on the _id field). For example:\n   \n   { _id: <value>, <shardkeyfield>: null } // _id of the document missing shard key\n   \n    REPLACE DOCUMENT \nStarting in MongoDB 4.2, when replacing a document, update attempts to target a shard, first by using the query filter. If the operation cannot target a single shard by the query filter, it then attempts to target by the replacement document. In earlier versions, the operation attempts to target using the replacement document. SHARD KEY MODIFICATION \nStarting in MongoDB 4.2, you can update a document's shard key value unless the shard key field is the immutable _id field. In MongoDB 4.2 and earlier, a document's shard key field value is immutable. To modify the existing shard key value with update:  * You must run on a mongos. Do not issue the operation directly on the shard.  * You must run either in a transaction or as a retryable write.\n\n  * You must include an equality query filter on the full shard key. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also upsert on a Sharded Collection. MISSING SHARD KEY \nStarting in version 4.4, documents in a sharded collection can be missing the shard key fields. To use update to set the document's missing shard key, you must run on a mongos. Do not issue the operation directly on the shard. In addition, the following requirements also apply: Task\nRequirements\nTo set to null  * Can specify multi: true.  * Requires equality filter on the full shard key if upsert: true is specified. To set to a non-null value:  * Must be performed either inside a transaction or as a retryable write.  * Must specify multi: false.  * Requires equality filter on the full shard key if either:\n   \n   * upsert: true, or\n   \n   * if using a replacement document and the new shard key value belongs to a different shard. \nTIP Since a missing key value is returned as part of a null equality match, to avoid updating a null-valued key, include additional query conditions (such as on the _id field) as appropriate. See also:  * upsert on a Sharded Collection  * Missing Shard Key Fields \nTRANSACTIONS \nupdate can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. UPSERT WITHIN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, update with upsert: true can be run on an existing collection or a non-existing collection. If run on a non-existing collection, the operation creates the collection. In MongoDB 4.2 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nEXAMPLES  UPDATE SPECIFIC FIELDS OF ONE DOCUMENT \nUse update operators to update only the specified fields of a document. For example, create a members collection with the following documents: db.members.insertMany([   { _id: 1, member: \"abc123\", status: \"Pending\", points: 0, misc1: \"note to self: confirm status\", misc2: \"Need to activate\" },   { _id: 2, member: \"xyz123\", status: \"D\", points: 59, misc1: \"reminder: ping me at 100pts\", misc2: \"Some random comment\" },])  The following command uses the $set and $inc update operators to update the status and the points fields of a document where the member equals \"abc123\": db.runCommand(   {      update: \"members\",      updates: [         {           q: { member: \"abc123\" }, u: { $set: { status: \"A\" }, $inc: { points: 1 } }         }      ],      ordered: false,      writeConcern: { w: \"majority\", wtimeout: 5000 }   })  Because <update> document does not specify the optional multi field, the update only modifies one document, even if more than one document matches the q match condition. The returned document shows that the command found and updated a single document. The command returns: { \"n\" : 1, \"nModified\" : 1, \"ok\" : 1, <additional fields if run on a replica set/sharded cluster> } See Output for details.\n\n { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"A\", \"points\" : 1, \"misc1\" : \"note to self: confirm status\", \"misc2\" : \"Need to activate\" }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"D\", \"points\" : 59, \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" }  \nUPDATE SPECIFIC FIELDS OF MULTIPLE DOCUMENTS \nUse update operators to update only the specified fields of a document, and include the multi field set to true in the update statement. For example, a members collection contains the following documents: { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"A\", \"points\" : 1, \"misc1\" : \"note to self: confirm status\", \"misc2\" : \"Need to activate\" }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"D\", \"points\" : 59, \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" }  The following command uses the $set and $inc update operators to modify the status and the points fields respectively of all documents in the collection: db.runCommand(   {      update: \"members\",      updates: [         { q: { }, u: { $set: { status: \"A\" }, $inc: { points: 1 } }, multi: true }      ],      ordered: false,      writeConcern: { w: \"majority\", wtimeout: 5000 }   })  The update modifies all documents that match the query specified in the q field, namely the empty query which matches all documents in the collection. The returned document shows that the command found and updated multiple documents. For a replica set, the command returns: { \"n\" : 2, \"nModified\" : 2, \"ok\" : 1,  <additional fields if run on a replica set/sharded cluster> } See Output for details. After the command, the collection contains the following documents: { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"A\", \"points\" : 2, \"misc1\" : \"note to self: confirm status\", \"misc2\" : \"Need to activate\" }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60, \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" }  \nUPDATE WITH AGGREGATION PIPELINE \nStarting in MongoDB 4.2, the update command can use an aggregation pipeline for the update. The pipeline can consist of the following stages:  * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. Using the aggregation pipeline allows for a more expressive update statement, such as expressing conditional updates based on current field values or updating one field using the value of another field(s). EXAMPLE 1 \nThe following examples uses the aggregation pipeline to modify a field using the values of the other fields in the document. A members collection contains the following documents: { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"A\", \"points\" : 2, \"misc1\" : \"note to self: confirm status\", \"misc2\" : \"Need to activate\" }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60, \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" }  Assume that instead of separate misc1 and misc2 fields, you want to gather these into a new comments field. The following update operation uses an aggregation pipeline to add the new comments field and remove the misc1 and misc2 fields for all documents in the collection.  * First, set the status field to \"Modified\" and add a new field comments that contains the current contents of two other fields misc1 and misc2 fields.  * Second, remove the misc1 and misc2 fields.\n\n  \nNOTE The $set and $unset used in the pipeline refers to the aggregation stages $set and $unset respectively, and not the update operators $set and $unset. The returned document shows that the command found and updated multiple documents. The command returns: { \"n\" : 2, \"nModified\" : 2, \"ok\" : 1, <additional fields if run on a replica set/sharded cluster> } See Output for details. After the command, the collection contains the following documents: { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"Modified\", \"points\" : 2, \"comments\" : [ \"note to self: confirm status\", \"Need to activate\" ] }{ \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"Modified\", \"points\" : 60, \"comments\" : [ \"reminder: ping me at 100pts\", \"Some random comment\" ] }  EXAMPLE 2 \nThe aggregation pipeline allows the update to perform conditional updates based on the current field values as well as use current field values to calculate a separate field value. db.students.insertMany( [   { \"_id\" : 1, \"tests\" : [ 95, 92, 90 ] },   { \"_id\" : 2, \"tests\" : [ 94, 88, 90 ] },   { \"_id\" : 3, \"tests\" : [ 70, 75, 82 ] }] );  Using an aggregation pipeline, you can update the documents with the calculated grade average and letter grade. db.runCommand(   {      update: \"students\",      updates: [         {           q: { },           u: [                 { $set: { average : { $avg: \"$tests\" } } },                 { $set: { grade: { $switch: {                                       branches: [                                           { case: { $gte: [ \"$average\", 90 ] }, then: \"A\" },                                           { case: { $gte: [ \"$average\", 80 ] }, then: \"B\" },                                           { case: { $gte: [ \"$average\", 70 ] }, then: \"C\" },                                           { case: { $gte: [ \"$average\", 60 ] }, then: \"D\" }                                       ],                                       default: \"F\"                 } } } }           ],           multi: true         }      ],      ordered: false,      writeConcern: { w: \"majority\", wtimeout: 5000 }   })  \nNOTE The $set used in the pipeline refers to the aggregation stage $set, and not the update operators $set.\n\n The returned document shows that the command found and updated multiple documents. The command returns: { \"n\" : 3, \"nModified\" : 3, \"ok\" : 1, <additional fields if run on a replica set/sharded cluster> } After the command, the collection contains the following documents: { \"_id\" : 1, \"tests\" : [ 95, 92, 90 ], \"average\" : 92.33333333333333, \"grade\" : \"A\" }{ \"_id\" : 2, \"tests\" : [ 94, 88, 90 ], \"average\" : 90.66666666666667, \"grade\" : \"A\" }{ \"_id\" : 3, \"tests\" : [ 70, 75, 82 ], \"average\" : 75.66666666666667, \"grade\" : \"C\" }  \nBULK UPDATE \nThe following example performs multiple update operations on the members collection: db.runCommand(   {      update: \"members\",      updates: [         { q: { status: \"P\" }, u: { $set: { status: \"D\" } }, multi: true },         { q: { _id: 5 }, u: { _id: 5, name: \"abc123\", status: \"A\" }, upsert: true }      ],      ordered: false,      writeConcern: { w: \"majority\", wtimeout: 5000 }   })  The returned document shows that the command modified 10 documents and inserted a document with the _id value 5. See Output for details. {   \"ok\" : 1,   \"nModified\" : 10,   \"n\" : 11,   \"upserted\" : [      {         \"index\" : 1,         \"_id\" : 5      }   ]}  \nSPECIFY COLLATION \nCollation allows users to specify language-specific rules for string comparison, such as rules for lettercase and accent marks. A collection myColl has the following documents: { _id: 1, category: \"café\", status: \"A\" }{ _id: 2, category: \"cafe\", status: \"a\" }{ _id: 3, category: \"cafE\", status: \"a\" }  The following operation includes the collation option: db.runCommand({   update: \"myColl\",   updates: [     { q: { category: \"cafe\", status: \"a\" }, u: { $set: { status: \"Updated\" } }, collation: { locale: \"fr\", strength: 1 } }   ]})  \nSPECIFY ARRAYFILTERS FOR ARRAY UPDATE OPERATIONS \nStarting in MongoDB 3.6, when updating an array field, you can specify arrayFilters that determine which array elements to update. UPDATE ELEMENTS MATCH ARRAYFILTERS CRITERIA \nCreate a collection students with the following documents: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] },   { \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] },   { \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }] );  To modify all elements that are greater than or equal to 100 in the grades array, use the filtered positional operator $[<identifier>] with the arrayFilters option: db.runCommand( {   update: \"students\",   updates: [     { q: { grades: { $gte: 100 } }, u: { $set: { \"grades.$[element]\" : 100 } }, arrayFilters: [ { \"element\": { $gte: 100 } } ], multi: true}   ]} )  After the operation, the collection contains the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 100 ] }{ \"_id\" : 3, \"grades\" : [ 95, 100, 100 ] }  UPDATE SPECIFIC ELEMENTS OF AN ARRAY OF DOCUMENTS \nCreate a collection students2 with the following documents:\n\n  To modify the value of the mean field for all elements in the grades array where the grade is greater than or equal to 85, use the filtered positional operator $[<identifier>] with the arrayFilters: db.runCommand({   update: \"students2\",   updates: [     { q: { }, u: { $set: { \"grades.$[elem].mean\" : 100 } }, arrayFilters: [ { \"elem.grade\": { $gte: 85 } } ], multi: true }   ]})  After the operation, the collection has the following documents: {   \"_id\" : 1,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 6 }   ]}{   \"_id\" : 2,   \"grades\" : [      { \"grade\" : 90, \"mean\" : 100, \"std\" : 6 },      { \"grade\" : 87, \"mean\" : 100, \"std\" : 3 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 }   ]}  \nSPECIFY HINT FOR UPDATE OPERATIONS \nNew in version 4.2. Create a sample members collection with the following documents: db.members.insertMany([   { \"_id\" : 1, \"member\" : \"abc123\", \"status\" : \"P\", \"points\" :  0,  \"misc1\" : null, \"misc2\" : null },   { \"_id\" : 2, \"member\" : \"xyz123\", \"status\" : \"A\", \"points\" : 60,  \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" },   { \"_id\" : 3, \"member\" : \"lmn123\", \"status\" : \"P\", \"points\" :  0,  \"misc1\" : null, \"misc2\" : null },   { \"_id\" : 4, \"member\" : \"pqr123\", \"status\" : \"D\", \"points\" : 20,  \"misc1\" : \"Deactivated\", \"misc2\" : null },   { \"_id\" : 5, \"member\" : \"ijk123\", \"status\" : \"P\", \"points\" :  0,  \"misc1\" : null, \"misc2\" : null },   { \"_id\" : 6, \"member\" : \"cde123\", \"status\" : \"A\", \"points\" : 86,  \"misc1\" : \"reminder: ping me at 100pts\", \"misc2\" : \"Some random comment\" }])  Create the following indexes on the collection: db.members.createIndex( { status: 1 } )db.members.createIndex( { points: 1 } )  The following update operation explicitly hints to use the index { status: 1 }: \nNOTE If you specify an index that does not exist, the operation errors. db.runCommand({   update: \"members\",   updates: [     { q: { \"points\": { $lte: 20 }, \"status\": \"P\" }, u: { $set: { \"misc1\": \"Need to activate\" } }, hint: { status: 1 }, multi: true }   ]})  The update command returns the following: { \"n\" : 3, \"nModified\" : 3, \"ok\" : 1 }  To see the index used, run explain on the operation: db.runCommand(   {     explain: {       update: \"members\",       updates: [         { q: { \"points\": { $lte: 20 }, \"status\": \"P\" }, u: { $set: { \"misc1\": \"Need to activate\" } }, hint: { status: 1 }, multi: true }       ]     },     verbosity: \"queryPlanner\"   })  The explain does not modify the documents. \nUSE VARIABLES IN LET OPTION OR C FIELD \nNew in version 5.0.\n\n \nNOTE To filter results using a variable, you must access the variable within the $expr operator. Create a collection cakeFlavors: db.cakeFlavors.insertMany( [   { _id: 1, flavor: \"chocolate\" },   { _id: 2, flavor: \"strawberry\" },   { _id: 3, flavor: \"cherry\" }] )  The following example defines targetFlavor and newFlavor variables in let and uses the variables to change the cake flavor from cherry to orange: db.runCommand( {   update: db.cakeFlavors.getName(),   updates: [      { q: { $expr: { $eq: [ \"$flavor\", \"$$targetFlavor\" ] } },        u: [ { $set: { flavor: \"$$newFlavor\" } } ] }   ],   let : { targetFlavor: \"cherry\", newFlavor: \"orange\" }} )  The next example defines targetFlavor and newFlavor variables in c and uses the variables to change the cake flavor from chocolate to vanilla: db.runCommand( {   update: db.cakeFlavors.getName(),   updates: [      { q: { $expr: { $eq: [ \"$flavor\", \"$$targetFlavor\" ] } },        u: [ { $set: { flavor: \"$$newFlavor\" } } ],        c: { targetFlavor: \"chocolate\", newFlavor: \"vanilla\" } }    ]} )  \nOUTPUT \nThe returned document contains a subset of the following fields: update.ok \nThe status of the command. update.n \nThe number of documents selected for update. If the update operation results in no change to the document, e.g. $set expression updates the value to the current value, n can be greater than nModified. update.nModified \nThe number of documents updated. If the update operation results in no change to the document, such as setting the value of the field to its current value, nModified can be less than n. update.upserted \nAn array of documents that contains information for each document inserted through the update with upsert: true. Each document contains the following information: update.upserted.index \nAn integer that identifies the update with upsert:true statement in the updates array, which uses a zero-based index. update.upserted._id \nThe _id value of the added document. update.writeErrors \nAn array of documents that contains information regarding any error encountered during the update operation. The writeErrors array contains an error document for each update statement that errors. Each error document contains the following fields: update.writeErrors.index \nAn integer that identifies the update statement in the updates array, which uses a zero-based index. update.writeErrors.code \nAn integer value identifying the error. update.writeErrors.errmsg \nA description of the error. update.writeConcernError \nDocument that describe error related to write concern and contains the field: update.writeConcernError.code \nAn integer value identifying the cause of the write concern error. update.writeConcernError.errmsg \nA description of the cause of the write concern error. update.writeConcernError.errInfo.writeConcern \nNew in version 4.4. The write concern object used for the corresponding operation. For information on write concern object fields, see Write Concern Specification. The write concern object may also contain the following field, indicating the source of the write concern: update.writeConcernError.errInfo.writeConcern.provenance \nA string value indicating where the write concern originated (known as write concern provenance). The following table shows the possible values for this field and their significance: Provenance\nDescription\nclientSupplied\nThe write concern was specified in the application.\ncustomDefault\nThe write concern originated from a custom defined default value. See setDefaultRWConcern.\ngetLastErrorDefaults\nThe write concern originated from the replica set's settings.getLastErrorDefaults field.\nimplicitDefault\nThe write concern originated from the server in absence of all other write concern specifications. In addition to the aforementioned update specific return fields, the db.runCommand() includes additional information:  * for replica sets: optime, electionId, $clusterTime, and operationTime.  * for sharded clusters: operationTime and $clusterTime. See db.runCommand Response for details on these fields.\n\n {   \"ok\" : 1,   \"nModified\" : 0,   \"n\" : 1,   \"upserted\" : [      {         \"index\" : 0,         \"_id\" : ObjectId(\"52ccb2118908ccd753d65882\")      }   ]}  The following is an example document returned for a bulk update involving three update statements, where one update statement was successful and two other update statements encountered errors: {   \"ok\" : 1,   \"nModified\" : 1,   \"n\" : 1,   \"writeErrors\" : [      {         \"index\" : 1,         \"code\" : 16837,         \"errmsg\" : \"The _id field cannot be changed from {_id: 1.0} to {_id: 5.0}.\"      },      {         \"index\" : 2,         \"code\" : 16837,         \"errmsg\" : \"The _id field cannot be changed from {_id: 2.0} to {_id: 6.0}.\"      },   ]} \n←  resetErrorQuery Plan Cache Commands → On this page  * Definition\n * Syntax\n * Command Fields\n * Access Control\n * Behavior\n * Examples\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setDefaultRWConcern/": " Docs Home → MongoDB Manual \nSETDEFAULTRWCONCERN \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Example \nDEFINITION \nNew in version 4.4. setDefaultRWConcern \nThe setDefaultRWConcern administrative command sets the global default read or write concern configuration for a replica set or sharded cluster. setDefaultRWConcern must be run against the admin database.      * For replica sets, issue the setDefaultRWConcern command on the primary mongod.  * For sharded clusters, issue the setDefaultRWConcern on a mongos. \nSYNTAX \nThe command has the following syntax: db.adminCommand(  {    setDefaultRWConcern : 1,    defaultReadConcern: { <read concern> },    defaultWriteConcern: { <write concern> },    writeConcern: { <write concern> },    comment: <any>  })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nsetDefaultRWConcern\nint Set to 1. defaultReadConcern\nobject Document containing the global read concern configuration. Specify a valid read concern object.  * setDefaultRWConcern supports the following subset of level:\n   \n   * \"local\"\n   \n   * \"available\"\n   \n   * \"majority\"  * To unset the currently configured default read concern, specify an empty document {}.  * setDefaultRWConcern only supports the levels read concern setting. You cannot specify any other read concern setting in the default read concern. Omit this document to leave the current global read concern unmodified. If omitted, setDefaultRWConcern must specify defaultWriteConcern. defaultWriteConcern\nobject Document containing the global default write concern configuration.  * For the write concern w setting, setDefaultRWConcern supports all write concern values except w : 0.  * For the write concern wtimeout setting, setDefaultRWConcern defaults to 0 if the setting is omitted. Operations block until the requested write concern is met. If specifying a global default wtimeout, ensure the value is large enough to allow write operations to achieve the requested write concern.  * To unset the currently configured write concern, specify an empty document {}. Omit this document to leave the current global write concern unmodified. If omitted, setDefaultRWConcern must specify defaultReadConcern. writeConcern\nobject Optional. A document that specifies the write concern to be used by the setDefaultRWConcern command itself. If omitted, setDefaultRWConcern uses the previously set global default write concern if one was configured. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. setDefaultRWConcern returns an object that contains the currently configured global default read and write concern. See getDefaultRWConcern for more complete documentation on the returned fields. \nBEHAVIOR  NOTE \nREQUIRES FEATURECOMPATIBILITYVERSION 4.4+ Each mongod in the replica set or sharded cluster must have featureCompatibilityVersion set to at least 4.4 to use setDefaultRWConcern. Starting in MongoDB 5.0, once the Cluster Wide Write Concern (CWWC) is set via the setDefaultRWConcern command the write concern cannot be unset. MongoDB only applies the global default read or write concern to operations which do not explicitly specify a read or write concern. If MongoDB applies the global default read or write concern to an operation, that operation behaves as if that read or write concern were explicitly specified by the issuing client. \nREPLICA SETS \nIssue setDefaultRWConcern against the replica set primary. The primary replicates the new global default settings to the remaining members of the replica set. Secondaries which have not yet replicated the updated global default settings continue using their local 'stale' copy of the defaults. Issue the setDefaultRWConcern command with a writeConcern of w : \"majority\" to ensure the command only returns after the changes have propagated to a majority of replica set members. \nSHARDED CLUSTERS\n\n Issue the setDefaultRWConcern command with a writeConcern of w : \"majority\" to ensure the command only returns after the changes have propagated to a majority of CSRS members. When an application issues an operation against the mongos without explicitly specifying a read or write concern setting, the mongos applies the corresponding global default setting. The global default settings do not propagate to the individual shards. You cannot run setDefaultRWConcern against a shard. \nIMPORTANT setDefaultRWConcern requires featureCompatibilityVersion 4.4+. If you downgrade your deployment's featureCompatibilityVersion from 4.4 to 4.2, all cluster-wide read and write concern defaults are lost, but mongos instances may continue applying the defaults for up to 30 seconds. SHARDING ADMINISTRATIVE COMMANDS OVERRIDE WRITE CONCERN SETTINGS \nSharding administrative commands that perform write operations on the config server, such as the enableSharding or addShard commands, have specific behavior with global default write concern settings:  * The commands use \"majority\" regardless of the configured global default write concern.  * The commands use a minimum wtimeout of 60000. The commands only use the global default write concern wtimeout if it is greater than 60000. \nACCESS CONTROL \nFor replica sets or sharded clusters enforcing Authentication, setDefaultRWConcern requires that the authenticated user have the setDefaultRWConcern privilege action. The clusterManager built-in role provides the required privileges to run setDefaultRWConcern. \nEXAMPLE  SET GLOBAL DEFAULT WRITE CONCERN \nThe following operation sets the global write concern to the following:  * w: 2 db.adminCommand({  \"setDefaultRWConcern\" : 1,  \"defaultWriteConcern\" : {    \"w\" : 2  }})  The operation returns a document similar to the following: {  \"defaultWriteConcern\" : {   \"w\" : 2  },  \"updateOpTime\" : Timestamp(1586290895, 1),  \"updateWallClockTime\" : ISODate(\"2020-04-07T20:21:41.849Z\"),  \"localUpdateWallClockTime\" : ISODate(\"2020-04-07T20:21:41.862Z\"),  \"ok\" : 1,  \"$clusterTime\" : { ... }  \"operationTime\" : Timestamp(1586290925, 1)} \nSET GLOBAL DEFAULT READ CONCERN \nThe following operation sets the global read concern to \"majority\": db.adminCommand({  \"setDefaultRWConcern\" : 1,  \"defaultReadConcern\" : { \"level\" : \"majority\" }})  The operation returns a document similar to the following: {  \"defaultReadConcern\" : {    \"level\" : \"majority\"  },  \"updateOpTime\" : Timestamp(1586290895, 1),  \"updateWallClockTime\" : ISODate(\"2020-04-07T20:21:41.849Z\"),  \"localUpdateWallClockTime\" : ISODate(\"2020-04-07T20:21:41.862Z\"),  \"ok\" : 1,  \"$clusterTime\" : { ... }  \"operationTime\" : Timestamp(1586290925, 1)} \nSET GLOBAL DEFAULT READ AND WRITE CONCERN \nThe following operation sets the global default read and write concern to the following:  * w: 2 write concern  * level: \"majority\" read concern. db.adminCommand({  \"setDefaultRWConcern\" : 1,  \"defaultWriteConcern\" : {    \"w\" : 2  },  \"defaultReadConcern\" : { \"level\" : \"majority\" }})  The operation returns a document similar to the following: \"defaultWriteConcern\" : {  \"w\" : 2},\"defaultReadConcern\" : {  \"level\" : \"majority\"} \nUNSET GLOBAL DEFAULT READ AND WRITE CONCERN \nYou can:  * Unset the global default read concern.  * Only unset the global default write concern if you haven't already set it. For example, assume the global default read concern is set to level: \"majority\". To unset the global default read concern, use an empty document {}: db.adminCommand( {   \"setDefaultRWConcern\" : 1,   \"defaultReadConcern\" : {}} )  The operation returns a document that indicates the operation was successful:\n\n You can only unset the global default write concern if you haven't already set it. To unset the global default write concern, use an empty document {}: db.adminCommand( {   \"setDefaultRWConcern\" : 1,   \"defaultWriteConcern\" : {}} )  If the global default write concern is:  * Unset, the operation succeeds.  * Already set, the operation returns the following error. MongoServerError: The global default write concern cannot be unsetonce it is set. ←  setParametersetUserWriteBlockMode → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/connPoolStats/": " Docs Home → MongoDB Manual \nCONNPOOLSTATS \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example\n * Output \nDEFINITION \nconnPoolStats \nThe command connPoolStats returns information regarding the open outgoing connections from the current database instance to other members of the sharded cluster or replica set. To run connPoolStats, use the db.runCommand( { <command> } ) method. \nNOTE connPoolStats only returns meaningful results for mongos instances and for mongod instances in sharded clusters. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     connPoolStats: 1   })  The value of the argument (i.e. 1 ) does not affect the output of the command. \nBEHAVIOR \nconnPoolStats includes aggregated statistics in its output:      * The hosts field displays the information aggregated by host.  * The pools field displays the information aggregated by pool. \nNOTE To avoid interference with any running operations, connPoolStats does not take any locks. As such, the counts may change slightly as connPoolStats gathers information, resulting in slight differences between the hosts and pools connection counts. \nEXAMPLE \nThe following operation uses the db.runCommand() method to run the connPoolStats command on a mongos of a sharded cluster. The sharded cluster has 2 shards, each a single-member replica set, and a config server replica set. The mongos runs on a 4-core machine. db.runCommand( { \"connPoolStats\" : 1 } )  The command returns the output of the following form: \nNOTE The connPoolStats output varies depending on the deployment and the member against which you run connPoolStats among other factors.\n\n  \nOUTPUT \nconnPoolStats.totalAvailable \nReports the total number of available outgoing connections from the current mongod / mongos instance to other members of the sharded cluster or replica set. connPoolStats.totalCreated \nReports the total number of outgoing connections ever created by the current mongod / mongos instance to other members of the sharded cluster or replica set. connPoolStats.totalInUse \nReports the total number of outgoing connections from the current mongod / mongos instance to other members of the sharded cluster or replica set that are currently in use. connPoolStats.totalRefreshing \nReports the total number of outgoing connections from the current mongod / mongos instance to other members of the sharded cluster or replica set that are currently being refreshed. connPoolStats.totalWasNeverUsed \nReports the total number of outgoing connections from the current mongod / mongos instance to other members of the sharded cluster or replica set that were closed having never been used. connPoolStats.replicaSetMatchingStrategy \nNew in version 5.0: (Also available starting in 4.4.5 and 4.2.13) On a mongos instance, this value reports the policy used by the instance to determine the minimum size limit of its connection pools to nodes within replica sets. On a mongod instance, this value reports the policy used by the instance to determine the minimum size limit of its connection pools to nodes within other replica sets. The policy for mongos and mongod can be set with the ShardingTaskExecutorPoolReplicaSetMatching parameter. \nNOTE If the ShardingTaskExecutorPoolReplicaSetMatching is set to \"automatic\", the replicaSetMatchingStrategy still describes the actual policy being used, not \"automatic\". To find the value of the ShardingTaskExecutorPoolReplicaSetMatching, use getParameter which returns the value of the server parameter: db.adminCommand( { getParameter : 1, \"ShardingTaskExecutorPoolReplicaSetMatching\" : 1 } ) \nconnPoolStats.numClientConnection \nReports the number of active and stored outgoing synchronous connections from the current mongod / mongos instance to other members of the sharded cluster or replica set. These connections are a part of a pool that is a subset of the data reported by totalAvailable, totalCreated, and totalInUse. connPoolStats.numAScopedConnection \nReports the number of active and stored outgoing scoped synchronous connections from the current mongod / mongos instance to other members of the sharded cluster or replica set. These connections are a part of a pool that is a subset of the data reported by totalAvailable, totalCreated, and totalInUse. connPoolStats.pools \nReports on connection statistics (in use/available/created/refreshing) grouped by the connection pools. A mongod or mongos has two distinct families of outgoing connection pools:  * DBClient-based pools (the \"write path\") and  * NetworkInterfaceTL-based pools (the \"read path\"). For each pool, the command returns a document that resembles the following: {  \"poolInUse\" : <num>,  \"poolAvailable\" : <num>,  \"poolCreated\" : <num>,  \"poolRefreshing\" : <num>,  \"[host1]\" : { \"inUse\" : <num>, \"available\" : <num>, \"created\" : <num>, \"refreshing\" : <num> },  \"[host2]\" : { \"inUse\" : <num>, \"available\" : <num>, \"created\" : <num>, \"refreshing\" : <num> },  ...}  \nNOTE If there are no connections (in use/available/created/refreshing) for a particular pool, the connPoolStats does not return statistics for that pool. connPoolStats.pools.NetworkInterfaceTL-TaskExecutorPool-[n] \nDisplays connection statics related to TaskExecutor pools. Typically, there will be one TaskExecutorPool per core, e.g. pools.NetworkInterfaceTL-TaskExecutorPool-0 ... pools.NetworkInterfaceTL-TaskExecutorPool-7 for an 8-core processor. \nTIP \nSEE ALSO: taskExecutorPoolSize connPoolStats.pools.NetworkInterfaceTL-ShardRegistry \nAvailable if the command is run on a member of a sharded cluster. Displays the pool statistics for the connections between the current mongod / mongos instance and other members of the sharded cluster. connPoolStats.pools.NetworkInterfaceTL-Replication\n\n Displays the pool statistics for the connections between the current mongod instance and the other members of the replica set. connPoolStats.pools.global \nDisplays the DBClient-based pool statistics. connPoolStats.pools.[pool].poolWasNeverUsed \nDisplays the number of connections in this connection pool that were closed having never been used. connPoolStats.hosts \nReports on connection statistics (in use/available/created/refreshing) grouped by the hosts. Contains documents that represent a report of connections between the current mongod / mongos instance and each member of the sharded cluster or replica set. connPoolStats.hosts.[host].available \nReports the total number of connections available for connecting to the [host]. connPoolStats.hosts.[host].created \nReports the number of connections to the [host] ever created. connPoolStats.hosts.[host].inUse \nReports the number of connections to the [host] that are currently in use. connPoolStats.hosts.[host].wasNeverUsed \nReports the total number of outgoing connections from host to other members of the sharded cluster or replica set that were closed having never been used. connPoolStats.replicaSets \nContains documents that represent a report of information related to each replica set connected to the current mongod / mongos. connPoolStats.replicaSets.replicaSet \nReports on each replica set connected to the current mongod / mongos. connPoolStats.replicaSets.[replicaSets].hosts \nHolds an array of documents that reports on each member in the replica set. These values derive from the replica set status values. connPoolStats.replicaSets.[replicaSet].hosts[n].addr \nReports the address for the member in the replica set in [hostname]:[port] format. connPoolStats.replicaSets.[replicaSet].hosts[n].ok \nReports false when:  * the current mongos or mongod cannot connect to instance.  * the current mongos or mongod received a connection exception or error. This field is for internal use. connPoolStats.replicaSets.[replicaSet].hosts[n].ismaster \nReports true if this host is the primary member of the replica set. connPoolStats.replicaSets.[replicaSet].hosts[n].hidden \nReports true if this host is a hidden member of the replica set. connPoolStats.replicaSets.[replicaSet].hosts[n].secondary \nReports true if this host is a secondary member of the replica set. connPoolStats.replicaSets.[replicaSet].hosts[n].pingTimeMillis \nReports the ping time in milliseconds from the mongos or mongod to this host. connPoolStats.replicaSets.[replicaSet].hosts[n].tags \nReports the members[n].tags, if this member of the set has tags configured. See also Response for details on the ok status field, the operationTime field and the $clusterTime field. host ←  collStatsconnectionStatus → On this page  * Definition\n * Syntax\n * Behavior\n * Example\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/configureCollectionBalancing/": " Docs Home → MongoDB Manual \nCONFIGURECOLLECTIONBALANCING \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \nconfigureCollectionBalancing \nNew in version 5.3. Configures balancer settings for a sharded collection, such as setting the chunk size for and defragmenting the collection. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     configureCollectionBalancing: \"<db>.<collection>\",     chunkSize: <num>,     defragmentCollection: <bool>   })  \nCOMMAND FIELDS \nconfigureCollectionBalancing has the following fields: Field\nType\nDescription\nconfigureCollectionBalancing\nstring\nRequired: The name of the database and sharded collection to configure.\nchunkSize\ninteger\nOptional: Sets the chunk size in MiB for the collection. The recommended size is 256, 512, or larger.\ndefragmentCollection\nboolean\nOptional: Causes the balancer to defragment the collection. For more information, see Data Partitioning with Chunks. \nBEHAVIOR  NO COLLECTION CHUNKSIZE SPECIFIED \nIf you do not specify chunkSize for a collection and no custom size has been set previously, the global default chunkSize is used for balancing. \nSPECIFYING CHUNKSIZE: 0 \nIf you use configureCollectionBalancing with chunkSize: 0, the per-collection chunkSize is reset and the global default chunkSize is used for balancing. For more information on configuring default chunkSize, see Modify Range Size in a Sharded Cluster. \nEXAMPLES  CONFIGURE CHUNK SIZE \nTo change the chunk size for a sharded collection, use the chunkSize option: db.adminCommand( {   configureCollectionBalancing: \"test.students\",   chunkSize: 256} )  Use this command to change the chunk size for the given collection. \nWARNING By default, MongoDB cannot move a chunk if the number of documents in the chunk is greater than 1.3 times the result of dividing the configured chunk size by the average document size. To find the average document size, see the avgObjSize field in the output of the db.collection.stats() method. For more information, see Range Size. \nDEFRAGMENT COLLECTIONS  WARNING We do not recommend using defragmentCollection to defragment sharded collections for MongoDB 6.0.0 to 6.0.3 and MongoDB 6.1.0 to 6.1.1, as the defragmentation process on these releases can make databases and collections unavailable for extended periods of time. To tell the balancer to defragment a sharded collection, use the defragmentCollection option: db.adminCommand( {   configureCollectionBalancing: \"test.students\",   defragmentCollection: true} )  Use this command to have the balancer defragment a sharded collection. To monitor the chunk defragmentation process, use the balancerCollectionStatus command. \nRECONFIGURE AND DEFRAGMENT COLLECTIONS \nTo defragment a sharded collection while updating the chunk size, use the defragmentCollection option and the chunkSize option together: db.adminCommand( {   configureCollectionBalancing: \"test.students\",   chunkSize: 512,   defragmentCollection: true} ) \n←  commitReshardCollectionenableSharding → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dropAllRolesFromDatabase/": " Docs Home → MongoDB Manual \nDROPALLROLESFROMDATABASE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \ndropAllRolesFromDatabase \nDeletes all user-defined roles on the database where you run the command. \nWARNING The dropAllRolesFromDatabase removes all user-defined roles from the database. \nTIP In mongosh, this command can also be run through the db.dropAllRoles() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dropAllRolesFromDatabase: 1,     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\ndropAllRolesFromDatabase\ninteger\nSpecify 1 to drop all user-defined roles from the database where the command is run.\nwriteConcern\ndocument\n/includes/source/fact-write-concern-spec-link.rst\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nREQUIRED ACCESS \nYou must have the dropRole action on a database to drop a role from that database. \nEXAMPLE \nThe following operations drop all user-defined roles from the products database: use productsdb.runCommand(   {     dropAllRolesFromDatabase: 1,     writeConcern: { w: \"majority\" }   })  The n field in the results document reports the number of roles dropped: { \"n\" : 4, \"ok\" : 1 } \n←  dropRolegrantPrivilegesToRole → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/moveChunk/": " Docs Home → MongoDB Manual \nMOVECHUNK \nOn this page    \n * Definition\n   \n * Considerations\n * Behavior \nDEFINITION \nmoveChunk \nInternal administrative command. Moves chunks between shards. Issue the moveChunk command via a mongos instance while using the admin database. Use the following forms: \nTIP In mongosh, this command can also be run through the sh.moveChunk() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. db.adminCommand( { moveChunk : <namespace> ,                 find : <query> ,                 to : <string>,                 forceJumbo: <boolean>, // Starting in MongoDB 4.4                 _secondaryThrottle : <boolean>,                 writeConcern: <document>,                 _waitForDelete : <boolean> } )  Alternatively: db.adminCommand( { moveChunk : <namespace> ,                 bounds : <array> ,                 to : <string>,                 forceJumbo: <boolean>, // Starting in MongoDB 4.4                 _secondaryThrottle : <boolean>,                 writeConcern: <document>,                 _waitForDelete : <boolean> } )  The moveChunk command has the following fields: Field\nType\nDescription\nmoveChunk\nstring\nThe namespace of the collection where the chunk exists. Specify the collection's full namespace, including the database name.\nfind\ndocument\nAn equality match on the shard key that specifies the shard-key value of the chunk to move. Specify either the bounds field or the find field but not both. Do not use the find field to select chunks in collections that use a hashed shard key.\nbounds\narray\nThe bounds of a specific chunk to move. The array must consist of two documents that specify the lower and upper shard key values of a chunk to move. Specify either the bounds field or the find field but not both. Use bounds to select chunks in collections that use a hashed shard key.\nto\nstring\nThe name of the destination shard for the chunk.\nforceJumbo\nboolean Optional. A flag that determines if the command can move a chunk that is too large to migrate. The chunk may or may not be labeled as jumbo.      * If true, the command can move the chunk.  * If false, the command cannot move the chunk. The default is false. \nWARNING When you run moveChunk with forceJumbo: true, write operations to the collection may block for a long period of time during the migration. To migrate these large chunks without this long blocking period, see Balance Ranges that Exceed Size Limit instead. New in version 4.4. _secondaryThrottle\nboolean Optional. Starting in MongoDB 3.4, for WiredTiger, defaults to false.  * If true, then by default, each document move during chunk migration propagates to at least one secondary before the balancer proceeds with the next document. This is equivalent to a write concern of { w: 2 }.\n   \n   Use the writeConcern option to specify a different write concern.  * If false, the balancer does not wait for replication to a secondary and instead continues with the next document. For more information, see Secondary Throttle. writeConcern\ndocument Optional. A document that expresses the write concern that the _secondaryThrottle will use to wait for secondaries during the chunk migration. writeConcern requires _secondaryThrottle: true.\n\n The value of bounds takes the form: [ { hashedField : <minValue> } ,  { hashedField : <maxValue> } ]  The chunk migration section describes how chunks move between shards on MongoDB. \nTIP \nSEE ALSO:  * split  * sh.moveChunk()  * sh.splitAt()  * sh.splitFind() \nCONSIDERATIONS \nOnly use the moveChunk in special circumstances such as preparing your sharded cluster for an initial ingestion of data, or a large bulk import operation. In most cases allow the balancer to create and balance chunks in sharded clusters. See Create Ranges in a Sharded Cluster for more information. \nBEHAVIOR  INDEXES \nmoveChunk requires that all indexes exist on the target (i.e. to ) shard before migration and returns an error if a required index does not exist. \nMETA DATA ERROR \nmoveChunk returns the following error message if another metadata operation is in progress on the chunks collection: errmsg: \"The collection's metadata lock is already taken.\"  If another process, such as a balancer process, changes meta data while moveChunk is running, you may see this error. You may retry the moveChunk operation without side effects. \nMAXCATCHUPPERCENTAGEBEFOREBLOCKINGWRITES SERVER PARAMETER \nStarting in MongoDB 5.0 (and 4.4.7, 4.2.15, 4.0.26), you can set the maxCatchUpPercentageBeforeBlockingWrites to specify the maximum allowed percentage of data not yet migrated during a moveChunk operation when compared to the total size (in MBs) of the chunk being transferred. ←  medianKeymovePrimary → On this page  * Definition\n * Considerations\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/clearJumboFlag/": " Docs Home → MongoDB Manual \nCLEARJUMBOFLAG \nOn this page    \n * Definition\n   \n * Syntax\n * Access Control\n * Example \nDEFINITION \nclearJumboFlag \nAvailable starting in 4.2.3 and 4.0.15 Clears the jumbo flag for a chunk. To use the command, issue the clearJumboFlag command on a mongos instance. \nSYNTAX \nTh command has the following syntax: db.adminCommand(   {     clearJumboFlag: \"<database>.<collection>\",     bounds: <array>   })  -OR- // Cannot use for collections with hashed shard keys\ndb.adminCommand(   {     clearJumboFlag: \"<database>.<collection>\",     find: <query>   })  \nCOMMAND FIELDS \nThe clearJumboFlag command takes the following fields as arguments: Field\nType\nDescription\nclearJumboFlag\nstring The namespace of the sharded collection with the jumbo chunk(s). Specify the collection's full namespace, including the database name (i.e. \"<database>.<collection>\") bounds\narray The exact bounds of a specific chunk. The array must consist of two documents that specify the lower and upper shard key values of a chunk to move: [ { <shardKey> : <minValue> },{ <shardKey> : <maxValue> } ] \nNOTE      * Specify either the bounds field or the find field but not both.  * If the collection uses a hashed shard key, use the bounds field. find\ndocument A specific shard key and its value contained in the jumbo chunk. { <shardKey> : <value> } \nNOTE  * Specify either the bounds field or the find field but not both.  * If the collection uses a hashed shard key, do not use the find field. Use bounds instead. \nACCESS CONTROL \nOn systems running with authorization, the user must have the clearJumboFlag privilege actions on the { db: \"\", collection: \"\" } resource. The built-in role clusterManager provides the appropriate privileges. \nEXAMPLE  CLEAR JUMBO FLAG FOR A CHUNK (RANGE-BASED SHARD KEY) \nThe sh.status() includes the following sh.status.databases.<collection>.chunk-details for the test.jumbo collection. ... // Content omitted for brevity\ntest.jumbo         shard key: { \"x\" : 1 }         unique: false         balancing: true         chunks:                  shardA   2                  shardB   2         { \"x\" : { \"$minKey\" : 1 } } -->> { \"x\" : 1 } on : shardB Timestamp(3, 0)         { \"x\" : 1 } -->> { \"x\" : 2 } on : shardA Timestamp(6, 1) jumbo         { \"x\" : 2 } -->> { \"x\" : 3 } on : shardA Timestamp(5, 1) jumbo         { \"x\" : 3 } -->> { \"x\" : { \"$maxKey\" : 1 } } on : shardB Timestamp(6, 0) The following clearJumboFlag command specifies the bounds of the { \"x\" : 1 } -->> { \"x\" : 2 } chunk: db.adminCommand( {   clearJumboFlag: \"test.jumbo\",   bounds: [{ \"x\" : 1 }, { \"x\" : 2 }]} )  Upon success, the command returns \"ok\": 1 in its output:\n\n The following clearJumboFlag command specifies the find field to find the chunk that contains the shard key { \"x\" : 2 } : db.adminCommand( {   clearJumboFlag: \"test.jumbo\",   find: { \"x\" : 2 }} )  Upon success, the command returns \"ok\": 1 in its output: {   \"ok\" : 1,   \"operationTime\" : Timestamp(1580191819, 5),   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1580191819, 5),      \"signature\" : {         \"hash\" : BinData(0,\"N6x6drN7HUq5MR5ezUJns1rfeqY=\"),         \"keyId\" : NumberLong(\"6786859092951433239\")      }   }} To verify the operation, run sh.status() again. The jumbo flag should no longer appear in its output. ... // Content omitted for brevity\ntest.jumbo         shard key: { \"x\" : 1 }         unique: false         balancing: true         chunks:                  shardA   2                  shardB   2         { \"x\" : { \"$minKey\" : 1 } } -->> { \"x\" : 1 } on : shardB Timestamp(3, 0)         { \"x\" : 1 } -->> { \"x\" : 2 } on : shardA Timestamp(7, 0)         { \"x\" : 2 } -->> { \"x\" : 3 } on : shardA Timestamp(8, 0)         { \"x\" : 3 } -->> { \"x\" : { \"$maxKey\" : 1 } } on : shardB Timestamp(6, 0) \nCLEAR JUMBO FLAG FOR A CHUNK (HASHED SHARD KEY) \nThe sh.status() includes the following sh.status.databases.<collection>.chunk-details for the test.jumboHashed collection. The collection uses a hashed shard key. ... // Content omitted for brevity\ntest.jumboHashed         shard key: { \"x\" : \"hashed\" }         unique: false         balancing: true         chunks:                  shardA   2                  shardB   2         { \"x\" : { \"$minKey\" : 1 } } -->> { \"x\" : NumberLong(0) } on : shardA Timestamp(1, 0)         { \"x\" : NumberLong(0) } -->> { \"x\" : NumberLong(\"848411777775835583\") } on : shardA Timestamp(4, 0)         { \"x\" : NumberLong(\"848411777775835583\") } -->> { \"x\" : NumberLong(\"5902408780260971510\") } on : shardB Timestamp(4, 1) jumbo         { \"x\" : NumberLong(\"5902408780260971510\") } -->> { \"x\" : { \"$maxKey\" : 1 } } on : shardB Timestamp(2, 2) To clear the jumbo flag for a chunk if the collection uses a hashed shard key, use clearJumboFlag with the bounds field: db.adminCommand( {   clearJumboFlag: \"test.jumboHashed\",   bounds: [{ \"x\" : NumberLong(\"848411777775835583\") }, { \"x\" : NumberLong(\"5902408780260971510\") }]} ) \n\n {   \"ok\" : 1,   \"operationTime\" : Timestamp(1580194290, 5),   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1580194290, 5),      \"signature\" : {         \"hash\" : BinData(0,\"nWCqOYVrab7NEGHWoo2NYENqHR4=\"),         \"keyId\" : NumberLong(\"6786875525496307742\")      }   }} To verify the operation, run sh.status() again. The jumbo flag should no longer appear in its output. ... // Content omitted for brevity\ntest.jumboHashed         shard key: { \"x\" : \"hashed\" }         unique: false         balancing: true         chunks:                  shardA     2                  shardB     2         { \"x\" : { \"$minKey\" : 1 } } -->> { \"x\" : NumberLong(0) } on : shardA Timestamp(1, 0)         { \"x\" : NumberLong(0) } -->> { \"x\" : NumberLong(\"848411777775835583\") } on : shardA Timestamp(4, 0)         { \"x\" : NumberLong(\"848411777775835583\") } -->> { \"x\" : NumberLong(\"5902408780260971510\") } on : shardB Timestamp(5, 0)         { \"x\" : NumberLong(\"5902408780260971510\") } -->> { \"x\" : { \"$maxKey\" : 1 } } on : shardB Timestamp(2, 2) \nTIP \nSEE ALSO: Clear jumbo Flag ←  checkShardingIndexcleanupOrphaned → On this page  * Definition\n * Syntax\n * Access Control\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-sessions/": " Docs Home → MongoDB Manual \nSESSIONS COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Command\nDescription\nabortTransaction Abort transaction. New in version 4.0. commitTransaction Commit transaction. New in version 4.0. endSessions\nExpire sessions before the sessions' timeout period.\nkillAllSessions\nKill all sessions.\nkillAllSessionsByPattern\nKill all sessions that match the specified pattern\nkillSessions\nKill specified sessions.\nrefreshSessions\nRefresh idle sessions.\nstartSession\nStarts a new session. ←  updateZoneKeyRangeabortTransaction → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getParameter/": " Docs Home → MongoDB Manual \nGETPARAMETER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Examples \nDEFINITION \ngetParameter \ngetParameter is an administrative command for retrieving the values of parameters. Use the db.adminCommand( { command } ) method to run the getParameter command in the admin database. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     getParameter: <value>,     <parameter> : <value>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ngetParameter\nint, string, document Specify a value of:      * 1 (or any integer value) to return the value for the specified <parameter>.  * '*' to return values for all parameters available to getParameter, ignoring the <parameter> field.  * { showDetails: true } to return a document containing:\n   \n   * value, the value that <parameter> is set to\n   \n   * settableAtRuntime, whether or not <parameter> can be set at runtime\n   \n   * settableAtStartup, whether or not <parameter> can be set at startup  * { showDetails: true, allParameters: true } to return a document containing showDetails fields for all parameters. <parameter>\nstring String name of the parameter to retrieve. The value for <value> does not affect output. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR \ngetParameter runs on the admin database only, and returns an error if run on any other database. The possible value for <parameter> may vary depending on what version and storage engine in use. See Retrieve All Parameters for an example of listing the available parameters. \nEXAMPLES  RETRIEVE SINGLE PARAMETER \nThe following operation runs getParameter on the admin database using a value of saslHostName to retrieve the value for that parameter: db.adminCommand( { getParameter : 1, \"saslHostName\" : 1 } )  The command returns the following output: \nNOTE The output may vary depending on the version and specific configuration of your MongoDB instance. { \"saslHostName\" : \"www.example.net:27018\", \"ok\" : 1 }  \nRETRIEVE ALL PARAMETERS \nThe following operation runs getParameter with a value of '*' to retrieve all parameters: db.adminCommand( { getParameter : '*' } )  \nNOTE The output may vary depending on the version of MongoDB and the specific configuration of the running MongoDB instance. \nTIP \nSEE ALSO: setParameter for more about these parameters. \nREPORT DETAILS ON A SINGLE PARAMETER \nThe following example runs getParameter with {showDetails: true} to report details on saslHostName. db.adminCommand( { getParameter : { showDetails: true }, \"saslHostName\" : 1 } )  Example output: {  saslHostName: {    value: '<hostname>',    settableAtRuntime: false,    settableAtStartup: true  },  ok: 1} \nNOTE The output may vary depending on the version and specific configuration of your MongoDB instance. \nREPORT DETAILS FOR ALL PARAMETERS \nThe following example runs getParameter with {showDetails: true, allParameters: true} to report details on all parameters. db.adminCommand( { getParameter : { showDetails: true, allParameters: true } } ) \n←  getDefaultRWConcernkillCursors → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/dbStats/": " Docs Home → MongoDB Manual \nDBSTATS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Output\n * Examples \nDEFINITION \ndbStats \nThe dbStats command returns storage statistics for a given database. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     dbStats: 1,     scale: <number>,     freeStorage: 0   })  \nCOMMAND FIELDS \nThe command takes the following fields: Fields\nDescription\ndbStats\n1\nscale Optional. The scale factor for the various size data. The scale defaults to 1 to return size data in bytes. To display kilobytes rather than bytes, specify a scale value of 1024. If you specify a non-integer scale factor, MongoDB uses the integer part of the specified factor. For example, if you specify a scale factor of 1023.999, MongoDB uses 1023 as the scale factor. Starting in version 4.2, the output includes the scaleFactor used to scale the size values. freeStorage Optional. To return details on free space allocated to collections, set freeStorage to 1. If the instance has a large number of collections or indexes, obtaining free space usage data may cause processing delays. To gather dbStats information without free space details, either set freeStorage to 0 or do not include the field. In mongosh, the db.stats() function provides a wrapper around dbStats. \nBEHAVIOR \nThe time required to run the command depends on the total size of the database. Because the command must touch all data files, the command may take several seconds to run. \nACCURACY AFTER UNEXPECTED SHUTDOWN \nAfter an unclean shutdown of a mongod using the Wired Tiger storage engine, count and size statistics reported by dbStats may be inaccurate. The amount of drift depends on the number of insert, update, or delete operations performed between the last checkpoint and the unclean shutdown. Checkpoints usually occur every 60 seconds. However, mongod instances running with non-default --syncdelay settings may have more or less frequent checkpoints. Run validate on each collection on the mongod to restore statistics after an unclean shutdown. After an unclean shutdown:      * validate updates the count statistic in the collStats output with the latest value.  * Other statistics like the number of documents inserted or removed in the collStats output are estimates. \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, dbStats operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nOUTPUT \ndbStats.db \nName of the database. dbStats.collections \nNumber of collections in the database. dbStats.views \nNumber of views in the database. dbStats.objects \nNumber of objects (specifically, documents) in the database across all collections. dbStats.avgObjSize \nAverage size of each document in bytes. This is the dataSize divided by the number of documents. The scale argument does not affect the avgObjSize value. dbStats.dataSize \nTotal size of the uncompressed data held in the database. The dataSize decreases when you remove documents. For databases using the WiredTiger storage engine, dataSize may be larger than storageSize if compression is enabled. The dataSize decreases when documents shrink. dbStats.storageSize \nSum of the space allocated to all collections in the database for document storage, including free space. The storageSize does not decrease as you remove or shrink documents. This value may be smaller than dataSize for databases using the WiredTiger storage engine with compression enabled. storageSize does not include space allocated to indexes. See indexSize for the total index size. dbStats.freeStorageSize \nSum of the free space allocated to all collections in the database for document storage. Free database storage space is allocated to the collection but does not contain data. freeStorageSize does not include free space allocated to indexes. See indexFreeStorageSize for the total free index size. To include this value in the dbStats output, set freeStorage to 1. Updated in version 5.3.0, 5.2.1, and 5.0.6. dbStats.indexes \nTotal number of indexes across all collections in the database. dbStats.indexSize\n\n dbStats.indexFreeStorageSize \nSum of the free space allocated to all indexes in the database. Free database storage space is allocated to the index but does not contain data. indexFreeStorageSize does not include free space allocated to document storage. See freeStorageSize for the total free document storage size. To include this value in the dbStats output, set freeStorage to 1. Updated in version 5.3.0, 5.2.1, and 5.0.6. dbStats.totalSize \nSum of the space allocated for both documents and indexes in all collections in the database. Includes used and free storage space. This is the sum of storageSize and indexSize. New in version 4.4. dbStats.totalFreeStorageSize \nSum of the free storage space allocated for both documents and indexes in all collections in the database. This is the sum of freeStorageSize and indexFreeStorageSize. To include this value in the dbStats output, set freeStorage to 1. Updated in version 5.3.0, 5.2.1, and 5.0.6. dbStats.scaleFactor \nscale value used by the command. If you specified a non-integer scale factor, MongoDB uses the integer part of the specified factor. For example, if you specify a scale factor of 1023.999, MongoDB uses 1023 as the scale factor. New in version 4.2. dbStats.fsUsedSize \nTotal size of all disk space in use on the filesystem where MongoDB stores data. \nTIP \nSEE ALSO: --dbpath dbStats.fsTotalSize \nTotal size of all disk capacity on the filesystem where MongoDB stores data. \nEXAMPLES \nThe following examples demonstrate dbStats usage. \nLIMIT DATA RETURNED \nTo limit the data returned to a single field, append the field name to the dbStats command. This example returns the indexSize value: db.runCommand( { dbStats: 1 } ).indexSize  \nVIEW FREE SPACE ALLOCATED TO COLLECTIONS \nTo view free storage usage, set freeStorage to 1. db.runCommand( { dbStats: 1, scale: 1024, freeStorage: 1 } )  Example output: {  db: 'test',  collections: 2,  views: 0,  objects: 1689,  avgObjSize: 52.56542332741267,  dataSize: 86.7021484375,  storageSize: 100,  freeStorageSize: 32,  indexes: 2,  indexSize: 116,  indexFreeStorageSize: 36,  totalSize: 216,  totalFreeStorageSize: 68,  scaleFactor: 1024,  fsUsedSize: 60155820,  fsTotalSize: 61255492,  ok: 1,  '$clusterTime': {    clusterTime: Timestamp({ t: 1646085664, i: 1 }),    signature: {      hash: Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),      keyId: Long(\"0\")    }  },  operationTime: Timestamp({ t: 1646085664, i: 1 })} The freeStorage field enables the collection and display of the highlighted metrics. The scale field sets the displayed values to kilobytes. ←  dbHashdriverOIDTest → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Output\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setAuditConfig/": " Docs Home → MongoDB Manual \nSETAUDITCONFIG \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Examples \nDEFINITION \nsetAuditConfig \nNew in version 5.0. setAuditConfig is an administrative command that sets new audit configurations for mongod and mongos server instances at runtime. Use the db.adminCommand( { command } ) method to run setAuditConfig against the admin database. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     setAuditConfig: 1,     filter: <Filter Document>,     auditAuthorizationSuccess: <Boolean>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\nsetAuditConfig\ninteger\nRun setAuditConfig\nfilter\ndocument\nAn audit filter\nauditAuthorizationSuccess\nboolean\nLog all, or only failed access authorizations \nBEHAVIOR \nEnable Auditing to use setAuditConfig at runtime. auditAuthorizationSuccess enables auditing of authorization success for the authCheck action. The parameter value must be true to audit read and write operations. However, when auditAuthorizationSuccess is false auditing has less performance impact because the audit system only logs authorization failures. Configuration updates are distributed via the oplog mechanism which means updates on mongod nodes are distributed to secondary nodes very quickly. There is a different distribution mechanism on mongos nodes. mongos nodes have to poll the primary server at regular intervals for configuration updates. You may see stale data due to polling delay if you run setAuditConfig on the primary server and getAuditConfig on a shard before the shard has polled the primary server for updated configuration details. \nEXAMPLES \nIn these examples the audit messages have been reformatted. They appear on a single line in the log file. \nAUDIT COLLECTION CREATION AND DELETION \nEnable auditing when a collection is created or deleted. db.admin.runCommand(   {      setAuditConfig: 1,      filter:         {            atype:               {                  $in: [ \"createCollection\", \"dropCollection\" ]               }         },      auditAuthorizationSuccess: false   })  When the inventory collection is created in the sales database, the audit system will log a message like this: {   \"atype\" : \"createCollection\",   \"ts\" : { \"$date\" : \"2021-08-09T13:45:05.372+00:00\" },   \"uuid\" : { \"$binary\" : \"RKU/YLizS6K9se2GUU7ZVQ==\", \"$type\" : \"04\" },   \"local\" : { \"ip\" : \"127.0.0.1\", \"port\" : 27502 },   \"remote\" : { \"ip\" : \"127.0.0.1\", \"port\" : 51918 },   \"users\" : [],   \"roles\" : [],   \"param\" : { \"ns\" : \"sales.inventory\" },   \"result\" : 0}  When the inventory collection is dropped from the sales database, the audit system will log a message like this: {   \"atype\" : \"dropCollection\",   \"ts\" : { \"$date\" : \"2021-08-09T13:45:00.661+00:00\" },   \"uuid\" : { \"$binary\" : \"0gle4/pSQli+LUcz43ykag==\", \"$type\" : \"04\" },   \"local\" : { \"ip\" : \"127.0.0.1\", \"port\" : 27502 },   \"remote\" : { \"ip\" : \"127.0.0.1\", \"port\" : 51928 },   \"users\" : [],   \"roles\" : [],   \"param\" : { \"ns\" : \"sales.inventory\" },   \"result\" : 0}  \nAUDIT DOCUMENT INTERACTIONS\n\n db.admin.runCommand(   {      setAuditConfig: 1,      filter:         {            atype: \"authCheck\",            \"param.command\":               {                  $in: [ \"find\", \"insert\", \"delete\", \"update\", \"findandmodify\" ]               }         },      auditAuthorizationSuccess: true   })  Search the inventory collection in the sales database using the find command to create an audit log entry like this one: {   \"atype\" : \"authCheck\",   \"ts\" : { \"$date\" : \"2021-08-09T15:28:10.788+00:00\" },   \"uuid\" : { \"$binary\" : \"ngwRt5CRTZqgE4TsfleoqQ==\", \"$type\" : \"04\" },   \"local\" : { \"ip\" : \"127.0.0.1\", \"port\" : 27502 },   \"remote\" : { \"ip\" : \"127.0.0.1\", \"port\" : 51930 },   \"users\" : [],   \"roles\" : [],   \"param\" : {      \"command\" : \"find\",      \"ns\" : \"sales.inventory\",      \"args\" : {         \"find\" : \"inventory\",         \"filter\" : { \"widget\" : 1 },         \"lsid\" : { \"id\" : { \"$binary\" : \"FNWNxiitQ8GHKrHx8eJSbg==\", \"$type\" : \"04\" } },         \"$clusterTime\" : { \"clusterTime\" : { \"$timestamp\" : { \"t\" : 1628521381, \"i\" : 1 } },         \"signature\" : { \"hash\" : { \"$binary\" : \"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\", \"$type\" : \"00\" },         \"keyId\" : { \"$numberLong\" : \"0\" } } },         \"$db\" : \"sales\"      }   },   \"result\" : 0}  \nTIP \nSEE ALSO: db.adminCommand(), getAuditConfig, configure audit filters ←  rotateCertificatessetClusterParameter → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetGetConfig/": " Docs Home → MongoDB Manual \nREPLSETGETCONFIG \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Output Example \nDEFINITION \nreplSetGetConfig \nReturns a document that describes the current configuration of the replica set. \nTIP In mongosh, this command can also be run through the rs.conf() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nTo run, replSetGetConfig must be issued against the admin database. The command has the following syntax: db.adminCommand(   {     replSetGetConfig: 1,     commitmentStatus: <boolean>,     comment: <any>   })  \nCOMMAND FIELDS \nField\nType\nDescription\nreplSetGetConfig\nany\nAny value\ncommitmentStatus\nboolean Optional. Specify true to include a commitmentStatus field in the output. The commitmentStatus output field indicates whether the replica set's previous reconfig has been committed, so that the replica set is ready to be reconfigured again. For details, see commitmentStatus Output Field. You can only specify commitmentStatus: true option when running the command on the primary. The command errors if run with commitmentStatus: true on a secondary. New in version 4.4. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. mongosh provides the rs.conf() method that wraps the replSetGetConfig command: rs.conf();  \nOUTPUT EXAMPLE \nThe following is an example output of the replSetGetConfig command run with commitmentStatus: true on the primary:\n\n \nField\nDescription\nconfig\nThe replica set configuration. For description of each configuration settings, see Replica Set Configuration.\ncommitmentStatus A boolean that indicates whether the most recent replica set configuration has been committed; i.e.  * The most recent replica set configuration for the primary has propagated to a majority of members, and  * The last write operation to the replica set with the previous configuration has been majority-commited in the new configuration. If true, then the configuration has been committed, and the replica set can be reconfigured. To reconfigure the replica set, see replSetReconfig command or the mongosh method rs.reconfig(). If false, then the configuration has not been committed, and the replica set cannot be reconfigured. ok\nA number that indicates whether the command has succeeded (1) or failed (0).\noperationTime\n$clusterTime\nReturned with every command for a replica set. See db.adminCommand Response for details. \nTIP \nSEE ALSO:  * rs.conf()  * rs.reconfig() ←  replSetFreezereplSetGetStatus → On this page  * Definition\n * Syntax\n * Command Fields\n * Output Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getLastError/": " Docs Home → MongoDB Manual \nGETLASTERROR \ngetLastError \nRemoved in MongoDB 5.1. Any code explicitly using getLastError, db.getLastError(), or db.getLastErrorObj() should instead use the CRUD API to issue the write with the desired write concern. Information about the success or failure of the write operation will be provided directly by the driver as a return value. ←  findAndModifygetMore → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getAuditConfig/": " Docs Home → MongoDB Manual \nGETAUDITCONFIG \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \ngetAuditConfig \nNew in version 5.0. getAuditConfig is an administrative command that retrieves audit configurations from mongod and mongos server instances. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     getAuditConfig: 1   })  \nBEHAVIOR \nAuditing must be enabled in order to use getAuditConfig. Nodes that are not participating in a runtime audit configuration return their current configuration file settings for auditLog.filter and setParameter.auditAuthorizationSuccess. Nodes that are participating in the runtime audit synthesize their current configuration from memory. Configuration updates are distributed via the oplog mechanism which means updates on mongod nodes are distributed to secondary nodes very quickly. However, the distribution mechanism is different on mongos nodes. mongos nodes have to poll the primary server at regular intervals for configuration updates. You may see stale data due to polling delay if you run setAuditConfig on the primary server and getAuditConfig on a shard before the shard has polled the primary server for updated configuration details. \nNOTE If you are writing automated audit scripts, note that the quoting style and the types used to represent the cluster signature differ between mongosh and the legacy mongo shell. In mongosh the types are Binary and Long. The corresponding types in the legacy shell are BinData and NumberLong. // mongoshsignature: {   hash: Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),   keyId: Long(\"0\")}\n// mongo\"signature\" : {                        \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"),                        \"keyId\" : NumberLong(0)              } \nEXAMPLES \nRun getAuditConfig on the admin database . db.adminCommand({getAuditConfig: 1})  The example server is configured to audit read and write operations. It has a filter which captures the desired operations and the auditAuthorizationSuccess value has been set to true. {  generation: ObjectId(\"60e73e74680a655705f16525\"),  filter: {    atype: 'authCheck',    'param.command': {      '$in': [ 'find', 'insert', 'delete', 'update', 'findandmodify' ]    }  },  auditAuthorizationSuccess: true,  ok: 1,  '$clusterTime': {    clusterTime: Timestamp(1, 1625767540),    signature: {      hash: Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),      keyId: Long(\"0\")    }  },  operationTime: Timestamp(1, 1625767540)} \nTIP \nSEE ALSO: db.adminCommand(), setAuditConfig, configure audit filters ←  fsyncUnlockgetClusterParameter → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-user-management/": " Docs Home → MongoDB Manual \nUSER MANAGEMENT COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\ncreateUser\nCreates a new user.\ndropAllUsersFromDatabase\nDeletes all users associated with a database.\ndropUser\nRemoves a single user.\ngrantRolesToUser\nGrants a role and its privileges to a user.\nrevokeRolesFromUser\nRemoves a role from a user.\nupdateUser\nUpdates a user's data.\nusersInfo\nReturns information about the specified users. ←  logoutcreateUser → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getCmdLineOpts/": " Docs Home → MongoDB Manual \nGETCMDLINEOPTS \nOn this page    \n * Definition\n   \n * Syntax\n * Output \nDEFINITION \ngetCmdLineOpts \nThe getCmdLineOpts command returns a document containing command line options used to start the given mongod or mongos. Run getCmdLineOpts in the admin database. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     getCmdLineOpts: 1   })  \nOUTPUT \nThis command returns a document with two fields, argv and parsed. The argv field contains an array with each item from the command string used to invoke mongod or mongos. The document in the parsed field includes all runtime options, including those parsed from the command line and those specified in the configuration file, if specified. Consider the following example output of getCmdLineOpts: {   \"argv\" : [      \"/usr/bin/mongod\",      \"--config\",      \"/etc/mongod.conf\"   ],   \"parsed\" : {      \"config\" : \"/etc/mongod.conf\",      \"net\" : {         \"bindIp\" : \"127.0.0.1\",         \"port\" : 27017      },      \"processManagement\" : {         \"fork\" : true      },      \"storage\" : {         \"dbPath\" : \"/data/db\"      },      \"systemLog\" : {         \"destination\" : \"file\",         \"logAppend\" : true,         \"path\" : \"/var/log/mongodb/mongod.log\"      }   },   \"ok\" : 1} \n←  featuresgetLog → On this page  * Definition\n * Syntax\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/updateRole/": " Docs Home → MongoDB Manual \nUPDATEROLE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Example \nDEFINITION \nupdateRole \nUpdates a user-defined role. The updateRole command must run on the role's database. \nTIP In mongosh, this command can also be run through the db.updateRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. An update to a field completely replaces the previous field's values. To grant or remove roles or privileges without replacing all values, use one or more of the following commands:      * grantRolesToRole  * grantPrivilegesToRole  * revokeRolesFromRole  * revokePrivilegesFromRole \nWARNING An update to the privileges or roles array completely replaces the previous array's values. \nSYNTAX \nTo update a role, you must provide the privileges array, roles array, or both. The command uses the following syntax: db.runCommand(   {     updateRole: \"<role>\",     privileges:         [           { resource: { <resource> }, actions: [ \"<action>\", ... ] },           ...         ],     roles:         [           { role: \"<role>\", db: \"<database>\" } | \"<role>\",           ...         ],     authenticationRestrictions:         [           {             clientSource: [\"<IP>\" | \"<CIDR range>\", ...],             serverAddress: [\"<IP>\", ...]           },           ...         ]     writeConcern: <write concern document>,     comment: <any>   } )  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nupdateRole\nstring\nThe name of the user-defined role role to update.\nprivileges\narray\nOptional. Required if you do not specify roles array. The privileges to grant the role. An update to the privileges array overrides the previous array's values. For the syntax for specifying a privilege, see the privileges array.\nroles\narray\nOptional. Required if you do not specify privileges array. The roles from which this role inherits privileges. An update to the roles array overrides the previous array's values.\nauthenticationRestrictions\narray Optional. The authentication restrictions the server enforces on the role. Specifies a list of IP addresses and CIDR ranges users granted this role are allowed to connect to and/or which they can connect from. writeConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where updateRole runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field:\n\n \nIMPORTANT If a user inherits multiple roles with incompatible authentication restrictions, that user becomes unusable. For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. \nBEHAVIOR \nA role's privileges apply to the database where the role is created. The role can inherit privileges from other roles in its database. A role created on the admin database can include privileges that apply to all databases or to the cluster and can inherit privileges from roles in other databases. \nREQUIRED ACCESS \nYou must have the revokeRole action on all databases in order to update a role. You must have the grantRole action on the database of each role in the roles array to update the array. You must have the grantRole action on the database of each privilege in the privileges array to update the array. If a privilege's resource spans databases, you must have grantRole on the admin database. A privilege spans databases if the privilege is any of the following:  * a collection in all databases  * all collections and all database  * the cluster resource You must have the setAuthenticationRestriction action on the database of the target role to update a role's authenticationRestrictions document. \nEXAMPLE \nThe following is an example of the updateRole command that updates the myClusterwideAdmin role on the admin database. While the privileges and the roles arrays are both optional, at least one of the two is required: db.adminCommand(   {     updateRole: \"myClusterwideAdmin\",     privileges:         [           {             resource: { db: \"\", collection: \"\" },             actions: [ \"find\" , \"update\", \"insert\", \"remove\" ]           }         ],     roles:         [           { role: \"dbAdminAnyDatabase\", db: \"admin\" }         ],     writeConcern: { w: \"majority\" }   })  To view a role's privileges, use the rolesInfo command. ←  rolesInfoReplication Commands → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setParameter/": " Docs Home → MongoDB Manual \nSETPARAMETER \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior \nDEFINITION \nsetParameter \nsetParameter is an administrative command for modifying options normally set on the command line. You must issue the setParameter command against the admin database. \nSYNTAX \ndb.adminCommand(   {     setParameter: 1,     <parameter>: <value>   })  For the available parameters, including examples, see MongoDB Server Parameters. \nBEHAVIOR  PERSISTENCE \nCommands issued by the admin command setParameter do not survive server restarts. For a persistent option use the --setParameter command line option or the setParameter configuration file setting. \nSTABLE API \nWhen using Stable API V1 with apiStrict set to true, you cannot use setParameter to modify server parameters. ←  setIndexCommitQuorumsetDefaultRWConcern → On this page  * Definition\n * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/compactStructuredEncryptionData/": " Docs Home → MongoDB Manual \nCOMPACTSTRUCTUREDENCRYPTIONDATA \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \ncompactStructuredEncryptionData \nCompacts documents specified in the state collections and deletes redundant documents. \nSYNTAX \nThe command has the following syntax: db.runCommand(  {    compactStructuredEncryptionData: <collection>,    compactionTokens: {       encryptedFieldPath: bindata,       ...    },  })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ncompactStructuredEncryptionData\nstring\nThe name of the collection.\ncompactionTokens\ndocument\nA document that maps index fields to compaction tokens. The mongosh provides a wrapper method db.collection.compactStructuredEncryptionData(). \nREQUIRED ACCESS \nThe built-in roles readWriteAnyDatabase and dbOwner provide compactStructuredEncryptionData actions on resources. \nEXAMPLE \nSee index compaction for an example. What is MongoDB? → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/renameCollection/": " Docs Home → MongoDB Manual \nRENAMECOLLECTION \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Example \nDEFINITION \nrenameCollection \nChanges the name of an existing collection. Specify collection names to renameCollection in the form of a complete namespace (<database>.<collection>). \nTIP In mongosh, this command can also be run through the renameCollection() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. Issue the renameCollection command against the admin database. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     renameCollection: \"<source_namespace>\",     to: \"<target_namespace>\",     dropTarget: <true|false>,     writeConcern: <document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command contains the following fields: Field\nType\nDescription\nrenameCollection\nstring\nThe namespace of the collection to rename. The namespace is a combination of the database name and the name of the collection.\nto\nstring\nThe new namespace of the collection. If the new namespace specifies a different database, the renameCollection command copies the collection to the new database and drops the source collection. See Naming Restrictions.\ndropTarget\nboolean\nOptional. If true, mongod will drop the target of renameCollection prior to renaming the collection. The default value is false.\nwriteConcern\ndocument Optional. A document that expresses the write concern for the operation. Omit to use the default write concern. When issued on a sharded cluster, mongos converts the write concern of the renameCollection command and its helper db.collection.renameCollection() to \"majority\". comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR  SHARDED COLLECTIONS \nStarting in MongoDB 5.0, you can use the renameCollection command to change the name of a sharded collection. The target database must be the same as the source database. \nUNSHARDED COLLECTIONS \nYou can use renameCollection to rename an unsharded collection in a sharded cluster as long as the source and target databases are on the same primary shard. \nTIME SERIES COLLECTIONS \nYou cannot use renameCollection to rename a time series collection. For more information, see Time Series Collection Limitations. \nEXISTING TARGET COLLECTION \nrenameCollection fails if target is the name of an existing collection and you do not specify dropTarget: true. \nPERFORMANCE \nChanged in version 3.6. renameCollection has different performance implications depending on the target namespace. If the target database is the same as the source database, renameCollection simply changes the namespace. This is a quick operation. If the target database differs from the source database, renameCollection copies all documents from the source collection to the target collection. Depending on the size of the collection, this may take longer to complete. \nRESOURCE LOCKING IN SHARDED CLUSTERS \nChanged in version 5.0. When renaming a sharded or unsharded collection in a sharded cluster, the source and target collections are exclusively locked on every shard. Subsequent operations on the source and target collections must wait until the rename operation completes. For more information on locking in MongoDB, see FAQ: Concurrency. \nRESOURCE LOCKING IN REPLICA SETS \nChanged in version 4.2. If renaming a collection within the same database, renameCollection obtains an exclusive lock on the source and target collections for the duration of the operation. All subsequent operations on the collections must wait until renameCollection completes. Prior to MongoDB 4.2, renaming a collection within the same database with renameCollection required an exclusive database lock. If renaming a collection between different databases, renameCollection locking behavior depends on the MongoDB version:\n\n  * For MongoDB 4.2.1 and earlier, renameCollection obtains an exclusive (W) global lock. Subsequent operations on the mongod must wait until renameCollection releases the global lock. For more information on locking in MongoDB, see FAQ: Concurrency. \nLOCAL DATABASE \n * You cannot rename a collection from a replicated database to the local database, which is not replicated.  * You cannot rename a collection from the local database, which is not replicated, to a replicated database. \nOPEN CURSORS  WARNING The db.collection.renameCollection() method and renameCollection command will invalidate open cursors which interrupts queries that are currently returning data. \nCHANGE STREAMS \nFor Change Streams, the db.collection.renameCollection() method and renameCollection command create an invalidate for any existing Change Streams opened on the source or target collection. \nINTERACTION WITH MONGODUMP \nA mongodump started with --oplog fails if a client issues the renameCollection command during the dump process. See mongodump.--oplog for more information. \nEXAMPLE \nThe following example renames a collection named orders in the test database to orders2014 in the test database. db.adminCommand( { renameCollection: \"test.orders\", to: \"test.orders2014\" } )  mongosh provides the db.collection.renameCollection() helper for the command to rename collections within the same database. The following is equivalent to the previous example: use testdb.orders.renameCollection( \"orders2014\" ) \n←  reIndexrotateCertificates → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/cleanupReshardCollection/": " Docs Home → MongoDB Manual \nCLEANUPRESHARDCOLLECTION \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \ncleanupReshardCollection \nNew in version 5.0. The cleanupReshardCollection command cleans up metadata of a failed resharding operation. You only need to run this command if a primary failover occurred while you ran a resharding operation. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     cleanupReshardCollection: \"<database>.<collection>\"   })  \nEXAMPLE  CLEAN UP A FAILED RESHARDING OPERATION \nThe following example cleans up metadata of a failed resharding operation on the sales.orders collection: db.adminCommand({  cleanupReshardCollection: \"sales.orders\"})  \nTIP \nSEE ALSO: Reshard a Collection ←  cleanupOrphanedcommitReshardCollection → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-aggregation/": " Docs Home → MongoDB Manual \nAGGREGATION COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\naggregate\nPerforms aggregation tasks such as $group using an aggregation pipeline.\ncount\nCounts the number of documents in a collection or a view.\ndistinct\nDisplays the distinct values found for a specified key in a collection or a view.\nmapReduce\nPerforms map-reduce aggregation for large data sets. For a detailed comparison of the different approaches, see Aggregation Commands Comparison. ←  Database Commandsaggregate → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-auditing/": " Docs Home → MongoDB Manual \nSYSTEM EVENTS AUDITING COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\nlogApplicationMessage\nPosts a custom message to the audit log. ←  setFreeMonitoringlogApplicationMessage → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/applyOps/": " Docs Home → MongoDB Manual \nAPPLYOPS \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access \nDEFINITION \napplyOps \nApplies specified oplog entries to a mongod instance. The applyOps command is an internal command. \nBEHAVIOR  WARNING This command obtains a global write lock and will block other operations until it has completed. \nREQUIRED ACCESS \nIf the specified oplog entries contain collection UUIDs, executing this command requires both the useUUID and forceUUID privileges on the cluster resource to which the oplog entries are attempting to be written. ←  Replication Commandshello → On this page  * Definition\n * Behavior\n * Required Access Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/listIndexes/": " Docs Home → MongoDB Manual \nLISTINDEXES \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Behavior\n * Output\n * Examples \nDEFINITION \nlistIndexes \nReturns information about the indexes on the specified collection. Returned index information includes the keys and options used to create the index, as well as hidden indexes. You can optionally set the batch size for the first batch of results. \nTIP In mongosh, this command can also be run through the db.collection.getIndexes() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand (   {      listIndexes: \"<collection-name>\",      cursor: { batchSize: <int> },      comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nlistIndexes\nstring\nThe name of the collection.\ncursor.batchSize\ninteger\nOptional. Specifies the cursor batch size.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nREQUIRED ACCESS \nIf access control is enforced, the built-in read role provides the required privileges to run listIndexes for the collections in a database. \nBEHAVIOR  CLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued listIndexes disconnects before the operation completes, MongoDB marks listIndexes for termination using killOp. \nREPLICA SET MEMBER STATE RESTRICTION \nStarting in MongoDB 4.4, to run on a replica set member, listIndexes operations require the member to be in PRIMARY or SECONDARY state. If the member is in another state, such as STARTUP2, the operation errors. In previous versions, the operations also run when the member is in STARTUP2. The operations wait until the member transitioned to RECOVERING. \nWILDCARD INDEXES \nStarting in MongoDB 6.3, 6.0.5, and 5.0.16, the wildcardProjection field stores the index projection in its submitted form. Earlier versions of the server may have stored the projection in a normalized form. The server uses the index the same way, but you may notice a difference in the output of the listIndexes and db.collection.getIndexes() commands. \nOUTPUT \nlistIndexes.cursor \nA result set returned in the batch size specified by your cursor. Each document in the batch output contains the following fields: Field\nType\nDescription\nid\ninteger\nA 64-bit integer. If zero, there are no more batches of information. If non-zero, a cursor ID, usable in a getMore command to get the next batch of index information.\nns\nstring\nThe database and collection name in the following format: <database-name>.<collection-name>\nfirstBatch\ndocument Index information includes the keys and options used to create the index. The index option hidden, available starting in MongoDB 4.4, is only present if the value is true. Use getMore to retrieve additional results as needed. listIndexes.ok \nThe return value for the command. A value of 1 indicates success. \nEXAMPLES  LIST DATABASE INDEXES \nThis example lists indexes for the contacts collection without specifying the cursor batch size. 1db.runCommand (2  {3     listIndexes: \"contacts\"4  }5) \nHIDE OUTPUT\n\n \nSPECIFY RESULT BATCH SIZE \nThis example lists indexes for the contacts collection, and specifies a cursor batch size of 1. 1db.runCommand (2   {3      listIndexes: \"contacts\", cursor: { batchSize: 1 }4   }5) \nHIDE OUTPUT 1{2   cursor: {3     id: Long(\"4809221676960028307\"),4     ns: 'test.contacts',5    firstBatch: [ { v: 2, key: { _id: 1 }, name: '_id_', ns: 'test.contacts' } ]6  },7  ok: 18} \nRETRIEVE ADDITIONAL RESULTS \nThis example uses getMore to retrieve additional result batches from the contacts collection. 1db.runCommand(2   {3      getMore: Long(\"4809221676960028307\"), collection: \"contacts\"4   }5) \nHIDE OUTPUT 1{2   cursor: {3    nextBatch: [ { v: 2, key: { a: 1 }, name: 'a_1', ns: 'test.contacts' } ],4    id: Long(\"0\"),5    ns: 'test.contacts'6  },7  ok: 18} ←  listDatabaseslogRotate → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Behavior\n * Output\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/revokePrivilegesFromRole/": " Docs Home → MongoDB Manual \nREVOKEPRIVILEGESFROMROLE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Example \nDEFINITION \nrevokePrivilegesFromRole \nRemoves the specified privileges from the user-defined role on the database where the command is run. \nTIP In mongosh, this command can also be run through the db.revokePrivilegesFromRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     revokePrivilegesFromRole: \"<role>\",     privileges: [       { resource: { <resource> }, actions: [ \"<action>\", ... ] },       ...     ],     writeConcern: <write concern document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nrevokePrivilegesFromRole\nstring\nThe user-defined role to revoke privileges from.\nprivileges\narray\nAn array of privileges to remove from the role. See privileges for more information on the format of the privileges.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR \nTo revoke a privilege, the resource document pattern must match exactly the resource field of that privilege. The actions field can be a subset or match exactly. For example, consider the role accountRole in the products database with the following privilege that specifies the products database as the resource: {  \"resource\" : {      \"db\" : \"products\",      \"collection\" : \"\"  },  \"actions\" : [      \"find\",      \"update\"  ]}  You cannot revoke find and/or update from just one collection in the products database. The following operations result in no change to the role: use productsdb.runCommand(    {      revokePrivilegesFromRole: \"accountRole\",      privileges:        [          {            resource : {                db : \"products\",                collection : \"gadgets\"            },            actions : [                \"find\",                \"update\"            ]          }        ]    })\ndb.runCommand(    {      revokePrivilegesFromRole: \"accountRole\",      privileges:        [          {            resource : {                db : \"products\",                collection : \"gadgets\"            },            actions : [                \"find\"            ]          }        ]    }) \n\n use productsdb.runCommand(    {      revokePrivilegesFromRole: \"accountRole\",      privileges:        [          {            resource : {                db : \"products\",                collection : \"\"            },            actions : [                \"find\"            ]          }        ]    })  \nREQUIRED ACCESS \nYou must have the revokeRole action on the database a privilege targets in order to revoke that privilege. If the privilege targets multiple databases or the cluster resource, you must have the revokeRole action on the admin database. \nEXAMPLE \nThe following operation removes multiple privileges from the associates role in the products database: use productsdb.runCommand(   {     revokePrivilegesFromRole: \"associate\",     privileges:      [        {          resource: { db: \"products\", collection: \"\" },          actions: [ \"createCollection\", \"createIndex\", \"find\" ]        },        {          resource: { db: \"products\", collection: \"orders\" },          actions: [ \"insert\" ]        }      ],     writeConcern: { w: \"majority\" }   }) \n←  invalidateUserCacherevokeRolesFromRole → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/grantPrivilegesToRole/": " Docs Home → MongoDB Manual \nGRANTPRIVILEGESTOROLE \nOn this page    \n * Definition\n   \n * Command Fields\n * Behavior\n * Required Access\n * Example \nDEFINITION \ngrantPrivilegesToRole \nAssigns additional privileges to a user-defined role defined on the database on which the command is run. \nTIP In mongosh, this command can also be run through the db.grantPrivilegesToRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. The grantPrivilegesToRole command uses the following syntax: db.runCommand(   {     grantPrivilegesToRole: \"<role>\",     privileges: [       {         resource: { <resource> }, actions: [ \"<action>\", ... ]       },       ...     ],     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\ngrantPrivilegesToRole\nstring\nThe name of the user-defined role to grant privileges to.\nprivileges\narray\nThe privileges to add to the role. For the format of a privilege, see privileges.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR \nA role's privileges apply to the database where the role is created. A role created on the admin database can include privileges that apply to all databases or to the cluster. \nREQUIRED ACCESS \nYou must have the grantRole action on the database a privilege targets in order to grant the privilege. To grant a privilege on multiple databases or on the cluster resource, you must have the grantRole action on the admin database. \nEXAMPLE \nThe following grantPrivilegesToRole command grants two additional privileges to the service role that exists in the products database: use productsdb.runCommand(   {     grantPrivilegesToRole: \"service\",     privileges: [         {           resource: { db: \"products\", collection: \"\" }, actions: [ \"find\" ]         },         {           resource: { db: \"products\", collection: \"system.js\" }, actions: [ \"find\" ]         }     ],     writeConcern: { w: \"majority\" , wtimeout: 5000 }   })  The first privilege in the privileges array allows the user to search on all non-system collections in the products database. The privilege does not allow queries on system collections, such as the system.js collection. To grant access to these system collections, explicitly provision access in the privileges array. See Resource Document. The second privilege explicitly allows the find action on system.js collections on all databases. ←  dropAllRolesFromDatabasegrantRolesToRole → On this page  * Definition\n * Command Fields\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/commitTransaction/": " Docs Home → MongoDB Manual \nCOMMITTRANSACTION \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior \nDEFINITION \ncommitTransaction \nNew in version 4.0. Saves the changes made by the operations in the multi-document transaction and ends the transaction. \nTIP In mongosh, this command can also be run through the Session.commitTransaction() and Session.withTransaction() helper methods. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. To run the commitTransaction, the command must be run against the admin database and run within a Session(). \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     commitTransaction: 1,     txnNumber: <long>,     writeConcern: <document>,     autocommit: false,     comment: <any>   })  \nBEHAVIOR  WRITE CONCERN \nWhen committing the transaction, the session uses the write concern specified at the transaction start. See Session.startTransaction(). If you commit using the \"w: 1\" write concern, your transaction can be rolled back if there is a failover.. \nATOMICITY \nWhen a transaction commits, all data changes made in the transaction are saved and visible outside the transaction. That is, a transaction will not commit some of its changes while rolling back others. Until a transaction commits, the data changes made in the transaction are not visible outside the transaction. However, when a transaction writes to multiple shards, not all outside read operations need to wait for the result of the committed transaction to be visible across the shards. For example, if a transaction is committed and write 1 is visible on shard A but write 2 is not yet visible on shard B, an outside read at read concern \"local\" can read the results of write 1 without seeing write 2. ←  abortTransactionendSessions → On this page  * Definition\n * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/balancerCollectionStatus/": " Docs Home → MongoDB Manual \nBALANCERCOLLECTIONSTATUS \nOn this page    \n * Definition\n   \n * Syntax\n * Access Control\n * Output Document\n * Examples \nDEFINITION \nbalancerCollectionStatus \nNew in version 4.4. Returns a document that contains information about whether the chunks of a sharded collection are balanced (i.e. do not need to be moved) as of the time the command is run or need to be moved because of draining shards, zone violation or imbalance of chunks across shards. You can only issue the balancerCollectionStatus against the admin database. \nTIP In mongosh, this command can also be run through the sh.balancerCollectionStatus() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     balancerCollectionStatus: \"<db>.<collection>\"   })  Specify the full namespace (\"<db>.<collection>\") of the sharded collection. mongosh provides a wrapper method sh.balancerCollectionStatus(). \nACCESS CONTROL \nWhen running with access control, the user must have the enableSharding privilege actions on database and/or collection to run the command. That is, a user must have a role that grants the following privilege: { resource: { db: <database>, collection: <collection> }, actions: [ \"enableSharding\" ] }  The built-in clusterManager role provides the appropriate privileges. \nOUTPUT DOCUMENT \nThe following is an example of a document returned by the command: {   \"balancerCompliant\" : false,   \"firstComplianceViolation\" : \"chunksImbalance\",   \"ok\" : 1,   \"operationTime\" : Timestamp(1583192967, 16),   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1583192967, 16),      \"signature\" : {         \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"),         \"keyId\" : NumberLong(0)      }   }} \nField\nDescription\n\"balancerCompliant\"\nA boolean that indicates whether the chunks do not need to be moved (true) or need to be moved (false).\n\"firstComplianceViolation\" A string that indicates the reason chunks for this namespace need to be moved. The field is only available if \"balancerCompliant\" is false. Possible values are: Value\nDescription\n\"chunksImbalance\"\nThe difference in the number of chunks between the shard with the most chunks for the collection and the shard with the fewest chunks for the collection exceed the migration threshold.\n\"defragmentingChunks\"\nThe queried namespace is currently going through the chunk defragmentation process. Defragmentation can be triggered by the configureCollectionBalancing command.\n\"draining\"\nA remove shard operation is in progress and MongoDB must drain chunks off the removed shard to other shard(s).\n\"zoneViolation\"\nChunks violate the defined zone ranges for a shard. \nNOTE This field only returns information on the first violation observed by MongoDB. There may be additional pending chunk migrations due to a different reason than the one reported in firstComplianceViolation. \"details\" An object containing information on the ongoing defragmentation process. This object indicates the current phase of the defragmentation and how many chunks are left to process in that phase. For example output, see Ongoing Defragmentation Process. This field is only returned when firstComplianceViolation is defragmentingChunks. In addition to the command-specific return fields, the command also returns the ok status field, the operationTime field, and the $clusterTime field for the operation. For details on these fields, see Response. \nEXAMPLES \nTo check whether the chunks of a sharded collection test.contacts is currently in balance, connect to a mongos instance and issue the following command: db.adminCommand( { balancerCollectionStatus: \"test.contacts\" } )  If the chunks for the collection do not need to be moved, the command returns an output similar to the following:\n\n \nONGOING DEFRAGMENTATION PROCESS \nIf the queried namespace is going through chunk defragmentation, the balancerCollectionStatus command returns output similar to the following: {   \"balancerCompliant\": false,   \"firstComplianceViolation\": \"defragmentingChunks\",   \"details\": {      \"currentPhase\": \"moveAndMergeChunks\",      \"progress\": { \"remainingChunksToProcess\": 1 }   }} \nNOTE Chunk defragmentation occurs in multiple phases. The progress field only pertains to the current phase. ←  addShardToZonebalancerStart → On this page  * Definition\n * Syntax\n * Access Control\n * Output Document\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/convertToCapped/": " Docs Home → MongoDB Manual \nCONVERTTOCAPPED \nOn this page    \n * Definition\n   \n * Syntax\n * Example \nDEFINITION \nconvertToCapped  WARNING \nDO NOT RUN THIS COMMAND IN SHARDED CLUSTERS MongoDB does not support the convertToCapped command in a sharded cluster. The convertToCapped command converts an existing, non-capped collection to a capped collection within the same database. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     convertToCapped: <collection>,     size: <capped size>,     writeConcern: <document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nDescription\nconvertToCapped\nThe name of the existing collection to convert.\nsize\nThe maximum size, in bytes, for the capped collection.\nwriteConcern\nOptional. A document expressing the write concern of the drop command. Omit to use the default write concern.\ncomment Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. convertToCapped takes an existing collection (<collection>) and transforms it into a capped collection with a maximum size in bytes, specified by the size argument (<capped size>). During the conversion process, the convertToCapped command exhibits the following behavior:  * MongoDB traverses the documents in the original collection in natural order and loads the documents into a new capped collection.  * If the capped size specified for the capped collection is smaller than the size of the original uncapped collection, then MongoDB will overwrite documents in the capped collection based on insertion order, or first in, first out order.  * Internally, to convert the collection, MongoDB uses the following procedure\n   \n   * cloneCollectionAsCapped command creates the capped collection and imports the data.\n   \n   * MongoDB drops the original collection.\n   \n   * renameCollection renames the new capped collection to the name of the original collection.  * This holds a database exclusive lock for the duration of the operation. Other operations which lock the same database will be blocked until the operation completes. See What locks are taken by some common client operations? for operations that lock the database. \nWARNING The convertToCapped will not recreate indexes from the original collection on the new collection, other than the index on the _id field. If you need indexes on this collection you will need to create these indexes after the conversion is complete. \nEXAMPLE  CONVERT A COLLECTION \nThe following example uses db.collection.insertOne() to create an events collection, and db.collection.stats() to obtain information about the collection: db.events.insertOne( { click: 'button-1', time: new Date() } )db.events.stats()  MongoDB will return the following: {        \"ns\" : \"test.events\",        ...        \"capped\" : false,        ...}  To convert the events collection into a capped collection and view the updated collection information, run the following commands: db.runCommand( { convertToCapped: 'events', size: 8192 } )db.events.stats()  MongoDB will return the following: {     \"ns\" : \"test.events\",     ...     \"capped\" : true,     \"max\" : NumberLong(\"9223372036854775807\"),     \"maxSize\" : 8192,     ...}  The convertToCapped will not recreate indexes from the original collection on the new collection, other than the index on the _id field. If you need indexes on this collection you will need to create these indexes after the conversion is complete. \nTIP \nSEE ALSO: create ←  compactcreate → On this page  * Definition\n * Syntax\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getDefaultRWConcern/": " Docs Home → MongoDB Manual \nGETDEFAULTRWCONCERN \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Output\n * Behavior\n * Example \nDEFINITION \nNew in version 4.4. getDefaultRWConcern \nThe getDefaultRWConcern administrative command retrieves the global default read or write concern settings.      * For sharded clusters, issue the getDefaultRWConcern on a mongos. \nSYNTAX \nThe command has the following form: db.adminCommand(   {     getDefaultRWConcern: 1 ,     inMemory: <boolean>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\ngetDefaultRWConcern\nint Set to 1. inMemory\nboolean Optional. Set to true to return the in-memory cached copy of the global default read or write concern settings. The instance uses the in-memory copy when applying the global defaults to an operation. Set to false to return the on-disk copy of the deployment's global default read or write concern. Defaults to false. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nOUTPUT \nThe output includes the following fields: Field\nType\nDescription\ndefaultWriteConcern\nobject The global default write concern configuration. If this field is absent, the deployment has no global default write concern settings. defaultReadConcern\nobject The global default read concern configuration. If this field is absent, the deployment has no global default read concern settings. defaultWriteConcernSource\nString The source of the default write concern. By default, the value is \"implicit\". Once you set the default write concern with setDefaultRWConcern, the value becomes \"global\". defaultReadConcernSource\nString The source of the default read concern. By default, the value is \"implicit\". Once you set the default read concern with setDefaultRWConcern, the value becomes \"global\". updateOpTime\nTimestamp The operation timestamp of when any global default read or write concern setting was last modified. Present if a default has ever been set for the cluster. updateWallClockTime\nDate The wall clock date when an administrator last set the global default read or write concern. This value is informational and should not be used for any recency comparisons. localUpdateWallClockTime\nDate The wall clock date when the mongod or mongos last updated its local copy of the global default read and write concern settings. This value is informational and should not be used for any recency comparisons. \nTIP \nSEE ALSO: setDefaultRWConcern \nBEHAVIOR  NOTE \nREQUIRES FEATURECOMPATIBILITYVERSION 4.4+ Each mongod in the replica set or sharded cluster must have featureCompatibilityVersion set to at least 4.4 to use getDefaultRWConcern. If you downgrade your deployment's featureCompatibilityVersion from 4.4 to 4.2, all cluster-wide read and write concern defaults are lost, but mongos instances may continue applying the defaults for up to 30 seconds. \nREPLICA SETS \nYou can issue getDefaultRWConcern against any data-bearing member of the replica set (i.e. not against an arbiter). A secondary can return a 'stale' version of the global default settings if it has not yet replicated the latest changes from the primary. \nSHARDED CLUSTERS \nIssue the setDefaultRWConcern against a mongos in the cluster. Each mongos periodically refreshes its local copy of the global default settings. A mongos can return a 'stale' version of the global default settings if it has not yet refreshed its local copy after a recent update to the global default settings or if it fetched its settings from a lagged config server secondary. The global default settings do not propagate to the individual shards. You cannot run getDefaultRWConcern against a shard. \nACCESS CONTROL \nFor replica sets or sharded clusters enforcing Authentication, getDefaultRWConcern requires that the authenticated user have the getDefaultRWConcern privilege action.\n\n \nEXAMPLE \nThe following operation retrieves the currently configured global default read and write concern for the mongod. db.adminCommand({  \"getDefaultRWConcern\": 1})  The operation returns output similar to the following: {  \"defaultWriteConcern\" : {    \"w\" : \"majority\"  },  \"defaultReadConcern\" : {    \"level\" : \"majority\"  },  \"defaultWriteConcernSource\" : \"global\",  \"defaultReadConcernSource\" : \"global\",  \"updateOpTime\" : Timestamp(1586290895, 1),  \"updateWallClockTime\" : ISODate(\"2020-04-07T20:21:41.849Z\"),  \"localUpdateWallClockTime\" : ISODate(\"2020-04-07T20:21:41.862Z\"),  \"ok\" : 1,  \"$clusterTime\" : { ... }  \"operationTime\" : Timestamp(1586290925, 1)} \n←  getClusterParametergetParameter → On this page  * Definition\n * Syntax\n * Command Fields\n * Output\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/killSessions/": " Docs Home → MongoDB Manual \nKILLSESSIONS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * View Existing Sessions\n * Behavior\n * Example \nDEFINITION \nkillSessions \nThe killSessions command kills the specified sessions. If access control is enabled, the command only kills the sessions owned by the user. [1] \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     killSessions: [ { id : <UUID> }, ... ]   })  \nCOMMAND FIELDS \nThe command takes an array of documents that specify the UUID portion of the session id. Specify an empty array [ ] to kill all sessions, or if access control is enabled, all sessions owned by the user. [1] \nVIEW EXISTING SESSIONS \nTo view existing sessions, see $listSessions operation or $listLocalSessions. [1](1, 2) The killSessions operation ignores sessions that have transactions in prepared state. See Behavior for details. \nBEHAVIOR  SESSION IDENTIFICATION \nMongoDB concatenates each of the specified UUIDs with the hash of the authenticated user credentials to identify the user's sessions to kill. If the user has no session that match, the killSessions has no effect. IN-PROGRESS OPERATIONS \nKilling a session kills any in-progress operations in the session and closes any open cursors associated with these operations. KILLED SESSION AVAILABILITY \nThe killed session may still be listed as a current session, and future operations may use the killed session. To view existing sessions, see $listSessions operation or $listLocalSessions. SESSIONS WITH TRANSACTIONS IN PREPARED STATE \nThe killSessions operation ignores sessions that have transactions in prepared state. Transactions in prepared state refer to transactions with write operations that span multiple shards whose commit coordinator has completed the \"sendingPrepare\" action. \nEXAMPLE \nThe following operation kills the specified session for the user: db.runCommand( { killSessions: [ { id: UUID(\"f9b3d8d9-9496-4fff-868f-04a6196fc58a\") } ] } )  \nTIP \nSEE ALSO: Kill Write Operations ←  killAllSessionsByPatternrefreshSessions → On this page  * Definition\n * Syntax\n * Command Fields\n * View Existing Sessions\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetResizeOplog/": " Docs Home → MongoDB Manual \nREPLSETRESIZEOPLOG \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Examples \nDEFINITION \nreplSetResizeOplog \nNew in version 4.4: replSetResizeOplog also supports specifying the minimum number of hours to preserve an oplog entry. Changed in version 5.0: To set the replSetOplog size in mongosh, use the Double() constructor. replSetResizeOplog enables you to resize the oplog or its minimum retention period dynamically without restarting the mongod process. You must run this command against the admin database. \nSYNTAX \nThe command has the following form: db.adminCommand(   {     replSetResizeOplog: <int>,     size: <double>,     minRetentionHours: <double>   } )  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nreplSetResizeOplog\nint\nSet to 1.\nsize\ndouble The maximum size of the oplog in megabytes.      * The minimum size you can specify is 990 megabytes.  * The maximum size you can specify is 1 petabytes.  * Explicitly cast the size as a double in mongosh with Double(). See Change the Maximum Oplog Size. minRetentionHours\ndouble Optional. The minimum number of hours to preserve an oplog entry, where decimal values represent the fractions of an hour. For example, a value of 1.5 represents one hour and thirty minutes. The value must be greater than or equal to 0. A value of 0 indicates that the mongod should truncate the oplog starting with the oldest entries to maintain the configured maximum oplog size. A mongod configured with minRetentionHours only removes an oplog entry if:  * The oplog has reached the maximum configured size, and  * The oplog entry is older that the configured number of hours based on the host system clock. To check the currently configured minimum oplog retention period, see the oplogTruncation.minRetentionHours in the output of the serverStatus command. New in version 4.4. \nTIP \nSEE ALSO:  * --oplogSize / replication.oplogSizeMB  * --oplogMinRetentionHours / storage.oplogMinRetentionHours \nBEHAVIOR \nYou can only use replSetResizeOplog on mongod instances running with the Wired Tiger storage engine. See the Change the Size of the Oplog tutorial for a procedure on using replSetResizeOplog command to resize the oplog. The oplog can grow past its configured size limit to avoid deleting the majority commit point. Starting in MongoDB 4.0, MongoDB forbids dropping the local.oplog.rs collection. For more information on this restriction, see Oplog Collection Behavior. replSetResizeOplog overrides the maximum oplog size or minimum oplog retention period set at startup by:  * replication.oplogSizeMB / --oplogSizeMB, and  * storage.oplogMinRetentionHours / --oplogMinRetentionHours respectively. The new oplog size persists after a server restart, unless you use:  * storage.oplogMinRetentionHours, or  * --oplogMinRetentionHours. \nIMPORTANT Reducing the maximum oplog size results in truncation of the oldest oplog entries until the oplog reaches the new configured size. Similarly, reducing the minimum oplog retention period (new in 4.4) results in truncation of oplog entries older that the specified period if the oplog has exceeded the maximum configured size. Oplog truncation due to reduced oplog size or retention period can result in unexpected behavior from clients still reading those oplog entries, including:  * Open change streams may become invalidated  * Secondaries which have not replicated those oplog entries may require resynchronization.  * Backups using mongodump with --oplog against the member may not capture entries prior to truncation. \nMINIMUM OPLOG RETENTION PERIOD \nA mongod has the following behavior when configured with a minimum oplog retention period (New in 4.4):  * The oplog can grow without constraint so as to retain oplog entries for the configured number of hours. This may result in reduction or exhaustion of system disk space due to a combination of high write volume and large retention period.\n\n  * The mongod compares the system wall clock to an oplog entries creation wall clock time when enforcing oplog entry retention. Clock drift between cluster components may result in unexpected oplog retention behavior. See Clock Synchronization for more information on clock synchronization across cluster members. \nREPLSETRESIZEOPLOG DOES NOT REPLICATE TO OTHER MEMBERS \nChanging the oplog size or minimum oplog retention period (new in 4.4) of a given replica set member with replSetResizeOplog does not change the oplog size of any other member in the replica set. You must run replSetResizeOplog on each replica set member in your cluster to change the oplog size or minimum retention period for all members. \nREDUCING OPLOG SIZE DOES NOT IMMEDIATELY RETURN DISK SPACE \nReducing the oplog size does not immediately reclaim that disk space. This includes oplog size reduction due to truncation of oplog events older than of the minimum oplog retention period (New in 4.4). To immediately free unused disk space after reducing the oplog size, run compact against the oplog.rs collection in the local database during a maintenance period. compact blocks all operations on the database it runs against. Running compact against oplog.rs therefore prevents oplog synchronization. For a procedure on resizing the oplog and compacting oplog.rs, see Change the Size of the Oplog. \nRESOURCE LOCKING \nChanged in version 4.2.2.  * For MongoDB 4.2.2 and later, replSetResizeOplog takes an exclusive (W) lock on the oplog and blocks other operations on the collection until it finishes.  * For MongoDB 4.2.1 and earlier, replSetResizeOplog takes a global exclusive (W) lock and blocks all other operations until it finishes. For more information on locking in MongoDB, see FAQ: Concurrency. \nEXAMPLES  CHANGE THE MAXIMUM OPLOG SIZE \nUse the db.collection.stats() mongosh method to display the current maximum oplog size, maxSize, in megabytes. For example: db.getSiblingDB(\"local\").oplog.rs.stats(1024*1024).maxSize  The above command returns the oplog size of this member in megabytes: 990  The following command uses replSetResizeOplog to change the oplog size of this member to 16384 megabytes: db.adminCommand({ \"replSetResizeOplog\": 1, size: Double(16384)})  To verify the new oplog size, rerun the db.collection.stats() method: db.getSiblingDB(\"local\").oplog.rs.stats(1024*1024).maxSize  The above command returns: \"maxSize\": NumberLong(\"16834\")  \nWARNING Reducing the size of the oplog in a node removes data from it. This may cause replica members syncing with that node to become stale. To resync those members, see Resync a Member of a Replica Set. \nCHANGE THE MINIMUM OPLOG RETENTION PERIOD \n 1. Connect mongosh to the mongod replica set member.  2. Optional. Use the db.serverStatus() command to verify the current minimum oplog retention value as oplogTruncation.minRetentionHours:\n    \n    db.getSiblingDB(\"admin\").serverStatus().oplogTruncation.minRetentionHours\n    \n    \n    \n    The command returns the currently configured minimum oplog retention period for the mongod. For example:\n    \n    1.5\n    \n    \n    \n    If the mongod has no minimum oplog retention period, the operation returns an empty result.  3. Use the replSetResizeOplog command to modify the configured minimum oplog retention period. For example, the following sets the minimum oplog retention period to 2 hours:\n    \n    db.adminCommand({  \"replSetResizeOplog\" : 1,  \"minRetentionHours\" : 2})\n    \n     ←  replSetReconfigreplSetStepDown → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/hello/": " Docs Home → MongoDB Manual \nHELLO \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Output \nDEFINITION \nhello \nNew in version 5.0: (and 4.4.2, 4.2.10, 4.0.21, and 3.6.21) hello returns a document that describes the role of the mongod instance. If the optional field saslSupportedMechs is specified, the command also returns an array of SASL mechanisms used to create the specified user's credentials. If the instance is a member of a replica set, then hello returns a subset of the replica set configuration and status including whether or not the instance is the primary of the replica set. When sent to a mongod instance that is not a member of a replica set, hello returns a subset of this information. MongoDB drivers and clients use hello to determine the state of the replica set members and to discover additional members of a replica set. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     hello: 1   })  The hello command accepts optional fields saslSupportedMechs: <db.user> to return an additional field hello.saslSupportedMechs in its result and comment <any> to add a log comment associated with the command. db.runCommand(   {     hello: 1,     saslSupportedMechs: \"<db.username>\",     comment: <any>   })  The db.hello() method in mongosh provides a wrapper around hello. \nBEHAVIOR  CLIENT DISCONNECTION \nStarting in MongoDB 4.2, if the client that issued hello disconnects before the operation completes, MongoDB marks hello for termination using killOp. \nOUTPUT  ALL INSTANCES \nThe following hello fields are common across all roles: hello.isWritablePrimary \nA boolean value that reports when this node is writable. If true, then this instance is a primary in a replica set, or a mongos instance, or a standalone mongod. This field will be false if the instance is a secondary member of a replica set or if the member is an arbiter of a replica set. hello.topologyVersion \nFor internal use by MongoDB. hello.maxBsonObjectSize \nThe maximum permitted size of a BSON object in bytes for this mongod process. If not provided, clients should assume a max size of \"16 * 1024 * 1024\". hello.maxMessageSizeBytes \nThe maximum permitted size of a BSON wire protocol message. The default value is 48000000 bytes. hello.maxWriteBatchSize \nThe maximum number of write operations permitted in a write batch. If a batch exceeds this limit, the client driver divides the batch into smaller groups each with counts less than or equal to the value of this field. The value of this limit is 100,000 writes. hello.localTime \nReturns the local server time in UTC. This value is an ISO date. hello.logicalSessionTimeoutMinutes \nThe time in minutes that a session remains active after its most recent use. Sessions that have not received a new read/write operation from the client or been refreshed with refreshSessions within this threshold are cleared from the cache. State associated with an expired session may be cleaned up by the server at any time. Only available when featureCompatibilityVersion is \"3.6\" or greater. See Backwards Incompatible Features. hello.connectionId \nAn identifier for the mongod / mongos instance's outgoing connection to the client. hello.minWireVersion \nThe earliest version of the wire protocol that this mongod or mongos instance is capable of using to communicate with clients. Clients may use minWireVersion to help negotiate compatibility with MongoDB. hello.maxWireVersion \nThe latest version of the wire protocol that this mongod or mongos instance is capable of using to communicate with clients. Clients may use maxWireVersion to help negotiate compatibility with MongoDB. hello.readOnly \nA boolean value that, when true, indicates that the mongod or mongos is running in read-only mode. hello.compression \nAn array listing the compression algorithms used or available for use (i.e. common to both the client and the mongod or mongos instance) to compress the communication between the client and the mongod or mongos instance. The field is only available if compression is used. For example:    \n\n  * If the mongod is enabled to use both the snappy,zlib compressors and a client has specified zlib,snappy, the compression field would contain:\n   \n   \"compression\": [ \"zlib\", \"snappy\" ]\n   \n     * If the mongod is enabled to use the snappy compressor and a client has specified zlib,snappy, the compression field would contain :\n   \n   \"compression\": [ \"snappy\" ]\n   \n     * If the mongod is enabled to use the snappy compressor and a client has specified zlib or the client has specified no compressor, the field is omitted. That is, if the client does not specify compression or if the client specifies a compressor not enabled for the connected mongod or mongos instance, the field does not return. hello.saslSupportedMechs \nAn array of SASL mechanisms used to create the user's credential or credentials. Supported SASL mechanisms are:  * GSSAPI  * SCRAM-SHA-256  * SCRAM-SHA-1 The field is returned only when the command is run with the saslSupportedMechs field: db.runCommand( { hello: 1, saslSupportedMechs: \"<db.username>\" } )  \nSHARDED INSTANCES \nmongos instances add the following field to the hello response document: hello.msg \nContains the value isdbgrid when hello returns from a mongos instance. \nREPLICA SETS \nhello contains these fields when returned by a member of a replica set: hello.hosts \nAn array of strings in the format of \"[hostname]:[port]\" that lists all members of the replica set that are neither hidden, passive, nor arbiters. Drivers use this array and the hello.passives to determine which members to read from. hello.setName \nThe name of the current :replica set. hello.setVersion \nThe current replica set config version. hello.secondary \nA boolean value that, when true, indicates if the mongod is a secondary member of a replica set. hello.passives \nAn array of strings in the format of \"[hostname]:[port]\" listing all members of the replica set which have a members[n].priority of 0. This field only appears if there is at least one member with a members[n].priority of 0. Drivers use this array and the hello.hosts to determine which members to read from. hello.arbiters \nAn array of strings in the format of \"[hostname]:[port]\" listing all members of the replica set that are arbiters. This field only appears if there is at least one arbiter in the replica set. hello.primary \nA string in the format of \"[hostname]:[port]\" listing the current primary member of the replica set. hello.arbiterOnly \nA boolean value that , when true, indicates that the current instance is an arbiter. The arbiterOnly field is only present, if the instance is an arbiter. hello.passive \nA boolean value that, when true, indicates that the current instance is passive. The passive field is only present for members with a members[n].priority of 0. hello.hidden \nA boolean value that, when true, indicates that the current instance is hidden. The hidden field is only present for hidden members. hello.tags \nA tags document contains user-defined tag field and value pairs for the replica set member. { \"<tag1>\": \"<string1>\", \"<tag2>\": \"<string2>\",... }  * For read operations, you can specify a tag set in the read preference to direct the operations to replica set member(s) with the specified tag(s).  * For write operations, you can create a customize write concern using settings.getLastErrorModes and settings.getLastErrorDefaults. For more information, see Configure Replica Set Tag Sets. \nTIP \nSEE ALSO: members[n].tags hello.me \nThe [hostname]:[port] of the member that returned hello. hello.electionId \nA unique identifier for each election. Included only in the output of hello for the primary. Used by clients to determine when elections occur. hello.lastWrite \nA document containing optime and date information for the database's most recent write operation. hello.lastWrite.opTime \nAn object giving the optime of the last write operation. hello.lastWrite.lastWriteDate \nA date object containing the time of the last write operation. hello.lastWrite.majorityOpTime \nAn object giving the optime of the last write operation readable by majority reads.\n\n \nA date object containing the time of the last write operation readable by majority reads. For details on the ok status field, the operationTime field, and the $clusterTime field, see Command Response. ←  applyOpsreplSetAbortPrimaryCatchUp → On this page  * Definition\n * Syntax\n * Behavior\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/revokeRolesFromUser/": " Docs Home → MongoDB Manual \nREVOKEROLESFROMUSER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \nrevokeRolesFromUser \nRemoves one or more roles from a user on the database where the roles exist. \nTIP In mongosh, this command can also be run through the db.revokeRolesFromUser() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     revokeRolesFromUser: \"<user>\",     roles: [       { role: \"<role>\", db: \"<database>\" } | \"<role>\",       ...     ],     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nrevokeRolesFromUser\nstring\nThe user to remove roles from.\nroles\narray\nThe roles to remove from the user.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. In the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where revokeRolesFromUser runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nREQUIRED ACCESS \nYou must have the revokeRole action on a database to revoke a role on that database. \nEXAMPLE \nThe accountUser01 user in the products database has the following roles: \"roles\" : [    { \"role\" : \"assetsReader\",      \"db\" : \"assets\"    },    { \"role\" : \"read\",      \"db\" : \"stock\"    },    { \"role\" : \"readWrite\",      \"db\" : \"products\"    }]  The following revokeRolesFromUser command removes the two of the user's roles: the read role on the stock database and the readWrite role on the products database, which is also the database on which the command runs: use productsdb.runCommand( { revokeRolesFromUser: \"accountUser01\",                 roles: [                          { role: \"read\", db: \"stock\" },                          \"readWrite\"                 ],                 writeConcern: { w: \"majority\" }             } )  The user accountUser01 in the products database now has only one remaining role: \"roles\" : [    { \"role\" : \"assetsReader\",      \"db\" : \"assets\"    }] \n←  grantRolesToUserupdateUser → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setAllowMigrations/": " Docs Home → MongoDB Manual \nSETALLOWMIGRATIONS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Example \nDEFINITION \nsetAllowMigrations \nPrevents the start of new automatic migrations on a collection, prevents in-flight manual migrations from committing, and excludes the collection from new balancer rounds. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {    setAllowMigrations: \"<db>.<collection>\",    allowMigrations: <true|false>   })  \nCOMMAND FIELDS \nThe command takes the following parameters: Field\nType\nDescription\nsetAllowMigrations\nstring The collection to modify. allowMigrations\nboolean If false:      * MongoDB prevents new automatic migrations on the collection  * in-flight manual migrations will not be committed  * the collection will be excluded from new balancer rounds If true:  * MongoDB allows new automatic migrations on the collection  * in-flight manual migrations will be committed  * the collection will be included in new balancer rounds \nBEHAVIOR \nsetAllowMigrations requires the same privileges as moveChunk. \nEXAMPLE \nThis operation prevents migrations on the store.inventory collection: db.adminCommand( {   setAllowMigrations: \"store.inventory\",   allowMigrations: false} ) \n←  reshardCollectionsetShardVersion → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/ping/": " Docs Home → MongoDB Manual \nPING \nOn this page    \n * Definition\n   \n * Syntax \nDEFINITION \nping \nThe ping command is a no-op used to test whether a server is responding to commands. This command will return immediately even if the server is write-locked: \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     ping: 1   })  The value (e.g. 1 above) does not impact the behavior of the command. ←  netstatprofile → On this page  * Definition\n * Syntax Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/nav-geospatial/": " Docs Home → MongoDB Manual \nGEOSPATIAL COMMANDS  NOTE For details on a specific command, including syntax and examples, click on the link to the command's reference page. Name\nDescription\ngeoSearch\nRemoved in MongoDB 5.0. Performs a geospatial query that uses MongoDB's haystack index functionality. ←  mapReducegeoSearch → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/revokeRolesFromRole/": " Docs Home → MongoDB Manual \nREVOKEROLESFROMROLE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Example \nDEFINITION \nrevokeRolesFromRole \nRemoves the specified inherited roles from a role. \nTIP In mongosh, this command can also be run through the db.revokeRolesFromRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     revokeRolesFromRole: \"<role>\",     roles: [       { role: \"<role>\", db: \"<database>\" } | \"<role>\",       ...     ],     writeConcern: { <write concern> },     comment: <any>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\nrevokeRolesFromRole\nstring\nThe role from which to remove inherited roles.\nroles\narray\nThe inherited roles to remove.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. In the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where revokeRolesFromRole runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nREQUIRED ACCESS \nYou must have the revokeRole action on a database to revoke a role on that database. \nEXAMPLE \nThe purchaseAgents role in the emea database inherits privileges from several other roles, as listed in the roles array: {   \"_id\" : \"emea.purchaseAgents\",   \"role\" : \"purchaseAgents\",   \"db\" : \"emea\",   \"privileges\" : [],   \"roles\" : [      {         \"role\" : \"readOrdersCollection\",         \"db\" : \"emea\"      },      {         \"role\" : \"readAccountsCollection\",         \"db\" : \"emea\"      },      {         \"role\" : \"writeOrdersCollection\",         \"db\" : \"emea\"      }   ]}  The following revokeRolesFromRole operation on the emea database removes two roles from the purchaseAgents role: use emeadb.runCommand( { revokeRolesFromRole: \"purchaseAgents\",                 roles: [                          \"writeOrdersCollection\",                          \"readOrdersCollection\"                        ],                  writeConcern: { w: \"majority\" , wtimeout: 5000 }             } )  The purchaseAgents role now contains just one role:\n\n \n←  revokePrivilegesFromRolerolesInfo → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/whatsmyuri/": " Docs Home → MongoDB Manual \nWHATSMYURI \nwhatsmyuri \nwhatsmyuri is an internal command. ←  validateDBMetadataFree Monitoring Commands → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/profile/": " Docs Home → MongoDB Manual \nPROFILE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Example \nDEFINITION \nprofile \nChanged in version 5.0. For a mongod instance, the command enables, disables, or configures the database profiler. The profiler captures and records data on the performance of write operations, cursors, and database commands on a running mongod instance. If the profiler is disabled, the command configures how slow operations are logged to the diagnostic log. On mongod, if the database profiler level is 2, full logging is enabled on the profiler and the diagnostic log. At database profiler level 1, the following settings modify both the profiler and the diagnostic log:      * slowms  * sampleRate  * filter If the database profiler level is 0, the database profiler is disabled. At level 0 the following settings only modify the diagnostic log:  * slowms  * sampleRate  * filter Starting in MongoDB 4.0, for a mongos instance, the command only configures how operations get written to the diagnostic log. You cannot enable the database profiler on a mongos instance because mongos does not have any collections that the profiler can write to. Starting in MongoDB 5.0 (also available starting in 4.4.2, and 4.2.12), changes made to the database profiler level, slowms, sampleRate, or filter using the profile command or db.setProfilingLevel() wrapper method are recorded in the log file. On mongos, you can set profile level to:  * 0 to set the slowms, sampleRate, and filter for the diagnostic log;  * -1 to read the current settings. The profiler is off by default. \nIMPORTANT Profiling can impact performance and shares settings with the system log. Carefully consider any performance and security implications before configuring and enabling the profiler on a production deployment. See Profiler Overhead for more information on potential performance degradation. \nSYNTAX \nThe command has the following syntax: db.runCommand(  {    profile: <level>,    slowms: <threshold>,    sampleRate: <rate>,    filter: <filter expression>  })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nprofile\nint Configures the profiler level. The following profiler levels are available: Level\nDescription\n0\nThe profiler is off and does not collect any data. This is the default profiler level.\n1 The profiler collects data for operations that take longer than the value of slowms or that match a filter. When a filter is set:  * The slowms and sampleRate options are not used for profiling.  * The profiler only captures operations that match the filter. 2\nThe profiler collects data for all operations. Since profiling is not available on mongos, the profile command cannot be used to set the profiling level to a value other than 0 on a mongos instance. slowms\nint Optional. Default: 100 The slow operation time threshold, in milliseconds. Operations that run for longer than this threshold are considered slow. When logLevel is set to 0, MongoDB records slow operations to the diagnostic log at a rate determined by slowOpSampleRate. At higher logLevel settings, all operations appear in the diagnostic log regardless of their latency with the following exception: the logging of slow oplog entry messages by the secondaries. The secondaries log only the slow oplog entries; increasing the logLevel does not log all oplog entries. \nNOTE This argument affects the same setting as the configuration option operationProfiling.slowOpThresholdMs. sampleRate\ndouble Optional. Default: 1.0 The fraction of slow operations that should be profiled or logged.\nsampleRate accepts values between 0 and 1, inclusive. \nNOTE This argument affects the same setting as the configuration option operationProfiling.slowOpSampleRate and does not affect the slow oplog entry log messages on secondaries (available starting in MongoDB 4.2). filter\nobject Optional. A query that determines which operations are profiled or logged. The filter query takes the following form: { <field1>: <expression1>, ... }  The query can be any legal find() operation where the query <field> matches a field in the profiler output. \nNOTE This argument affects the same setting as the configuration option operationProfiling.filter. When filter is set, the slowms and sampleRate options are not used for profiling and slow-query log lines. New in version 4.4.2.\n\n \nBEHAVIOR \nThe profile command obtains a write lock on the affected database while enabling or disabling the profiler. This is typically a short operation. The lock blocks other operations until the profile command has completed. Starting in MongoDB 4.4.2, when connected to a sharded cluster through mongos, you can run the profile command against any database. In previous versions of MongoDB, when connected through mongos, you can only run the profile command against the admin database. \nTIP \nSEE ALSO: Database Profiling \nEXAMPLE \nEnable profiling and filter the logged data: db.runCommand(   {     profile: 1,     filter:        {           $or:           [              { millis: { $gte: 100 } },              { user: \"testuser@admin\" }           ]        }   })  The filter only selects operations that are:  * at least 100 milliseconds long, or  * submitted by the testuser. ←  pingserverStatus → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/hostInfo/": " Docs Home → MongoDB Manual \nHOSTINFO \nOn this page    \n * Definition\n   \n * Syntax\n * Output \nDEFINITION \nhostInfo \nReturns:A document with information about the underlying system that the mongod or mongos runs on. Some of the returned fields are only included on some platforms. You must run the hostInfo command, which takes no arguments, against the admin database. \nSYNTAX \ndb.adminCommand(   {     hostInfo: 1   })  \nOUTPUT \nIn mongosh you can use db.hostInfo() as a helper to access hostInfo. The output of hostInfo on a Linux system will resemble the following: {   \"system\" : {          \"currentTime\" : ISODate(\"<timestamp>\"),          \"hostname\" : \"<hostname>\",          \"cpuAddrSize\" : <number>,          \"memSizeMB\" : <number>,          \"memLimitMB\" : <number>,  // Available starting in MongoDB 4.0.9 (and 3.6.13)          \"numCores\" : <number>,          \"cpuArch\" : \"<identifier>\",          \"numaEnabled\" : <boolean>   },   \"os\" : {          \"type\" : \"<string>\",          \"name\" : \"<string>\",          \"version\" : \"<string>\"   },   \"extra\" : {          \"versionString\" : \"<string>\",          \"libcVersion\" : \"<string>\",          \"kernelVersion\" : \"<string>\",          \"cpuFrequencyMHz\" : \"<string>\",          \"cpuFeatures\" : \"<string>\",          \"pageSize\" : <number>,          \"numPages\" : <number>,          \"maxOpenFiles\" : <number>   },   \"ok\" : <return>}  \nOUTPUT FIELDS \nhostInfo \nThe document returned by the hostInfo. hostInfo.system \nAn embedded document providing information about the underlying environment of the system running the mongod or mongos hostInfo.system.currentTime \nA timestamp of the current system time. hostInfo.system.hostname \nThe system name, which should correspond to the output of hostname -f on Linux systems. hostInfo.system.cpuAddrSize \nA number reflecting the architecture of the system. Either 32 or 64. hostInfo.system.memSizeMB \nThe total amount of system memory (RAM) in megabytes. hostInfo.system.memLimitMB \nThe memory usage limit in megabytes. For example, running in a container may impose memory limits that are lower than the total system memory. This memory limit, rather than the total system memory, is used as the maximum RAM available to calculate WiredTiger internal cache. Available starting in MongoDB 4.0.9 (and 3.6.13) hostInfo.system.numCores \nThe total number of available logical processor cores. hostInfo.system.cpuArch \nA string that represents the system architecture. Either x86 or x86_64. hostInfo.system.numaEnabled \nA boolean value. false if NUMA is interleaved (i.e. disabled), otherwise true. hostInfo.os \nAn embedded document that contains information about the operating system running the mongod and mongos. hostInfo.os.type \nA string representing the type of operating system, such as Linux or Windows. hostInfo.os.name \nIf available, returns a display name for the operating system. hostInfo.os.version \nIf available, returns the name of the distribution or operating system. hostInfo.extra \nAn embedded document with extra information about the operating system and the underlying hardware. The content of the extra embedded document depends on the operating system. hostInfo.extra.cpuString \nA string containing a human-readable description of the system's processor.\n\n hostInfo.extra.versionString \nA complete string of the operating system version and identification. On Linux and macOS systems, this contains output similar to uname -a. hostInfo.extra.libcVersion \nThe release of the system libc. libcVersion only appears on Linux systems. hostInfo.extra.kernelVersion \nThe release of the Linux kernel in current use. kernelVersion only appears on Linux systems. hostInfo.extra.alwaysFullSync \nalwaysFullSync only appears on macOS systems. hostInfo.extra.nfsAsync \nnfsAsync only appears on macOS systems. hostInfo.extra.cpuFrequencyMHz \nReports the clock speed of the system's processor in megahertz. hostInfo.extra.cpuFeatures \nReports the processor feature flags. On Linux systems this the same information that /proc/cpuinfo includes in the flags fields. hostInfo.extra.pageSize \nReports the default system page size in bytes. hostInfo.extra.physicalCores \nReports the number of physical, non-HyperThreading, cores available on the system. physicalCores only appears on macOS systems. hostInfo.extra.numPages \nnumPages only appears on Linux systems. hostInfo.extra.maxOpenFiles \nReports the current system limits on open file handles. See UNIX ulimit Settings for more information. maxOpenFiles only appears on Linux systems. hostInfo.extra.scheduler \nReports the active I/O scheduler. scheduler only appears on OS X systems. ←  getLogisSelf → On this page  * Definition\n * Syntax\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/buildInfo/": " Docs Home → MongoDB Manual \nBUILDINFO \nOn this page    \n * Definition\n   \n * Syntax\n * Usage\n * Output \nDEFINITION \nbuildInfo \nThe buildInfo command is an administrative command which returns a build summary for the current mongod. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     buildInfo: 1   })  \nUSAGE \nIn mongosh, call buildInfo in the following form: db.runCommand( { buildInfo: 1 } )  The output document of buildInfo has the following form: {  \"version\" : \"<string>\",  \"gitVersion\" : \"<string>\",  \"sysInfo\" : \"<string>\",  \"loaderFlags\" : \"<string>\",  \"compilerFlags\" : \"<string>\",  \"allocator\" : \"<string>\",  \"versionArray\" : [ <num>, <num>, <...> ],  \"openssl\" : <document>,  \"javascriptEngine\" : \"<string>\",  \"bits\" : <num>,  \"debug\" : <boolean>,  \"maxBsonObjectSize\" : <num>,  \"storageEngines\" : [ \"<string>\", \"<string>\", \"<...>\" ],  \"ok\" : <num>}  \nOUTPUT \nbuildInfo \nThe document returned by the buildInfo command. \nSUPPORTED \nThese fields are stable and should provide consistent behavior. buildInfo.gitVersion \nThe commit identifier that identifies the state of the code used to build the mongod. buildInfo.versionArray \nAn array that conveys version information about the mongod instance. See version for a more readable version of this string. buildInfo.version \nA string that conveys version information about the mongod instance. If you need to present version information to a human, this field is preferable to versionArray. This string will take the format <major>.<minor>.<patch> in the case of a release, but development builds may contain additional information. buildInfo.storageEngines \nA list of storage engines available to the mongod server. buildInfo.javascriptEngine \nA string that reports the JavaScript engine used in the mongod instance. By default, this is mozjs after version 3.2, and previously V8. buildInfo.bits \nA number that reflects the target processor architecture of the mongod binary. buildInfo.debug \nA boolean. true when built with debugging options. buildInfo.maxBsonObjectSize \nA number that reports the Maximum BSON Document Size. buildInfo.openssl \nAn embedded document describing the version of the TLS/SSL library that mongod was built with and is currently using. buildInfo.modules \nA list of add-on modules that mongod was built with. Possible values currently include \"enterprise\" and \"rocksdb\". \nUNSTABLE \nThese fields are for internal use only, and you should not expect their behavior or existence to remain consistent on any level. buildInfo.sysInfo \nDeprecated since version 3.2. buildInfo.sysInfo no longer contains useful information. buildInfo.allocator \nThe memory allocator that mongod uses. By default this is tcmalloc. buildInfo.buildEnvironment \nAn embedded document containing various debugging information about the mongod build environment. ←  Diagnostic CommandscollStats → On this page  * Definition\n * Syntax\n * Usage\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetFreeze/": " Docs Home → MongoDB Manual \nREPLSETFREEZE \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior \nDEFINITION \nreplSetFreeze \nThe replSetFreeze command prevents a replica set member from seeking election for the specified number of seconds. Use this command in conjunction with the replSetStepDown command to make a different node in the replica set a primary. \nTIP In mongosh, this command can also be run through the rs.freeze() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     replSetFreeze: <seconds>   })  \nBEHAVIOR \nIf you want to unfreeze a replica set member before the specified number of seconds has elapsed, you can issue the command with a seconds value of 0: db.runCommand(   {     replSetFreeze: 0   })  Restarting the mongod process also unfreezes a replica set member. replSetFreeze is an administrative command, and you must issue it against the admin database. ←  replSetAbortPrimaryCatchUpreplSetGetConfig → On this page  * Definition\n * Syntax\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/planCacheListFilters/": " Docs Home → MongoDB Manual \nPLANCACHELISTFILTERS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Required Access\n * Output \nDEFINITION \nplanCacheListFilters \nLists the index filters associated with query shapes for a collection. Returns:Document listing the index filters. See Output. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     planCacheListFilters: <collection>   })  \nCOMMAND FIELDS \nThe command has the following fields: Field\nType\nDescription\nplanCacheListFilters\nstring\nThe name of the collection.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nREQUIRED ACCESS \nA user must have access that includes the planCacheIndexFilter action. \nOUTPUT \nThe planCacheListFilters command returns the document with the following form: {   \"filters\" : [      {         \"query\" : <query>         \"sort\" : <sort>,         \"projection\" : <projection>,         \"collation\" : <collation>,         \"indexes\" : [            <index1>,            ...         ]      },      ...   ],   \"ok\" : 1} \nplanCacheListFilters.filters \nThe array of documents that contain the index filter information. Each document contains the following fields: planCacheListFilters.filters.query \nThe query predicate associated with this filter. Although the query shows the specific values used to create the index filter, the values in the predicate are insignificant; i.e. query predicates cover similar queries that differ only in the values. For instance, a query predicate of { \"type\": \"electronics\", \"status\" : \"A\" } covers the following query predicates: { type: \"food\", status: \"A\" }{ type: \"utensil\", status: \"D\" } \nplanCacheListFilters.filters.sort \nThe sort associated with this filter. Can be an empty document. planCacheListFilters.filters.projection \nThe projection associated with this filter. Can be an empty document. planCacheListFilters.filters.collation \nThe collation associated with this filter. Can be an empty document. planCacheListFilters.filters.indexes \nThe array of indexes for the query shape. The query shape is the combination of these fields:  * query  * sort  * projection  * collation To choose the best query plan, the query optimizer evaluates the indexes and the collection scan. planCacheListFilters.ok \nThe status of the command. \nTIP \nSEE ALSO:  * planCacheClearFilters  * planCacheSetFilter ←  planCacheClearFiltersplanCacheSetFilter → On this page  * Definition\n * Syntax\n * Command Fields\n * Required Access\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/updateUser/": " Docs Home → MongoDB Manual \nUPDATEUSER \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Example \nDEFINITION \nupdateUser \nUpdates the user's profile on the database on which you run the command. An update to a field completely replaces the previous field's values, including updates to the user's roles and authenticationRestrictions arrays. \nTIP In mongosh, this command can also be run through the db.changeUserPassword() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nWARNING When you update the roles array, you completely replace the previous array's values. To add or remove roles without replacing all the user's existing roles, use the grantRolesToUser or revokeRolesFromUser commands. To update a user, you must specify the updateUser field and at least one other field, other than writeConcern. \nSYNTAX \nThe command uses the following syntax:  db.runCommand(    {      updateUser: \"<username>\",      pwd: passwordPrompt(),      // Or  \"<cleartext password>\"      customData: { <any information> },      roles: [        { role: \"<role>\", db: \"<database>\" } | \"<role>\",        ...      ],      authenticationRestrictions: [         {           clientSource: [\"<IP>\" | \"<CIDR range>\", ...],           serverAddress: [\"<IP>\", | \"<CIDR range>\", ...]         },         ...      ],      mechanisms: [ \"<scram-mechanism>\", ... ],      digestPassword: <boolean>,      writeConcern: { <write concern> },      comment: <any>    })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nupdateUser\nstring\nThe name of the user to update.\npwd\nstring Optional. The user's password. The value can be either:      * the user's password in cleartext string, or  * passwordPrompt() to prompt for the user's password. \nTIP Starting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell. customData\ndocument\nOptional. Any arbitrary information.\nroles\narray\nOptional. The roles granted to the user. An update to the roles array overrides the previous array's values.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. authenticationRestrictions\narray\nOptional. The authentication restrictions the server enforces upon the user. Specifies a list of IP addresses and CIDR ranges from which the user is allowed to connect to the server or from which the server can accept users.\nmechanisms\narray Optional. The specific SCRAM mechanism or mechanisms for the user credentials. If authenticationMechanisms is specified, you can only specify a subset of the authenticationMechanisms. If updating the mechanisms field without the password, you can only specify a subset of the user's current mechanisms, and only the existing user credentials for the specified mechanism or mechanisms are retained. If updating the password along with the mechanisms, new set of credentials are stored for the user. Valid values are:  * \"SCRAM-SHA-1\"\n   \n   * Uses the SHA-1 hashing function.  * \"SCRAM-SHA-256\"\n   \n   * Uses the SHA-256 hashing function.\n   \n   * Requires featureCompatibilityVersion set to 4.0.\n   \n   * Requires digestPassword to be true. digestPassword\nboolean Optional. Indicates whether the server or the client digests the password.\n\n If false, the client digests the password and passes the digested password to the server. Not compatible with SCRAM-SHA-256 comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nROLES \nIn the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where updateUser runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nAUTHENTICATION RESTRICTIONS \nThe authenticationRestrictions document can contain only the following fields. The server throws an error if the authenticationRestrictions document contains an unrecognized field: Field Name\nValue\nDescription\nclientSource\nArray of IP addresses and/or CIDR ranges\nIf present, when authenticating a user, the server verifies that the client's IP address is either in the given list or belongs to a CIDR range in the list. If the client's IP address is not present, the server does not authenticate the user.\nserverAddress\nArray of IP addresses and/or CIDR ranges\nA list of IP addresses or CIDR ranges to which the client can connect. If present, the server will verify that the client's connection was accepted via an IP address in the given list. If the connection was accepted via an unrecognized IP address, the server does not authenticate the user. \nIMPORTANT If a user inherits multiple roles with incompatible authentication restrictions, that user becomes unusable. For example, if a user inherits one role in which the clientSource field is [\"198.51.100.0\"] and another role in which the clientSource field is [\"203.0.113.0\"] the server is unable to authenticate the user. For more information on authentication in MongoDB, see Authentication. \nBEHAVIOR  WARNING By default, updateUser sends all specified data to the MongoDB instance in cleartext, even if using passwordPrompt(). Use TLS transport encryption to protect communications between clients and the server, including the password sent by updateUser. For instructions on enabling TLS transport encryption, see Configure mongod and mongos for TLS/SSL. MongoDB does not store the password in cleartext. The password is only vulnerable in transit between the client and the server, and only if TLS transport encryption is not enabled. \nREQUIRED ACCESS \nYou must have access that includes the revokeRole action on all databases in order to update a user's roles array. You must have the grantRole action on a role's database to add a role to a user. To change another user's pwd or customData field, you must have the changePassword and changeCustomData actions respectively on that user's database. To modify your own password and custom data, you must have privileges that grant changeOwnPassword and changeOwnCustomData actions respectively on the user's database. \nEXAMPLE \nGiven a user appClient01 in the products database with the following user info: {   \"_id\" : \"products.appClient01\",   \"userId\" : UUID(\"c5d88855-3f1e-46cb-9c8b-269bef957986\"),   \"user\" : \"appClient01\",   \"db\" : \"products\",   \"customData\" : { \"empID\" : \"12345\", \"badge\" : \"9156\" },   \"roles\" : [       { \"role\" : \"readWrite\",         \"db\" : \"products\"       },       { \"role\" : \"read\",         \"db\" : \"inventory\"       }   ],   \"mechanisms\" : [      \"SCRAM-SHA-1\",      \"SCRAM-SHA-256\"   ]}  The following updateUser command completely replaces the user's customData and roles data:\n\n  The user appClient01 in the products database now has the following user information: {   \"_id\" : \"products.appClient01\",   \"userId\" : UUID(\"c5d88855-3f1e-46cb-9c8b-269bef957986\"),   \"user\" : \"appClient01\",   \"db\" : \"products\",   \"customData\" : { \"employeeId\" : \"0x3039\" },   \"roles\" : [       { \"role\" : \"read\",         \"db\" : \"assets\"       }   ],   \"mechanisms\" : [      \"SCRAM-SHA-1\",      \"SCRAM-SHA-256\"   ]\n} \n←  revokeRolesFromUserusersInfo → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/lockInfo/": " Docs Home → MongoDB Manual \nLOCKINFO \nOn this page    \n * Definition\n   \n * Syntax\n * Output Example\n * Output Fields \nDEFINITION \nlockInfo \nReturns information on locks that are currently being held or pending. lockInfo is an internal command available on mongod instances only. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     lockInfo: 1   })  \nOUTPUT EXAMPLE \nThe following is an example of the output from the lockInfo:\n\n  \nOUTPUT FIELDS \nlockInfo \nAn array of documents that report on the lock information. Each document includes:      * resourceId  * granted  * pending lockInfo.resourceId \nThe resource on which the locks are being held or pending. lockInfo.granted \nAn array of documents. Each document provides information on locks that are currently granted on the resource. lockInfo.pending \nAn array of documents. Each document provides information on locks that are currently pending on the resource. \nLOCK INFORMATION \nFor each granted or pending lock document, information include: Field\nDescription\nmode Specifies the lock mode: Lock Mode\nDescription\nS\nShared\nX\nExclusive\nIS\nIntent Shared\nIX\nIntent Exclusive (IX) convertMode\nSpecifies the new lock mode for conversion.\nenqueueAtFront\nA boolean that indicates whether to put the lock at the front of the queue or the back in case of conflict.\ncompatibleFirst\nA boolean that indicates to grant lock requests based on compatibility with already granted locks or to use the first-in-first-out (FIFO) order.\ndebugInfo\nInformation about the operation that issued the lock request.\nclientInfo\nA document detailing the client information. ←  listCommandsnetstat → On this page  * Definition\n * Syntax\n * Output Example\n * Output Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/startSession/": " Docs Home → MongoDB Manual \nSTARTSESSION \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Output \nDEFINITION \nstartSession \nThe startSession command starts a new logical session for a sequence of operations. \nTIP In mongosh, this command can also be run through the Mongo.startSession() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     startSession: 1   })  To run startSession, use the db.runCommand( { <command> } ) method. db.runCommand(   {     startSession: 1   })  \nIMPORTANT featureCompatibilityVersion must be 3.6 or greater to use the startSession command. \nTIP \nSEE ALSO: Mongo.startSession() \nBEHAVIOR \nIf the deployment enforces authentication/authorization, you must be authenticated to run the startSession command. The user who runs startSession owns the created session, and only that user can use the session. If the deployment does not enforce authentication/authorization, a created session has no owner and can be used by any user on any connection. If the user authenticates and creates a session for a deployment that does not enforce authentication/authorization, the user owns the session. However, any user on any connection may use the session. If the deployment transitions to auth without any downtime, any sessions without an owner cannot be used. \nOUTPUT \nIn addition to the status and operation time of the command, the startSession returns the following session specific information: Field\nType\nDescription\nid\nDocument The document that contains the 16-byte Universally Unique Identifier (UUID) portion of the session's identifier. id: { id: <UUID> }  The session's identifier consists of this UUID concatenated with the hash of the authenticated user credentials. timeoutMinutes\nnumber The amount of time, in minutes, since the last client use that a session remains active before expiring. By default, sessions have an expiration timeout of 30 minutes. To change the value, set the localLogicalSessionTimeoutMinutes parameter when starting up mongod. For replica sets and sharded clusters, you must specify the same value on every member. ←  refreshSessionsAdministration Commands → On this page  * Definition\n * Syntax\n * Behavior\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/replSetAbortPrimaryCatchUp/": " Docs Home → MongoDB Manual \nREPLSETABORTPRIMARYCATCHUP \nOn this page    \n * Definition\n   \n * Syntax \nDEFINITION \nreplSetAbortPrimaryCatchUp \nThe replSetAbortPrimaryCatchUp command forces the elected primary member of the replica set to abort sync (catch up) then complete the transition to primary. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     replSetAbortPrimaryCatchUp: 1   }) \n←  helloreplSetFreeze → On this page  * Definition\n * Syntax Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setFreeMonitoring/": " Docs Home → MongoDB Manual \nSETFREEMONITORING \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Access Control \nDEFINITION \nsetFreeMonitoring \nNew in version 4.0: Available for MongoDB Community Edition. Enables or disables free Cloud monitoring for standalones and replica sets. The command is run against the admin database. \nIMPORTANT To run setFreeMonitoring, you must have specified --enableFreeMonitoring command-line option or cloud.monitoring.free.state configuration file set to runtime. Otherwise, you can only enable or disable at startup. See --enableFreeMonitoring command-line option or cloud.monitoring.free.state for details. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     setFreeMonitoring: 1,     action: \"<enable|disable>\"   })  \nBEHAVIOR \nOnce enabled, the free monitoring state remains enabled until explicitly disabled. That is, you do not need to re-enable each time you start the server. mongosh provides the following helper methods for setFreeMonitoring:      * db.enableFreeMonitoring()  * db.disableFreeMonitoring() You can use db.getFreeMonitoringStatus() to check your free monitoring status. \nACCESS CONTROL \nWhen running with access control, the user must have the setFreeMonitoring privilege actions on the cluster. That is, a user must have a role that grants the following privilege: { resource: { cluster : true }, actions: [ \"setFreeMonitoring\" ] }  The built-in role clusterMonitor role provides this privilege. ←  getFreeMonitoringStatusSystem Events Auditing Commands → On this page  * Definition\n * Syntax\n * Behavior\n * Access Control Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/mergeChunks/": " Docs Home → MongoDB Manual \nMERGECHUNKS \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Access Control\n * Behavior\n * Return Messages \nDEFINITION \nmergeChunks \nFor a sharded collection, mergeChunks combines contiguous chunk ranges on a shard into a single chunk. Issue the mergeChunks command on the admin database from a mongos instance. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     mergeChunks: <namespace>,     bounds : [       { <shardKeyField>: <minFieldValue> },       { <shardKeyField>: <maxFieldValue> }     ]   })  For compound shard keys, you must include the full shard key in the bounds specification. For example, if the shard key is { x: 1, y: 1 }, mergeChunks has the following form: db.adminCommand(   {     mergeChunks: <namespace>,     bounds: [       { x: <minValue>, y: <minValue> },       { x: <maxValue>, y: <maxValue> }     ]   } )  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nmergeChunks\nnamespace\nThe fully qualified namespace of the collection where both chunks exist. Namespaces take form of <database>.<collection>.\nbounds\narray\nAn array that contains the minimum and maximum key values of the new chunk. \nACCESS CONTROL \nOn deployments running with authorization, the built-in role clusterManager provides the required privileges. \nBEHAVIOR  NOTE Use the mergeChunks only in special circumstances. For instance, when cleaning up your sharded cluster after removing many documents. In order to successfully merge chunks, the following must be true:      * In the bounds field, <minkey> and <maxkey> must correspond to the lower and upper bounds of the chunks to merge.  * The chunks must reside on the same shard.  * The chunks must be contiguous. mergeChunks returns an error if these conditions are not satisfied. \nRETURN MESSAGES \nOn success, mergeChunks returns this document: {  \"ok\" : 1,  \"$clusterTime\" : {     \"clusterTime\" : Timestamp(1510767081, 1),     \"signature\" : {         \"hash\" : BinData(0,\"okKHD0QuzcpbVQg7mP2YFw6lM04=\"),         \"keyId\" : NumberLong(\"6488693018630029321\")      }  },  \"operationTime\" : Timestamp(1510767081, 1)}  \nANOTHER OPERATION IN PROGRESS \nmergeChunks returns the following error message if another metadata operation is in progress on the chunks collection: errmsg: \"The collection's metadata lock is already taken.\"  If another process, such as balancer process, changes metadata while mergeChunks is running, you may see this error. You can retry the mergeChunks operation without side effects. \nCHUNKS ON DIFFERENT SHARDS \nIf the input chunks are not on the same shard, mergeChunks returns an error similar to the following: {  \"ok\" : 0,  \"errmsg\" : \"could not merge chunks, collection test.users does not contain a chunk ending at { username: \\\"user63169\\\" }\",  \"$clusterTime\" : {     \"clusterTime\" : Timestamp(1510767081, 1),     \"signature\" : {         \"hash\" : BinData(0,\"okKHD0QuzcpbVQg7mP2YFw6lM04=\"),         \"keyId\" : NumberLong(\"6488693018630029321\")      }  },  \"operationTime\" : Timestamp(1510767081, 1)}  \nNONCONTIGUOUS CHUNKS \nIf the input chunks are not contiguous, mergeChunks returns an error similar to the following:\n\n \n←  moveRangerefineCollectionShardKey → On this page  * Definition\n * Syntax\n * Command Fields\n * Access Control\n * Behavior\n * Return Messages Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/shutdown/": " Docs Home → MongoDB Manual \nSHUTDOWN \nOn this page    \n * Syntax\n   \n * Command Fields\n * Behavior\n * Access Control\n * Examples Changed in version 5.0. shutdown \nThe shutdown command cleans up all database resources and then terminates the process. You must issue the shutdown command against the admin database. \nSYNTAX \nThe command has the following syntax: db.adminCommand(   {     shutdown: 1,     force: <boolean>     timeoutSecs: <int>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes these fields: Field\nDescription\nshutdown Specify 1. force Optional. Specify true to force the mongod or mongos to shut down. Force shutdown interrupts any ongoing operations on the mongod or mongos and may result in unexpected behavior. timeoutSecs Optional. Starting in MongoDB 5.0, mongod and mongos enter a quiesce period to allow any ongoing database operations to complete before shutting down. If a mongod primary receives a shut down request, the primary:       1. Attempts to step down to a secondary.\n    \n    If the step down fails and a:\n    \n    * shutdown or db.shutdownServer() command was run, mongod only continues the shut down steps if the force field is true, or a\n    \n    * SIGTERM signal was sent to mongod, mongod always continues the shut down steps.  2. Enters the quiesce period.  3. Ends any remaining database operations.  4. Shuts down. For a mongod secondary or mongos shut down request, the quiesce period is entered after a shut down was requested. The quiesce period is specified by the:  * timeoutSecs field if a shutdown or db.shutdownServer() command was run, or  * shutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongod, or  * mongosShutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongos. The timeoutSecs field defaults to 15 seconds in MongoDB 5.0. Clients cannot open new connections to a mongod or mongos that is shutting down. In MongoDB 4.4 and earlier, for a mongod primary, timeoutSecs is the number of seconds the primary should wait for a secondary to catch up. If no secondaries catch up within the specified time, the command fails. Defaults to 10 seconds. comment Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nTIP \nSEE ALSO: db.shutdownServer() \nBEHAVIOR \nFor a mongod started with Authentication, you must run shutdown over an authenticated connection. See Access Control for more information. For a mongod started without Authentication, you must run shutdown from a client connected to the localhost interface. For example, run mongosh with the --host \"127.0.0.1\" option on the same host machine as the mongod. \nSHUTDOWN ON REPLICA SET MEMBERS \nshutdown fails if the replica set member is running certain operations such as index builds. You can specify force: true to force the member to interrupt those operations and shut down. SHUTTING DOWN THE REPLICA SET PRIMARY, SECONDARY, OR MONGOS \nStarting in MongoDB 5.0, mongod and mongos enter a quiesce period to allow any ongoing database operations to complete before shutting down. If a mongod primary receives a shut down request, the primary:\n\n  2. Enters the quiesce period.  3. Ends any remaining database operations.  4. Shuts down. For a mongod secondary or mongos shut down request, the quiesce period is entered after a shut down was requested. The quiesce period is specified by the:  * timeoutSecs field if a shutdown or db.shutdownServer() command was run, or  * shutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongod, or  * mongosShutdownTimeoutMillisForSignaledShutdown server parameter if a SIGTERM signal was sent to mongos. The timeoutSecs field defaults to 15 seconds in MongoDB 5.0. Clients cannot open new connections to a mongod or mongos that is shutting down. In MongoDB 4.4 and earlier, if running shutdown against the replica set primary, the operation implicitly uses replSetStepDown to step down the primary before shutting down the mongod. If no secondary in the replica set can catch up to the primary within 10 seconds, the shutdown operation fails. You can issue shutdown with force: true to shut down the primary even if the step down fails. \nWARNING Force shutdown of the primary can result in the rollback of any writes not yet replicated to a secondary. \nACCESS CONTROL \nTo run shutdown on a mongod enforcing Authentication, the authenticated user must have the shutdown privilege. For example, a user with the built-in role hostManager has the appropriate permissions. \nEXAMPLES  SHUT DOWN A MONGOD \ndb.adminCommand({ \"shutdown\" : 1 })  \nFORCE SHUT DOWN A MONGOD \ndb.adminCommand({ \"shutdown\" : 1, \"force\" : true })  \nSHUT DOWN A PRIMARY MONGOD WITH LONGER TIMEOUT \ndb.adminCommand({ \"shutdown\" : 1, timeoutSecs: 60 }) \n←  setUserWriteBlockModeDiagnostic Commands → On this page  * Syntax\n * Command Fields\n * Behavior\n * Access Control\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/grantRolesToRole/": " Docs Home → MongoDB Manual \nGRANTROLESTOROLE \nOn this page    \n * Definition\n   \n * Behavior\n * Required Access\n * Example \nDEFINITION \ngrantRolesToRole \nGrants roles to a user-defined role. \nTIP In mongosh, this command can also be run through the db.grantRolesToRole() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. The grantRolesToRole command affects roles on the database where the command runs. The command has the following syntax: db.runCommand(   {     grantRolesToRole: \"<role>\",     roles: [                { role: \"<role>\", db: \"<database>\" },                ...            ],     writeConcern: { <write concern> },     comment: <any>   })  The command has the following fields: Field\nType\nDescription\ngrantRolesToRole\nstring\nThe name of a role to add subsidiary roles.\nroles\narray\nAn array of roles from which to inherit.\nwriteConcern\ndocument Optional. The level of write concern for the operation. See Write Concern Specification. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. In the roles field, you can specify both built-in roles and user-defined roles. To specify a role that exists in the same database where grantRolesToRole runs, you can either specify the role with the name of the role: \"readWrite\"  Or you can specify the role with a document, as in: { role: \"<role>\", db: \"<database>\" }  To specify a role that exists in a different database, specify the role with a document. \nBEHAVIOR \nA role can inherit privileges from other roles in its database. A role created on the admin database can inherit privileges from roles in any database. \nREQUIRED ACCESS \nYou must have the grantRole action on a database to grant a role on that database. \nEXAMPLE \nThe following grantRolesToRole command updates the productsReaderWriter role in the products database to inherit the privileges of the productsReader role in the products database: use productsdb.runCommand(   { grantRolesToRole: \"productsReaderWriter\",     roles: [              \"productsReader\"     ],     writeConcern: { w: \"majority\" , wtimeout: 5000 }   }) \n←  grantPrivilegesToRoleinvalidateUserCache → On this page  * Definition\n * Behavior\n * Required Access\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/setIndexCommitQuorum/": " Docs Home → MongoDB Manual \nSETINDEXCOMMITQUORUM \nOn this page    \n * Syntax\n   \n * Command Fields\n * Behavior\n * Examples New in version 4.4. setIndexCommitQuorum \nChanges the minimum number of data-bearing members (i.e commit quorum), including the primary, that must complete an in-progress simultaneous index build before the primary marks those indexes as ready. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     setIndexCommitQuorum: <string>,     indexNames: [ <document> ],     commitQuorum: <int> | <string>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nsetIndexCommitQuorum\nstring The name of the collection for which the indexes are being built. indexNames\narray An array of in-progress index builds to modify. Each element of the array must be the name of the index. The indexes specified to indexNames must be the entire set of in-progress builds associated to a given index builder, i.e. the indexes built by a single createIndexes or db.collection.createIndexes() operation. commitQuorum\nint or string The minimum number of data-bearing replica set members (i.e. commit quorum), including the primary, that must report a successful index build before the primary marks the indexes as ready. Starting in MongoDB v5.0, it's possible to resume some interrupted index builds when the commit quorum is set to \"votingMembers\". Replica set nodes in a commit quorum must have members[n].buildIndexes set to true. If any voting nodes have members[n].buildIndexes set to false, you can't use the default \"votingMembers\" commit quorum. Either configure all nodes with members[n].buildIndexes set to true, or select a different commit quorum. Supports the following values:      * \"votingMembers\" - all data-bearing voting replica set members (Default). A \"voting\" member is any replica set member where members[n].votes is greater than 0.  * \"majority\" - a simple majority of data-bearing replica set members.  * <int> - a specific number of data-bearing replica set members. Specify an integer greater than 0.  * A replica set tag name. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR  NOTE \nREQUIRES FEATURECOMPATIBILITYVERSION 4.4+ Each mongod in the replica set or sharded cluster must have featureCompatibilityVersion set to at least 4.4 to start index builds simultaneously across replica set members. MongoDB 4.4 running featureCompatibilityVersion: \"4.2\" builds indexes on the primary before replicating the index build to secondaries. Issuing setIndexCommitQuorum has no effect on index builds started with commitQuorum of 0. \nIMPORTANT Replica set nodes with buildIndexes set to false can't be included in a commit quorum. \nEXAMPLES \nStarting with MongoDB 4.4, index builds on a replica set or sharded cluster build simultaneously across all data-bearing replica set members. For sharded clusters, the index build occurs only on shards containing data for the collection being indexed. The primary requires a minimum number of data-bearing voting members (i.e commit quorum), including itself, that must complete the build before marking the index as ready for use. See Index Builds in Replicated Environments for more information. The following operation starts an index build of two indexes: db.getSiblingDB(\"examples\").invoices.createIndexes(  [    { \"invoices\" : 1 },    { \"fulfillmentStatus\" : 1 }  ])  By default, index builds use \"votingMembers\" commit quorum, or all data-bearing voting replica set members. The following operation modifies the index build commit quorum to \"majority\", or a simple majority of data-bearing voting members:.\n\n   * The indexes specified to indexNames must be the entire set of in-progress builds associated to a given index builder, i.e. the createIndexes() operation.  * The indexNames field specifies the names of the indexes. Since the indexes were created without an explicit name, MongoDB generated an index name by concatenating the names of the indexed fields and the sort order. ←  setFeatureCompatibilityVersionsetParameter → On this page  * Syntax\n * Command Fields\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/isSelf/": " Docs Home → MongoDB Manual \nISSELF \n_isSelf \n_isSelf is an internal command. ←  hostInfolistCommands → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/insert/": " Docs Home → MongoDB Manual \nINSERT \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Behavior\n * Examples\n * Output \nDEFINITION \ninsert \nThe insert command inserts one or more documents and returns a document containing the status of all inserts. The insert methods provided by the MongoDB drivers use this command internally. \nTIP In mongosh, this command can also be run through the db.collection.insertOne() and db.collection.insertMany() helper methods. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. Returns:A document that contains the status of the operation. See Output for details. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {      insert: <collection>,      documents: [ <document>, <document>, <document>, ... ],      ordered: <boolean>,      writeConcern: { <write concern> },      bypassDocumentValidation: <boolean>,      comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\ninsert\nstring\nThe name of the target collection.\ndocuments\narray\nAn array of one or more documents to insert into the named collection.\nordered\nboolean\nOptional. If true, then when an insert of a document fails, return without inserting any remaining documents listed in the inserts array. If false, then when an insert of a document fails, continue to insert the remaining documents. Defaults to true.\nwriteConcern\ndocument Optional. A document that expresses the write concern of the insert command. Omit to use the default write concern. Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. bypassDocumentValidation\nboolean\nOptional. Enables insert to bypass document validation during the operation. This lets you insert documents that do not meet the validation requirements.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nBEHAVIOR  SIZE LIMIT \nThe total size of all the documents array elements must be less than or equal to the maximum BSON document size. The total number of documents in the documents array must be less than or equal to the maximum bulk size. \nDOCUMENT VALIDATION \nThe insert command adds support for the bypassDocumentValidation option, which lets you bypass document validation when inserting or updating documents in a collection with validation rules. \nTRANSACTIONS \ninsert can be used inside multi-document transactions. \nIMPORTANT In most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transactions should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions. For additional transactions usage considerations (such as runtime limit and oplog size limit), see also Production Considerations. COLLECTION CREATION IN TRANSACTIONS \nStarting in MongoDB 4.4, you can create collections and indexes inside a multi-document transaction if the transaction is not a cross-shard write transaction. Specifically, in MongoDB 4.4 and greater, if you specify an insert on a non-existing collection in a transaction, the collection is implicitly created. In MongoDB 4.4 and earlier, the operation must be run on an existing collection. \nTIP \nSEE ALSO: Create Collections and Indexes In a Transaction WRITE CONCERNS AND TRANSACTIONS \nDo not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. \nINSERT INACCURACIES \nEven if you encounter a server error during an insert, some documents may have been inserted.\n\n \nEXAMPLES  INSERT A SINGLE DOCUMENT \nInsert a document into the users collection: db.runCommand(   {      insert: \"users\",      documents: [ { _id: 1, user: \"abc123\", status: \"A\" } ]   })  The returned document shows that the command successfully inserted a document. See Output for details. { \"ok\" : 1, \"n\" : 1 }  \nBULK INSERT \nInsert three documents into the users collection: db.runCommand(   {      insert: \"users\",      documents: [         { _id: 2, user: \"ijk123\", status: \"A\" },         { _id: 3, user: \"xyz123\", status: \"P\" },         { _id: 4, user: \"mop123\", status: \"P\" }      ],      ordered: false,      writeConcern: { w: \"majority\", wtimeout: 5000 }   })  The returned document shows that the command successfully inserted the three documents. See Output for details. { \"ok\" : 1, \"n\" : 3 }  \nUSING INSERT WITH BYPASSDOCUMENTVALIDATION \nIf schema validation validationActions are set to error, inserts to a collection return errors for documents that violate the schema validation rules. To insert documents which would violate these rules set bypassDocumentValidation: true. Create the user collection with a validation rule on the status fields. The validation rule validates that the status must be \"Unknown\" or \"Incomplete\": db.createCollection(\"users\", {   validator:      {         status: {            $in: [ \"Unknown\", \"Incomplete\" ]         }      }})  Attempt to insert a document which violates the validation rule: db.runCommand({      insert: \"users\",      documents: [ {user: \"123\", status: \"Active\" } ]})  The insert returns a write error message: {   n: 0,   writeErrors: [      {         index: 0,         code: 121,         errInfo: {            failingDocumentId: ObjectId('6197a7f2d84e85d1cc90d270'),            details: {               operatorName: '$in',               specifiedAs: { status: { '$in': [Array] } },               reason: 'no matching value found in array',               consideredValue: 'Active'            }         },         errmsg: 'Document failed validation'      }   ],   ok: 1} Set bypassDocumentValidation : true and rerun the insert: db.runCommand({   insert: \"users\",   documents: [ {user: \"123\", status: \"Active\" } ],   bypassDocumentValidation: true})  The operation succeeds. To check for documents that violate schema validation rules, use the validate command. \nOUTPUT \nThe returned document contains a subset of the following fields: insert.ok \nThe status of the command. insert.n \nThe number of documents inserted. insert.writeErrors \nAn array of documents that contains information regarding any error encountered during the insert operation. The writeErrors array contains an error document for each insert that errors. Each error document contains the following fields: insert.writeErrors.index \nAn integer that identifies the document in the documents array, which uses a zero-based index. insert.writeErrors.code \nAn integer value identifying the error. insert.writeErrors.errmsg \nA description of the error. insert.writeConcernError \nDocument that describe error related to write concern and contains the field: insert.writeConcernError.code\n\n insert.writeConcernError.errmsg \nA description of the cause of the write concern error. insert.writeConcernError.errInfo.writeConcern \nNew in version 4.4. The write concern object used for the corresponding operation. For information on write concern object fields, see Write Concern Specification. The write concern object may also contain the following field, indicating the source of the write concern: insert.writeConcernError.errInfo.writeConcern.provenance \nA string value indicating where the write concern originated (known as write concern provenance). The following table shows the possible values for this field and their significance: Provenance\nDescription\nclientSupplied\nThe write concern was specified in the application.\ncustomDefault\nThe write concern originated from a custom defined default value. See setDefaultRWConcern.\ngetLastErrorDefaults\nThe write concern originated from the replica set's settings.getLastErrorDefaults field.\nimplicitDefault\nThe write concern originated from the server in absence of all other write concern specifications. The following is an example document returned for a successful insert of a single document: { ok: 1, n: 1 }  The following is an example document returned for an insert of two documents that successfully inserted one document but encountered an error with the other document: {   \"ok\" : 1,   \"n\" : 1,   \"writeErrors\" : [      {         \"index\" : 1,         \"code\" : 11000,         \"errmsg\" : \"insertDocument :: caused by :: 11000 E11000 duplicate key error index: test.users.$_id_  dup key: { : 1.0 }\"      }   ]} \n←  getMoreresetError → On this page  * Definition\n * Syntax\n * Command Fields\n * Behavior\n * Examples\n * Output Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/usersInfo/": " Docs Home → MongoDB Manual \nUSERSINFO \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * usersInfo: <various>\n * Required Access\n * Output\n * Examples \nDEFINITION \nusersInfo \nReturns information about one or more users. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     usersInfo: <various>,     showCredentials: <Boolean>,     showCustomData: <Boolean>,     showPrivileges: <Boolean>,     showAuthenticationRestrictions: <Boolean>,     filter: <document>,     comment: <any>   })  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nusersInfo\nvarious The user(s) about whom to return information. The argument to usersInfo has multiple forms depending on the requested information. See usersInfo: <various>. showCredentials\nboolean Optional. Set to true to display the user's password hash. By default, this field is false. showCustomData\nboolean Optional. Set to false to omit the user's customData from the output. By default, this field is true. New in version 5.2. showPrivileges\nboolean Optional. Set to true to show the user's full set of privileges, including expanded information for the inherited roles. By default, this field is false. If viewing all users, you cannot specify this field. showAuthenticationRestrictions\nboolean Optional. Set to true to show the user's authentication restrictions. By default, this field is false. If viewing all users, you cannot specify this field. filter\ndocument\nOptional. A document that specifies $match stage conditions to return information for users that match the filter conditions.\ncomment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:      * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). New in version 4.4. \nUSERSINFO: <VARIOUS> \n{ usersInfo: <various> }  The argument to usersInfo has multiple forms depending on the requested information: Argument\nReturns\n{ usersInfo: 1 } Returns information about the users in the database where the command is run. mongosh provides the db.getUsers() helper for this invocation of the command. { usersInfo: <username> } Return information about the a specific user that exists in the database where the command is run. mongosh provides the db.getUser() helper for this invocation of the command. { usersInfo: { user: <name>, db: <db> } }\nReturns information about the user specified by the name and database.\n{ usersInfo: [ { user: <name>, db: <db> }, ... ] }\n{ usersInfo: [ <username1>, ... ] }\nReturns information about the specified users.\n{ forAllDBs: true }\nReturns information about users in all databases. \nREQUIRED ACCESS \nUsers can always view their own information. To view another user's information, the user running the command must have privileges that include the viewUser action on the other user's database. \nOUTPUT \nThe following information can be returned by the usersInfo depending on the options specified:\n\n  \nEXAMPLES  VIEW SPECIFIC USERS \nTo see information and privileges, but not the credentials, for the user \"Kari\" defined in \"home\" database, run the following command: db.runCommand(   {     usersInfo:  { user: \"Kari\", db: \"home\" },     showPrivileges: true   })  To view a user that exists in the current database, you can specify the user by name only. For example, if you are in the home database and a user named \"Kari\" exists in the home database, you can run the following command: db.getSiblingDB(\"home\").runCommand(   {     usersInfo:  \"Kari\",     showPrivileges: true   })  \nVIEW MULTIPLE USERS \nTo view info for several users, use an array, with or without the optional fields showPrivileges and showCredentials. For example: db.runCommand( {   usersInfo: [ { user: \"Kari\", db: \"home\" }, { user: \"Li\", db: \"myApp\" } ],   showPrivileges: true} )  \nVIEW ALL USERS FOR A DATABASE \nTo view all users on the database the command is run, use a command document that resembles the following: db.runCommand( { usersInfo: 1 } )  When viewing all users, you can specify the showCredentials option but not the showPrivileges or the showAuthenticationRestrictions options. \nVIEW ALL USERS FOR A DATABASE THAT MATCH THE SPECIFIED FILTER \nThe usersInfo command can accept a filter document to return information for users that match the filter condition. To view all users in the current database who have the specified role, use a command document that resembles the following: db.runCommand( { usersInfo: 1, filter: { roles: { role: \"root\", db: \"admin\" } } } )  When viewing all users, you can specify the showCredentials option but not the showPrivileges or the showAuthenticationRestrictions options. \nVIEW ALL USERS WITH SCRAM-SHA-1 CREDENTIALS \nThe usersInfo command can accept a filter document to return information for users that match the filter condition. The following operation returns all users that have SCRAM-SHA-1 credentials. Specifically, the command returns all users across all databases and then uses the $match stage to apply the specified filter to the users. db.runCommand( { usersInfo: { forAllDBs: true}, filter: { mechanisms: \"SCRAM-SHA-1\" } } )  When viewing all users, you can specify the showCredentials option but not the showPrivileges or the showAuthenticationRestrictions options. \nOMIT CUSTOM DATA FROM OUTPUT \nNew in version 5.2: To omit users' custom data from the usersInfo output, set the showCustomData option to false. Use the createUser command to create a user named accountAdmin01 on the products database: db.getSiblingDB(\"products\").runCommand( {   createUser: \"accountAdmin01\",   pwd: passwordPrompt(),   customData: { employeeId: 12345 },   roles: [ { role: 'readWrite', db: 'products' } ]} )  The user contains a customData field of { employeeId: 12345 }. To retrieve the user but omit the custom data from the output, run usersInfo with showCustomData set to false: db.getSiblingDB(\"products\").runCommand ( {   usersInfo: \"accountAdmin01\",   showCustomData: false} )  Example output: {   users: [      {         _id: 'products.accountAdmin01',         userId: UUID(\"0955afc1-303c-4683-a029-8e17dd5501f4\"),         user: 'accountAdmin01',         db: 'products',         roles: [ { role: 'readWrite', db: 'products' } ],         mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]      }   ],   ok: 1} ←  updateUserRole Management Commands → On this page\n\n Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/getMore/": " Docs Home → MongoDB Manual \nGETMORE \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Output\n * Behavior \nDEFINITION \ngetMore \nUse in conjunction with commands that return a cursor, e.g. find and aggregate, to return subsequent batches of documents currently pointed to by the cursor. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {      getMore: <long>,      collection: <string>,      batchSize: <int>,      maxTimeMS: <int>,      comment: <any>   })  \nCOMMAND FIELDS \nThe command accepts the following fields: Field\nType\nDescription\ngetMore\nlong\nThe cursor id.\ncollection\nstring\nThe name of the collection over which the cursor is operating.\nbatchSize\npositive integer\nOptional. The number of documents to return in the batch.\nmaxTimeMS\nnon-negative integer Optional. Specifies a time limit in milliseconds for processing operations on a cursor. If you do not specify a value for maxTimeMS, operations will not time out. A value of 0 explicitly specifies the default unbounded behavior. MongoDB terminates operations that exceed their allotted time limit using the same mechanism as db.killOp(). MongoDB only terminates an operation at one of its designated interrupt points. With tailable cursors:      * maxTimeMS on getMore specifies the maximum amount of time MongoDB waits for new documents to be added to the cursor for that specific getMore command. If no value is provided, this defaults to 1000 milliseconds.  * The command that creates the cursor only sets maxTimeMS for the initial operation. It does not set maxTimeMS for subsequent operations.  * Set maxTimeMS individually for each call to getMore.  * A timeout on getMore will retain the documents accumulated before the timeout occurred in the cursor. With non-tailable cursors:  * The command that creates the cursor sets maxTimeMS. This is the maximum amount of time that the initial operation, and any subsequent operations, can spend on the entire cursor.  * A timeout on getMore raises an error.  * maxTimeMS cannot be set when getMore is called on a non-tailable cursor. comment\nany Optional. A user-provided comment to attach to this command. Once set, this comment appears alongside records of this command in the following locations:  * mongod log messages, in the attr.command.cursor.comment field.  * Database profiler output, in the command.comment field.  * currentOp output, in the command.comment field. A comment can be any valid BSON type (string, integer, object, array, etc). \nNOTE If omitted, getMore inherits any comment set on the originating find or aggregate command. New in version 4.4. \nOUTPUT \nThe command returns a document that contains the cursor information as well as the next batch. For example, a document similar to the one below may be returned when getMore is run on a cursor that was originally created by a find operation on a sharded cluster: {   \"cursor\" : {      \"id\" : NumberLong(\"678960441858272731\"),      \"ns\" : \"test.contacts\",      \"nextBatch\" : [         {            \"_id\" : ObjectId(\"5e8e501e1a32d227f9085857\"),            \"zipcode\" : \"220000\"         }      ],      \"partialResultsReturned\" : true,      \"postBatchResumeToken\": \"< Resume Token >\"   },   \"ok\" : 1,   \"operationTime\" : Timestamp(1586385239, 2),   \"$clusterTime\" : {      \"clusterTime\" : Timestamp(1586385239, 2),      \"signature\" : {         \"hash\" : BinData(0,\"lLjejeW6AQGReR9x1PD8xU+tP+A=\"),         \"keyId\" : NumberLong(\"6813467763969884181\")      }   }}\n\n Contains the cursor information, including the cursor ID as well as the nextBatch of documents. Starting in 4.4, if the cursor from a find command returns partial results due to the unavailability of the queried shard(s), the cursor document includes a partialResultsReturned field. To return partial results, rather than error, due to the unavailability of the queried shard(s), the initial find command must run with allowPartialResults set to true. See allowPartialResults. If the queried shards are initially available for the find command but one or more shards become unavailable in subsequent getMore commands, only the getMore commands run when a queried shard or shards are unavailable include the partialResultsReturned flag in the output. The postBatchResumeToken field can be used with the $changeStream pipeline to start or resume a change stream from this point. \"ok\"\nIndicates whether the command has succeeded (1) or failed (0). In addition to the aforementioned getMore-specific fields, the db.runCommand() includes the following information for replica sets and sharded clusters:  * $clusterTime  * operationTime See db.runCommand() Results for details. \nBEHAVIOR  ACCESS CONTROL \nIf authentication is turned on, you can only issue a getMore against cursors you created. \nSESSIONS \nNew in version 4.0. For cursors created inside a session, you cannot call getMore outside the session. Similarly, for cursors created outside of a session, you cannot call getMore inside a session. TRANSACTIONS \nNew in version 4.0. For multi-document transactions:  * For cursors created outside of a transaction, you cannot call getMore inside the transaction.  * For cursors created in a transaction, you cannot call getMore outside the transaction. \nSLOW QUERIES \nStarting in MongoDB 5.1, when a getMore command is logged as a slow query, the queryHash and planCacheKey fields are added to the slow query log message and the profiler log message. ←  getLastErrorinsert → On this page  * Definition\n * Syntax\n * Command Fields\n * Output\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query-comparison/": " Docs Home → MongoDB Manual \nCOMPARISON QUERY OPERATORS \nComparison operators return data based on value comparisons. \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. For comparison of different BSON type values, see the specified BSON comparison order. Name\nDescription\n$eq\nMatches values that are equal to a specified value.\n$gt\nMatches values that are greater than a specified value.\n$gte\nMatches values that are greater than or equal to a specified value.\n$in\nMatches any of the values specified in an array.\n$lt\nMatches values that are less than a specified value.\n$lte\nMatches values that are less than or equal to a specified value.\n$ne\nMatches all values that are not equal to a specified value.\n$nin\nMatches none of the values specified in an array. ←  Query and Projection Operators$eq → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/shardCollection/": " Docs Home → MongoDB Manual \nSHARDCOLLECTION \nOn this page    \n * Definition\n   \n * Syntax\n * Command Fields\n * Considerations\n * Example \nDEFINITION \nshardCollection \nShards a collection to distribute its documents across shards. The shardCollection command must be run against the admin database. \nNOTE Changed in version 6.0. Starting in MongoDB 6.0, sharding a collection does not require you to first run the enableSharding command to configure the database. \nTIP In mongosh, this command can also be run through the sh.shardCollection() helper method. Helper methods are convenient for mongosh users, but they may not return the same level of information as database commands. In cases where the convenience is not needed or the additional return fields are required, use the database command. \nSYNTAX \nTo run shardCollection, use the db.runCommand( { <command> } ) method. The command has the following form: db.adminCommand(   {     shardCollection: \"<database>.<collection>\",     key: { <field1>: <1|\"hashed\">, ... },     unique: <boolean>,     numInitialChunks: <integer>,     presplitHashedZones: <boolean>,     collation: { locale: \"simple\" },     timeseries: <object>   } )  \nCOMMAND FIELDS \nThe command takes the following fields: Field\nType\nDescription\nshardCollection\nstring\nThe namespace of the collection to shard in the form <database>.<collection>.\nkey\ndocument The document that specifies the field or fields to use as the shard key. { <field1>: <1|\"hashed\">, ... } Set the field values to either:      * 1 for ranged based sharding  * \"hashed\" to specify a hashed shard key. shard key must be supported by an index. Unless the collection is empty, the index must exist prior to the shardCollection command. If the collection is empty, MongoDB creates the index prior to sharding the collection if the index that can support the shard key does not already exist. See also Shard Key Indexes unique\nboolean Specify true to ensure that the underlying index enforces a unique constraint. Defaults to false. You cannot specify true when using hashed shard keys. numInitialChunks\ninteger Specifies the initial number of chunks to create across all shards in the cluster when sharding an empty collection with a hashed shard key. MongoDB will then create and balance chunks across the cluster. The numInitialChunks must result in less than 8192 per shard. If the collection is not empty or the shard key does not contain a hashed field, the operation returns an error.  * If sharding with presplitHashedZones: true, MongoDB attempts to evenly distribute the specified number of chunks across the zones in the cluster.  * If sharding with presplitHashedZones: false or omitted and no zones and zone ranges are defined for the empty collection, MongoDB attempts to evenly distributed the specified number of chunks across the shards in the cluster.  * If sharding with presplitHashedZones: false or omitted and zones and zone ranges have been defined for the empty collection, numInitChunks has no effect. Changed in version 4.4. collation\ndocument\nOptional. If the collection specified to shardCollection has a default collation, you must include a collation document with { locale : \"simple\" }, or the shardCollection command fails. At least one of the indexes whose fields support the shard key pattern must have the simple collation.\npresplitHashedZones\nboolean Optional. Specify true to perform initial chunk creation and distribution for an empty or non-existing collection based on the defined zones and zone ranges for the collection. For hashed sharding only. shardCollection with presplitHashedZones: true returns an error if any of the following are true:  * The shard key does not contain a hashed field (i.e. is not a single field hashed index or compound hashed index).  * The collection has no defined zones or zone ranges.  * The defined zone range or ranges do not meet the requirements. New in version 4.4. timeseries\nobject Optional. Specify this option to create a new sharded time series collection. To shard an existing time series collection, omit this parameter.\n\n For detailed syntax, see Time Series Options. New in version 5.1. \nTIME SERIES OPTIONS \nNew in version 5.1. To create a new time series collection that is sharded, specify the timeseries option to shardCollection. The timeseries option takes the following fields: Field\nType\nDescription\ntimeField\nstring Required. The name of the field which contains the date in each time series document. Documents in a time series collection must have a valid BSON date as the value for the timeField. metaField\nstring Optional. The name of the field which contains metadata in each time series document. The metadata in the specified field should be data that is used to label a unique series of documents. The metadata should rarely, if ever, change. The name of the specified field may not be _id or the same as the timeseries.timeField. The field can be of any type. granularity\nstring Optional. Possible values are:  * \"seconds\"  * \"minutes\"  * \"hours\" By default, MongoDB sets the granularity to \"seconds\" for high-frequency ingestion. Manually set the granularity parameter to improve performance by optimizing how data in the time series collection is stored internally. To select a value for granularity, choose the closest match to the time span between consecutive incoming measurements. If you specify the timeseries.metaField, consider the time span between consecutive incoming measurements that have the same unique value for the metaField field. Measurements often have the same unique value for the metaField field if they come from the same source. If you do not specify timeseries.metaField, consider the time span between all measurements that are inserted in the collection. \nCONSIDERATIONS  USE \nDo not run more than one shardCollection command on the same collection at the same time. Once a collection has been sharded, MongoDB provides no method to unshard a sharded collection. \nSHARD KEYS \nWhile you can change your shard key later, it is important to carefully consider your shard key choice to avoid scalability and perfomance issues. \nTIP \nSEE ALSO:  * Choose a Shard Key  * Shard Keys SHARD KEYS ON TIME SERIES COLLECTIONS \nWhen sharding time series collections, you can only specify the following fields in the shard key:  * The metaField  * Sub-fields of metaField  * The timeField You may specify combinations of these fields in the shard key. No other fields, including _id, are allowed in the shard key pattern. When you specify the shard key:  * metaField can be either a:\n   \n   * Hashed shard key\n   \n   * Ranged shard key  * timeField must be:\n   \n   * A ranged shard key\n   \n   * At the end of the shard key pattern \nTIP Avoid specifying only the timeField as the shard key. Since the timeField increases monotonically, it may result in all writes appearing on a single chunk within the cluster. Ideally, data is evenly distributed across chunks. To learn how to best choose a shard key, see:  * Choose a Shard Key  * MongoDB Blog: On Selecting a Shard Key for MongoDB. \nHASHED SHARD KEYS \nHashed shard keys use a hashed index or a compound hashed index as the shard key. Use the form field: \"hashed\" to specify a hashed shard key field. \nNOTE If chunk migrations are in progress while creating a hashed shard key collection, the initial chunk distribution may be uneven until the balancer automatically balances the collection. \nTIP \nSEE ALSO: Hashed Sharding \nZONE SHARDING AND INITIAL CHUNK DISTRIBUTION \nThe shard collection operation (i.e. shardCollection command and the sh.shardCollection() helper) can perform initial chunk creation and distribution for an empty or a non-existing collection if zones and zone ranges have been defined for the collection. Initial chunk distribution allows for a faster setup of zoned sharding. After the initial distribution, the balancer manages the chunk distribution going forward per usual. See Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection for an example. If sharding a collection using a ranged or single-field hashed shard key, the numInitialChunks option has no effect if zones and zone ranges have been defined for the empty collection. To shard a collection using a compound hashed index, see Zone Sharding and Compound Hashed Indexes. ZONE SHARDING AND COMPOUND HASHED INDEXES\n\n The numInitialChunks option has no effect if zones and zone ranges have been defined for the empty collection and presplitHashedZones is false. See Pre-Define Zones and Zone Ranges for an Empty or Non-Existing Collection for an example. \nTIP \nSEE ALSO:  * Initial Chunks  * balancerCollectionStatus \nUNIQUENESS \nIf specifying unique: true:  * If the collection is empty, shardCollection creates the unique index on the shard key if such an index does not already exist.  * If the collection is not empty, you must create the index first before using shardCollection. Although you can have a unique compound index where the shard key is a prefix, if using unique parameter, the collection must have a unique index that is on the shard key. See also Sharded Collection and Unique Indexes COLLATION \nChanged in version 3.4. If the collection has a default collation, the shardCollection command must include a collation parameter with the value { locale: \"simple\" }. For non-empty collections with a default collation, you must have at least one index with the simple collation whose fields support the shard key pattern. You do not need to specify the collation option for collections without a collation. If you do specify the collation option for a collection with no collation, it will have no effect. \nWRITE CONCERN \nmongos uses \"majority\" for the write concern of the shardCollection command and its helper sh.shardCollection(). \nEXAMPLE \nThe following operation enables sharding for the people collection in the records database and uses the zipcode field as the shard key: db.adminCommand( { shardCollection: \"records.people\", key: { zipcode: 1 } } )  \nTIP \nSEE ALSO:  * refineCollectionShardKey  * sh.balancerCollectionStatus()  * sh.shardCollection()  * Sharding ←  setShardVersionshardingState → On this page  * Definition\n * Syntax\n * Command Fields\n * Considerations\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query-logical/": " Docs Home → MongoDB Manual \nLOGICAL QUERY OPERATORS \nLogical operators return data based on expressions that evaluate to true or false. \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$and\nJoins query clauses with a logical AND returns all documents that match the conditions of both clauses.\n$not\nInverts the effect of a query expression and returns documents that do not match the query expression.\n$nor\nJoins query clauses with a logical NOR returns all documents that fail to match both clauses.\n$or\nJoins query clauses with a logical OR returns all documents that match the conditions of either clause. ←  $nin$and → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update-field/": " Docs Home → MongoDB Manual \nFIELD UPDATE OPERATORS  NOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$currentDate\nSets the value of a field to current date, either as a Date or a Timestamp.\n$inc\nIncrements the value of the field by the specified amount.\n$min\nOnly updates the field if the specified value is less than the existing field value.\n$max\nOnly updates the field if the specified value is greater than the existing field value.\n$mul\nMultiplies the value of the field by the specified amount.\n$rename\nRenames a field.\n$set\nSets the value of a field in a document.\n$setOnInsert\nSets the value of a field if an update results in an insert of a document. Has no effect on update operations that modify existing documents.\n$unset\nRemoves the specified field from a document. ←  Update Operators$currentDate → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query-evaluation/": " Docs Home → MongoDB Manual \nEVALUATION QUERY OPERATORS \nEvaluation operators return data based on evaluations of either individual fields or the entire collection's documents. \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$expr\nAllows use of aggregation expressions within the query language.\n$jsonSchema\nValidate documents against the given JSON Schema.\n$mod\nPerforms a modulo operation on the value of a field and selects documents with a specified result.\n$regex\nSelects documents where values match a specified regular expression.\n$text\nPerforms text search.\n$where\nMatches documents that satisfy a JavaScript expression. ←  $type$expr → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/": " Docs Home → MongoDB Manual \nAGGREGATION PIPELINE STAGES \nOn this page    \n * Stages\n   \n * Alphabetical Listing of Stages In the db.collection.aggregate() method and db.aggregate() method, pipeline stages appear in an array. Documents pass through the stages in sequence. \nSTAGES  DB.COLLECTION.AGGREGATE() STAGES \nAll except the $out, $merge, $geoNear, and $changeStream stages can appear multiple times in a pipeline. \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. db.collection.aggregate( [ { <stage> }, ... ] ) \nStage\nDescription\n$addFields Adds new fields to documents. Similar to $project, $addFields reshapes each document in the stream; specifically, by adding new fields to output documents that contain both the existing fields from the input documents and the newly added fields. $set is an alias for $addFields. $bucket\nCategorizes incoming documents into groups, called buckets, based on a specified expression and bucket boundaries.\n$bucketAuto\nCategorizes incoming documents into a specific number of groups, called buckets, based on a specified expression. Bucket boundaries are automatically determined in an attempt to evenly distribute the documents into the specified number of buckets.\n$changeStream\nReturns a Change Stream cursor for the collection. This stage can only occur once in an aggregation pipeline and it must occur as the first stage.\n$collStats\nReturns statistics regarding a collection or view.\n$count Returns a count of the number of documents at this stage of the aggregation pipeline. Distinct from the $count aggregation accumulator. $densify Creates new documents in a sequence of documents where certain values in a field are missing. $documents\nReturns literal documents from input expressions.\n$facet\nProcesses multiple aggregation pipelines within a single stage on the same set of input documents. Enables the creation of multi-faceted aggregations capable of characterizing data across multiple dimensions, or facets, in a single stage.\n$fill Populates null and missing field values within documents. $geoNear Returns an ordered stream of documents based on the proximity to a geospatial point. Incorporates the functionality of $match, $sort, and $limit for geospatial data. The output documents include an additional distance field and can include a location identifier field. $graphLookup\nPerforms a recursive search on a collection. To each output document, adds a new array field that contains the traversal results of the recursive search for that document.\n$group\nGroups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group. Consumes all input documents and outputs one document per each distinct group. The output documents only contain the identifier field and, if specified, accumulated fields.\n$indexStats\nReturns statistics regarding the use of each index for the collection.\n$limit\nPasses the first n documents unmodified to the pipeline where n is the specified limit. For each input document, outputs either one document (for the first n documents) or zero documents (after the first n documents).\n$listSessions\nLists all sessions that have been active long enough to propagate to the system.sessions collection.\n$lookup\nPerforms a left outer join to another collection in the same database to filter in documents from the \"joined\" collection for processing.\n$match\nFilters the document stream to allow only matching documents to pass unmodified into the next pipeline stage. $match uses standard MongoDB queries. For each input document, outputs either one document (a match) or zero documents (no match).\n$merge Writes the resulting documents of the aggregation pipeline to a collection. The stage can incorporate (insert new documents, merge documents, replace documents, keep existing documents, fail the operation, process documents with a custom update pipeline) the results into an output collection. To use the $merge stage, it must be the last stage in the pipeline. New in version 4.2. $out\nWrites the resulting documents of the aggregation pipeline to a collection. To use the $out stage, it must be the last stage in the pipeline.\n$planCacheStats\nReturns plan cache information for a collection.\n$project Reshapes each document in the stream, such as by adding new fields or removing existing fields. For each input document, outputs one document. See also $unset for removing existing fields.\n\n Replaces a document with the specified embedded document. The operation replaces all existing fields in the input document, including the _id field. Specify a document embedded in the input document to promote the embedded document to the top level. $replaceWith is an alias for $replaceRoot stage. $replaceWith Replaces a document with the specified embedded document. The operation replaces all existing fields in the input document, including the _id field. Specify a document embedded in the input document to promote the embedded document to the top level. $replaceWith is an alias for $replaceRoot stage. $sample\nRandomly selects the specified number of documents from its input.\n$search Performs a full-text search of the field or fields in an Atlas collection. \nNOTE $search is only available for MongoDB Atlas clusters, and is not available for self-managed deployments. To learn more, see Atlas Search Aggregation Pipeline Stages. $searchMeta Returns different types of metadata result documents for the Atlas Search query against an Atlas collection. \nNOTE $searchMeta is only available for MongoDB Atlas clusters running MongoDB v4.4.9 or higher, and is not available for self-managed deployments. To learn more, see Atlas Search Aggregation Pipeline Stages. $set Adds new fields to documents. Similar to $project, $set reshapes each document in the stream; specifically, by adding new fields to output documents that contain both the existing fields from the input documents and the newly added fields. $set is an alias for $addFields stage. $setWindowFields Groups documents into windows and applies one or more operators to the documents in each window. New in version 5.0. $skip\nSkips the first n documents where n is the specified skip number and passes the remaining documents unmodified to the pipeline. For each input document, outputs either zero documents (for the first n documents) or one document (if after the first n documents).\n$sort\nReorders the document stream by a specified sort key. Only the order changes; the documents remain unmodified. For each input document, outputs one document.\n$sortByCount\nGroups incoming documents based on the value of a specified expression, then computes the count of documents in each distinct group.\n$unionWith Performs a union of two collections; i.e. combines pipeline results from two collections into a single result set. New in version 4.4. $unset Removes/excludes fields from documents. $unset is an alias for $project stage that removes fields. $unwind\nDeconstructs an array field from the input documents to output a document for each element. Each output document replaces the array with an element value. For each input document, outputs n documents where n is the number of array elements and can be zero for an empty array. For aggregation expression operators to use in the pipeline stages, see Aggregation Pipeline Operators. \nDB.AGGREGATE() STAGES \nMongoDB also provides the db.aggregate() method: db.aggregate( [ { <stage> }, ... ] )  The following stages use the db.aggregate() method and not the db.collection.aggregate() method. Stage\nDescription\n$changeStream\nReturns a Change Stream cursor for the collection. This stage can only occur once in an aggregation pipeline and it must occur as the first stage.\n$currentOp\nReturns information on active and/or dormant operations for the MongoDB deployment.\n$listLocalSessions\nLists all active sessions recently in use on the currently connected mongos or mongod instance. These sessions may have not yet propagated to the system.sessions collection.\n$documents\nReturns literal documents from input values. \nSTAGES AVAILABLE FOR UPDATES \nStarting in MongoDB 4.2, you can use the aggregation pipeline for updates in: Command\nmongosh Methods\nfindAndModify\ndb.collection.findOneAndUpdate()\ndb.collection.findAndModify()\nupdate\ndb.collection.updateOne()\ndb.collection.updateMany()\nBulk.find.update()\nBulk.find.updateOne()\nBulk.find.upsert() For the updates, the pipeline can consist of the following stages:      * $addFields and its alias $set  * $project and its alias $unset  * $replaceRoot and its alias $replaceWith. \nALPHABETICAL LISTING OF STAGES\n\n Returns a count of the number of documents at this stage of the aggregation pipeline. Distinct from the $count aggregation accumulator. $currentOp\nReturns information on active and/or dormant operations for the MongoDB deployment. To run, use the db.aggregate() method.\n$densify Creates new documents in a sequence of documents where certain values in a field are missing. $documents\nReturns literal documents from input values.\n$facet\nProcesses multiple aggregation pipelines within a single stage on the same set of input documents. Enables the creation of multi-faceted aggregations capable of characterizing data across multiple dimensions, or facets, in a single stage.\n$fill Populates null and missing field values within documents. $geoNear Returns an ordered stream of documents based on the proximity to a geospatial point. Incorporates the functionality of $match, $sort, and $limit for geospatial data. The output documents include an additional distance field and can include a location identifier field. $graphLookup\nPerforms a recursive search on a collection. To each output document, adds a new array field that contains the traversal results of the recursive search for that document.\n$group\nGroups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group. Consumes all input documents and outputs one document per each distinct group. The output documents only contain the identifier field and, if specified, accumulated fields.\n$indexStats\nReturns statistics regarding the use of each index for the collection.\n$limit\nPasses the first n documents unmodified to the pipeline where n is the specified limit. For each input document, outputs either one document (for the first n documents) or zero documents (after the first n documents).\n$listLocalSessions\nLists all active sessions recently in use on the currently connected mongos or mongod instance. These sessions may have not yet propagated to the system.sessions collection.\n$listSessions\nLists all sessions that have been active long enough to propagate to the system.sessions collection.\n$lookup\nPerforms a left outer join to another collection in the same database to filter in documents from the \"joined\" collection for processing.\n$match\nFilters the document stream to allow only matching documents to pass unmodified into the next pipeline stage. $match uses standard MongoDB queries. For each input document, outputs either one document (a match) or zero documents (no match).\n$merge Writes the resulting documents of the aggregation pipeline to a collection. The stage can incorporate (insert new documents, merge documents, replace documents, keep existing documents, fail the operation, process documents with a custom update pipeline) the results into an output collection. To use the $merge stage, it must be the last stage in the pipeline. New in version 4.2. $out\nWrites the resulting documents of the aggregation pipeline to a collection. To use the $out stage, it must be the last stage in the pipeline.\n$planCacheStats\nReturns plan cache information for a collection.\n$project\nReshapes each document in the stream, such as by adding new fields or removing existing fields. For each input document, outputs one document.\n$redact\nReshapes each document in the stream by restricting the content for each document based on information stored in the documents themselves. Incorporates the functionality of $project and $match. Can be used to implement field level redaction. For each input document, outputs either one or zero documents.\n$replaceRoot\nReplaces a document with the specified embedded document. The operation replaces all existing fields in the input document, including the _id field. Specify a document embedded in the input document to promote the embedded document to the top level.\n$replaceWith Replaces a document with the specified embedded document. The operation replaces all existing fields in the input document, including the _id field. Specify a document embedded in the input document to promote the embedded document to the top level. Alias for $replaceRoot. $sample\nRandomly selects the specified number of documents from its input.\n$search Performs a full-text search of the field or fields in an Atlas collection. \nNOTE $search is only available for MongoDB Atlas clusters, and is not available for self-managed deployments. To learn more, see Atlas Search Aggregation Pipeline Stages. $searchMeta Returns different types of metadata result documents for the Atlas Search query against an Atlas collection. \nNOTE $searchMeta is only available for MongoDB Atlas clusters running MongoDB v4.4.9 or higher, and is not available for self-managed deployments. To learn more, see Atlas Search Aggregation Pipeline Stages. $set\n\n Alias for $addFields. $setWindowFields Groups documents into windows and applies one or more operators to the documents in each window. New in version 5.0. $shardedDataDistribution Provides data and size distribution information on sharded collections. New in version 6.0.3. $skip\nSkips the first n documents where n is the specified skip number and passes the remaining documents unmodified to the pipeline. For each input document, outputs either zero documents (for the first n documents) or one document (if after the first n documents).\n$sort\nReorders the document stream by a specified sort key. Only the order changes; the documents remain unmodified. For each input document, outputs one document.\n$sortByCount\nGroups incoming documents based on the value of a specified expression, then computes the count of documents in each distinct group.\n$unionWith Performs a union of two collections; i.e. combines pipeline results from two collections into a single result set. New in version 4.4. $unset Removes/exludes fields from documents. Alias for $project stage that excludes/removes fields. $unwind\nDeconstructs an array field from the input documents to output a document for each element. Each output document replaces the array with an element value. For each input document, outputs n documents where n is the number of array elements and can be zero for an empty array. ←  $bit$addFields (aggregation) → On this page  * Stages\n * Alphabetical Listing of Stages Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query-miscellaneous/": " Docs Home → MongoDB Manual \nMISCELLANEOUS QUERY OPERATORS  NOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$comment\nAdds a comment to a query predicate.\n$rand\nGenerates a random float between 0 and 1.\n$natural\nA special hint that can be provided via the sort() or hint() methods that can be used to force either a forward or reverse collection scan. ←  $slice (projection)$comment → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update-bitwise/": " Docs Home → MongoDB Manual \nBITWISE UPDATE OPERATOR  NOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$bit\nPerforms bitwise AND, OR, and XOR updates of integer values. ←  $sort$bit → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/": " Docs Home → MongoDB Manual \nUPDATE OPERATORS \nOn this page    \n * Update Operators The following modifiers are available for use in update operations, for example, in db.collection.updateMany() and db.collection.findAndModify(). Specify the operator expression in a document of the form: {   <operator1>: { <field1>: <value1>, ... },   <operator2>: { <field2>: <value2>, ... },   ...}  \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. \nUPDATE OPERATORS  BEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. In MongoDB 4.4 and earlier, update operators process all document fields in lexicographic order. Consider this example $set command: { $set: { \"a.2\": <new value>, \"a.10\": <new value>, } }  In MongoDB 5.0 and later, \"a.2\" is processed before \"a.10\" because 2 comes before 10 in numeric order. In MongoDB 4.4 and earlier, \"a.10\" is processed before \"a.2\" because 10 comes before 2 in lexicographic order. \nFIELDS \nName\nDescription\n$currentDate\nSets the value of a field to current date, either as a Date or a Timestamp.\n$inc\nIncrements the value of the field by the specified amount.\n$min\nOnly updates the field if the specified value is less than the existing field value.\n$max\nOnly updates the field if the specified value is greater than the existing field value.\n$mul\nMultiplies the value of the field by the specified amount.\n$rename\nRenames a field.\n$set\nSets the value of a field in a document.\n$setOnInsert\nSets the value of a field if an update results in an insert of a document. Has no effect on update operations that modify existing documents.\n$unset\nRemoves the specified field from a document. \nARRAY \nOPERATORS \nName\nDescription\n$\nActs as a placeholder to update the first element that matches the query condition.\n$[]\nActs as a placeholder to update all elements in an array for the documents that match the query condition.\n$[<identifier>]\nActs as a placeholder to update all elements that match the arrayFilters condition for the documents that match the query condition.\n$addToSet\nAdds elements to an array only if they do not already exist in the set.\n$pop\nRemoves the first or last item of an array.\n$pull\nRemoves all array elements that match a specified query.\n$push\nAdds an item to an array.\n$pullAll\nRemoves all matching values from an array. MODIFIERS \nName\nDescription\n$each\nModifies the $push and $addToSet operators to append multiple items for array updates.\n$position\nModifies the $push operator to specify the position in the array to add elements.\n$slice\nModifies the $push operator to limit the size of updated arrays.\n$sort\nModifies the $push operator to reorder documents stored in an array. \nBITWISE \nName\nDescription\n$bit\nPerforms bitwise AND, OR, and XOR updates of integer values. ←  $naturalField Update Operators → On this page  * Update Operators Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/projection/": " Docs Home → MongoDB Manual \nPROJECTION OPERATORS \nProjection operators specify the fields returned by an operation. \nNOTE find() operations on views do not support the following projection operators:      * $  * $elemMatch  * $slice  * $meta \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$\nProjects the first element in an array that matches the query condition.\n$elemMatch\nProjects the first element in an array that matches the specified $elemMatch condition.\n$meta\nProjects the available per-document metadata.\n$slice\nLimits the number of elements projected from an array. Supports skip and limit slices. ←  $bitsAnySet$ (projection) → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query-geospatial/": " Docs Home → MongoDB Manual \nGEOSPATIAL QUERY OPERATORS \nOn this page    \n * Operators Geospatial operators return data based on geospatial expression conditions. \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. \nOPERATORS  QUERY SELECTORS \nName\nDescription\n$geoIntersects\nSelects geometries that intersect with a GeoJSON geometry. The 2dsphere index supports $geoIntersects.\n$geoWithin\nSelects geometries within a bounding GeoJSON geometry. The 2dsphere and 2d indexes support $geoWithin.\n$near\nReturns geospatial objects in proximity to a point. Requires a geospatial index. The 2dsphere and 2d indexes support $near.\n$nearSphere\nReturns geospatial objects in proximity to a point on a sphere. Requires a geospatial index. The 2dsphere and 2d indexes support $nearSphere. \nGEOMETRY SPECIFIERS \nName\nDescription\n$box\nSpecifies a rectangular box using legacy coordinate pairs for $geoWithin queries. The 2d index supports $box.\n$center\nSpecifies a circle using legacy coordinate pairs to $geoWithin queries when using planar geometry. The 2d index supports $center.\n$centerSphere\nSpecifies a circle using either legacy coordinate pairs or GeoJSON format for $geoWithin queries when using spherical geometry. The 2dsphere and 2d indexes support $centerSphere.\n$geometry\nSpecifies a geometry in GeoJSON format to geospatial query operators.\n$maxDistance\nSpecifies a maximum distance to limit the results of $near and $nearSphere queries. The 2dsphere and 2d indexes support $maxDistance.\n$minDistance\nSpecifies a minimum distance to limit the results of $near and $nearSphere queries. For use with 2dsphere index only.\n$polygon\nSpecifies a polygon to using legacy coordinate pairs for $geoWithin queries. The 2d index supports $center. ←  $where$geoIntersects → On this page  * Operators Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query-array/": " Docs Home → MongoDB Manual \nARRAY QUERY OPERATORS \nArray operators return data based on array conditions. \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. Name\nDescription\n$all\nMatches arrays that contain all elements specified in the query.\n$elemMatch\nSelects documents if element in the array field matches all the specified $elemMatch conditions.\n$size\nSelects documents if the array field is a specified size. For examples on querying array fields, see:      * Query an Array  * Query an Array of Embedded Documents ←  $polygon$all → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/anyElementTrue/": " Docs Home → MongoDB Manual \n$ANYELEMENTTRUE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$anyElementTrue \nEvaluates an array as a set and returns true if any of the elements are true and false otherwise. An empty array returns false. $anyElementTrue has the following syntax: { $anyElementTrue: [ <expression> ] }  The <expression> itself must resolve to an array, separate from the outer array that denotes the argument list. For more information on expressions, see Expressions. \nBEHAVIOR \nIf a set contains a nested array element, $anyElementTrue does not descend into the nested array but evaluates the array at top-level. In addition to the false boolean value, $anyElementTrue evaluates as false the following: null, 0, and undefined values. The $anyElementTrue evaluates all other values as true, including non-zero numeric values and arrays. Example\nResult\n{ $anyElementTrue: [ [ true, false ] ] }\ntrue\n{ $anyElementTrue: [ [ [ false ] ] ] }\ntrue\n{ $anyElementTrue: [ [ null, false, 0 ] ] }\nfalse\n{ $anyElementTrue: [ [ ] ] }\nfalse \nEXAMPLE \nCreate an example collection named survey with the following documents: db.survey.insertMany([   { \"_id\" : 1, \"responses\" : [ true ] },   { \"_id\" : 2, \"responses\" : [ true, false ] },   { \"_id\" : 3, \"responses\" : [ ] },   { \"_id\" : 4, \"responses\" : [ 1, true, \"seven\" ] },   { \"_id\" : 5, \"responses\" : [ 0 ] },   { \"_id\" : 6, \"responses\" : [ [ ] ] },   { \"_id\" : 7, \"responses\" : [ [ 0 ] ] },   { \"_id\" : 8, \"responses\" : [ [ false ] ] },   { \"_id\" : 9, \"responses\" : [ null ] },   { \"_id\" : 10, \"responses\" : [ undefined ] }])  The following operation uses the $anyElementTrue operator to determine if the responses array contains any value that evaluates to true: db.survey.aggregate(   [     { $project: { responses: 1, isAnyTrue: { $anyElementTrue: [ \"$responses\" ] }, _id: 0 } }   ])  The operation returns the following results: { \"responses\" : [ true ], \"isAnyTrue\" : true }{ \"responses\" : [ true, false ], \"isAnyTrue\" : true }{ \"responses\" : [ ], \"isAnyTrue\" : false }{ \"responses\" : [ 1, true, \"seven\" ], \"isAnyTrue\" : true }{ \"responses\" : [ 0 ], \"isAnyTrue\" : false }{ \"responses\" : [ [ ] ], \"isAnyTrue\" : true }{ \"responses\" : [ [ 0 ] ], \"isAnyTrue\" : true }{ \"responses\" : [ [ false ] ], \"isAnyTrue\" : true }{ \"responses\" : [ null ], \"isAnyTrue\" : false }{ \"responses\" : [ undefined ], \"isAnyTrue\" : false } \n←  $and (aggregation)$arrayElemAt (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/": " Docs Home → MongoDB Manual \nQUERY AND PROJECTION OPERATORS \nOn this page    \n * Query Selectors\n   \n * Projection Operators\n * Miscellaneous Operators \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. \nQUERY SELECTORS  COMPARISON \nFor comparison of different BSON type values, see the specified BSON comparison order. Name\nDescription\n$eq\nMatches values that are equal to a specified value.\n$gt\nMatches values that are greater than a specified value.\n$gte\nMatches values that are greater than or equal to a specified value.\n$in\nMatches any of the values specified in an array.\n$lt\nMatches values that are less than a specified value.\n$lte\nMatches values that are less than or equal to a specified value.\n$ne\nMatches all values that are not equal to a specified value.\n$nin\nMatches none of the values specified in an array. \nLOGICAL \nName\nDescription\n$and\nJoins query clauses with a logical AND returns all documents that match the conditions of both clauses.\n$not\nInverts the effect of a query expression and returns documents that do not match the query expression.\n$nor\nJoins query clauses with a logical NOR returns all documents that fail to match both clauses.\n$or\nJoins query clauses with a logical OR returns all documents that match the conditions of either clause. \nELEMENT \nName\nDescription\n$exists\nMatches documents that have the specified field.\n$type\nSelects documents if a field is of the specified type. \nEVALUATION \nName\nDescription\n$expr\nAllows use of aggregation expressions within the query language.\n$jsonSchema\nValidate documents against the given JSON Schema.\n$mod\nPerforms a modulo operation on the value of a field and selects documents with a specified result.\n$regex\nSelects documents where values match a specified regular expression.\n$text\nPerforms text search.\n$where\nMatches documents that satisfy a JavaScript expression. \nGEOSPATIAL \nName\nDescription\n$geoIntersects\nSelects geometries that intersect with a GeoJSON geometry. The 2dsphere index supports $geoIntersects.\n$geoWithin\nSelects geometries within a bounding GeoJSON geometry. The 2dsphere and 2d indexes support $geoWithin.\n$near\nReturns geospatial objects in proximity to a point. Requires a geospatial index. The 2dsphere and 2d indexes support $near.\n$nearSphere\nReturns geospatial objects in proximity to a point on a sphere. Requires a geospatial index. The 2dsphere and 2d indexes support $nearSphere. \nARRAY \nName\nDescription\n$all\nMatches arrays that contain all elements specified in the query.\n$elemMatch\nSelects documents if element in the array field matches all the specified $elemMatch conditions.\n$size\nSelects documents if the array field is a specified size. \nBITWISE \nName\nDescription\n$bitsAllClear\nMatches numeric or binary values in which a set of bit positions all have a value of 0.\n$bitsAllSet\nMatches numeric or binary values in which a set of bit positions all have a value of 1.\n$bitsAnyClear\nMatches numeric or binary values in which any bit from a set of bit positions has a value of 0.\n$bitsAnySet\nMatches numeric or binary values in which any bit from a set of bit positions has a value of 1. \nPROJECTION OPERATORS \nName\nDescription\n$\nProjects the first element in an array that matches the query condition.\n$elemMatch\nProjects the first element in an array that matches the specified $elemMatch condition.\n$meta\nProjects the document's score assigned during $text operation.\n$slice\nLimits the number of elements projected from an array. Supports skip and limit slices. \nMISCELLANEOUS OPERATORS \nName\nDescription\n$comment\nAdds a comment to a query predicate.\n$rand\nGenerates a random float between 0 and 1. ←  OperatorsComparison Query Operators → On this page  * Query Selectors\n * Projection Operators\n * Miscellaneous Operators Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/floor/": " Docs Home → MongoDB Manual \n$FLOOR (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$floor \nReturns the largest integer less than or equal to the specified number. $floor has the following syntax: { $floor: <number> }  The <number> expression can be any valid expression as long as it resolves to a number. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the argument resolves to a value of null or refers to a field that is missing, $floor returns null. If the argument resolves to NaN, $floor returns NaN. Example\nResults\n{ $floor: 1 }\n1\n{ $floor: 7.80 }\n7\n{ $floor: -2.8 }\n-3 \nEXAMPLE \nCreate a collection named samples with the following documents: db.samples.insertMany(   [      { _id: 1, value: 9.25 },      { _id: 2, value: 8.73 },      { _id: 3, value: 4.32 },      { _id: 4, value: -5.34 }   ])  The following example returns both the original value and the floor value: db.samples.aggregate([   { $project: { value: 1, floorValue: { $floor: \"$value\" } } }])  The operation returns the following results: { \"_id\" : 1, \"value\" : 9.25, \"floorValue\" : 9 }{ \"_id\" : 2, \"value\" : 8.73, \"floorValue\" : 8 }{ \"_id\" : 3, \"value\" : 4.32, \"floorValue\" : 4 }{ \"_id\" : 4, \"value\" : -5.34, \"floorValue\" : -6 } \n←  $firstN (array operator)$function (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/function/": " Docs Home → MongoDB Manual \n$FUNCTION (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Considerations\n * Examples \nDEFINITION \n$function \nNew in version 4.4. Defines a custom aggregation function or expression in JavaScript. You can use the $function operator to define custom functions to implement behavior not supported by the MongoDB Query Language. See also $accumulator. \nIMPORTANT Executing JavaScript inside an aggregation expression may decrease performance. Only use the $function operator if the provided pipeline operators cannot fulfill your application's needs. \nSYNTAX \nThe $function operator has the following syntax: {  $function: {    body: <code>,    args: <array expression>,    lang: \"js\"  }} \nField\nType\nDescription\nbody\nString or Code The function definition. You can specify the function definition as either BSON type Code or String. See also lang. function(arg1, arg2, ...) { ... } or \"function(arg1, arg2, ...) { ... }\" args\nArray Arguments passed to the function body. If the body function does not take an argument, you can specify an empty array [ ]. The array elements can be any BSON type, including Code. See Example 2: Alternative to $where. lang\nString The language used in the body. You must specify lang: \"js\". \nCONSIDERATIONS  SCHEMA VALIDATION RESTRICTION \nYou cannot use $function as part of schema validation query expression. \nJAVASCRIPT ENABLEMENT \nTo use $function, you must have server-side scripting enabled (default). If you do not use $function (or $accumulator, $where, or mapReduce), disable server-side scripting:      * For a mongod instance, see security.javascriptEnabled configuration option or --noscripting command-line option.  * For a mongos instance, see security.javascriptEnabled configuration option or the --noscripting command-line option starting in MongoDB 4.4.\n   \n   In earlier versions, MongoDB does not allow JavaScript execution on mongos instances. See also ➤ Run MongoDB with Secure Configuration Options. \nALTERNATIVE TO $WHERE \nThe query operator $where can also be used to specify JavaScript expression. However:  * The $expr operator allows the use of aggregation expressions within the query language.  * Starting in MongoDB 4.4, the $function and $accumulator allows users to define custom aggregation expressions in JavaScript if the provided pipeline operators cannot fulfill your application's needs. Given the available aggregation operators:  * The use of $expr with aggregation operators that do not use JavaScript (i.e. non-$function and non-$accumulator operators) is faster than $where because it does not execute JavaScript and should be preferred if possible.  * However, if you must create custom expressions, $function is preferred over $where. \nUNSUPPORTED ARRAY AND STRING FUNCTIONS \nMongoDB 6.0 upgrades the internal JavaScript engine used for server-side JavaScript, $accumulator, $function, and $where expressions and from MozJS-60 to MozJS-91. Several deprecated, non-standard array and string functions that existed in MozJS-60 are removed in MozJS-91. For the complete list of removed array and string functions, see the 6.0 compatibility notes. \nEXAMPLES  EXAMPLE 1: USAGE EXAMPLE \nCreate a sample collection named players with the following documents: db.players.insertMany([   { _id: 1, name: \"Miss Cheevous\",  scores: [ 10, 5, 10 ] },   { _id: 2, name: \"Miss Ann Thrope\", scores: [ 10, 10, 10 ] },   { _id: 3, name: \"Mrs. Eppie Delta \", scores: [ 9, 8, 8 ] }])  The following aggregation operation uses $addFields to add new fields to each document:  * isFound whose value is determined by the custom $function expression that checks whether the MD5 hash of the name is equal to a specified hash.  * message whose value is determined by the custom $function expression that format a string message using a template.\n\n  The operation returns the following documents: { \"_id\" : 1, \"name\" : \"Miss Cheevous\", \"scores\" : [ 10, 5, 10 ], \"isFound\" : false, \"message\" : \"Hello Miss Cheevous.  Your total score is 25.\" }{ \"_id\" : 2, \"name\" : \"Miss Ann Thrope\", \"scores\" : [ 10, 10, 10 ], \"isFound\" : true, \"message\" : \"Hello Miss Ann Thrope.  Your total score is 30.\" }{ \"_id\" : 3, \"name\" : \"Mrs. Eppie Delta \", \"scores\" : [ 9, 8, 8 ], \"isFound\" : false, \"message\" : \"Hello Mrs. Eppie Delta .  Your total score is 25.\" } \nEXAMPLE 2: ALTERNATIVE TO $WHERE  NOTE \nAGGREGATION ALTERNATIVES PREFERRED OVER $WHERE The $expr operator allows the use of aggregation expressions within the query language. And, starting in MongoDB 4.4, the $function and $accumulator allows users to define custom aggregation expressions in JavaScript if the provided pipeline operators cannot fulfill your application's needs. Given the available aggregation operators:  * The use of $expr with aggregation operators that do not use JavaScript (i.e. non-$function and non-$accumulator operators) is faster than $where because it does not execute JavaScript and should be preferred if possible.  * However, if you must create custom expressions, $function is preferred over $where. As an alternative to a query that uses the $where operator, you can use $expr and $function. For example, consider the following $where example. db.players.find( { $where: function() {   return (hex_md5(this.name) == \"15b0a220baa16331e8d80e15367677ad\")} } );  The db.collection.find() operation returns the following document: { \"_id\" : 2, \"name\" : \"Miss Ann Thrope\", \"scores\" : [ 10, 10, 10 ] } The example can be expressed using $expr and $function: db.players.find( {$expr: { $function: {      body: function(name) { return hex_md5(name) == \"15b0a220baa16331e8d80e15367677ad\"; },      args: [ \"$name\" ],      lang: \"js\"} } } ) \n←  $floor (aggregation)$getField (aggregation) → On this page  * Definition\n * Syntax\n * Considerations\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/second/": " Docs Home → MongoDB Manual \n$SECOND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$second \nReturns the second portion of a date as a number between 0 and 59, but can be 60 to account for leap seconds. The $second expression has the following operator expression syntax: { $second: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $second: new Date(\"2012-11-06T00:14:20\") } 20 { $second: { date: new Date(\"Jan 7, 2003\") } } 0 { $second: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 0 { $second: ISODate(\"1998-11-07T00:00:42Z\") } 42 { $second: {    date: ISODate(\"1998-11-07T00:00:09Z\"),    timezone: \"+0530\"} } 9 { $second: \"March 28, 1976\" } error { $second: Date(\"2016-01-01\") } error { $second: \"2009-04-09\" } error \nNOTE \n$SECOND CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $second and other date expressions to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $sampleRate (aggregation)$setDifference (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/command/serverStatus/": " Docs Home → MongoDB Manual \nSERVERSTATUS \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Initialization\n * Include mirroredReads\n * Include latchAnalysis\n * Change tcmalloc Verbosity\n * Output\n * asserts\n * bucketCatalog\n * connections\n * defaultRWConcern\n * electionMetrics\n * extra_info\n * flowControl\n * freeMonitoring\n * globalLock\n * hedgingMetrics\n * indexBulkBuilder\n * Instance Information\n * latchAnalysis\n * locks\n * logicalSessionRecordCache\n * mem\n * metrics\n * mirroredReads\n * network\n * opLatencies\n * opReadConcernCounters\n * opWriteConcernCounters\n * opcounters\n * opcountersRepl\n * oplogTruncation\n * readConcernCounters\n * repl\n * security\n * sharding\n * shardingStatistics\n * shardedIndexConsistency\n * storageEngine\n * transactions\n * transportSecurity\n * watchdog\n * wiredTiger\n * writeBacksQueued \nDEFINITION \nserverStatus \nThe serverStatus command returns a document that provides an overview of the database's state. Monitoring applications can run this command at a regular interval to collect statistics about the instance. \nSYNTAX \nThe command has the following syntax: db.runCommand(   {     serverStatus: 1   })  The value (i.e. 1 above) does not affect the operation of the command. The db.serverStatus() command returns a large amount of data. To return a specific object or field from the output append the object or field name to the command. For example: db.runCommand({ serverStatus: 1}).metricsdb.runCommand({ serverStatus: 1}).metrics.commandsdb.runCommand({ serverStatus: 1}).metrics.commands.update  mongosh provides the db.serverStatus() wrapper for the serverStatus command. \nTIP \nSEE ALSO: Much of the output of serverStatus is also displayed dynamically by mongostat. See the mongostat command for more information. \nBEHAVIOR \nBy default, serverStatus excludes in its output:      * some content in the repl document.  * mirroredReads document. (Available starting in version 4.4) To include fields that are excluded by default, specify the top-level field and set it to 1 in the command. To exclude fields that are included by default, specify the field and set to 0. You can specify either top-level or embedded fields. For example, the following operation excludes the repl, metrics and locks information in the output. db.runCommand( { serverStatus: 1, repl: 0, metrics: 0, locks: 0 } )  For example, the following operation excludes the embedded histogram field in the output. db.runCommand( { serverStatus: 1, metrics: { query: { multiPlanner: { histograms: false } } } } )  The following example includes all repl information in the output: db.runCommand( { serverStatus: 1,  repl: 1 } )  \nINITIALIZATION \nThe statistics reported by serverStatus are reset when the mongod server is restarted. This command will always return a value, even on a fresh database. The related command db.serverStatus() does not always return a value unless a counter has started to increment for a particular metric. After you run an update query, db.serverStatus() and db.runCommand({ serverStatus: 1}) both return the same values. {   \"arrayFilters\" : NumberLong(0),   \"failed\" : NumberLong(0),   \"pipeline\" : NumberLong(0),   \"total\" : NumberLong(1)} \nINCLUDE MIRROREDREADS \nBy default, the mirroredReads information (available starting in version 4.4) is not included in the output. To return mirroredReads information, you must explicitly specify the inclusion: db.runCommand( { serverStatus: 1, mirroredReads: 1 } )  \nINCLUDE LATCHANALYSIS \nBy default, the latchAnalysis information (available starting in version 4.4) is not included in the output. To return latchAnalysis information, you must explicitly specify the inclusion: db.runCommand( { serverStatus: 1, latchAnalysis: 1 } )  \nCHANGE TCMALLOC VERBOSITY\n\n db.runCommand( { serverStatus: 1, tcmalloc: <int> } )  The command has the following behavior:  * By default, tcmalloc is set to 1.  * Passing a larger integer value increases the verbosity.  * Passing 0 or false omits the tcmalloc section from the output.  * Passing any other input results in the default setting. For more information about how MongoDB allocates memory, see TCMalloc. \nOUTPUT  NOTE The output fields vary depending on the version of MongoDB, underlying operating system platform, the storage engine, and the kind of node, including mongos, mongod or replica set member. For the serverStatus output specific to the version of your MongoDB, refer to the appropriate version of the MongoDB Manual. \nASSERTS \n\"asserts\" : {   \"regular\" : <num>,   \"warning\" : <num>,   \"msg\" : <num>,   \"user\" : <num>,   \"rollovers\" : <num>}, \nasserts \nA document that reports on the number of assertions raised since the MongoDB process started. While assert errors are typically uncommon, if there are non-zero values for the asserts, you should examine the log file for more information. In many cases, these errors are trivial, but are worth investigating. asserts.regular \nThe number of regular assertions raised since the MongoDB process started. Examine the log file for more information about these messages. asserts.warning \nThis field always returns zero 0. asserts.msg \nThe number of message assertions raised since the MongoDB process started. Examine the log file for more information about these messages. asserts.user \nThe number of \"user asserts\" that have occurred since the last time the MongoDB process started. These are errors that user may generate, such as out of disk space or duplicate key. You can prevent these assertions by fixing a problem with your application or deployment. Examine the MongoDB log for more information. asserts.rollovers \nThe number of times that the assert counters have rolled over since the last time the MongoDB process started. The counters will roll over to zero after 2 30 assertions. Use this value to provide context to the other values in the asserts data structure. \nBUCKETCATALOG \n\"bucketCatalog\" : {    \"numBuckets\" : <num>,    \"numOpenBuckets\" : <num>,    \"numIdleBuckets\" : <num>,    \"memoryUsage\" : <num> }  New in version 5.0. A document that reports metrics related to the internal storage of time series collections. The bucketCatalog returns the following metrics: Metric\nDescription\nnumBuckets\nThe number of buckets that store time series data internally.\nnumOpenBuckets\nThe number of active, uncommitted writes to buckets.\nnumIdleBuckets\nThe number of buckets that are not full and can store incoming time series data.\nmemoryUsage\nThe number of bytes used by internal bucketing data structures. New in version 5.0. \nCONNECTIONS \n\"connections\" : {   \"current\" : <num>,   \"available\" : <num>,   \"totalCreated\" : <num>,   \"active\" : <num>,   \"threaded\" : <num>,   \"exhaustIsMaster\" : <num>,   \"exhaustHello\" : <num>,   \"awaitingTopologyChanges\" : <num>,   \"loadBalanced\" : <num>}, \nconnections \nA document that reports on the status of the connections. Use these values to assess the current load and capacity requirements of the server. connections.current \nThe number of incoming connections from clients to the database server. This number includes the current shell session. Consider the value of connections.available to add more context to this datum. The value will include all incoming connections including any shell connections or connections from other servers, such as replica set members or mongos instances. connections.available \nThe number of unused incoming connections available. Consider this value in combination with the value of connections.current to understand the connection load on the database, and the UNIX ulimit Settings document for more information about system thresholds on available connections. connections.totalCreated \nCount of all incoming connections created to the server. This number includes connections that have since closed. connections.active \nThe number of active client connections to the server. Active client connections refers to client connections that currently have operations in progress. connections.threaded \nThe number of incoming connections from clients that are assigned to threads that service client requests.\n\n connections.exhaustIsMaster \nThe number of connections whose last request was an isMaster request with exhaustAllowed. \nNOTE If you are running MongoDB 5.0 or later, do not use the isMaster command. Instead, use hello. New in version 4.4. connections.exhaustHello \nThe number of connections whose last request was a hello request with exhaustAllowed. New in version 5.0: (and 4.4.2) connections.awaitingTopologyChanges \nThe number of clients currently waiting in a hello or isMaster request for a topology change. \nNOTE If you are running MongoDB 5.0 or later, do not use the isMaster command. Instead, use hello. New in version 4.4. connections.loadBalanced \nNew in version 5.3. The current number of incoming connections received through the load balancer. \nDEFAULTRWCONCERN \nAvailable starting in 4.4 The defaultRWConcern section provides information on the local copy of the global default read or write concern settings. The data may be stale or out of date. See getDefaultRWConcern for more information. \"defaultRWConcern\" : {  \"defaultReadConcern\" : {    \"level\" : <string>  },  \"defaultWriteConcern\" : {    \"w\" : <string> | <int>,    \"wtimeout\" : <int>,    \"j\" : <bool>  },  \"defaultWriteConcernSource\": <string>,  \"defaultReadConcernSource\": <string>,  \"updateOpTime\" : Timestamp,  \"updateWallClockTime\" : Date,  \"localUpdateWallClockTime\" : Date} \ndefaultRWConcern \nThe last known global default read or write concern settings. New in version 4.4. defaultRWConcern.defaultReadConcern \nThe last known global default read concern setting. If serverStatus does not return this field, the global default read concern has either not been set or has not yet propagated to the instance. New in version 4.4. defaultRWConcern.defaultReadConcern.level \nThe last known global default read concern level setting. If serverStatus does not return this field, the global default for this setting has either not been set or has not yet propagated to the instance. New in version 4.4. defaultRWConcern.defaultWriteConcern \nThe last known global default write concern setting. If serverStatus does not return this field, the global default write concern has either not been set or has not yet propagated to the instance. New in version 4.4. defaultRWConcern.defaultWriteConcern.w \nThe last known global default w setting. If serverStatus does not return this field, the global default for this setting has either not been set or has not yet propagated to the instance. New in version 4.4. defaultRWConcern.defaultWriteConcern.wtimeout \nThe last known global default wtimeout setting. If serverStatus does not return this field, the global default for this setting has either not been set or has not yet propagated to the instance. New in version 4.4. defaultRWConcern.defaultWriteConcernSource \nThe source of the default write concern. By default, the value is \"implicit\". Once you set the default write concern with setDefaultRWConcern, the value becomes \"global\". New in version 5.0. defaultRWConcern.defaultReadConcernSource \nThe source of the default read concern. By default, the value is \"implicit\". Once you set the default read concern with setDefaultRWConcern, the value becomes \"global\". New in version 5.0. defaultRWConcern.updateOpTime \nThe timestamp when the instance last updated its copy of any global read or write concern settings. If the defaultRWConcern.defaultReadConcern and defaultRWConcern.defaultWriteConcern fields are absent, this field indicates the timestamp when the defaults were last unset. New in version 4.4. defaultRWConcern.updateWallClockTime \nThe wall clock time when the instance last updated its copy of any global read or write concern settings. If the defaultRWConcern.defaultReadConcern and defaultRWConcern.defaultWriteConcern fields are absent, this field indicates the time when the defaults were last unset. New in version 4.4. defaultRWConcern.localUpdateWallClockTime\n\n New in version 4.4. \nELECTIONMETRICS \nNew in version 4.2.1. The electionMetrics section provides information on elections called by this mongod instance in a bid to become the primary: \"electionMetrics\" : {   \"stepUpCmd\" : {      \"called\" : NumberLong(<num>),      \"successful\" : NumberLong(<num>)   },   \"priorityTakeover\" : {      \"called\" : NumberLong(<num>),      \"successful\" : NumberLong(<num>)   },   \"catchUpTakeover\" : {      \"called\" : NumberLong(<num>),      \"successful\" : NumberLong(<num>)   },   \"electionTimeout\" : {      \"called\" : NumberLong(<num>),      \"successful\" : NumberLong(<num>)   },   \"freezeTimeout\" : {      \"called\" : NumberLong(<num>),      \"successful\" : NumberLong(<num>)   },   \"numStepDownsCausedByHigherTerm\" : NumberLong(<num>),   \"numCatchUps\" : NumberLong(<num>),   \"numCatchUpsSucceeded\" : NumberLong(<num>),   \"numCatchUpsAlreadyCaughtUp\" : NumberLong(<num>),   \"numCatchUpsSkipped\" : NumberLong(<num>),   \"numCatchUpsTimedOut\" : NumberLong(<num>),   \"numCatchUpsFailedWithError\" : NumberLong(<num>),   \"numCatchUpsFailedWithNewTerm\" : NumberLong(<num>),   \"numCatchUpsFailedWithReplSetAbortPrimaryCatchUpCmd\" : NumberLong(<num>),   \"averageCatchUpOps\" : <double>} \nelectionMetrics.stepUpCmd \nMetrics on elections that were called by the mongod instance as part of an election handoff when the primary stepped down. The stepUpCmd includes both the number of elections called and the number of elections that succeeded. New in version 4.2.1. electionMetrics.priorityTakeover \nMetrics on elections that were called by the mongod instance because its priority is higher than the primary's. The electionMetrics.priorityTakeover includes both the number of elections called and the number of elections that succeeded. New in version 4.2.1. electionMetrics.catchUpTakeover \nMetrics on elections called by the mongod instance because it is more current than the primary. The catchUpTakeover includes both the number of elections called and the number of elections that succeeded. \nTIP \nSEE ALSO: settings.catchUpTakeoverDelayMillis New in version 4.2.1. electionMetrics.electionTimeout \nMetrics on elections called by the mongod instance because it has not been able to reach the primary within settings.electionTimeoutMillis. The electionTimeout includes both the number of elections called and the number of elections that succeeded. \nTIP \nSEE ALSO: settings.electionTimeoutMillis New in version 4.2.1. electionMetrics.freezeTimeout \nMetrics on elections called by the mongod instance after its freeze period (during which the member cannot seek an election) has expired. The electionMetrics.freezeTimeout includes both the number of elections called and the number of elections that succeeded. ..versionadded:: 4.2.1 electionMetrics.numStepDownsCausedByHigherTerm \nNumber of times the mongod instance stepped down because it saw a higher term (specifically, other member(s) participated in additional elections). New in version 4.2.1. electionMetrics.numCatchUps \nNumber of elections where the mongod instance as the newly-elected primary had to catch up to the highest known oplog entry. New in version 4.2.1. electionMetrics.numCatchUpsSucceeded \nNumber of times the mongod instance as the newly-elected primary successfully caught up to the highest known oplog entry. New in version 4.2.1. electionMetrics.numCatchUpsAlreadyCaughtUp\n\n New in version 4.2.1. electionMetrics.numCatchUpsSkipped \nNumber of times the mongod instance as the newly-elected primary skipped the catchup process. New in version 4.2.1. electionMetrics.numCatchUpsTimedOut \nNumber of times the mongod instance as the newly-elected primary concluded its catchup process because of the settings.catchUpTimeoutMillis limit. New in version 4.2.1. electionMetrics.numCatchUpsFailedWithError \nNumber of times the newly-elected primary's catchup process failed with an error. New in version 4.2.1. electionMetrics.numCatchUpsFailedWithNewTerm \nNumber of times the newly-elected primary's catchup process concluded because another member(s) had a higher term (specifically, other member(s) participated in additional elections). New in version 4.2.1. electionMetrics.numCatchUpsFailedWithReplSetAbortPrimaryCatchUpCmd \nNumber of times the newly-elected primary's catchup process concluded because the mongod received the replSetAbortPrimaryCatchUp command. New in version 4.2.1. electionMetrics.averageCatchUpOps \nAverage number of operations applied during the newly-elected primary's catchup processes. New in version 4.2.1. \nEXTRA_INFO \n\"extra_info\" : {   \"note\" : \"fields vary by platform.\",   \"page_faults\" : <num>}, \nextra_info \nA document that provides additional information about the underlying system. extra_info.note \nA string with the text \"fields vary by platform.\" extra_info.page_faults \nThe total number of page faults. The extra_info.page_faults counter may increase dramatically during moments of poor performance and may correlate with limited memory environments and larger data sets. Limited and sporadic page faults do not necessarily indicate an issue. Windows differentiates \"hard\" page faults involving disk I/O from \"soft\" page faults that only require moving pages in memory. MongoDB counts both hard and soft page faults in this statistic. \nFLOWCONTROL \nNew in version 4.2. \"flowControl\" : {   \"enabled\" : <boolean>,   \"targetRateLimit\" : <int>,   \"timeAcquiringMicros\" : NumberLong(<num>),   \"locksPerKiloOp\" : <double>,  // Available in 4.4+. In 4.2, returned locksPerOp instead.   \"sustainerRate\" : <int>,   \"isLagged\" : <boolean>,   \"isLaggedCount\" : <int>,   \"isLaggedTimeMicros\" : NumberLong(<num>)}, \nflowControl \nA document that returns statistics on the Flow Control. With flow control enabled, as the majority commit point lag grows close to the flowControlTargetLagSeconds, writes on the primary must obtain tickets before taking locks. As such, the metrics returned are meaningful when run on the primary. New in version 4.2. flowControl.enabled \nA boolean that indicates whether Flow Control is enabled (true) or disabled (false). See also enableFlowControl. New in version 4.2. flowControl.targetRateLimit \nWhen run on the primary, the maximum number of tickets that can be acquired per second. When run on a secondary, the returned number is a placeholder. New in version 4.2. flowControl.timeAcquiringMicros \nWhen run on the primary, the total time write operations have waited to acquire a ticket. When run on a secondary, the returned number is a placeholder. New in version 4.2. flowControl.locksPerKiloOp  NOTE \nSTARTING IN MONGODB 4.4 locksPerKiloOp replaces locksPerOp field. locksPerOp field is available only on version 4.2. When run on the primary, an approximation of the number of locks taken per 1000 operations. When run on a secondary, the returned number is a placeholder. New in version 4.4. flowControl.locksPerOp  NOTE \nAVAILABLE ON MONGODB 4.2 ONLY MongoDB 4.4 replaces locksPerOp with flowControl.locksPerKiloOp. When run on the primary, an approximation of the number of locks taken per operation. When run on a secondary, the returned number is a placeholder. New in version 4.2. flowControl.sustainerRate\n\n When run on a secondary, the returned number is a placeholder. New in version 4.2. flowControl.isLagged \nWhen run on the primary, a boolean that indicates whether flow control has engaged. Flow control engages when the majority committed lag is greater than some percentage of the configured flowControlTargetLagSeconds. Replication lag can occur without engaging flow control. An unresponsive secondary might lag without the replica set receiving sufficient load to engage flow control, leaving the flowControl.isLagged value at false. For additional information, see Flow Control. New in version 4.2. flowControl.isLaggedCount \nWhen run on a primary, the number of times flow control has engaged since the last restart. Flow control engages when the majority committed lag is greater than some percentage of the flowControlTargetLagSeconds. When run on a secondary, the returned number is a placeholder. New in version 4.2. flowControl.isLaggedTimeMicros \nWhen run on the primary, the amount of time flow control has spent being engaged since the last restart. Flow control engages when the majority committed lag is greater than some percentage of the flowControlTargetLagSeconds. When run on a secondary, the returned number is a placeholder. New in version 4.2. \nFREEMONITORING \n\"freeMonitoring\" : {   \"state\" : <string>,   \"retryIntervalSecs\" : NumberLong(<num>),   \"lastRunTime\" : <string>,   \"registerErrors\" : NumberLong(<num>),   \"metricsErrors\" : NumberLong(<num>)}, \nfreeMonitoring \nA document that reports on the free Cloud monitoring. freeMonitoring.state \nThe enablement state of free monitoring. The values can be one of the following:  * \"enabled\"  * \"disabled\"  * \"pending\" if the enable free monitoring encountered a registeration error. freeMonitoring.retryIntervalSecs \nThe frequency, in seconds, at which data is uploaded. freeMonitoring.lastRunTime \nThe date and time of the last run of the metrics upload. freeMonitoring.registerErrors \nThe number of registration errors, incremented on unexpected HTTP status or network errors. freeMonitoring.metricsErrors \nThe number of errors encountered when uploading metrics. \nGLOBALLOCK \n\"globalLock\" : {   \"totalTime\" : NumberLong(<num>),   \"currentQueue\" : {      \"total\" : <num>,      \"readers\" : <num>,      \"writers\" : <num>   },   \"activeClients\" : {      \"total\" : <num>,      \"readers\" : <num>,      \"writers\" : <num>   }}, \nglobalLock \nA document that reports on the database's lock state. Generally, the locks document provides more detailed data on lock uses. globalLock.totalTime \nThe time, in microseconds, since the database last started and created the globalLock. This is approximately equivalent to the total server uptime. globalLock.currentQueue \nA document that provides information concerning the number of operations queued because of a lock. globalLock.currentQueue.total \nThe total number of operations queued waiting for the lock (i.e., the sum of globalLock.currentQueue.readers and globalLock.currentQueue.writers). A consistently small queue, particularly of shorter operations, should cause no concern. The globalLock.activeClients readers and writers information provides context for this data. globalLock.currentQueue.readers \nThe number of operations that are currently queued and waiting for the read lock. A consistently small read queue, particularly of shorter operations, should cause no concern. globalLock.currentQueue.writers \nThe number of operations that are currently queued and waiting for the write lock. A consistently small write queue, particularly of shorter operations, is no cause for concern. globalLock.activeClients \nA document that provides information about the number of connected clients and the read and write operations performed by these clients. Use this data to provide context for the globalLock.currentQueue data. globalLock.activeClients.total \nThe total number of internal client connections to the database including system threads as well as queued readers and writers. This metric will be higher than the total of activeClients.readers and activeClients.writers due to the inclusion of system threads. globalLock.activeClients.readers \nThe number of the active client connections performing read operations.\n\n \nThe number of active client connections performing write operations. \nHEDGINGMETRICS \nNew in version 4.4: For mongos instances only. \"hedgingMetrics\" : {   \"numTotalOperations\" : <num>,   \"numTotalHedgedOperations\" : <num>,   \"numAdvantageouslyHedgedOperations\" : <num>}, \nhedgingMetrics \nProvides metrics on hedged reads for the mongos instance. New in version 4.4: For mongos instances only. hedgingMetrics.numTotalOperations \nThe total number of the operations issued with the hedged read option enabled to this mongos instance. New in version 4.4: For mongos instances only. hedgingMetrics.numTotalHedgedOperations \nThe number of all operations for which the mongos instance hedged the read operation; i.e. sent the operation to an additional member of each queried shard. New in version 4.4: For mongos instances only. hedgingMetrics.numAdvantageouslyHedgedOperations \nThe total number of the operation where the additional read sent to hedge the read operation fulfilled the client request. New in version 4.4: For mongos instances only. \nINDEXBULKBUILDER \n\"indexBulkBuilder\": {   \"count\": <long>,   \"resumed\": <long>,   \"filesOpenedForExternalSort\": <long>,   \"filesClosedForExternalSort\": <long>,   \"spilledRanges\": <long>,   \"bytesSpilled\": <long>} \nindexBulkBuilder \nProvides metrics for index bulk builder operations. Use these metrics to diagnose index build issues with createIndexes, collection cloning during initial sync, index builds that resume after startup, and statistics on disk usage by the external sorter. indexBuildBuilder.bytesSpilled \nNew in version 6.0.4. The number of bytes written to disk by the external sorter. indexBuilder.bytesSpilledUncompressed \nNew in version 6.0.4. The number of bytes to be written to disk by the external sorter before compression. indexBulkBuilder.count \nThe number of instances of the bulk builder created. indexBulkBuilder.filesClosedForExternalSort \nThe number of times the external sorter closed a file handle to spill data to disk. Combine this value with filesOpenedForExternalSort to determine the number of open file handles in use by the external sorter. indexBulkBuilder.filesOpenedForExternalSort \nThe number of times the external sorter opened a file handle to spill data to disk. Combine this value with filesClosedForExternalSort to determine the number of open file handles in use by the external sorter. indexBulkBuilder.resumed \nThe number of times the bulk builder was created for a resumable index build. indexBulkBuilder.spilledRanges \nNew in version 6.0.4. The number of times the external sorter spilled to disk. \nINSTANCE INFORMATION \n\"host\" : <string>,\"advisoryHostFQDNs\" : <array>,\"version\" : <string>,\"process\" : <\"mongod\"|\"mongos\">,\"pid\" : NumberLong(<num>),\"uptime\" : <num>,\"uptimeMillis\" : NumberLong(<num>),\"uptimeEstimate\" : NumberLong(<num>),\"localTime\" : ISODate(\"\"), \nhost \nThe system's hostname. In Unix/Linux systems, this should be the same as the output of the hostname command. advisoryHostFQDNs \nAn array of the system's fully qualified domain names (FQDNs). version \nThe MongoDB version of the current MongoDB process. process \nThe current MongoDB process. Possible values are: mongos or mongod. pid \nThe process ID number. uptime \nThe number of seconds that the current MongoDB process has been active. uptimeMillis \nThe number of milliseconds that the current MongoDB process has been active. uptimeEstimate \nThe uptime in seconds as calculated from MongoDB's internal course-grained time keeping system. localTime \nThe ISODate representing the current time, according to the server, in UTC. \nLATCHANALYSIS \nNew in version 4.4.\n\n \nlatchAnalysis \nA document that reports on metrics related to internal locking primitives (also known as latches). To return latchAnalysis information, you must explicitly specify the inclusion: db.runCommand( { serverStatus: 1, latchAnalysis: 1 } )  The latchAnalysis returns for each latch the <latch name> and the following associated metrics: Metric\nDescription\ncreated\nNumber of times the latch was created.\ndestroyed\nNumber of times the latch was destroyed.\nacquired\nNumber of times the latch was acquired.\nreleased\nNumber of times the latch was released.\ncontended\nNumber of times the latch had contention.\nhierarchicalAcquisitionLevelViolations Only returned if violations exist Contains the following fields:  * onAcquire\n   \n   * Number of times the latch hierarchy (i.e. the latch level ordering) has been violated during latch acquisition.  * onRelease\n   \n   * Number of times the latch hierarchy (i.e. the latch level ordering) has been violated during latch release. New in version 4.4. \nLOCKS \n\"locks\" : {   <type> : {         \"acquireCount\" : {            <mode> : NumberLong(<num>),            ...         },         \"acquireWaitCount\" : {            <mode> : NumberLong(<num>),            ...         },         \"timeAcquiringMicros\" : {            <mode> : NumberLong(<num>),            ...         },         \"deadlockCount\" : {            <mode> : NumberLong(<num>),            ...         }   },   ... \nlocks \nA document that reports for each lock <type>, data on lock <modes>. The possible lock <types> are: Lock Type\nDescription\nParallelBatchWriterMode Represents a lock for parallel batch writer mode. In earlier versions, PBWM information was reported as part of the Global lock information. New in version 4.2. ReplicationStateTransition Represents lock taken for replica set member state transitions. New in version 4.2. Global\nRepresents global lock.\nDatabase\nRepresents database lock.\nCollection\nRepresents collection lock.\nMutex\nRepresents mutex.\nMetadata\nRepresents metadata lock.\noplog\nRepresents lock on the oplog. The possible <modes> are: Lock Mode\nDescription\nR\nRepresents Shared (S) lock.\nW\nRepresents Exclusive (X) lock.\nr\nRepresents Intent Shared (IS) lock.\nw\nRepresents Intent Exclusive (IX) lock. All values are of the NumberLong() type. locks.<type>.acquireCount \nNumber of times the lock was acquired in the specified mode. locks.<type>.acquireWaitCount \nNumber of times the locks.<type>.acquireCount lock acquisitions encountered waits because the locks were held in a conflicting mode. locks.<type>.timeAcquiringMicros \nCumulative wait time in microseconds for the lock acquisitions. locks.<type>.timeAcquiringMicros divided by locks.<type>.acquireWaitCount gives an approximate average wait time for the particular lock mode. locks.<type>.deadlockCount \nNumber of times the lock acquisitions encountered deadlocks. \nLOGICALSESSIONRECORDCACHE\n\n \nlogicalSessionRecordCache \nProvides metrics around the caching of server sessions. logicalSessionRecordCache.activeSessionsCount \nThe number of all active local sessions cached in memory by the mongod or mongos instance since the last refresh period. \nTIP \nSEE ALSO:  * $listLocalSessions  * logicalSessionRefreshMillis logicalSessionRecordCache.sessionsCollectionJobCount \nThe number that tracks the number of times the refresh process has run on the config.system.sessions collection. \nTIP \nSEE ALSO: logicalSessionRefreshMillis logicalSessionRecordCache.lastSessionsCollectionJobDurationMillis \nThe length in milliseconds of the last refresh. logicalSessionRecordCache.lastSessionsCollectionJobTimestamp \nThe time at which the last refresh occurred. logicalSessionRecordCache.lastSessionsCollectionJobEntriesRefreshed \nThe number of sessions that were refreshed during the last refresh. logicalSessionRecordCache.lastSessionsCollectionJobEntriesEnded \nThe number of sessions that ended during the last refresh. logicalSessionRecordCache.lastSessionsCollectionJobCursorsClosed \nThe number of cursors that were closed during the last config.system.sessions collection refresh. logicalSessionRecordCache.transactionReaperJobCount \nThe number that tracks the number of times the transaction record cleanup process has run on the config.transactions collection. logicalSessionRecordCache.lastTransactionReaperJobDurationMillis \nThe length (in milliseconds) of the last transaction record cleanup. logicalSessionRecordCache.lastTransactionReaperJobTimestamp \nThe time of the last transaction record cleanup. logicalSessionRecordCache.lastTransactionReaperJobEntriesCleanedUp \nThe number of entries in the config.transactions collection that were deleted during the last transaction record cleanup. logicalSessionRecordCache.sessionCatalogSize  * For a mongod instance,The size of its in-memory cache of the config.transactions entries. This corresponds to retryable writes or transactions whose sessions have not expired within the localLogicalSessionTimeoutMinutes.\n * For a mongos instance,The number of the in-memory cache of its sessions that have had transactions within the most recent localLogicalSessionTimeoutMinutes interval. New in version 4.2. \nMEM \n\"mem\" : {   \"bits\" : <int>,   \"resident\" : <int>,   \"virtual\" : <int>,   \"supported\" : <boolean>}, \nmem \nA document that reports on the system architecture of the mongod and current memory use. mem.bits \nA number, either 64 or 32, that indicates whether the MongoDB instance is compiled for 64-bit or 32-bit architecture. mem.resident \nThe value of mem.resident is roughly equivalent to the amount of RAM, in mebibyte (MiB), currently used by the database process. During normal use, this value tends to grow. In dedicated database servers, this number tends to approach the total amount of system memory. mem.virtual \nmem.virtual displays the quantity, in mebibyte (MiB), of virtual memory used by the mongod process. mem.supported \nA boolean that indicates whether the underlying system supports extended memory information. If this value is false and the system does not support extended memory information, then other mem values may not be accessible to the database server. mem.note \nThe field mem.note appears if mem.supported is false. The mem.note field contains the text: \"not all mem info support on this platform\". \nMETRICS\n\n \nmetrics \nA document that returns various statistics that reflect the current use and state of a running mongod instance. metrics.aggStageCounters \nA document that reports on the use of aggregation pipeline stages. The fields in metrics.aggStageCounters are the names of aggregation pipeline stages. For each pipeline stage, serverStatus reports the number of times that stage has been executed. New in version 4.4 (4.2.6 and 4.0.19). Updated in version 5.2 (and 5.0.6). metrics.apiVersions \nA document with client applications and the version of Stable API which they are configured with. Consider the following when viewing metrics.apiVersions:  * The only possible returned values for each appname are default or 1.  * API version metrics are retained for 24 hours. If no commands are issued with a specific API version from an application in the past 24 hours, that appname and API version will be removed from the metrics. This also applies to the default API version metric.  * Set the appname when connecting to a MongoDB instance by specifying the appname in the connection URI. ?appName=ZZZ sets the appname to ZZZZ.  * Drivers accessing the Stable API can set a default appname.  * If no appname is configured, a default value will be automatically populated based on the product. For example, for a MongoDB Compass connection with no appname in the URI, the metric returns: 'MongoDB Compass': [ 'default' ]. New in version 5.0. metrics.operatorCounters \nA document that reports on the use of aggregation pipeline operators and expressions. metrics.operatorCounters.expressions \nA document with a number that indicates how often Expression Operators ran. To get metrics for a specific operator, such as the greater-than operator ($gt), append the operator to the command: db.runCommand( { serverStatus: 1 } ).metrics.operatorCounters.expressions.$gt  New in version 5.0. metrics.operatorCounters.match \nA document with a number that indicates how often match expressions ran. Match expression operators also increment as part of an aggregation pipeline $match stage. If the $match stage uses the $expr operator, the counter for $expr increments, but the component counters do not increment. Consider the following query: db.matchCount.aggregate(   [      { $match:         {  $expr: { $gt: [ \"$_id\", 0 ] } }      }   ]) The counter for $expr increments when the query runs. The counter for $gt does not. New in version 5.1. metrics.commands \nA document that reports on the use of database commands. The fields in metrics.commands are the names of database commands. For each command, the serverStatus reports the total number of executions and the number of failed executions. metrics.commands includes replSetStepDownWithForce (i.e. the replSetStepDown command with force: true) as well as the overall replSetStepDown. In earlier versions, the command reported only overall replSetStepDown metrics. metrics.commands.<command>.failed \nThe number of times <command> failed on this mongod. metrics.commands.<command>.total \nThe number of times <command> executed on this mongod. metrics.commands.update.pipeline \nThe number of times an aggregation pipeline was used to update documents on this mongod. Subtract this value from the total number of updates to get the number of updates made with document syntax. The pipeline counter is only available for update and findAndModify operations. metrics.commands.findAndModify.pipeline \nThe number of times findAndModify() was used in an aggregation pipeline to update documents on this mongod. The pipeline counter is only available for update and findAndModify operations. metrics.commands.update.arrayFilters \nThe number of times an arrayFilter was used to update documents on this mongod. The arrayFilters counter is only available for update and findAndModify operations. metrics.commands.findAndModify.arrayFilters \nThe number of times an arrayFilter was used with findAndModify() to update documents on this mongod. The arrayFilters counter is only available for update and findAndModify operations. metrics.document \nA document that reflects document access and modification patterns. Compare these values to the data in the opcounters document, which track total number of operations. metrics.document.deleted \nThe total number of documents deleted.\n\n \nThe total number of documents inserted. metrics.document.returned \nThe total number of documents returned by queries. metrics.document.updated \nThe total number of documents updated. metrics.dotsAndDollarsFields \nA document with a number that indicates how often insert or update operations ran using a dollar ($) prefixed name. The value does not report the exact number of operations. When an upsert operation creates a new document, it is considered to be an insert rather than an update. New in version 5.0. metrics.executor \nA document that reports on various statistics for the replication executor. metrics.getLastError \nA document that reports on write concern use. metrics.getLastError.wtime \nA document that reports write concern operation counts with a w argument greater than 1. metrics.getLastError.wtime.num \nThe total number of operations with a specified write concern (i.e. w) that wait for one or more members of a replica set to acknowledge the write operation (i.e. a w value greater than 1.) metrics.getLastError.wtime.totalMillis \nThe total amount of time in milliseconds that the mongod has spent performing write concern operations with a write concern (i.e. w) that waits for one or more members of a replica set to acknowledge the write operation (i.e. a w value greater than 1.) metrics.getLastError.wtimeouts \nThe number of times that write concern operations have timed out as a result of the wtimeout threshold. This number increments for both default and non-default write concern specifications. metrics.getLastError.default \nA document that reports on when a default write concern was used (meaning, a non-clientSupplied write concern). The possible origins of a default write concern are:  * implicitDefault  * customDefault  * getLastErrorDefaults Refer to the following table for information on each possible write concern origin, or provenance: Provenance\nDescription\nclientSupplied\nThe write concern was specified in the application.\ncustomDefault\nThe write concern originated from a custom defined default value. See setDefaultRWConcern.\ngetLastErrorDefaults\nThe write concern originated from the replica set's settings.getLastErrorDefaults field.\nimplicitDefault\nThe write concern originated from the server in absence of all other write concern specifications. metrics.getLastError.default.unsatisfiable \nNumber of times that a non-clientSupplied write concern returned the UnsatisfiableWriteConcern error code. metrics.getLastError.default.wtimeouts \nNumber of times a non-clientSupplied write concern timed out. metrics.mongos \nA document that contains metrics about mongos. metrics.mongos.cursor \nA document that contains metrics for cursors used by mongos. metrics.mongos.cursor.moreThanOneBatch \nThe total number of cursors that have returned more than one batch since mongos started. Additional batches are retrieved using the getMore command. New in version 5.0. metrics.mongos.cursor.totalOpened \nThe total number of cursors that have been opened since mongos started, including cursors currently open. Differs from metrics.cursor.open.total, which is the number of currently open cursors only. New in version 5.0. metrics.operation \nA document that holds counters for several types of update and query operations that MongoDB handles using special operation types. metrics.operation.scanAndOrder \nThe total number of queries that return sorted numbers that cannot perform the sort operation using an index. metrics.operation.writeConflicts \nThe total number of queries that encountered write conflicts. metrics.query.multiPlanner \nProvides detailed query planning data for the slot-based query execution engine and the classic query engine. For more information on the slot-based query execution engine see: Slot-Based Query Execution Engine Pipeline Optimizations. These metrics are primarily intended for internal use by MongoDB. New in version 6.0.0, 5.0.9, and 4.4.15 query.multiPlanner.classicMicros \nAggregates the total number of microseconds spent in the classic multiplanner. query.multiPlanner.classicWorks \nAggregates the total number of \"works\" performed in the classic multiplanner. query.multiPlanner.classicCount \nAggregates the total number of invocations of the classic multiplanner. query.multiPlanner.sbeMicros \nAggregates the total number of microseconds spent in the slot-based engine multiplanner.\n\n \nAggregates the total number of reads done in the slot-based engine multiplanner. query.multiPlanner.sbeCount \nAggregates the total number of invocations of the slot-based engine multiplanner. query.multiPlanner.histograms.classicMicros \nA histogram measuring the number of microseconds spent in an invocation of the classic multiplanner. query.multiPlanner.histograms.classicWorks \nA histogram measuring the number of \"works\" performed during an invocation of the classic multiplanner. query.multiPlanner.histograms.classicNumPlans \nA histogram measuring the number of plans in the candidate set during an invocation of the classic multiplanner. query.multiPlanner.histograms.sbeMicros \nA histogram measuring the number of microseconds spent in an invocation of the slot-based engine multiplanner. query.multiPlanner.histograms.sbeNumReads \nA histogram measuring the number of reads during an invocation of the slot-based engine multiplanner. query.multiPlanner.histograms.sbeNumPlans \nA histogram measuring the number of plans in the candidate set during an invocation of the slot-based engine multiplanner. metrics.queryExecutor \nA document that reports data from the query execution system. metrics.queryExecutor.scanned \nThe total number of index items scanned during queries and query-plan evaluation. This counter is the same as totalKeysExamined in the output of explain(). metrics.queryExecutor.scannedObjects \nThe total number of documents scanned during queries and query-plan evaluation. This counter is the same as totalDocsExamined in the output of explain(). metrics.queryExecutor.collectionScans \nA document that reports on the number of queries that performed a collection scan. New in version 4.4. metrics.queryExecutor.collectionScans.nonTailable \nThe number of queries that performed a collection scan that did not use a tailable cursor. New in version 4.4. metrics.queryExecutor.collectionScans.total \nThe total number queries that performed a collection scan. The total consists of queries that did and did not use a tailable cursor. New in version 4.4. metrics.record \nA document that reports on data related to record allocation in the on-disk memory files. metrics.repl \nA document that reports metrics related to the replication process. metrics.repl document appears on all mongod instances, even those that aren't members of replica sets. metrics.repl.apply \nA document that reports on the application of operations from the replication oplog. metrics.repl.apply.batchSize \nThe total number of oplog operations applied. The metrics.repl.apply.batchSize is incremented with the number of operations in a batch at the batch boundaries instead of being incremented by one after each operation. For finer granularity, see metrics.repl.apply.ops. metrics.repl.apply.batches \nmetrics.repl.apply.batches reports on the oplog application process on secondaries members of replica sets. See Multithreaded Replication for more information on the oplog application processes. metrics.repl.apply.batches.num \nThe total number of batches applied across all databases. metrics.repl.apply.batches.totalMillis \nThe total amount of time in milliseconds the mongod has spent applying operations from the oplog. metrics.repl.apply.ops \nThe total number of oplog operations applied. metrics.repl.apply.ops is incremented after each operation. \nTIP \nSEE ALSO: metrics.repl.apply.batchSize metrics.repl.buffer \nMongoDB buffers oplog operations from the replication sync source buffer before applying oplog entries in a batch. metrics.repl.buffer provides a way to track the oplog buffer. See Multithreaded Replication for more information on the oplog application process. metrics.repl.buffer.count \nThe current number of operations in the oplog buffer. metrics.repl.buffer.maxSizeBytes \nThe maximum size of the buffer. This value is a constant setting in the mongod, and is not configurable. metrics.repl.buffer.sizeBytes \nThe current size of the contents of the oplog buffer. metrics.repl.network \nmetrics.repl.network reports network use by the replication process. metrics.repl.network.bytes \nmetrics.repl.network.bytes reports the total amount of data read from the replication sync source.\n\n \nmetrics.repl.network.getmores reports on the getmore operations, which are requests for additional results from the oplog cursor as part of the oplog replication process. metrics.repl.network.getmores.num \nmetrics.repl.network.getmores.num reports the total number of getmore operations, which are operations that request an additional set of operations from the replication sync source. metrics.repl.network.getmores.totalMillis \nmetrics.repl.network.getmores.totalMillis reports the total amount of time required to collect data from getmore operations. \nNOTE This number can be quite large, as MongoDB will wait for more data even if the getmore operation does not initial return data. metrics.repl.network.getmores.numEmptyBatches \nThe number of empty oplog batches a secondary receives from its sync source. A secondary receives an empty batch if it is fully synced with its source and either:  * The getmore times out waiting for more data, or  * The sync source's majority commit point has advanced since the last batch sent to this secondary. For a primary, if the instance was previously a secondary, the number reports on the empty batches received when it was a secondary. Otherwise, for a primary, this number is 0. New in version 4.4. metrics.repl.network.notPrimaryLegacyUnacknowledgedWrites \nThe number of unacknowledged (w: 0) legacy write operations (see Opcodes) that failed because the current mongod is not in PRIMARY state. New in version 4.2. metrics.repl.network.notPrimaryUnacknowledgedWrites \nThe number of unacknowledged (w: 0) write operations that failed because the current mongod is not in PRIMARY state. New in version 4.2. metrics.repl.network.oplogGetMoresProcessed \nA document that reports the number of getMore commands to fetch the oplog that a node processed as a sync source. New in version 4.4. metrics.repl.network.oplogGetMoresProcessed.num \nThe number of getMore commands to fetch the oplog that a node processed as a sync source. New in version 4.4. metrics.repl.network.oplogGetMoresProcessed.totalMillis \nThe time, in milliseconds, that a node spent processing the getMore commands counted in metrics.repl.network.oplogGetMoresProcessed.num. New in version 4.4. metrics.repl.network.ops \nThe total number of operations read from the replication source. metrics.repl.network.readersCreated \nThe total number of oplog query processes created. MongoDB will create a new oplog query any time an error occurs in the connection, including a timeout, or a network operation. Furthermore, metrics.repl.network.readersCreated will increment every time MongoDB selects a new source for replication. metrics.repl.network.replSetUpdatePosition \nA document that reports the number of replSetUpdatePosition commands a node sent to its sync source. New in version 4.4. metrics.repl.network.replSetUpdatePosition.num \nThe number of replSetUpdatePosition commands a node sent to its sync source. replSetUpdatePosition commands are internal replication commands that communicate replication progress from nodes to their sync sources. New in version 4.4. \nNOTE Replica set members in the STARTUP2 state do not send the replSetUpdatePosition command to their sync source. metrics.repl.reconfig \nA document containing the number of times that member newlyAdded fields were automatically removed by the primary. When a member is first added to the replica set, the member's newlyAdded field is set to true. New in version 5.0. metrics.repl.reconfig.numAutoReconfigsForRemovalOfNewlyAddedFields \nThe number of times that newlyAdded member fields were automatically removed by the primary. When a member is first added to the replica set, the member's newlyAdded field is set to true. After the primary receives the member's heartbeat response indicating the member state is SECONDARY, RECOVERING, or ROLLBACK, the primary automatically removes the member's newlyAdded field. The newlyAdded fields are stored in the local.system.replset collection. New in version 5.0. metrics.repl.stepDown \nInformation on user operations that were running when the mongod stepped down. New in version 4.2. metrics.repl.stepDown.userOperationsKilled\n\n New in version 4.2. metrics.repl.stepDown.userOperationsRunning \nThe number of user operations that remained running when the mongod stepped down. New in version 4.2. metrics.repl.syncSource \nInformation on a replica set node's sync source selection process. New in version 4.4. metrics.repl.syncSource.numSelections \nNumber of times a node attempted to choose a node to sync from among the available sync source options. A node attempts to choose a node to sync from if, for example, the sync source is re-evaluated or the node receives an error from its current sync source. New in version 4.4. metrics.repl.syncSource.numTimesChoseSame \nNumber of times a node kept its original sync source after re-evaluating if its current sync source was optimal. New in version 4.4. metrics.repl.syncSource.numTimesChoseDifferent \nNumber of times a node chose a new sync source after re-evaluating if its current sync source was optimal. New in version 4.4. metrics.repl.syncSource.numTimesCouldNotFind \nNumber of times a node could not find an available sync source when attempting to choose a node to sync from. New in version 4.4. metrics.storage.freelist.search.bucketExhausted \nThe number of times that mongod has examined the free list without finding a large record allocation. metrics.storage.freelist.search.requests \nThe number of times mongod has searched for available record allocations. metrics.storage.freelist.search.scanned \nThe number of available record allocations mongod has searched. metrics.ttl \nA document that reports on the operation of the resource use of the ttl index process. metrics.ttl.deletedDocuments \nThe total number of documents deleted from collections with a ttl index. metrics.ttl.passes \nThe number of times the background process removes documents from collections with a ttl index. metrics.cursor \nA document that contains data regarding cursor state and use. metrics.cursor.moreThanOneBatch \nThe total number of cursors that have returned more than one batch since the server process started. Additional batches are retrieved using the getMore command. New in version 5.0. metrics.cursor.timedOut \nThe total number of cursors that have timed out since the server process started. If this number is large or growing at a regular rate, this may indicate an application error. metrics.cursor.totalOpened \nThe total number of cursors that have been opened since the server process started, including cursors currently open. Differs from metrics.cursor.open.total, which is the number of currently open cursors only. New in version 5.0. metrics.cursor.lifespan \nA document that reports the number of cursors that have lifespans within specified time periods. The cursor lifespan is the time period from when the cursor is created to when the cursor is killed using the killCursors command or the cursor has no remaining objects in the batch. The lifespan time periods are:  * < 1 second  * >= 1 second to < 5 seconds  * >= 5 seconds to < 15 seconds  * >= 15 seconds to < 30 seconds  * >= 30 seconds to < 1 minute  * >= 1 minute to < 10 minutes  * >= 10 minutes New in version 5.0. metrics.cursor.lifespan.greaterThanOrEqual10Minutes \nThe number of cursors with a lifespan >= 10 minutes. New in version 5.0. metrics.cursor.lifespan.lessThan10Minutes \nThe number of cursors with a lifespan >= 1 minute to < 10 minutes. New in version 5.0. metrics.cursor.lifespan.lessThan15Seconds \nThe number of cursors with a lifespan >= 5 seconds to < 15 seconds. New in version 5.0. metrics.cursor.lifespan.lessThan1Minute \nThe number of cursors with a lifespan >= 30 seconds to < 1 minute. New in version 5.0. metrics.cursor.lifespan.lessThan1Second \nThe number of cursors with a lifespan < 1 second. New in version 5.0. metrics.cursor.lifespan.lessThan30Seconds \nThe number of cursors with a lifespan >= 15 seconds to < 30 seconds. New in version 5.0. metrics.cursor.lifespan.lessThan5Seconds\n\n New in version 5.0. metrics.cursor.open \nA document that contains data regarding open cursors. metrics.cursor.open.noTimeout \nThe number of open cursors with the option DBQuery.Option.noTimeout set to prevent timeout after a period of inactivity. metrics.cursor.open.pinned \nThe number of \"pinned\" open cursors. metrics.cursor.open.total \nThe number of cursors that MongoDB is maintaining for clients. Because MongoDB exhausts unused cursors, typically this value small or zero. However, if there is a queue, or stale tailable cursors, or a large number of operations this value may increase. metrics.cursor.open.singleTarget \nThe total number of cursors that only target a single shard. Only mongos instances report metrics.cursor.open.singleTarget values. metrics.cursor.open.multiTarget \nThe total number of cursors that only target more than one shard. Only mongos instances report metrics.cursor.open.multiTarget values. \nMIRROREDREADS \nAvailable on mongod only. \"mirroredReads\" : {      \"seen\" : <num>,      \"sent\" : <num>}, \nmirroredReads \nAvailable on mongod only. A document that reports on mirrored reads. To return mirroredReads information, you must explicitly specify the inclusion: db.runCommand( { serverStatus: 1, mirroredReads: 1 } ) \nmirroredReads.seen \nNew in version 4.4. The number of operations that support mirroring received by this member. \nTIP \nSEE ALSO: mirrorReads Parameter mirroredReads.sent \nNew in version 4.4. The number of mirrored reads sent by this member when primary. For example, if a read is mirrored and sent to two secondaries, the number of mirrored reads is 2. \nTIP \nSEE ALSO: mirrorReads Parameter \nNETWORK \n\"network\" : {   \"bytesIn\" : NumberLong(<num>),   \"bytesOut\" : NumberLong(<num>),   \"numSlowDNSOperations\" : NumberLong(<num>),   \"numSlowSSLOperations\" : NumberLong(<num>),   \"numRequests\" : NumberLong(<num>),   \"tcpFastOpen\" : {      \"kernelSetting\" : NumberLong(<num>),      \"serverSupported\" : <bool>,      \"clientSupported\" : <bool>,      \"accepted\" : NumberLong(<num>)   },   \"serviceExecutors\" : {      \"passthrough\" : {         \"threadsRunning\" : <num>,         \"clientsInTotal\" : <num>,         \"clientsRunning\" : <num>,         \"clientsWaitingForData\" : <num>      },      \"fixed\" : {         \"threadsRunning\" : <num>,         \"clientsInTotal\" : <num>,         \"clientsRunning\" : <num>,         \"clientsWaitingForData\" : <num>      }   },   \"listenerProcessingTime\" : { durationMicros: <num> }  // Added in MongoDB 6.1} \nnetwork \nA document that reports data on MongoDB's network use. These statistics measure ingress connections only, specifically the traffic seen by the mongod or mongos over network connections initiated by clients or other mongod or mongos instances. Traffic from network connections initiated by this mongod or mongos instance (specifically, egress connections) is not measured in these statistics. network.bytesIn \nThe total number of bytes that the server has received over network connections initiated by clients or other mongod or mongos instances. network.bytesOut \nThe total number of bytes that the server has sent over network connections initiated by clients or other mongod or mongos instances. network.numSlowDNSOperations \nNew in version 4.4. The total number of DNS resolution operations which took longer than 1 second.\n\n \nNew in version 4.4. The total number of SSL handshake operations which took longer than 1 second. network.numRequests \nThe total number of distinct requests that the server has received. Use this value to provide context for the network.bytesIn and network.bytesOut values to ensure that MongoDB's network utilization is consistent with expectations and application use. network.tcpFastOpen \nNew in version 4.4. A document that reports data on MongoDB's support and use of TCP Fast Open (TFO) connections. network.tcpFastOpen.kernelSetting \nNew in version 4.4. Linux only Returns the value of /proc/sys/net/ipv4/tcp_fastopen:  * 0 - TCP Fast Open is disabled on the system.  * 1 - TCP Fast Open is enabled for outgoing connections.  * 2 - TCP Fast Open is enabled for incoming connections.  * 3 - TCP Fast Open is enabled for incoming and outgoing connections. network.tcpFastOpen.serverSupported \nNew in version 4.4.  * Returns true if the host operating system supports inbound TCP Fast Open (TFO) connections.  * Returns false if the host operating system does not support inbound TCP Fast Open (TFO) connections. network.tcpFastOpen.clientSupported \nNew in version 4.4.  * Returns true if the host operating system supports outbound TCP Fast Open (TFO) connections.  * Returns false if the host operating system does not support outbound TCP Fast Open (TFO) connections. network.tcpFastOpen.accepted \nNew in version 4.4. The total number of accepted incoming TCP Fast Open (TFO) connections to the mongod or mongos since the mongod or mongos last started. network.serviceExecutors \nNew in version 5.0. A document that reports data on the service executors, which run operations for client requests. network.serviceExecutors.passthrough \nNew in version 5.0. A document that reports data about the threads and clients for the passthrough service executor. The passthrough service executor creates a new thread for each client and destroys the thread after the client ends. network.serviceExecutors.passthrough.threadsRunning \nNew in version 5.0. Number of threads running in the passthrough service executor. network.serviceExecutors.passthrough.clientsInTotal \nNew in version 5.0. Total number of clients allocated to the passthrough service executor. A client can be allocated to the passthrough service executor and not currently running requests. network.serviceExecutors.passthrough.clientsRunning \nNew in version 5.0. Number of clients currently using the passthrough service executor to run requests. network.serviceExecutors.passthrough.clientsWaitingForData \nNew in version 5.0. Number of clients using the passthrough service executor that are waiting for incoming data from the network. network.serviceExecutors.fixed \nNew in version 5.0. A document that reports data about the threads and clients for the fixed service executor. The fixed service executor has a fixed number of threads. A thread is temporarily assigned to a client and the thread is preserved after the client ends. network.serviceExecutors.fixed.threadsRunning \nNew in version 5.0. Number of threads running in the fixed service executor. network.serviceExecutors.fixed.clientsInTotal \nNew in version 5.0. Total number of clients allocated to the fixed service executor. A client can be allocated to the fixed service executor and not currently running requests. network.serviceExecutors.fixed.clientsRunning \nNew in version 5.0. Number of clients currently using the fixed service executor to run requests. network.serviceExecutors.fixed.clientsWaitingForData \nNew in version 5.0. Number of clients using the fixed service executor that are waiting for incoming data from the network. network.listenerProcessingTime \nNew in version 6.1. A document that reports the total time the database listener spends allocating incoming database connection requests to dedicated threads. network.listenerProcessingTime.durationMicros \nNew in version 6.1. Total time in microseconds the database listener spends allocating incoming database connection requests to dedicated threads that perform database operations. \nOPLATENCIES \nOnly for ``mongod`` instances \"opLatencies\" : {   \"reads\" : <document>,   \"writes\" : <document>,   \"commands\" : <document>}, \nopLatencies\n\n Only mongod instances report opLatencies. opLatencies.reads \nLatency statistics for read requests. opLatencies.writes \nLatency statistics for write operations. opLatencies.commands \nLatency statistics for database commands. \nOPREADCONCERNCOUNTERS  WARNING \nREMOVED Starting in version 5.0, opReadConcernCounters is replaced by readConcernCounters. Only for mongod instances \"opReadConcernCounters\" : {   \"available\" : NumberLong(<num>),   \"linearizable\" : NumberLong(<num>),   \"local\" : NumberLong(<num>),   \"majority\" : NumberLong(<num>),   \"snapshot\" : NumberLong(<num>),   \"none\" : NumberLong(<num>)} \nopReadConcernCounters \nRemoved in version 5.0. Replaced by readConcernCounters. A document that reports on the read concern level specified by query operations to the mongod instance since it last started. Specified w\nDescription\n\"available\"\nNumber of query operations that specified read concern level \"available\".\n\"linearizable\"\nNumber of query operations that specified read concern level \"linearizable\".\n\"local\"\nNumber of query operations that specified read concern level \"local\".\n\"majority\"\nNumber of query operations that specified read concern level \"majority\".\n\"snapshot\"\nNumber of query operations that specified read concern level \"snapshot\".\n\"none\"\nNumber of query operations that did not specify a read concern level and instead used the default read concern level. The sum of the opReadConcernCounters equals opcounters.query. \nOPWRITECONCERNCOUNTERS \nOnly for mongod instances\n\n \nopWriteConcernCounters \nA document that reports on the write concerns specified by write operations to the mongod instance since it last started. More specifically, the opWriteConcernCounters reports on the w: <value> specified by the write operations. The journal flag option (j) and the timeout option (wtimeout) of the write concerns does not affect the count. The count is incremented even if the operation times out. \nNOTE Only available when reportOpWriteConcernCountersInServerStatus parameter is set to true (false by default). opWriteConcernCounters.insert \nA document that reports on the w: <value> specified by insert operations to the mongod instance since it last started: \nNOTE Only available when reportOpWriteConcernCountersInServerStatus parameter is set to true (false by default). \"insert\" : {   \"wmajority\" : NumberLong(<num>),   \"wnum\" : {      \"<num>\" :  NumberLong(<num>),      ...   },   \"wtag\" : {      \"<tag1>\" :  NumberLong(<num>),      ...   },   \"none\" : NumberLong(<num>),   \"noneInfo\" : {      \"CWWC\" : {         \"wmajority\" : NumberLong(<num>),         \"wnum\" : {},         \"wtag\" : {}      },      \"implicitDefault\" : {         \"wmajority\" : NumberLong(<num>)         \"wnum\" : {}      }   }}, \nSpecified w\nDescription\n\"wmajority\"\nNumber of insert operations that specified w: \"majority\".\n\"wnum\"\nNumber of insert operations that specified w: <num>. The counts are grouped by the specific``<num>``.\n\"wtag\"\nNumber of insert operations that specified w: <tag>. The counts are grouped by the specific <tag>.\n\"none\"\nNumber of insert operations that did not specify w value. These operations use the default w value of \"majority\".\n\"noneInfo\" Number of non-transaction query operations that use default write concerns. The metrics track usage of the cluster wide write concern (the global default write concern) and the implicit-default write concern. The sum of the values in opWriteConcernCounters.noneInfo should equal the value of opWriteConcernCounters.none. The sum of the opWriteConcernCounters.insert equals opcounters.insert. opWriteConcernCounters.update \nA document that reports on the w: <value> specified by update operations to the mongod instance since it last started: \nNOTE Only available when reportOpWriteConcernCountersInServerStatus parameter is set to true (false by default). \"update\" : {   \"wmajority\" : NumberLong(<num>),   \"wnum\" : {      \"<num>\" :  NumberLong(<num>),      ...   },   \"wtag\" : {      \"<tag1>\" :  NumberLong(<num>),      ...   },   \"none\" : NumberLong(<num>),   \"noneInfo\" : {      \"CWWC\" : {         \"wmajority\" : NumberLong(<num>),         \"wnum\" : {},         \"wtag\" : {}      },      \"implicitDefault\" : {         \"wmajority\" : NumberLong(<num>)         \"wnum\" : {}      }   }},\n\n Number of non-transaction query operations that use default write concerns. The metrics track usage of the cluster wide write concern (the global default write concern) and the implicit-default write concern. The sum of the values in opWriteConcernCounters.noneInfo should equal the value of opWriteConcernCounters.none. The sum of the opWriteConcernCounters.update equals opcounters.update. opWriteConcernCounters.delete \nA document that reports on the w: <value> specified by delete operations to the mongod instance since it last started: \nNOTE Only available when reportOpWriteConcernCountersInServerStatus parameter is set to true (false by default). \"delete\" : {   \"wmajority\" :  NumberLong(<num>),   \"wnum\" : {      \"<num>\" :  NumberLong(<num>),      ...   },   \"wtag\" : {      \"<tag1>\" :  NumberLong(<num>),      ...   },   \"none\" : NumberLong(<num>),   \"noneInfo\" : {      \"CWWC\" : {         \"wmajority\" : NumberLong(<num>),         \"wnum\" : {},         \"wtag\" : {}      },      \"implicitDefault\" : {         \"wmajority\" : NumberLong(<num>)         \"wnum\" : {}      }   }} \nSpecified w\nDescription\n\"wmajority\"\nNumber of delete operations that specified w: \"majority\".\n\"wnum\"\nNumber of delete operations that specified w: <num>. The counts are grouped by the specific <num>.\n\"wtag\"\nNumber of delete operations that specified w: <tag>. The counts are grouped by the specific <tag>.\n\"none\"\nNumber of delete operations that did not specify w value. These operations use the default w value of 1.\n\"noneInfo\" Number of non-transaction query operations that use default write concerns. The metrics track usage of the cluster wide write concern (the global default write concern) and the implicit-default write concern. The sum of the values in opWriteConcernCounters.noneInfo should equal the value of opWriteConcernCounters.none. The sum of the opWriteConcernCounters.delete equals opcounters.delete. \nOPCOUNTERS \nStarting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. \"opcounters\" : {   \"insert\" : NumberLong(<num>),  // Starting in MongoDB 4.2, type is NumberLong   \"query\" : NumberLong(<num>),   // Starting in MongoDB 4.2, type is NumberLong   \"update\" : NumberLong(<num>),  // Starting in MongoDB 4.2, type is NumberLong   \"delete\" : NumberLong(<num>),  // Starting in MongoDB 4.2, type is NumberLong   \"getmore\" : NumberLong(<num>), // Starting in MongoDB 4.2, type is NumberLong   \"command\" : NumberLong(<num>), // Starting in MongoDB 4.2, type is NumberLong}, \nopcounters \nA document that reports on database operations by type since the mongod instance last started. These numbers will grow over time until next restart. Analyze these values over time to track database utilization. \nNOTE The data in opcounters treats operations that affect multiple documents, such as bulk insert or multi-update operations, as a single operation. See metrics.document for more granular document-level operation tracking. Additionally, these values reflect received operations, and increment even when operations are not successful. opcounters.insert \nThe total number of insert operations received since the mongod instance last started. Starting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. opcounters.query \nThe total number of queries received since the mongod instance last started. Starting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. opcounters.update\n\n Starting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. opcounters.delete \nThe total number of delete operations since the mongod instance last started. Starting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. opcounters.getmore \nThe total number of getMore operations since the mongod instance last started. This counter can be high even if the query count is low. Secondary nodes send getMore operations as part of the replication process. Starting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. opcounters.command \nThe total number of commands issued to the database since the mongod instance last started. opcounters.command counts all commands except the write commands: insert, update, and delete. Starting in MongoDB 4.2, the returned opcounters.* values are type NumberLong. In previous versions, the values are of type NumberInt. opcounters.deprecated \nThis section only appears in the db.serverStatus() output when one of the following, deprecated opcodes has been used: \"deprecated\": {   \"opQuery\": NumberLong(<num>),   \"opGetMore\": NumberLong(<num>),   \"opKillCursors\": NumberLong(<num>),   \"opDelete\": NumberLong(<num>),   \"opUpdate\": NumberLong(<num>),   \"opInsert\": NumberLong(<num>),   \"total\": NumberLong(<num>),}  These opcodes were deprecated in MongoDB 5.0 and support for these opcodes is removed in MongoDB 5.1. Starting in MongoDB 5.1, mongod refuses requests that use a deprecated opcode and increments the corresponding counter. OP_QUERY is an exception to the general rule. mongod continues to support hello and isMaster requests that use OP_QUERY. mongod refuses any other OP_QUERY requests. The counters are reset when mongod starts. \nOPCOUNTERSREPL \nStarting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. \"opcountersRepl\" : {   \"insert\" : NumberLong(<num>),  // Starting in MongoDB 4.2, type is NumberLong   \"query\" : NumberLong(<num>),   // Starting in MongoDB 4.2, type is NumberLong   \"update\" : NumberLong(<num>),  // Starting in MongoDB 4.2, type is NumberLong   \"delete\" : NumberLong(<num>),  // Starting in MongoDB 4.2, type is NumberLong   \"getmore\" : NumberLong(<num>), // Starting in MongoDB 4.2, type is NumberLong   \"command\" : NumberLong(<num>), // Starting in MongoDB 4.2, type is NumberLong}, \nopcountersRepl \nA document that reports on database replication operations by type since the mongod instance last started. These values only appear when the current host is a member of a replica set. These values will differ from the opcounters values because of how MongoDB serializes operations during replication. See Replication for more information on replication. These numbers will grow over time in response to database use until next restart. Analyze these values over time to track database utilization. Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. opcountersRepl.insert \nThe total number of replicated insert operations since the mongod instance last started. Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. opcountersRepl.query \nThe total number of replicated queries since the mongod instance last started. Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. opcountersRepl.update \nThe total number of replicated update operations since the mongod instance last started. Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. opcountersRepl.delete\n\n Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. opcountersRepl.getmore \nThe total number of getMore operations since the mongod instance last started. This counter can be high even if the query count is low. Secondary nodes send getMore operations as part of the replication process. Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. opcountersRepl.command \nThe total number of replicated commands issued to the database since the mongod instance last started. Starting in MongoDB 4.2, the returned opcountersRepl.* values are type NumberLong. In previous versions, the values are type NumberInt. \nOPLOGTRUNCATION \n\"oplogTruncation\" : {   \"totalTimeProcessingMicros\" : NumberLong(<num>),   \"processingMethod\" : <string>,   \"oplogMinRetentionHours\" : <double>   \"totalTimeTruncatingMicros\" : NumberLong(<num>),   \"truncateCount\" : NumberLong(<num>)}, \noplogTruncation \nA document that reports on oplog truncations. The field only appears when the current instance is a member of a replica set and uses either the WiredTiger Storage Engine or In-Memory Storage Engine. Changed in version 4.4: Also available in In-Memory Storage Engine. New in version 4.2.1: Available in the WiredTiger Storage Engine. oplogTruncation.totalTimeProcessingMicros \nThe total time taken, in microseconds, to scan or sample the oplog to determine the oplog truncation points. totalTimeProcessingMicros is only meaningful if the mongod instance started on existing data files (i.e. not meaningful for In-Memory Storage Engine). See oplogTruncation.processingMethod Changed in version 4.4: Also available in In-Memory Storage Engine. New in version 4.2.1: Available in the WiredTiger Storage Engine. oplogTruncation.processingMethod \nThe method used at start up to determine the oplog truncation points. The value can be either \"sampling\" or \"scanning\". processingMethod is only meaningful if the mongod instance started on existing data files (i.e. not meaningful for In-Memory Storage Engine). Changed in version 4.4: Also available in In-Memory Storage Engine. New in version 4.2.1: Available in the WiredTiger Storage Engine. oplogTruncation.minRetentionHours \nNew in version 4.4: The minimum retention period for the oplog in hours. If the oplog has exceeded the oplog size, the mongod only truncates oplog entries older than the configured retention value. Only visible if the mongod is a member of a replica set and:  * The mongod was started with the --oplogMinRetentionHours command line option or the storage.oplogMinRetentionHours configuration file option,\n   \n   or  * The minimum retention period was configured after startup using replSetResizeOplog. oplogTruncation.totalTimeTruncatingMicros \nThe cumulative time spent, in microseconds, performing oplog truncations. Changed in version 4.4: Also available in In-Memory Storage Engine. New in version 4.2.1: Available in the WiredTiger Storage Engine. oplogTruncation.truncateCount \nThe cumulative number of oplog truncations. Changed in version 4.4: Also available in In-Memory Storage Engine. New in version 4.2.1: Available in the WiredTiger Storage Engine. \nREADCONCERNCOUNTERS \nNew in version 5.0.\n\n \nreadConcernCounters \nA document that reports on the read concern level specified by query operations. This document contains the readConcernCounters.nonTransactionOps and readConcernCounters.transactionOps documents. readConcernCounters.nonTransactionOps \nA document that reports on the read concern level specified by non-transaction query operations performed after the database server last started. readConcernCounters.nonTransactionOps.none \nNumber of non-transaction query operations that did not specify a read concern level and instead used either:  * the default read concern level, or  * the global default read concern configuration if it was set by the setDefaultRWConcern command. readConcernCounters.nonTransactionOps.noneInfo \nThe number of non-transaction query operations that use the global default read concern and an implicit-default read concern. The sum of the values in readConcernCounters.nonTransactionOps.noneInfo should equal the value of readConcernCounters.nonTransactionOps.none. readConcernCounters.nonTransactionOps.local \nNumber of non-transaction query operations that specified the \"local\" read concern level. readConcernCounters.nonTransactionOps.available \nNumber of non-transaction query operations that specified the \"available\" read concern level. readConcernCounters.nonTransactionOps.majority \nNumber of non-transaction query operations that specified the \"majority\" read concern level. readConcernCounters.nonTransactionOps.snapshot \nDocument containing non-transaction query operations that specified the \"snapshot\" read concern level. readConcernCounters.nonTransactionOps.snapshot.withClusterTime \nNumber of non-transaction query operations that specified the \"snapshot\" read concern level and the cluster time, which specified a point in time. readConcernCounters.nonTransactionOps.snapshot.withoutClusterTime \nNumber of non-transaction query operations that specified the \"snapshot\" read concern level without the cluster time, which means a point in time was omitted and the server will read the most recently committed snapshot available to the node. readConcernCounters.nonTransactionOps.linearizable \nNumber of non-transaction query operations that specified the \"linearizable\" read concern level. readConcernCounters.transactionOps \nA document that reports on the read concern level specified by transaction query operations performed after the database server last started. readConcernCounters.transactionOps.none \nNumber of transaction query operations that did not specify a read concern level and instead used the default read concern level or the global default read or write concern configuration added with the setDefaultRWConcern command. readConcernCounters.transactionOps.noneInfo \nInformation about the global default read concern and implicit-default read concern used by transaction query operations. readConcernCounters.transactionOps.local \nNumber of transaction query operations that specified the \"local\" read concern level. readConcernCounters.transactionOps.available \nNumber of transaction query operations that specified the \"available\" read concern level. readConcernCounters.transactionOps.majority \nNumber of transaction query operations that specified the \"majority\" read concern level. readConcernCounters.transactionOps.snapshot \nDocument containing transaction query operations that specified the \"snapshot\" read concern level. readConcernCounters.transactionOps.snapshot.withClusterTime \nNumber of transaction query operations that specified the \"snapshot\" read concern level and the cluster time, which specified a point in time. readConcernCounters.transactionOps.snapshot.withoutClusterTime \nNumber of transaction query operations that specified the \"snapshot\" read concern level without the cluster time, which means a point in time was omitted and the server will read the most recently committed snapshot available to the node. \nREPL\n\n \nrepl \nA document that reports on the replica set configuration. repl only appear when the current host is a replica set. See Replication for more information on replication. repl.hosts \nAn array of the current replica set members' hostname and port information (\"host:port\"). repl.setName \nA string with the name of the current replica set. This value reflects the --replSet command line argument, or replSetName value in the configuration file. repl.isWritablePrimary \nA boolean that indicates whether the current node is the primary of the replica set. repl.secondary \nA boolean that indicates whether the current node is a secondary member of the replica set. repl.primary \nThe hostname and port information (\"host:port\") of the current primary member of the replica set. repl.me \nThe hostname and port information (\"host:port\") for the current member of the replica set. repl.primaryOnlyServices \nDocument that contains the number and status of instances of each primary service active on the server. Primary services can only start when a server is primary but can continue running to completion after the server changes state. New in version 5.0. repl.primaryOnlyServices.ReshardingRecipientService \nDocument that contains the state and number of instances of the ReshardingRecipientService. Recipients are the shards,that would own the chunks after as a result of the resharding operation, according to the new shard key and zones. The resharding coordinator instructs each donor and recipient shard primary, to rename the temporary sharded collection. The temporary collection becomes the new resharded collection. New in version 5.0. repl.primaryOnlyServices.RenameCollectionParticipantService \nDocument that contains the state and number of instances of the RenameCollectionParticipantService. The RenameCollectionParticipantService ensures that, after a shard receives a renameCollection request, the shard is able to resume the local rename in case of system failure. New in version 5.0. repl.primaryOnlyServices.ShardingDDLCoordinator \nDocument that contains the state and number of instances of the ShardingDDLCoordinator. The ShardingDDLCoordinator service manages DDL operations for primary databases such as: create database, drop database, renameCollection. The ShardingDDLCoordinator ensures that one DDL operation for each database can happen at any one specific point in time within a sharded cluster. New in version 5.0. repl.primaryOnlyServices.ReshardingDonorService \nDocument that contains the state and number of instances of the ReshardingDonorService. Donors are the shards that own chunks of the sharded collection before the rename operation completes. The resharding coordinator instructs each donor and recipient shard primary, to rename the temporary sharded collection. The temporary collection becomes the new resharded collection. New in version 5.0. repl.rbid \nRollback identifier. Used to determine if a rollback has happened for this mongod instance. repl.replicationProgress \nAn array with one document for each member of the replica set that reports replication process to this member. Typically this is the primary, or secondaries if using chained replication. To include this output, you must pass the repl option to the serverStatus, as in the following: db.serverStatus({ \"repl\": 1 })db.runCommand({ \"serverStatus\": 1, \"repl\": 1 })  The content of the repl.replicationProgress section depends on the source of each member's replication. This section supports internal operation and is for internal and diagnostic use only. repl.replicationProgress[n].rid \nAn ObjectId used as an ID for the members of the replica set. For internal use only. repl.replicationProgress[n].optime \nInformation regarding the last operation from the oplog that the member applied, as reported from this member. repl.replicationProgress[n].host \nThe name of the host in [hostname]:[port] format for the member of the replica set. repl.replicationProgress[n].memberID \nThe integer identifier for this member of the replica set. \nSECURITY\n\n \nsecurity \nA document that reports on:  * The number of times a given authentication mechanism has been used to authenticate against the mongod or mongos instance. (New in MongoDB 4.4)  * The mongod / mongos instance's TLS/SSL certificate. (Only appears for mongod or mongos instance with support for TLS) security.authentication.saslSupportedMechsReceived \nNew in version 5.0. The number of times a hello request includes a valid hello.saslSupportedMechs field. security.authentication.mechanisms \nA document that reports on the number of times a given authentication mechanism has been used to authenticate against the mongod or mongos instance. The values in the document distinguish standard authentication and speculative authentication. [1] New in version 4.4. \nNOTE The fields in the mechanisms document depend on the configuration of the authenticationMechanisms parameter. The mechanisms document includes a field for each authentication mechanism supported by your mongod or mongos instance. The following example shows the shape of the mechanisms document for a deployment that only supports x.509 authentication. security.authentication.mechanisms.MONGODB-X509 \nA document that reports on the number of times x.509 has been used to authenticate against the mongod or mongos instance. Includes total number of x.509 authentication attempts and the subset of those attempts which were speculative. [1] New in version 4.4. security.authentication.mechanisms.MONGODB-X509.speculativeAuthenticate.received \nNumber of speculative authentication attempts received using x.509. Includes both successful and failed speculative authentication attempts. [1] New in version 4.4. security.authentication.mechanisms.MONGODB-X509.speculativeAuthenticate.successful \nNumber of successful speculative authentication attempts received using x.509. [1] New in version 4.4. security.authentication.mechanisms.MONGODB-X509.authenticate.received \nNumber of successful and failed authentication attempts received using x.509. This value includes speculative authentication attempts received using x.509. New in version 4.4. security.authentication.mechanisms.MONGODB-X509.authenticate.successful \nNumber of successful authentication attempts received using x.509. This value includes successful speculative authentication attempts which used x.509. New in version 4.4. [1](1, 2, 3, 4) Speculative authentication minimizes the number of network round trips during the authentication process to optimize performance. security.SSLServerSubjectName \nThe subject name associated with the mongod / mongos instance's TLS/SSL certificate. security.SSLServerHasCertificateAuthority \nA boolean that is:  * true when the mongod / mongos instance's TLS/SSL certificate is associated with a certificate authority.  * false when the TLS/SSL certificate is self-signed. security.SSLServerCertificateExpirationDate \nThe expiration date and time of the mongod / mongos instance's TLS/SSL certificate. \nSHARDING \n{   \"configsvrConnectionString\" : \"csRS/cfg1.example.net:27019,cfg2.example.net:27019,cfg2.example.net:27019\",   \"lastSeenConfigServerOpTime\" : {      \"ts\" : <timestamp>,      \"t\" : NumberLong(<num>)   },   \"maxChunkSizeInBytes\" : NumberLong(<num>)} \nsharding \nA document with data regarding the sharded cluster. The lastSeenConfigServerOpTime is present only for a mongos or a shard member, not for a config server. sharding.configsvrConnectionString \nThe connection string for the config servers. sharding.lastSeenConfigServerOpTime \nThe latest optime of the CSRS primary that the mongos or the shard member has seen. The optime document includes:  * ts, the Timestamp of the operation.  * t, the term in which the operation was originally generated on the primary. The lastSeenConfigServerOpTime is present only if the sharded cluster uses CSRS. sharding.maxChunkSizeInBytes \nThe maximum size limit for a range to migrate. If this value has been updated recently on the config server, the maxChunkSizeInBytes may not reflect the most recent value. \nSHARDINGSTATISTICS  \nshardingStatistics \nA document which contains metrics on metadata refresh on sharded clusters.\n\n \nThe total number of times that threads hit stale config exception. Since a stale config exception triggers a refresh of the metadata, this number is roughly proportional to the number of metadata refreshes. Only present when run on a shard. shardingStatistics.countDonorMoveChunkStarted \nThe total number of times that the moveChunk command or moveRange have started on the shard, of which this node is a member, as part of a range migration process. This increasing number does not consider whether the chunk migrations succeed or not. Only present when run on a shard. shardingStatistics.totalDonorChunkCloneTimeMillis \nThe cumulative time, in milliseconds, taken by the clone phase of the range migrations from this shard, of which this node is a member. Specifically, for each migration from this shard, the tracked time starts with the moveRange and moveChunk commands and ends before the destination shard enters a catchup phase to apply changes that occurred during the range migrations. Only present when run on a shard. shardingStatistics.totalCriticalSectionCommitTimeMillis \nThe cumulative time, in milliseconds, taken by the update metadata phase of the range migrations from this shard, of which this node is a member. During the update metadata phase, all operations on the collection are blocked. Only present when run on a shard. shardingStatistics.totalCriticalSectionTimeMillis \nThe cumulative time, in milliseconds, taken by the catch-up phase and the update metadata phase of the range migrations from this shard, of which this node is a member. To calculate the duration of the catch-up phase, subtract totalCriticalSectionCommitTimeMillis from totalCriticalSectionTimeMillis: totalCriticalSectionTimeMillis - totalCriticalSectionCommitTimeMillis  Only present when run on a shard. shardingStatistics.countDocsClonedOnRecipient \nCumulative, always-increasing count of documents that have been cloned on this member where it acted as the primary of the recipient shard. Only present when run on a shard. New in version 4.2. shardingStatistics.countDocsClonedOnDonor \nCumulative, always-increasing count of documents that haves been cloned on this member where it acted as the primary of the donor shard. Only present when run on a shard. New in version 4.2. shardingStatistics.countRecipientMoveChunkStarted \nCumulative, always-increasing count of chunks this member, acting as the primary of the recipient shard, has started to receive (whether the move has succeeded or not). Only present when run on a shard. New in version 4.2. shardingStatistics.countDocsDeletedOnDonor \nCumulative, always-increasing count of documents that have been deleted on this member during chunk migration where the member acted as the primary of the donor shard. Only present when run on a shard. New in version 4.2. shardingStatistics.countDonorMoveChunkLockTimeout \nCumulative, always-increasing count of chunk migrations that were aborted due to lock acquisition timeouts, where the member acted as the primary of the donor shard. Only present when run on a shard. New in version 4.2. shardingStatistics.unfinishedMigrationFromPreviousPrimary \nThe number of unfinished migrations left by the previous primary after an election. This value is only updated after the newly-elected mongod completes the transition to primary. Only present when run on a shard. New in version 4.4. shardingStatistics.catalogCache \nA document with statistics about the cluster's routing information cache. shardingStatistics.catalogCache.numDatabaseEntries \nThe total number of database entries that are currently in the catalog cache. shardingStatistics.catalogCache.numCollectionEntries \nThe total number of collection entries (across all databases) that are currently in the catalog cache. shardingStatistics.catalogCache.countStaleConfigErrors \nThe total number of times that threads hit stale config exception. A stale config exception triggers a refresh of the metadata. shardingStatistics.catalogCache.totalRefreshWaitTimeMicros \nThe cumulative time, in microseconds, that threads had to wait for a refresh of the metadata. shardingStatistics.catalogCache.numActiveIncrementalRefreshes \nThe number of incremental catalog cache refreshes that are currently waiting to complete. shardingStatistics.countIncrementalRefreshesStarted \nThe cumulative number of incremental refreshes that have started. shardingStatistics.catalogCache.numActiveFullRefreshes \nThe number of full catalog cache refreshes that are currently waiting to complete.\n\n \nThe cumulative number of full refreshes that have started. shardingStatistics.catalogCache.countFailedRefreshes \nThe cumulative number of full or incremental refreshes that have failed. shardingStatistics.rangeDeleterTasks \nThe current total of the queued chunk range deletion tasks that are ready to run or are running as part of the range migration process. Inspect the documents in the config.rangeDeletions collection for information about the chunk ranges pending deletion from a shard after a chunk migration. Only present when run on a shard member. New in version 4.4. shardingStatistics.catalogCache.operationsBlockedByRefresh \nA document with statistics about operations blocked by catalog cache refresh activity on a mongos. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.catalogCache.operationsBlockedByRefresh.countAllOperations \nThe cumulative number of all operations ran on a mongos that were blocked at any point in their lifetime by a refresh of the catalog cache. This counter is the aggregate sum of all other operationsBlockedByRefresh metrics tracked below. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.catalogCache.operationsBlockedByRefresh.countInserts \nThe cumulative number of insert operations ran on a mongos that were blocked at any point in their lifetime by a refresh of the catalog cache. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.catalogCache.operationsBlockedByRefresh.countQueries \nThe cumulative number of query operations ran on a mongos that were blocked at any point in their lifetime by a refresh of the catalog cache. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.catalogCache.operationsBlockedByRefresh.countUpdates \nThe cumulative number of update operations ran on a mongos that were blocked at any point in their lifetime by a refresh of the catalog cache. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.catalogCache.operationsBlockedByRefresh.countDeletes \nThe cumulative number of delete operations ran on a mongos that were blocked at any point in their lifetime by a refresh of the catalog cache. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.catalogCache.operationsBlockedByRefresh.countCommands \nThe cumulative number of command operations ran on a mongos that were blocked at any point in their lifetime by a refresh of the catalog cache. Only present when run on a mongos. New in version 4.2.7. shardingStatistics.resharding \nA document with statistics about resharding operations. Each shard returns its own resharding operation statistics. If a shard is not involved in a resharding operation, then that shard will not contain statistics about the resharding operation. Only present when run on a shard or config server. New in version 5.0. shardingStatistics.resharding.countReshardingOperations \nThe sum of countReshardingSuccessful, countReshardingFailures, and countReshardingCanceled. The sum is further incremented by 1 if a resharding operation has started but has not yet completed. Sum is set to 0 when mongod is started or restarted. Only present when run on a shard or config server. New in version 5.0. shardingStatistics.resharding.countReshardingSuccessful \nNumber of successful resharding operations. Number is set to 0 when mongod is started or restarted. Only present when run on a shard or config server. New in version 5.0. shardingStatistics.resharding.countReshardingFailures \nNumber of failed resharding operations. Number is set to 0 when mongod is started or restarted. Only present when run on a shard or config server. New in version 5.0. shardingStatistics.resharding.countReshardingCanceled \nNumber of canceled resharding operations. Number is set to 0 when mongod is started or restarted. Only present when run on a shard or config server. New in version 5.0. shardingStatistics.resharding.totalOperationTimeElapsedMillis \nTotal elapsed time, in milliseconds, for the current resharding operation. Time is set to 0 when a new resharding operation starts.\n\n New in version 5.0. shardingStatistics.resharding.remainingOperationTimeEstimatedMillis \nEstimated remaining time, in milliseconds, for the current resharding operation. Time is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.approxDocumentsToCopy \nApproximate number of documents to copy from donor shards to recipient shards for the current resharding operation. Number is an estimate that is set when the resharding operation starts and the number is not updated as the operation progresses. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.documentsCopied \nNumber of documents copied from donor shards to recipient shards for the current resharding operation. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.approxBytesToCopy \nApproximate number of bytes to copy from donor shards to recipient shards for the current resharding operation. Number is an estimate that is set when the resharding operation starts and the number is not updated as the operation progresses. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.bytesCopied \nNumber of bytes copied from donor shards to recipient shards for the current resharding operation. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.totalCopyTimeElapsedMillis \nTotal elapsed time, in milliseconds, for ongoing data copy tasks from donor shards to recipient shards for the current resharding operation. Time is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.oplogEntriesFetched \nNumber of entries fetched from the oplog for the current resharding operation. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. shardingStatistics.resharding.oplogEntriesApplied \nNumber of entries applied to the oplog for the current resharding operation. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.totalApplyTimeElapsedMillis \nTotal elapsed time, in milliseconds, for the apply step of the current resharding operation. In the apply step, recipient shards modify their data based on new incoming writes from donor shards. Time is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.countWritesDuringCriticalSection \nNumber of writes perfomed in the critical section for the current resharding operation. The critical section prevents new incoming writes to the collection currently being resharded. Number is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.totalCriticalSectionTimeElapsedMillis \nTotal elapsed time, in milliseconds, for the critical section of the current resharding operation. The critical section prevents new incoming writes to the collection currently being resharded. Time is set to 0 when a new resharding operation starts. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.donorState \nState of the donor shard for the current resharding operation. Number is set to 0 when a new resharding operation starts.\n\n Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.resharding.recipientState \nState of the recipient shard for the current resharding operation. Number is set to 0 when a new resharding operation starts. Number Returned\nMeaning\nDescription\n0\nunused\nShard is not a recipient in the current resharding operation.\n1\nawaiting-fetch-timestamp\nThe recipient shard is waiting for the donor shards to be prepared to donate their data\n2\ncreating-collection\nThe recipient shard is creating the new sharded collection.\n3\ncloning\nThe recipient shard is receiving data from the donor shards.\n4\napplying\nThe recipient shard is applying oplog entries to modify its copy of the data based on the new incoming writes from donor shards.\n5\nerror\nAn error occurred during the resharding operation.\n6\nstrict-consistency\nThe recipient shard has all data changes stored in a temporary collection.\n7\ndone\nThe resharding operation is complete. Only present when run on a shard or config server. Returns 0 on a config server. New in version 5.0. shardingStatistics.numHostsTargeted \nIndicates the number of shards targeted for CRUD operations and aggregation commands. When a CRUD operation or aggregation command is run, the following metrics will be incremented. Name\nDescription\nallShards\nA command targeted all shards\nmanyShards\nA command targeted more than one shard\noneShard\nA command targeted one shard\nunsharded\nA command was run on an unsharded collection \nNOTE Running the serverStatus command on mongos will provide insight into the CRUD and aggregation operations that run on a sharded cluster. Multi-shard operations can either be scatter-gather or shard specific. Multi-shard scatter-gather operations can consume more resources. By using the shardingStatistics.numHostsTargeted metrics you can tune the aggregation queries that run on a sharded cluster. shardingStatistics.resharding.coordinatorState \nState of the resharding coordinator for the current resharding operation. The resharding coordinator is a thread that runs on the config server primary. Number is set to 0 when a new resharding operation starts. Number Returned\nMeaning\nDescription\n0\nunused\nThe shard is not the coordinator in the current resharding operation.\n1\ninitializing\nThe resharding coordinator has inserted the coordinator document into config.reshardingOperations and has added the reshardingFields to the config.collections entry for the original collection.\n2\npreparing-to-donate The resharding coordinator  * has created a config.collections entry for the temporary resharding collection.  * has inserted entries into config.chunks for ranges based on the new shard key.  * has inserted entries into config.tags for any zones associated with the new shard key. The coordinator informs participant shards to begin the resharding operation. The coordinator then waits until all donor shards have picked a minFetchTimestamp and are ready to donate. 3\ncloning\nThe resharding coordinator informs donor shards to donate data to recipient shards. The coordinator waits for all recipients to finish cloning the data from the donor.\n4\napplying\nThe resharding coordinator informs recipient shards to modify their copies of data based on new incoming writes from donor shards. The coordinator waits for all recipients to finish applying oplog entries.\n5\nblocking-writes\nThe resharding coordinator informs donor shards to prevent new incoming write operations to the collection being resharded. The coordinator then waits for all recipients to have all data changes.\n6\naborting\nAn unrecoverable error occurred during the resharding operation or the abortReshardCollection command (or the sh.abortReshardCollection() method) was run.\n6\ncommitting\nThe resharding coordinator removes the config.collections entry for the temporary resharding collection. The coordinator then adds the recipientFields to the source collection's entry. Only present when run on a shard or config server. New in version 5.0. shardingStatistics.resharding.opStatus \nStatus for the current resharding operation. Number Returned\nDescription\n-1\nResharding operation not in progress.\n0\nResharding operation succeeded.\n1\nResharding operation failed.\n2\nResharding operation canceled. Only present when run on a shard or config server. New in version 5.0.\n\n \nThis field contains the highest numeric difference for (maxNumChunksInShard - minNumChunksInShard) among all zones for the collection that was processed by the most recent resharding operation. See Range Size. Only updated on config servers. New in version 5.0. \nSHARDEDINDEXCONSISTENCY \n\"shardedIndexConsistency\" : {   \"numShardedCollectionsWithInconsistentIndexes\" : NumberLong(<num>)}, \nshardedIndexConsistency \nAvailable only on config server instances. A document that returns results of index consistency checks for sharded collections. The returned metrics are meaningful only when run on the primary of the config server replica set for a version 4.4+ (and 4.2.6+) sharded cluster. \nTIP \nSEE ALSO:  * enableShardedIndexConsistencyCheck parameter  * shardedIndexConsistencyCheckIntervalMS parameter New in version 4.4. (and 4.2.6) shardedIndexConsistency.numShardedCollectionsWithInconsistentIndexes \nAvailable only on config server instances. Number of sharded collections whose indexes are inconsistent across the shards. A sharded collection has an inconsistent index if the collection does not have the exact same indexes (including the index options) on each shard that contains chunks for the collection. To investigate if a sharded collection has inconsistent indexes, see Find Inconsistent Indexes Across Shards. The returned metrics are meaningful only when run on the primary of the config server replica set for a version 4.4+ (and 4.2.6+) sharded cluster. \nTIP \nSEE ALSO:  * enableShardedIndexConsistencyCheck parameter  * shardedIndexConsistencyCheckIntervalMS parameter New in version 4.4. (and 4.2.6) \nSTORAGEENGINE \n\"storageEngine\" : {   \"name\" : <string>,   \"supportsCommittedReads\" : <boolean>,   \"persistent\" : <boolean>}, \nstorageEngine \nA document with data about the current storage engine. storageEngine.name \nThe name of the current storage engine. storageEngine.supportsCommittedReads \nA boolean that indicates whether the storage engine supports \"majority\" read concern. storageEngine.persistent \nA boolean that indicates whether the storage engine does or does not persist data to disk. \nTRANSACTIONS  \ntransactions \nWhen run on a mongod, a document with data about the retryable writes and transactions. When run on a mongos, a document with data about the transactions run on the instance. transactions.retriedCommandsCount \nAvailable on mongod only. The total number of retry attempts that have been received after the corresponding retryable write command has already been committed. That is, a retryable write is attempted even though the write has previously succeeded and has an associated record for the transaction and session in the config.transactions collection, such as when the initial write response to the client is lost. \nNOTE MongoDB does not re-execute the committed writes. The total is across all sessions. The total does not include any retryable writes that may happen internally as part of a chunk migration. transactions.retriedStatementsCount \nAvailable on mongod only. The total number of write statements associated with the retried commands in transactions.retriedCommandsCount. \nNOTE MongoDB does not re-execute the committed writes. The total does not include any retryable writes that may happen internally as part of a chunk migration. transactions.transactionsCollectionWriteCount \nAvailable on mongod only. The total number of writes to the config.transactions collection, triggered when a new retryable write statement is committed. For update and delete commands, since only single document operations are retryable, there is one write per statement. For insert operations, there is one write per batch of documents inserted, except when a failure leads to each document being inserted separately. The total includes writes to a server's config.transactions collection that occur as part of a migration. transactions.currentActive \nChanged in version 4.2.1: This field is also available on mongos. The total number of open transactions currently executing a command. transactions.currentInactive \nChanged in version 4.2.1: This field is also available on mongos. The total number of open transactions that are not currently executing a command. transactions.currentOpen \nChanged in version 4.2.1: This field is also available on mongos.\n\n transactions.totalAborted \nFor the mongod, the total number of transactions aborted on this instance since its last startup. For the mongos, the total number of transactions aborted through this instance since its last startup. transactions.totalCommitted \nFor the mongod, the total number of transactions committed on the instance since its last startup. For the mongos,the total number of transactions committed through this instance since its last startup. transactions.totalStarted \nFor the mongod, the total number of transactions started on this instance since its last startup. For the mongos, the total number of transactions started on this instance since its last startup. transactions.abortCause \nAvailable on mongos only. Breakdown of the transactions.totalAborted by cause. If a client issues an explicit abortTransaction, the cause is listed as abort. For example: \"totalAborted\" : NumberLong(5),\"abortCause\" : {   \"abort\" : NumberLong(1),   \"DuplicateKey\" : NumberLong(1),   \"StaleConfig\" : NumberLong(3),   \"SnapshotTooOld\" : NumberLong(1)},  New in version 4.2. transactions.totalContactedParticipants \nAvailable on mongos only. The total number of shards contacted for all transactions started through this mongos since its last startup. The number of shards contacted during the transaction processes can include those shards that may not be included as part of the commit. New in version 4.2. transactions.totalParticipantsAtCommit \nAvailable on mongos only. Total number of shards involved in the commit for all transactions started through this mongos since its last startup. New in version 4.2. transactions.totalRequestsTargeted \nAvailable on mongos only. Total number of network requests targeted by the mongos as part of its transactions. New in version 4.2. transactions.commitTypes \nAvailable on mongos only. Breakdown of the commits by types. For example: \"noShards\" : {   \"initiated\" : NumberLong(0),   \"successful\" : NumberLong(0),   \"successfulDurationMicros\" : NumberLong(0)},\"singleShard\" : {   \"initiated\" : NumberLong(5),   \"successful\" : NumberLong(5),   \"successfulDurationMicros\" : NumberLong(203118)},\"singleWriteShard\" : {   \"initiated\" : NumberLong(0),   \"successful\" : NumberLong(0),   \"successfulDurationMicros\" : NumberLong(0)},\"readOnly\" : {   \"initiated\" : NumberLong(0),   \"successful\" : NumberLong(0),   \"successfulDurationMicros\" : NumberLong(0)},\"twoPhaseCommit\" : {   \"initiated\" : NumberLong(1),   \"successful\" : NumberLong(1),   \"successfulDurationMicros\" : NumberLong(179616)},\"recoverWithToken\" : {   \"initiated\" : NumberLong(0),   \"successful\" : NumberLong(0),   \"successfulDurationMicros\" : NumberLong(0)} The types of commit are: Type\nDescription\nnoShards\nCommits of transactions that did not contact any shards.\nsingleShard\nCommits of transactions that affected a single shard.\nsingleWriteShard\nCommits of transactions that contacted multiple shards but whose write operations only affected a single shard.\nreadOnly\nCommits of transactions that only involved read operations.\ntwoPhaseCommit\nCommits of transactions that included writes to multiple shards\nrecoverWithToken\nCommits that recovered the outcome of transactions from another instance or after this instance was restarted. For each commit type, the command returns the following metrics: Metrics\nDescription\ninitiated\nTotal number of times that commits of this type were initiated.\nsuccessful\nTotal number of times that commits of this type succeeded.\nsuccessfulDurationMicros\nTotal time, in microseconds, taken by successful commits of this type. New in version 4.2. transactions.totalPrepared \nAvailable on mongod only. The total number of transactions in prepared state on this server since the mongod process's last startup. New in version 4.2. transactions.totalPreparedThenCommitted \nAvailable on mongod only. The total number of transactions that were prepared and committed on this server since the mongod process's last startup.\n\n transactions.totalPreparedThenAborted \nAvailable on mongod only. The total number of transactions that were prepared and aborted on this server since the mongod process's last startup. New in version 4.2. transactions.currentPrepared \nAvailable on mongod only. The current number of transactions in prepared state on this server. New in version 4.2. transactions.lastCommittedTransaction \nAvailable on mongod only. The details of the last transaction committed when the mongod is primary. When returned from a secondary, lastCommittedTransaction returns the details of the last transaction committed when that secondary was a primary. \"lastCommittedTransaction\" : {   \"operationCount\" : NumberLong(1),   \"oplogOperationBytes\" : NumberLong(211),   \"writeConcern\" : {      \"w\" : \"majority\",      \"wtimeout\" : 0   }} \nMetrics\nDescription\noperationCount\nThe number of write operations in the transaction.\noplogOperationBytes\nThe size of the corresponding oplog entry or entries for the transaction. [2]\nwriteConcern\nThe write concern used for the transaction. New in version 4.2.2: (Also available in 4.0.9) [2] Starting in version 4.2, MongoDB creates as many oplog entries as necessary to encapsulate all write operations in a transaction whereas in version 4.0, the write operations in a transaction must fit within a single oplog entry. See Oplog Size Limit for details. \nTRANSPORTSECURITY \n\"transportSecurity\" : {   \"1.0\" : NumberLong(<num>),   \"1.1\" : NumberLong(<num>),   \"1.2\" : NumberLong(<num>),   \"1.3\" : NumberLong(<num>),   \"unknown\" : NumberLong(<num>)}, \ntransportSecurity.<version> \nThe cumulative number of TLS <version> connections that have been made to this mongod or mongos instance. The value is reset upon restart. \nWATCHDOG \n\"watchdog\" : {   \"checkGeneration\" : NumberLong(<num>),   \"monitorGeneration\" : NumberLong(<num>),   \"monitorPeriod\" : <num>}  \nNOTE The watchdog section is only present if the Storage Node Watchdog is enabled. watchdog \nA document reporting the status of the Storage Node Watchdog. watchdog.checkGeneration \nThe number of times the directories have been checked since startup. Directories are checked multiple times every monitoringPeriod. watchdog.monitorGeneration \nThe number of times the status of all filesystems used by mongod has been examined. This is incremented once every monitoringPeriod. watchdog.monitorPeriod \nThe value set by watchdogPeriodSeconds. This is the period between status checks. \nWIREDTIGER \nwiredTiger information only appears if using the WiredTiger storage engine. Some of the statistics roll up for the server.\n\n  \nNOTE The following is not an exhaustive list. wiredTiger.uri \nA string. For internal use by MongoDB. wiredTiger.async \nA document that returns statistics related to the asynchronous operations API. This is unused by MongoDB. wiredTiger.block-manager \nA document that returns statistics on the block manager operations. wiredTiger.cache \nA document that returns statistics on the cache and page evictions from the cache. The following describes some of the key wiredTiger.cache statistics: wiredTiger.cache.maximum bytes configured \nMaximum cache size. wiredTiger.cache.bytes currently in the cache \nSize in byte of the data currently in cache. This value should not be greater than the maximum bytes configured value. wiredTiger.cache.unmodified pages evicted \nMain statistics for page eviction. wiredTiger.cache.tracked dirty bytes in the cache \nSize in bytes of the dirty data in the cache. This value should be less than the bytes currently in the cache value. wiredTiger.cache.pages read into cache \nNumber of pages read into the cache. wiredTiger.cache.pages read into cache with the wiredTiger.cache.pages written from cache can provide an overview of the I/O activity. wiredTiger.cache.pages written from cache \nNumber of pages written from the cache. wiredTiger.cache.pages written from cache with the wiredTiger.cache.pages read into cache can provide an overview of the I/O activity. To adjust the size of the WiredTiger internal cache, see storage.wiredTiger.engineConfig.cacheSizeGB and --wiredTigerCacheSizeGB. Avoid increasing the WiredTiger internal cache size above its default value. wiredTiger.connection \nA document that returns statistics related to WiredTiger connections. wiredTiger.cursor \nA document that returns statistics on WiredTiger cursor. wiredTiger.data-handle \nA document that returns statistics on the data handles and sweeps. wiredTiger.log \nA document that returns statistics on WiredTiger's write ahead log (i.e. the journal). \nTIP \nSEE ALSO: Journaling and the WiredTiger Storage Engine wiredTiger.reconciliation \nA document that returns statistics on the reconciliation process. wiredTiger.session \nA document that returns the open cursor count and open session count for the session. wiredTiger.thread-yield \nA document that returns statistics on yields during page acquisitions. wiredTiger.transaction \nA document that returns statistics on transaction checkpoints and operations. wiredTiger.transaction.transaction checkpoint most recent time (msecs) \nAmount of time, in milliseconds, to create the most recent checkpoint. An increase in this value under stead write load may indicate saturation on the I/O subsystem. wiredTiger.concurrentTransactions \nA document that returns information on the number of concurrent of read and write transactions allowed into the WiredTiger storage engine. These settings are MongoDB-specific. To change the settings for concurrent reads and write transactions, see wiredTigerConcurrentReadTransactions and wiredTigerConcurrentWriteTransactions. \nWRITEBACKSQUEUED \n\"writeBacksQueued\" : <boolean>, \nwriteBacksQueued \nA boolean that indicates whether there are operations from a mongos instance queued for retrying. Typically, this value is false. See also writeBacks. ←  profileshardConnPoolStats → On this page  * Definition\n * Syntax\n * Behavior\n * Initialization\n * Include mirroredReads\n * Include latchAnalysis\n * Change tcmalloc Verbosity\n * Output\n * asserts\n * bucketCatalog\n * connections\n * defaultRWConcern\n * electionMetrics\n * extra_info\n * flowControl\n * freeMonitoring\n * globalLock\n * hedgingMetrics\n * indexBulkBuilder\n * Instance Information\n * latchAnalysis\n * locks\n * logicalSessionRecordCache\n * mem\n * metrics\n * mirroredReads\n * network\n * opLatencies\n * opReadConcernCounters\n * opWriteConcernCounters\n * opcounters\n * opcountersRepl\n * oplogTruncation\n * readConcernCounters\n * repl\n * security\n * sharding\n * shardingStatistics\n * shardedIndexConsistency\n * storageEngine\n * transactions\n * transportSecurity\n * watchdog\n * wiredTiger\n * writeBacksQueued Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/tan/": " Docs Home → MongoDB Manual \n$TAN (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $tan \nNew in version 4.2. Returns the tangent of a value that is measured in radians. $tan has the following syntax: { $tan: <expression> }  $tan takes any valid expression that resolves to a number. If the expression returns a value in degrees, use the $degreesToRadians operator to convert the result to radians. By default $tan returns values as a double. $tan can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $tan returns null. If the argument resolves to NaN, $tan returns NaN. If the argument resolves to negative or positive infinity, $tan throws an error. Example\nResults\n{ $tan: NaN }\nNaN\n{ $tan: null }\nnull { $tan : Infinity} or { $tan : -Infinity } Throws an error message resembling the following formatted output: \"errmsg\" :  \"Failed to optimize pipeline :: caused by :: cannot  apply $tan to -inf, value must in (-inf,inf)\" \nEXAMPLE  \n←  $switch (aggregation)$tanh (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/": " Docs Home → MongoDB Manual \nAGGREGATION PIPELINE OPERATORS  NOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. \nEXPRESSION OPERATORS \nThese expression operators are available to construct expressions for use in the aggregation pipeline stages. Operator expressions are similar to functions that take arguments. In general, these expressions take an array of arguments and have the following form: { <operator>: [ <argument1>, <argument2> ... ] }  If operator accepts a single argument, you can omit the outer array designating the argument list: { <operator>: <argument> }  To avoid parsing ambiguity if the argument is a literal array, you must wrap the literal array in a $literal expression or keep the outer array that designates the argument list. \nARITHMETIC EXPRESSION OPERATORS \nArithmetic expressions perform mathematic operations on numbers. Some arithmetic expressions can also support date arithmetic. Name\nDescription\n$abs\nReturns the absolute value of a number.\n$add\nAdds numbers to return the sum, or adds numbers and a date to return a new date. If adding numbers and a date, treats the numbers as milliseconds. Accepts any number of argument expressions, but at most, one expression can resolve to a date.\n$ceil\nReturns the smallest integer greater than or equal to the specified number.\n$divide\nReturns the result of dividing the first number by the second. Accepts two argument expressions.\n$exp\nRaises e to the specified exponent.\n$floor\nReturns the largest integer less than or equal to the specified number.\n$ln\nCalculates the natural log of a number.\n$log\nCalculates the log of a number in the specified base.\n$log10\nCalculates the log base 10 of a number.\n$mod\nReturns the remainder of the first number divided by the second. Accepts two argument expressions.\n$multiply\nMultiplies numbers to return the product. Accepts any number of argument expressions.\n$pow\nRaises a number to the specified exponent.\n$round\nRounds a number to to a whole integer or to a specified decimal place.\n$sqrt\nCalculates the square root.\n$subtract\nReturns the result of subtracting the second value from the first. If the two values are numbers, return the difference. If the two values are dates, return the difference in milliseconds. If the two values are a date and a number in milliseconds, return the resulting date. Accepts two argument expressions. If the two values are a date and a number, specify the date argument first as it is not meaningful to subtract a date from a number.\n$trunc\nTruncates a number to a whole integer or to a specified decimal place. \nARRAY EXPRESSION OPERATORS\n\n \nBOOLEAN EXPRESSION OPERATORS \nBoolean expressions evaluate their argument expressions as booleans and return a boolean as the result. In addition to the false boolean value, Boolean expression evaluates as false the following: null, 0, and undefined values. The Boolean expression evaluates all other values as true, including non-zero numeric values and arrays. Name\nDescription\n$and\nReturns true only when all its expressions evaluate to true. Accepts any number of argument expressions.\n$not\nReturns the boolean value that is the opposite of its argument expression. Accepts a single argument expression.\n$or\nReturns true when any of its expressions evaluates to true. Accepts any number of argument expressions. \nCOMPARISON EXPRESSION OPERATORS \nComparison expressions return a boolean except for $cmp which returns a number. The comparison expressions take two argument expressions and compare both value and type, using the specified BSON comparison order for values of different types. Name\nDescription\n$cmp\nReturns 0 if the two values are equivalent, 1 if the first value is greater than the second, and -1 if the first value is less than the second.\n$eq\nReturns true if the values are equivalent.\n$gt\nReturns true if the first value is greater than the second.\n$gte\nReturns true if the first value is greater than or equal to the second.\n$lt\nReturns true if the first value is less than the second.\n$lte\nReturns true if the first value is less than or equal to the second.\n$ne\nReturns true if the values are not equivalent. \nCONDITIONAL EXPRESSION OPERATORS \nName\nDescription\n$cond\nA ternary operator that evaluates one expression, and depending on the result, returns the value of one of the other two expressions. Accepts either three expressions in an ordered list or three named parameters.\n$ifNull\nReturns either the non-null result of the first expression or the result of the second expression if the first expression results in a null result. Null result encompasses instances of undefined values or missing fields. Accepts two expressions as arguments. The result of the second expression can be null.\n$switch\nEvaluates a series of case expressions. When it finds an expression which evaluates to true, $switch executes a specified expression and breaks out of the control flow. \nCUSTOM AGGREGATION EXPRESSION OPERATORS \nName\nDescription\n$accumulator Defines a custom accumulator function. New in version 4.4. $function Defines a custom function. New in version 4.4. \nDATA SIZE OPERATORS \nThe following operators return the size of a data element: Name\nDescription\n$binarySize\nReturns the size of a given string or binary data value's content in bytes.\n$bsonSize\nReturns the size in bytes of a given document (i.e. bsontype Object) when encoded as BSON. \nDATE EXPRESSION OPERATORS \nThe following operators returns date objects or components of a date object:\n\n Converts value to a Date. New in version 4.0. $week\nReturns the week number for a date as a number between 0 (the partial week that precedes the first Sunday of the year) and 53 (leap year).\n$year\nReturns the year for a date as a number (e.g. 2014). The following arithmetic operators can take date operands: Name\nDescription\n$add\nAdds numbers and a date to return a new date. If adding numbers and a date, treats the numbers as milliseconds. Accepts any number of argument expressions, but at most, one expression can resolve to a date.\n$subtract\nReturns the result of subtracting the second value from the first. If the two values are dates, return the difference in milliseconds. If the two values are a date and a number in milliseconds, return the resulting date. Accepts two argument expressions. If the two values are a date and a number, specify the date argument first as it is not meaningful to subtract a date from a number. \nLITERAL EXPRESSION OPERATOR \nName\nDescription\n$literal\nReturn a value without parsing. Use for values that the aggregation pipeline may interpret as an expression. For example, use a $literal expression to a string that starts with a dollar sign ($) to avoid parsing as a field path. \nMISCELLANEOUS OPERATORS \nName\nDescription\n$getField Returns the value of a specified field from a document. You can use $getField to retrieve the value of fields with names that contain periods (.) or start with dollar signs ($). New in version 5.0. $rand\nReturns a random float between 0 and 1\n$sampleRate\nRandomly select documents at a given rate. Although the exact number of documents selected varies on each run, the quantity chosen approximates the sample rate expressed as a percentage of the total number of documents. \nOBJECT EXPRESSION OPERATORS \nName\nDescription\n$mergeObjects\nCombines multiple documents into a single document.\n$objectToArray\nConverts a document to an array of documents representing key-value pairs.\n$setField Adds, updates, or removes a specified field in a document. You can use $setField to add, update, or remove fields with names that contain periods (.) or start with dollar signs ($). New in version 5.0. \nSET EXPRESSION OPERATORS \nSet expressions performs set operation on arrays, treating arrays as sets. Set expressions ignores the duplicate entries in each input array and the order of the elements. If the set operation returns a set, the operation filters out duplicates in the result to output an array that contains only unique entries. The order of the elements in the output array is unspecified. If a set contains a nested array element, the set expression does not descend into the nested array but evaluates the array at top-level. Name\nDescription\n$allElementsTrue\nReturns true if no element of a set evaluates to false, otherwise, returns false. Accepts a single argument expression.\n$anyElementTrue\nReturns true if any elements of a set evaluate to true; otherwise, returns false. Accepts a single argument expression.\n$setDifference\nReturns a set with elements that appear in the first set but not in the second set; i.e. performs a relative complement of the second set relative to the first. Accepts exactly two argument expressions.\n$setEquals\nReturns true if the input sets have the same distinct elements. Accepts two or more argument expressions.\n$setIntersection\nReturns a set with elements that appear in all of the input sets. Accepts any number of argument expressions.\n$setIsSubset\nReturns true if all elements of the first set appear in the second set, including when the first set equals the second set; i.e. not a strict subset. Accepts exactly two argument expressions.\n$setUnion\nReturns a set with elements that appear in any of the input sets. \nSTRING EXPRESSION OPERATORS \nString expressions, with the exception of $concat, only have a well-defined behavior for strings of ASCII characters. $concat behavior is well-defined regardless of the characters used.\n\n Removes whitespace or the specified characters from the beginning of a string. New in version 4.0. $regexFind Applies a regular expression (regex) to a string and returns information on the first matched substring. New in version 4.2. $regexFindAll Applies a regular expression (regex) to a string and returns information on the all matched substrings. New in version 4.2. $regexMatch Applies a regular expression (regex) to a string and returns a boolean that indicates if a match is found or not. New in version 4.2. $replaceOne Replaces the first instance of a matched string in a given input. New in version 4.4. $replaceAll Replaces all instances of a matched string in a given input. New in version 4.4. $rtrim Removes whitespace or the specified characters from the end of a string. New in version 4.0. $split\nSplits a string into substrings based on a delimiter. Returns an array of substrings. If the delimiter is not found within the string, returns an array containing the original string.\n$strLenBytes\nReturns the number of UTF-8 encoded bytes in a string.\n$strLenCP\nReturns the number of UTF-8 code points in a string.\n$strcasecmp\nPerforms case-insensitive string comparison and returns: 0 if two strings are equivalent, 1 if the first string is greater than the second, and -1 if the first string is less than the second.\n$substr\nDeprecated. Use $substrBytes or $substrCP.\n$substrBytes\nReturns the substring of a string. Starts with the character at the specified UTF-8 byte index (zero-based) in the string and continues for the specified number of bytes.\n$substrCP\nReturns the substring of a string. Starts with the character at the specified UTF-8 code point (CP) index (zero-based) in the string and continues for the number of code points specified.\n$toLower\nConverts a string to lowercase. Accepts a single argument expression.\n$toString Converts value to a string. New in version 4.0. $trim Removes whitespace or the specified characters from the beginning and end of a string. New in version 4.0. $toUpper\nConverts a string to uppercase. Accepts a single argument expression. \nTEXT EXPRESSION OPERATOR \nName\nDescription\n$meta\nAccess available per-document metadata related to the aggregation operation. \nTIMESTAMP EXPRESSION OPERATORS \nTimestamp expression operators return values from a timestamp. Name\nDescription\n$tsIncrement Returns the incrementing ordinal from a timestamp as a long. New in version 5.1. $tsSecond Returns the seconds from a timestamp as a long. New in version 5.1. \nTRIGONOMETRY EXPRESSION OPERATORS \nTrigonometry expressions perform trigonometric operations on numbers. Values that represent angles are always input or output in radians. Use $degreesToRadians and $radiansToDegrees to convert between degree and radian measurements.\n\n \nTYPE EXPRESSION OPERATORS \nName\nDescription\n$convert Converts a value to a specified type. New in version 4.0. $isNumber Returns boolean true if the specified expression resolves to an integer, decimal, double, or long. Returns boolean false if the expression resolves to any other BSON type, null, or a missing field. New in version 4.4. $toBool Converts value to a boolean. New in version 4.0. $toDate Converts value to a Date. New in version 4.0. $toDecimal Converts value to a Decimal128. New in version 4.0. $toDouble Converts value to a double. New in version 4.0. $toInt Converts value to an integer. New in version 4.0. $toLong Converts value to a long. New in version 4.0. $toObjectId Converts value to an ObjectId. New in version 4.0. $toString Converts value to a string. New in version 4.0. $type\nReturn the BSON data type of the field. \nACCUMULATORS ($GROUP, $BUCKET, $BUCKETAUTO, $SETWINDOWFIELDS) \nAggregation accumulator operators:      * Maintain their state as documents progress through the aggregation pipeline.  * Return totals, maxima, minima, and other values.  * Can be used in these aggregation pipeline stages:\n   \n   * $bucket\n   \n   * $bucketAuto\n   \n   * $group\n   \n   * $setWindowFields starting in MongoDB 5.0 (except when you are using the $accumulator or $mergeObjects operators, which cannot be used with $setWindowFields) Changed in version 5.0. Name\nDescription\n$accumulator\nReturns the result of a user-defined accumulator function.\n$addToSet Returns an array of unique expression values for each group. Order of the array elements is undefined. Changed in version 5.0: Available in $setWindowFields stage. $avg Returns an average of numerical values. Ignores non-numeric values. Changed in version 5.0: Available in $setWindowFields stage. $bottom Returns the bottom element within a group according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $bottomN Returns an aggregation of the bottom n fields within a group, according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $count Returns the number of documents in a group. Distinct from the $count pipeline stage. New in version 5.0: Available in $group and $setWindowFields stages. $first Returns a value from the first document for each group. Order is only defined if the documents are sorted. Distinct from the $first array operator. Changed in version 5.0: Available in $setWindowFields stage. $firstN Returns an aggregation of the first n elements within a group. Only meaningful when documents are in a defined order. Distinct from the $firstN array operator. New in version 5.2: Available in $group, expression and $setWindowFields stages. $last Returns a value from the last document for each group. Order is only defined if the documents are sorted. Distinct from the $last array operator. Changed in version 5.0: Available in $setWindowFields stage. $lastN Returns an aggregation of the last n elements within a group. Only meaningful when documents are in a defined order. Distinct from the $lastN array operator. New in version 5.2: Available in $group, expression and $setWindowFields stages. $max Returns the highest expression value for each group. Changed in version 5.0: Available in $setWindowFields stage. $maxN Returns an aggregation of the n maximum valued elements in a group. Distinct from the $maxN array operator. New in version 5.2. Available in $group, $setWindowFields and as an expression. $mergeObjects\nReturns a document created by combining the input documents for each group.\n$min Returns the lowest expression value for each group. Changed in version 5.0: Available in $setWindowFields stage. $push Returns an array of expression values for documents in each group. Changed in version 5.0: Available in $setWindowFields stage. $stdDevPop Returns the population standard deviation of the input values.\n\n $stdDevSamp Returns the sample standard deviation of the input values. Changed in version 5.0: Available in $setWindowFields stage. $sum Returns a sum of numerical values. Ignores non-numeric values. Changed in version 5.0: Available in $setWindowFields stage. $top Returns the top element within a group according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $topN Returns an aggregation of the top n fields within a group, according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. \nACCUMULATORS (IN OTHER STAGES) \nSome operators that are available as accumulators for the $group stage are also available for use in other stages but not as accumulators. When used in these other stages, these operators do not maintain their state and can take as input either a single argument or multiple arguments. For details, refer to the specific operator page. Changed in version 5.0. The following accumulator operators are also available in the $project, $addFields, $set, and, starting in MongoDB 5.0, the $setWindowFields stages. Name\nDescription\n$avg\nReturns an average of the specified expression or list of expressions for each document. Ignores non-numeric values.\n$max\nReturns the maximum of the specified expression or list of expressions for each document\n$min\nReturns the minimum of the specified expression or list of expressions for each document\n$stdDevPop\nReturns the population standard deviation of the input values.\n$stdDevSamp\nReturns the sample standard deviation of the input values.\n$sum\nReturns a sum of numerical values. Ignores non-numeric values. \nVARIABLE EXPRESSION OPERATORS \nName\nDescription\n$let Defines variables for use within the scope of a subexpression and returns the result of the subexpression. Accepts named parameters. Accepts any number of argument expressions. \nWINDOW OPERATORS \nNew in version 5.0. Window operators return values from a defined span of documents from a collection, known as a window. A window is defined in the $setWindowFields stage, available starting in MongoDB 5.0. The following window operators are available in the $setWindowFields stage. Name\nDescription\n$addToSet Returns an array of all unique values that results from applying an expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $avg Returns the average for the specified expression. Ignores non-numeric values. Changed in version 5.0: Available in $setWindowFields stage. $bottom Returns the bottom element within a group according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $bottomN Returns an aggregation of the bottom n fields within a group, according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $count Returns the number of documents in the group or window. Distinct from the $count pipeline stage. New in version 5.0. $covariancePop Returns the population covariance of two numeric expressions. New in version 5.0. $covarianceSamp Returns the sample covariance of two numeric expressions. New in version 5.0. $denseRank Returns the document position (known as the rank) relative to other documents in the $setWindowFields stage partition. There are no gaps in the ranks. Ties receive the same rank. New in version 5.0. $derivative Returns the average rate of change within the specified window. New in version 5.0. $documentNumber Returns the position of a document (known as the document number) in the $setWindowFields stage partition. Ties result in different adjacent document numbers. New in version 5.0. $expMovingAvg Returns the exponential moving average for the numeric expression. New in version 5.0. $first Returns the value that results from applying an expression to the first document in a group or window. Changed in version 5.0: Available in $setWindowFields stage. $integral Returns the approximation of the area under a curve. New in version 5.0. $last Returns the value that results from applying an expression to the last document in a group or window. Changed in version 5.0: Available in $setWindowFields stage. $linearFill Fills null and missing fields in a window using linear interpolation based on surrounding field values. Available in $setWindowFields stage. New in version 5.3.\n\n Last observation carried forward. Sets values for null and missing fields in a window to the last non-null value for the field. Available in $setWindowFields stage. New in version 5.2. $max Returns the maximum value that results from applying an expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $min Returns the minimum value that results from applying an expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $minN Returns an aggregation of the n minimum valued elements in a group. Distinct from the $minN array operator. New in version 5.2. Available in $group, $setWindowFields and as an expression. $push Returns an array of values that result from applying an expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $rank Returns the document position (known as the rank) relative to other documents in the $setWindowFields stage partition. New in version 5.0. $shift Returns the value from an expression applied to a document in a specified position relative to the current document in the $setWindowFields stage partition. New in version 5.0. $stdDevPop Returns the population standard deviation that results from applying a numeric expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $stdDevSamp Returns the sample standard deviation that results from applying a numeric expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $sum Returns the sum that results from applying a numeric expression to each document. Changed in version 5.0: Available in $setWindowFields stage. $top Returns the top element within a group according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $topN Returns an aggregation of the top n fields within a group, according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. \nALPHABETICAL LISTING OF EXPRESSION OPERATORS \nName\nDescription\n$abs\nReturns the absolute value of a number.\n$accumulator\nReturns the result of a user-defined accumulator function.\n$acos\nReturns the inverse cosine (arc cosine) of a value in radians.\n$acosh\nReturns the inverse hyperbolic cosine (hyperbolic arc cosine) of a value in radians.\n$add\nAdds numbers to return the sum, or adds numbers and a date to return a new date. If adding numbers and a date, treats the numbers as milliseconds. Accepts any number of argument expressions, but at most, one expression can resolve to a date.\n$addToSet Returns an array of unique expression values for each group. Order of the array elements is undefined. Changed in version 5.0: Available in $setWindowFields stage. $allElementsTrue\nReturns true if no element of a set evaluates to false, otherwise, returns false. Accepts a single argument expression.\n$and\nReturns true only when all its expressions evaluate to true. Accepts any number of argument expressions.\n$anyElementTrue\nReturns true if any elements of a set evaluate to true; otherwise, returns false. Accepts a single argument expression.\n$arrayElemAt\nReturns the element at the specified array index.\n$arrayToObject\nConverts an array of key value pairs to a document.\n$asin\nReturns the inverse sine (arc sine) of a value in radians.\n$asinh\nReturns the inverse hyperbolic sin (hyperbolic arc sine) of a value in radians.\n$atan\nReturns the inverse tangent (arc tangent) of a value in radians.\n$atan2\nReturns the inverse tangent (arc tangent) of y / x in radians, where y and x are the first and second values passed to the expression respectively.\n$atanh\nReturns the inverse hyperbolic tangent (hyperbolic arc tangent) of a value in radians.\n$avg Returns an average of numerical values. Ignores non-numeric values. Changed in version 5.0: Available in $setWindowFields stage. $binarySize Returns the size of a given string or binary data value's content in bytes. New in version 4.4. $bottom Returns the bottom element within a group according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $bottomN\n\n New in version 5.2. Available in $group and $setWindowFields stages. $bsonSize Returns the size in bytes of a given document (i.e. bsontype Object) when encoded as BSON. New in version 4.4. $ceil\nReturns the smallest integer greater than or equal to the specified number.\n$cmp\nReturns: 0 if the two values are equivalent, 1 if the first value is greater than the second, and -1 if the first value is less than the second.\n$concat\nConcatenates any number of strings.\n$concatArrays\nConcatenates arrays to return the concatenated array.\n$cond\nA ternary operator that evaluates one expression, and depending on the result, returns the value of one of the other two expressions. Accepts either three expressions in an ordered list or three named parameters.\n$convert\nConverts a value to a specified type.\n$cos\nReturns the cosine of a value that is measured in radians.\n$cosh\nReturns the hyperbolic cosine of a value that is measured in radians.\n$count Returns the number of documents in the group or window. New in version 5.0: Available in $group and $setWindowFields stages. Distinct from the $count pipeline stage. $covariancePop Returns the population covariance of two numeric expressions. New in version 5.0: Available in $setWindowFields stage. $covarianceSamp Returns the sample covariance of two numeric expressions. New in version 5.0: Available in $setWindowFields stage. $dateAdd\nAdds a number of time units to a date object.\n$dateDiff\nReturns the difference between two dates.\n$dateFromParts\nConstructs a BSON Date object given the date's constituent parts.\n$dateSubtract\nSubtracts a number of time units from a date object.\n$dateTrunc\nTruncates a date.\n$dateToParts\nReturns a document containing the constituent parts of a date.\n$dateFromString\nReturns a date/time as a date object.\n$dateToString\nReturns the date as a formatted string.\n$dayOfMonth\nReturns the day of the month for a date as a number between 1 and 31.\n$dayOfWeek\nReturns the day of the week for a date as a number between 1 (Sunday) and 7 (Saturday).\n$dayOfYear\nReturns the day of the year for a date as a number between 1 and 366 (leap year).\n$degreesToRadians\nConverts a value from degrees to radians.\n$denseRank Returns the relative position of a sorted document. There are no gaps in the positions. New in version 5.0: Available in $setWindowFields stage. $derivative Returns the average rate of change within the specified window. New in version 5.0: Available in $setWindowFields stage. $divide\nReturns the result of dividing the first number by the second. Accepts two argument expressions.\n$documentNumber Returns the position of a document (known as the document number) in the partition. New in version 5.0: Available in $setWindowFields stage. $eq\nReturns true if the values are equivalent.\n$exp\nRaises e to the specified exponent.\n$expMovingAvg Returns the exponential moving average for the numeric expression. New in version 5.0: Available in $setWindowFields stage. $filter\nSelects a subset of the array to return an array with only the elements that match the filter condition.\n$first Returns a value from the first document for each group. Order is only defined if the documents are sorted. Changed in version 5.0: Available in $setWindowFields stage. Distinct from the $first array operator. $first Returns the first array element. New in version 4.4. Distinct from the $first accumulator. $firstN Returns a specified number of elements from the beginning of an array. Distinct from the $firstN accumulator. New in version 5.2. $firstN Returns an aggregation of the first n fields within a group. Only meaningful when documents are in a defined order. Distinct from the $firstN array operator. New in version 5.2. Available in $group, expression and $setWindowFields stages. $floor\nReturns the largest integer less than or equal to the specified number.\n$function Defines a custom aggregation function. New in version 4.4. $getField\n\n $gt\nReturns true if the first value is greater than the second.\n$gte\nReturns true if the first value is greater than or equal to the second.\n$hour\nReturns the hour for a date as a number between 0 and 23.\n$ifNull\nReturns either the non-null result of the first expression or the result of the second expression if the first expression results in a null result. Null result encompasses instances of undefined values or missing fields. Accepts two expressions as arguments. The result of the second expression can be null.\n$in\nReturns a boolean indicating whether a specified value is in an array.\n$indexOfArray\nSearches an array for an occurrence of a specified value and returns the array index of the first occurrence. If the substring is not found, returns -1.\n$indexOfBytes\nSearches a string for an occurrence of a substring and returns the UTF-8 byte index of the first occurrence. If the substring is not found, returns -1.\n$indexOfCP\nSearches a string for an occurrence of a substring and returns the UTF-8 code point index of the first occurrence. If the substring is not found, returns -1.\n$integral Returns the approximation of the area under a curve. New in version 5.0: Available in $setWindowFields stage. $isArray\nDetermines if the operand is an array. Returns a boolean.\n$isNumber\nDetermines if the expression resolves to an integer, double, decimal, or long.\n$isoDayOfWeek\nReturns the weekday number in ISO 8601 format, ranging from 1 (for Monday) to 7 (for Sunday).\n$isoWeek\nReturns the week number in ISO 8601 format, ranging from 1 to 53. Week numbers start at 1 with the week (Monday through Sunday) that contains the year's first Thursday.\n$isoWeekYear\nReturns the year number in ISO 8601 format. The year starts with the Monday of week 1 (ISO 8601) and ends with the Sunday of the last week (ISO 8601).\n$last Returns a value from the last document for each group. Order is only defined if the documents are sorted. Changed in version 5.0: Available in $setWindowFields stage. Distinct from the $last array operator. $last Returns the last array element. New in version 4.4. Distinct from the $last accumulator. $lastN Returns a specified number of elements from the end of an array. Distinct from the $lastN accumulator. New in version 5.2. $lastN Returns an aggregation of the last n fields within a group. Only meaningful when documents are in a defined order. Distinct from the $lastN array operator. New in version 5.2. Available in $group, expression and $setWindowFields stages. $let Defines variables for use within the scope of a subexpression and returns the result of the subexpression. Accepts named parameters. Accepts any number of argument expressions. $linearFill Fills null and missing fields in a window using linear interpolation based on surrounding field values. New in version 5.3. $literal\nReturn a value without parsing. Use for values that the aggregation pipeline may interpret as an expression. For example, use a $literal expression to a string that starts with a $ to avoid parsing as a field path.\n$ln\nCalculates the natural log of a number.\n$locf Last observation carried forward. Sets values for null and missing fields in a window to the last non-null value for the field. New in version 5.2. $log\nCalculates the log of a number in the specified base.\n$log10\nCalculates the log base 10 of a number.\n$lt\nReturns true if the first value is less than the second.\n$lte\nReturns true if the first value is less than or equal to the second.\n$ltrim\nRemoves whitespace or the specified characters from the beginning of a string.\n$map\nApplies a subexpression to each element of an array and returns the array of resulting values in order. Accepts named parameters.\n$max Returns the highest expression value for each group. Changed in version 5.0: Available in $setWindowFields stage. $maxN Returns the n largest values in an array. Distinct from the $maxN accumulator. New in version 5.2. $maxN Returns an aggregation of the n maximum valued elements in a group. Distinct from the $maxN array operator. New in version 5.2: Available in $group, $setWindowFields and as an expression.\n\n Returns the lowest expression value for each group. Changed in version 5.0: Available in $setWindowFields stage. $minN Returns the n smallest values in an array. Distinct from the $minN accumulator. New in version 5.2. $minN Returns an aggregation of the n minimum valued elements in a group. Distinct from the $minN array operator. New in version 5.2: Available in $group, $setWindowFields and as an expression. $millisecond\nReturns the milliseconds of a date as a number between 0 and 999.\n$minute\nReturns the minute for a date as a number between 0 and 59.\n$mod\nReturns the remainder of the first number divided by the second. Accepts two argument expressions.\n$month\nReturns the month for a date as a number between 1 (January) and 12 (December).\n$multiply\nMultiplies numbers to return the product. Accepts any number of argument expressions.\n$ne\nReturns true if the values are not equivalent.\n$not\nReturns the boolean value that is the opposite of its argument expression. Accepts a single argument expression.\n$objectToArray\nConverts a document to an array of documents representing key-value pairs.\n$or\nReturns true when any of its expressions evaluates to true. Accepts any number of argument expressions.\n$pow\nRaises a number to the specified exponent.\n$push Returns an array of expression values for documents in each group. Changed in version 5.0: Available in $setWindowFields stage. $radiansToDegrees\nConverts a value from radians to degrees.\n$rand\nReturns a random float between 0 and 1.\n$range\nOutputs an array containing a sequence of integers according to user-defined inputs.\n$rank Returns the document position (known as the rank) relative to other sorted documents. New in version 5.0: Available in $setWindowFields stage. $reduce\nApplies an expression to each element in an array and combines them into a single value.\n$regexFind\nApplies a regular expression (regex) to a string and returns information on the first matched substring.\n$regexFindAll\nApplies a regular expression (regex) to a string and returns information on the all matched substrings.\n$regexMatch\nApplies a regular expression (regex) to a string and returns a boolean that indicates if a match is found or not.\n$replaceOne Replaces the first instance of a matched string in a given input. New in version 4.4. $replaceAll Replaces all instances of a matched string in a given input. New in version 4.4. $reverseArray\nReturns an array with the elements in reverse order.\n$round\nRounds a number to a whole integer or to a specified decimal place.\n$rtrim\nRemoves whitespace or the specified characters from the end of a string.\n$sampleRate\nRandomly select documents at a given rate. Although the exact number of documents selected varies on each run, the quantity chosen approximates the sample rate expressed as a percentage of the total number of documents.\n$second\nReturns the seconds for a date as a number between 0 and 60 (leap seconds).\n$setDifference\nReturns a set with elements that appear in the first set but not in the second set; i.e. performs a relative complement of the second set relative to the first. Accepts exactly two argument expressions.\n$setEquals\nReturns true if the input sets have the same distinct elements. Accepts two or more argument expressions.\n$setField Adds, updates, or removes a specified field in a document. You can use $setField to add, update, or remove fields with names that contain periods (.) or start with dollar signs ($). New in version 5.0. $setIntersection\nReturns a set with elements that appear in all of the input sets. Accepts any number of argument expressions.\n$setIsSubset\nReturns true if all elements of the first set appear in the second set, including when the first set equals the second set; i.e. not a strict subset. Accepts exactly two argument expressions.\n$setUnion\nReturns a set with elements that appear in any of the input sets.\n$shift Returns the value from an expression applied to a document in a specified position relative to the current document in the output. New in version 5.0: Available in $setWindowFields stage.\n\n Returns the population standard deviation of the input values. Changed in version 5.0: Available in $setWindowFields stage. $stdDevSamp Returns the sample standard deviation of the input values. Changed in version 5.0: Available in $setWindowFields stage. $strcasecmp\nPerforms case-insensitive string comparison and returns: 0 if two strings are equivalent, 1 if the first string is greater than the second, and -1 if the first string is less than the second.\n$strLenBytes\nReturns the number of UTF-8 encoded bytes in a string.\n$strLenCP\nReturns the number of UTF-8 code points in a string.\n$substr\nDeprecated. Use $substrBytes or $substrCP.\n$substrBytes\nReturns the substring of a string. Starts with the character at the specified UTF-8 byte index (zero-based) in the string and continues for the specified number of bytes.\n$substrCP\nReturns the substring of a string. Starts with the character at the specified UTF-8 code point (CP) index (zero-based) in the string and continues for the number of code points specified.\n$subtract\nReturns the result of subtracting the second value from the first. If the two values are numbers, return the difference. If the two values are dates, return the difference in milliseconds. If the two values are a date and a number in milliseconds, return the resulting date. Accepts two argument expressions. If the two values are a date and a number, specify the date argument first as it is not meaningful to subtract a date from a number.\n$sum Returns a sum of numerical values. Ignores non-numeric values. Changed in version 5.0: Available in $setWindowFields stage. $switch\nEvaluates a series of case expressions. When it finds an expression which evaluates to true, $switch executes a specified expression and breaks out of the control flow.\n$tan\nReturns the tangent of a value that is measured in radians.\n$tanh\nReturns the hyperbolic tangent of a value that is measured in radians.\n$toBool\nConverts value to a boolean.\n$toDate\nConverts value to a Date.\n$toDecimal\nConverts value to a Decimal128.\n$toDouble\nConverts value to a double.\n$toInt\nConverts value to an integer.\n$toLong\nConverts value to a long.\n$toObjectId\nConverts value to an ObjectId.\n$top Returns the top element within a group according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $topN Returns an aggregation of the top n fields within a group, according to the specified sort order. New in version 5.2. Available in $group and $setWindowFields stages. $toString\nConverts value to a string.\n$toLower\nConverts a string to lowercase. Accepts a single argument expression.\n$toUpper\nConverts a string to uppercase. Accepts a single argument expression.\n$trim\nRemoves whitespace or the specified characters from the beginning and end of a string.\n$trunc\nTruncates a number to a whole integer or to a specified decimal place.\n$tsIncrement Returns the incrementing ordinal from a timestamp as a long. New in version 5.1. $tsSecond Returns the seconds from a timestamp as a long. New in version 5.1. $type\nReturn the BSON data type of the field.\n$unsetField Removes a specified field from a document. An alias for $setField to remove fields with names that contain periods (.) or that start with dollar signs ($). New in version 5.0. $week\nReturns the week number for a date as a number between 0 (the partial week that precedes the first Sunday of the year) and 53 (leap year).\n$year\nReturns the year for a date as a number (e.g. 2014).\n$zip\nMerge two arrays together. For the pipeline stages, see Aggregation Pipeline Stages. ←  $unwind (aggregation)$abs (aggregation) → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/indexStats/": " Docs Home → MongoDB Manual \n$INDEXSTATS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$indexStats \nReturns statistics regarding the use of each index for the collection. If running with access control, the user must have privileges that include indexStats action. The $indexStats stage takes an empty document and has the following syntax: { $indexStats: { } }  For each index, the return document includes the following fields: Output Field\nDescription\nname\nIndex name.\nkey Index key specification. See also: spec. host The hostname and port of the mongod process. accesses Statistics on the index use:      * ops is the number of operations that used the index.  * since is the time from which MongoDB gathered the statistics. shard The name of the shard associated with the host Only available for a sharded cluster. New in version 4.2.4. spec The full specfication document for the index, which includes the index key specification document. The index option hidden, available starting in MongoDB 4.4, is only included if the value is true. New in version 4.2.4. building Indicates if the index is currently being built. Only available if true. New in version 4.2.4. Statistics for an index will be reset on mongod restart or index drop and recreation. \nNOTE Prior to version 3.2.3, the ops field value did not include $match or mapReduce operations that use indexes. \nBEHAVIOR  ACCESSES FIELD \nThe statistics reported by the accesses field only includes index access driven by user requests. It does not include internal operations like deletion via TTL Indexes or chunk split and migration operations. \nRESTRICTIONS \n * $indexStats must be the first stage in an aggregation pipeline.  * $indexStats is not allowed in transactions. \nINDEX MODIFICATIONS RESETS STATISTICS \nModification of an existing index (see collMod command) resets the statistics for that index. \nEXAMPLE \nFor example, a collection orders contains the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 12, \"quantity\" : 2, \"type\": \"apparel\" }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"quantity\" : 1, \"type\": \"electronics\" }{ \"_id\" : 3, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 5, \"type\": \"apparel\" }  Create the following two indexes on the collection: db.orders.createIndex( { item: 1, quantity: 1 } )db.orders.createIndex( { type: 1, item: 1 } )  Run some queries against the collection: db.orders.find( { type: \"apparel\"} )db.orders.find( { item: \"abc\" } ).sort( { quantity: 1 } )  To view statistics on the index use on the orders collection, run the following aggregation operation: db.orders.aggregate( [ { $indexStats: { } } ] )  The operation returns a document that contains usage statistics for each index:\n\n ←  $group (aggregation)$limit (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/match/": " Docs Home → MongoDB Manual \n$MATCH (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples\n * Additional Information \nDEFINITION \n$match \nFilters the documents to pass only the documents that match the specified condition(s) to the next pipeline stage. The $match stage has the following prototype form: { $match: { <query> } }  $match takes a document that specifies the query conditions. The query syntax is identical to the read operation query syntax; i.e. $match does not accept raw aggregation expressions. Instead, use a $expr query expression to include aggregation expression in $match. \nBEHAVIOR  PIPELINE OPTIMIZATION \n     * Place the $match as early in the aggregation pipeline as possible. Because $match limits the total number of documents in the aggregation pipeline, earlier $match operations minimize the amount of processing down the pipe.  * If you place a $match at the very beginning of a pipeline, the query can take advantage of indexes like any other db.collection.find() or db.collection.findOne(). \nRESTRICTIONS \n * The $match query syntax is identical to the read operation query syntax; i.e. $match does not accept raw aggregation expressions. To include aggregation expression in $match, use a $expr query expression:\n   \n   { $match: { $expr: { <aggregation expression> } } }\n   \n     * You cannot use $where in $match queries as part of the aggregation pipeline.  * You cannot use $near or $nearSphere in $match queries as part of the aggregation pipeline. As an alternative, you can either:\n   \n   * Use $geoNear stage instead of the $match stage.\n   \n   * Use $geoWithin query operator with $center or $centerSphere in the $match stage.  * To use $text in the $match stage, the $match stage has to be the first stage of the pipeline.\n   \n   Views do not support text search. \nEXAMPLES \nThe examples use a collection named articles with the following documents: { \"_id\" : ObjectId(\"512bc95fe835e68f199c8686\"), \"author\" : \"dave\", \"score\" : 80, \"views\" : 100 }{ \"_id\" : ObjectId(\"512bc962e835e68f199c8687\"), \"author\" : \"dave\", \"score\" : 85, \"views\" : 521 }{ \"_id\" : ObjectId(\"55f5a192d4bede9ac365b257\"), \"author\" : \"ahn\", \"score\" : 60, \"views\" : 1000 }{ \"_id\" : ObjectId(\"55f5a192d4bede9ac365b258\"), \"author\" : \"li\", \"score\" : 55, \"views\" : 5000 }{ \"_id\" : ObjectId(\"55f5a1d3d4bede9ac365b259\"), \"author\" : \"annT\", \"score\" : 60, \"views\" : 50 }{ \"_id\" : ObjectId(\"55f5a1d3d4bede9ac365b25a\"), \"author\" : \"li\", \"score\" : 94, \"views\" : 999 }{ \"_id\" : ObjectId(\"55f5a1d3d4bede9ac365b25b\"), \"author\" : \"ty\", \"score\" : 95, \"views\" : 1000 }  \nEQUALITY MATCH \nThe following operation uses $match to perform a simple equality match: db.articles.aggregate(    [ { $match : { author : \"dave\" } } ]);  The $match selects the documents where the author field equals dave, and the aggregation returns the following: { \"_id\" : ObjectId(\"512bc95fe835e68f199c8686\"), \"author\" : \"dave\", \"score\" : 80, \"views\" : 100 }{ \"_id\" : ObjectId(\"512bc962e835e68f199c8687\"), \"author\" : \"dave\", \"score\" : 85, \"views\" : 521 }  \nPERFORM A COUNT \nThe following example selects documents to process using the $match pipeline operator and then pipes the results to the $group pipeline operator to compute a count of the documents:\n\n  In the aggregation pipeline, $match selects the documents where either the score is greater than 70 and less than 90 or the views is greater than or equal to 1000. These documents are then piped to the $group to perform a count. The aggregation returns the following: { \"_id\" : null, \"count\" : 5 }  \nADDITIONAL INFORMATION \nRefer to the following pages for more information and use cases on aggregation. \nFILTER DATA ON ATLAS USING ATLAS SEARCH \nFor your $search queries against data on your Atlas cluster, you can use the Atlas Search compound operator filter option to match or filter documents. Running $match after $search is less performant than running $search with the compound operator filter option. To learn more about the filter option, see compound. \nAGGREGATION TUTORIALS \n * Aggregation with the Zip Code Data Set  * Aggregation with User Preference Data ←  $lookup (aggregation)$merge (aggregation) → On this page  * Definition\n * Behavior\n * Examples\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/concatArrays/": " Docs Home → MongoDB Manual \n$CONCATARRAYS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$concatArrays \nConcatenates arrays to return the concatenated array. $concatArrays has the following syntax: { $concatArrays: [ <array1>, <array2>, ... ] }  The <array> expressions can be any valid expression as long as they resolve to an array. For more information on expressions, see Expressions. If any argument resolves to a value of null or refers to a field that is missing, $concatArrays returns null. \nBEHAVIOR \nExample\nResults { $concatArrays: [   [ \"hello\", \" \"], [ \"world\" ]] } [ \"hello\", \" \", \"world\" ] { $concatArrays: [   [ \"hello\", \" \"],   [ [ \"world\" ], \"again\"]] } [ \"hello\", \" \", [ \"world\" ], \"again\" ] \nEXAMPLE \nA collection named warehouses contains the following documents: { \"_id\" : 1, instock: [ \"chocolate\" ], ordered: [ \"butter\", \"apples\" ] }{ \"_id\" : 2, instock: [ \"apples\", \"pudding\", \"pie\" ] }{ \"_id\" : 3, instock: [ \"pears\", \"pecans\"], ordered: [ \"cherries\" ] }{ \"_id\" : 4, instock: [ \"ice cream\" ], ordered: [ ] }  The following example concatenates the instock and the ordered arrays: db.warehouses.aggregate([   { $project: { items: { $concatArrays: [ \"$instock\", \"$ordered\" ] } } }])  { \"_id\" : 1, \"items\" : [ \"chocolate\", \"butter\", \"apples\" ] }{ \"_id\" : 2, \"items\" : null }{ \"_id\" : 3, \"items\" : [ \"pears\", \"pecans\", \"cherries\" ] }{ \"_id\" : 4, \"items\" : [ \"ice cream\" ] }  \nTIP \nSEE ALSO: $push ←  $concat (aggregation)$cond (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/map/": " Docs Home → MongoDB Manual \n$MAP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Examples\n * Add to Each Element of an Array\n * Truncate Each Array Element\n * Convert Celsius Temperatures to Fahrenheit\n * Learn More \nDEFINITION \n$map \nApplies an expression to each item in an array and returns an array with the applied results. The $map expression has the following syntax: { $map: { input: <expression>, as: <string>, in: <expression> } } \nField\nSpecification\ninput\nAn expression that resolves to an array.\nas\nOptional. A name for the variable that represents each individual element of the input array. If no name is specified, the variable name defaults to this.\nin\nAn expression that is applied to each element of the input array. The expression references each element individually with the variable name specified in as. For more information on expressions, see Expressions. \nEXAMPLES  ADD TO EACH ELEMENT OF AN ARRAY \nIn mongosh, create a sample collection named grades with the following documents: db.grades.insertMany( [  { quizzes: [ 5, 6, 7 ] },  { quizzes: [ ] },  { quizzes: [ 3, 8, 9 ] }] )  The following aggregation operation uses $map with the $add expression to increment each element in the quizzes array by 2. db.grades.aggregate( [   {      $project: {         adjustedGrades: {           $map: {             input: \"$quizzes\",             as: \"grade\",             in: { $add: [ \"$$grade\", 2 ] }           }         }      }   } ] )  This operation returns the following results: [  {    _id: ObjectId(\"6390b8f7237da390c6869a62\"),    adjustedGrades: [ 7, 8, 9 ]  },  {    _id: ObjectId(\"6390b8f7237da390c6869a63\"),    adjustedGrades: []  },  {    _id: ObjectId(\"6390b8f7237da390c6869a64\"),    adjustedGrades: [ 5, 10, 11 ]  }] \nTRUNCATE EACH ARRAY ELEMENT \nIn mongosh, create a sample collection named deliveries with the following documents: db.deliveries.insertMany( [  {    \"city\" : \"Bakersfield\",    \"distances\" : [ 34.57, 81.96, 44.24 ]  },  {    \"city\" : \"Barstow\",    \"distances\" : [ 73.28, 9.67, 124.36 ]  },  {    \"city\" : \"San Bernadino\",    \"distances\" : [ 16.04, 3.25, 6.82 ]  }] )  The following aggregation operation uses $map to truncate each element in the distances array to its integer. db.deliveries.aggregate( [   {      $project: {         city: \"$city\",         integerValues: {            $map: {               input: \"$distances\",               as: \"decimalValue\",               in: { $trunc: \"$$decimalValue\" }            }         }      }   } ] )  This operation returns the following results:\n\n \nCONVERT CELSIUS TEMPERATURES TO FAHRENHEIT \nIn mongosh, create a sample collection named temperatures with the following documents: db.temperatures.insertMany( [  {    \"date\" : ISODate(\"2019-06-23\"),    \"tempsC\" : [ 4, 12, 17 ]  },  {    \"date\" : ISODate(\"2019-07-07\"),    \"tempsC\" : [ 14, 24, 11 ]  },  {    \"date\" : ISODate(\"2019-10-30\"),    \"tempsC\" : [ 18, 6, 8 ]  }] )  The following aggregation operation uses the $addFields stage to add a new field to the documents called tempsF which contains Fahrenheit equivalents of the elements in the tempsC array. To convert from Celsius to Fahrenheit, the operation uses $map to $multiply the Celsius values by 9/5 and then $add 32.  db.temperatures.aggregate( [   {      $addFields: {         \"tempsF\": {            $map: {               input: \"$tempsC\",               as: \"tempInCelsius\",               in: {                  $add: [ { $multiply: [ \"$$tempInCelsius\", 9/5 ] }, 32 ]               }            }          }      }    }] )  This operation returns the following results: [  {    _id: ObjectId(\"6390ba11237da390c6869a68\"),    date: ISODate(\"2019-06-23T00:00:00.000Z\"),    tempsC: [ 4, 12, 17 ],    tempsF: [ 39.2, 53.6, 62.6 ]  },  {    _id: ObjectId(\"6390ba11237da390c6869a69\"),    date: ISODate(\"2019-07-07T00:00:00.000Z\"),    tempsC: [ 14, 24, 11 ],    tempsF: [ 57.2, 75.2, 51.8 ]  },  {    _id: ObjectId(\"6390ba11237da390c6869a6a\"),    date: ISODate(\"2019-10-30T00:00:00.000Z\"),    tempsC: [ 18, 6, 8 ],    tempsF: [ 64.4, 42.8, 46.4 ]  }] \nLEARN MORE \nTo learn more about expressions used in the previous examples, see:      * $add  * $let  * $multiply ←  $ltrim (aggregation)$max (aggregation) → On this page  * Definition\n * Examples\n * Add to Each Element of an Array\n * Truncate Each Array Element\n * Convert Celsius Temperatures to Fahrenheit\n * Learn More Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/count/": " Docs Home → MongoDB Manual \n$COUNT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$count \nPasses a document to the next stage that contains a count of the number of documents input to the stage. \nNOTE \nDISAMBIGUATION This page describes the $count aggregation pipeline stage. For the $count aggregation accumulator, see $count (aggregation accumulator). $count has the following prototype form: { $count: <string> }  <string> is the name of the output field which has the count as its value. <string> must be a non-empty string, must not start with $ and must not contain the . character. \nTIP \nSEE ALSO:      * db.collection.countDocuments()  * $collStats  * db.collection.estimatedDocumentCount()  * count  * db.collection.count() \nBEHAVIOR \nThe $count stage is equivalent to the following $group + $project sequence: db.collection.aggregate( [   { $group: { _id: null, myCount: { $sum: 1 } } },   { $project: { _id: 0 } }] )  where myCount would be the output field that contains the count. You can specify another name for the output field. \nTIP \nSEE ALSO: db.collection.countDocuments() which wraps the $group aggregation stage with a $sum expression. \nEXAMPLE \nA collection named scores has the following documents: { \"_id\" : 1, \"subject\" : \"History\", \"score\" : 88 }{ \"_id\" : 2, \"subject\" : \"History\", \"score\" : 92 }{ \"_id\" : 3, \"subject\" : \"History\", \"score\" : 97 }{ \"_id\" : 4, \"subject\" : \"History\", \"score\" : 71 }{ \"_id\" : 5, \"subject\" : \"History\", \"score\" : 79 }{ \"_id\" : 6, \"subject\" : \"History\", \"score\" : 83 }  The following aggregation operation has two stages:  1. The $match stage excludes documents that have a score value of less than or equal to 80 to pass along the documents with score greater than 80 to the next stage.  2. The $count stage returns a count of the remaining documents in the aggregation pipeline and assigns the value to a field called passing_scores. db.scores.aggregate(  [    {      $match: {        score: {          $gt: 80        }      }    },    {      $count: \"passing_scores\"    }  ])  The operation returns the following results: { \"passing_scores\" : 4 } \n←  $collStats (aggregation)$currentOp (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/firstN-array-element/": " Docs Home → MongoDB Manual \n$FIRSTN (ARRAY OPERATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example \nDEFINITION \n$firstN \nNew in version 5.2. Returns a specified number of elements from the beginning of an array. \nTIP \nSEE ALSO:      * $lastN  * $sortArray \nSYNTAX \n$firstN has the following syntax: { $firstN: { n: <expression>, input: <expression> } } \nField\nDescription\nn\nAn expression that resolves to a positive integer. The integer specifies the number of array elements that $firstN returns.\ninput\nAn expression that resolves to the array from which to return n elements. \nBEHAVIOR \n * $firstN returns elements in the same order they appear in the input array.  * $firstN does not filter out null values in the input array.  * You cannot specify a value of n less than 1.  * If the specified n is greater than or equal to the number of elements in the input array, $firstN returns the input array.  * If input resolves to a non-array value, the aggregation operation errors. \nEXAMPLE \nThe collection games has the following documents: db.games.insertMany([    { \"playerId\" : 1, \"score\" : [ 1, 2, 3 ] },    { \"playerId\" : 2, \"score\" : [ 12, 90, 7, 89, 8 ] },    { \"playerId\" : 3, \"score\" : [ null ] },    { \"playerId\" : 4, \"score\" : [ ] },    { \"playerId\" : 5, \"score\" : [ 1293, null, 3489, 9 ]},    { \"playerId\" : 6, \"score\" : [ \"12.1\", 2, NumberLong(\"2090845886852\"), 23 ]}])  The following example uses the $firstN operator to retrieve the first three scores for each player. The scores are returned in the new field firstScores created by $addFields. db.games.aggregate([   { $addFields: { firstScores: { $firstN: { n: 3, input: \"$score\" } } } }])  The operation returns the following results: [{  \"playerId\": 1,  \"score\": [ 1, 2, 3 ],  \"firstScores\": [ 1, 2, 3 ]},{  \"playerId\": 2,  \"score\": [ 12, 90, 7, 89, 8 ],  \"firstScores\": [ 12, 90, 7 ]},{  \"playerId\": 3,  \"score\": [ null ],  \"firstScores\": [ null ]},{  \"playerId\": 4,  \"score\": [ ],  \"firstScores\": [ ]},{  \"playerId\": 5,  \"score\": [ 1293, null, 3489, 9 ],  \"firstScores\": [ 1293, null, 3489 ]},{  \"playerId\": 6,  \"score\": [ \"12.1\", 2, NumberLong(\"2090845886852\"), 23 ],  \"firstScores\": [ \"12.1\", 2, NumberLong(\"2090845886852\") ] }] \n←  $first (array operator)$floor (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dateToParts/": " Docs Home → MongoDB Manual \n$DATETOPARTS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$dateToParts \nReturns a document that contains the constituent parts of a given BSON Date value as individual properties. The properties returned are year, month, day, hour, minute, second and millisecond. You can set the iso8601 property to true to return the parts representing an ISO week date instead. This will return a document where the properties are isoWeekYear, isoWeek, isoDayOfWeek, hour, minute, second and millisecond. The $dateToParts expression has the following syntax: {    $dateToParts: {        'date' : <dateExpression>,        'timezone' : <timezone>,        'iso8601' : <boolean>    }}  The $dateToParts takes a document with the following fields: Field\nRequired/Optional\nDescription\ndate\nRequired Changed in version 3.6. The input date for which to return parts. <dateExpression> can be any expression that resolves to a Date, a Timestamp, or an ObjectID. For more information on expressions, see Expressions. timezone\nOptional The timezone to use to format the date. By default, $dateToParts uses UTC. <timezone> can be any expression that evaluates to a string whose value is either:      * an Olson Timezone Identifier, such as \"Europe/London\" or \"America/New_York\", or  * a UTC offset in the form:\n   \n   * +/-[hh]:[mm], e.g. \"+04:45\", or\n   \n   * +/-[hh][mm], e.g. \"-0530\", or\n   \n   * +/-[hh], e.g. \"+03\". For more information on expressions, see Expressions. iso8601\nOptional\nIf set to true, modifies the output document to use ISO week date fields. Defaults to false. \nBEHAVIOR \nWhen using an Olson Timezone Identifier in the <timezone> field, MongoDB applies the DST offset if applicable for the specified timezone. For example, consider a sales collection with the following document: {   \"_id\" : 1,   \"item\" : \"abc\",   \"price\" : 20,   \"quantity\" : 5,   \"date\" : ISODate(\"2017-05-20T10:24:51.303Z\")}  The following aggregation illustrates how MongoDB handles the DST offset for the Olson Timezone Identifier. The example uses the $hour and $minute operators to return the corresponding portions of the date field: db.sales.aggregate([{   $project: {      \"nycHour\": {         $hour: { date: \"$date\", timezone: \"-05:00\" }       },       \"nycMinute\": {          $minute: { date: \"$date\", timezone: \"-05:00\" }       },       \"gmtHour\": {          $hour: { date: \"$date\", timezone: \"GMT\" }       },       \"gmtMinute\": {          $minute: { date: \"$date\", timezone: \"GMT\" } },       \"nycOlsonHour\": {          $hour: { date: \"$date\", timezone: \"America/New_York\" }       },       \"nycOlsonMinute\": {          $minute: { date: \"$date\", timezone: \"America/New_York\" }       }   }}])  The operation returns the following result: {   \"_id\": 1,   \"nycHour\" : 5,   \"nycMinute\" : 24,   \"gmtHour\" : 10,   \"gmtMinute\" : 24,   \"nycOlsonHour\" : 6,   \"nycOlsonMinute\" : 24} \n\n \nConsider a sales collection with the following document: {  \"_id\" : 2,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2017-01-01T01:29:09.123Z\")}  The following aggregation uses $dateToParts to return a document that contains the constituent parts of the date field.  db.sales.aggregate([ {    $project: {       date: {          $dateToParts: { date: \"$date\" }       },       date_iso: {          $dateToParts: { date: \"$date\", iso8601: true }       },       date_timezone: {          $dateToParts: { date: \"$date\", timezone: \"America/New_York\" }       }    }}])  The operation returns the following result: {   \"_id\" : 2,   \"date\" : {      \"year\" : 2017,      \"month\" : 1,      \"day\" : 1,      \"hour\" : 1,      \"minute\" : 29,      \"second\" : 9,      \"millisecond\" : 123   },   \"date_iso\" : {      \"isoWeekYear\" : 2016,      \"isoWeek\" : 52,      \"isoDayOfWeek\" : 7,      \"hour\" : 1,      \"minute\" : 29,      \"second\" : 9,      \"millisecond\" : 123   },   \"date_timezone\" : {      \"year\" : 2016,      \"month\" : 12,      \"day\" : 31,      \"hour\" : 20,      \"minute\" : 29,      \"second\" : 9,      \"millisecond\" : 123   }} \n←  $dateSubtract (aggregation)$dateToString (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/isArray/": " Docs Home → MongoDB Manual \n$ISARRAY (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$isArray \nDetermines if the operand is an array. Returns a boolean. $isArray has the following syntax: { $isArray: [ <expression> ] }  \nBEHAVIOR \nThe <expression> can be any valid expression. For more information on expressions, see Expressions. Example\nResults\nNotes\n{ $isArray: \"hello\" }\nfalse\n\"hello\" is a string, passed as a string.\n{ $isArray: [ \"hello\" ] }\nfalse\n\"hello\" is a string, passed as part of an argument array.\n{ $isArray: [ [ \"hello\" ] ] }\ntrue\n[ \"hello\" ] is an array, passed as part of an argument array. \nNOTE Aggregation expressions accept a variable number of arguments. These arguments are normally passed as an array. However, when the argument is a single value, you can simplify your code by passing the argument directly without wrapping it in an array. \nEXAMPLE \nCreate the warehouses collection: db.warehouses.insertMany( [   { \"_id\" : 1, instock: [ \"chocolate\" ], ordered: [ \"butter\", \"apples\" ] },   { \"_id\" : 2, instock: [ \"apples\", \"pudding\", \"pie\" ] },   { \"_id\" : 3, instock: [ \"pears\", \"pecans\"], ordered: [ \"cherries\" ] },   { \"_id\" : 4, instock: [ \"ice cream\" ], ordered: [ ] }] )  Check if the instock and the ordered fields are arrays. If both fields are arrays, concatenate them: db.warehouses.aggregate( [   { $project:      { items:          { $cond:            {              if: { $and: [ { $isArray: \"$instock\" },                            { $isArray: \"$ordered\" }                          ] },              then: { $concatArrays: [ \"$instock\", \"$ordered\" ] },              else: \"One or more fields is not an array.\"            }          }      }   }] )  { \"_id\" : 1, \"items\" : [ \"chocolate\", \"butter\", \"apples\" ] }{ \"_id\" : 2, \"items\" : \"One or more fields is not an array.\" }{ \"_id\" : 3, \"items\" : [ \"pears\", \"pecans\", \"cherries\" ] }{ \"_id\" : 4, \"items\" : [ \"ice cream\" ] }  \nTIP \nSEE ALSO:      * $cond  * $concatArrays ←  $integral (aggregation)$isNumber (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/skip/": " Docs Home → MongoDB Manual \n$SKIP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$skip \nSkips over the specified number of documents that pass into the stage and passes the remaining documents to the next stage in the pipeline. The $skip stage has the following prototype form: { $skip: <positive 64-bit integer> }  $skip takes a positive integer that specifies the maximum number of documents to skip. \nNOTE Starting in MongoDB 5.0, the $skip pipeline aggregation has a 64-bit integer limit. Values passed to the pipeline which exceed this limit will return a invalid argument error. \nBEHAVIOR  USING $SKIP WITH SORTED RESULTS \nIf using the $skip stage with any of:      * the $sort aggregation stage,  * the sort() method, or  * the sort field to the findAndModify command or the findAndModify() shell method, be sure to include at least one field in your sort that contains unique values, before passing results to the $skip stage. Sorting on fields that contain duplicate values may return a different sort order for those duplicate fields over multiple executions, especially when the collection is actively receiving writes. The easiest way to guarantee sort consistency is to include the _id field in your sort query. See the following for more information on each:  * Consistent sorting with $sort (aggregation)  * Consistent sorting with the sort() shell method  * Consistent sorting with the findAndModify command  * Consistent sorting with the findAndModify() shell method \nEXAMPLE \nConsider the following example: db.article.aggregate([    { $skip : 5 }]);  This operation skips the first 5 documents passed to it by the pipeline. $skip has no effect on the content of the documents it passes along the pipeline. \nTIP \nSEE ALSO:  * Aggregation with the Zip Code Data Set  * Aggregation with User Preference Data ←  $shardedDataDistribution (aggregation)$sort (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/graphLookup/": " Docs Home → MongoDB Manual \n$GRAPHLOOKUP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Considerations\n * Examples\n * Additional Resource \nDEFINITION \n$graphLookup \nChanged in version 5.1. Performs a recursive search on a collection, with options for restricting the search by recursion depth and query filter. The $graphLookup search process is summarized below:       1. Input documents flow into the $graphLookup stage of an aggregation operation.  2. $graphLookup targets the search to the collection designated by the from parameter (see below for full list of search parameters).  3. For each input document, the search begins with the value designated by startWith.  4. $graphLookup matches the startWith value against the field designated by connectToField in other documents in the from collection.  5. For each matching document, $graphLookup takes the value of the connectFromField and checks every document in the from collection for a matching connectToField value. For each match, $graphLookup adds the matching document in the from collection to an array field named by the as parameter.\n    \n    This step continues recursively until no more matching documents are found, or until the operation reaches a recursion depth specified by the maxDepth parameter. $graphLookup then appends the array field to the input document. $graphLookup returns results after completing its search on all input documents. $graphLookup has the following prototype form: {   $graphLookup: {      from: <collection>,      startWith: <expression>,      connectFromField: <string>,      connectToField: <string>,      as: <string>,      maxDepth: <number>,      depthField: <string>,      restrictSearchWithMatch: <document>   }}  $graphLookup takes a document with the following fields: Field\nDescription\nfrom Target collection for the $graphLookup operation to search, recursively matching the connectFromField to the connectToField. The from collection must be in the same database as any other collections used in the operation. Starting in MongoDB 5.1, the collection specified in the from parameter can be sharded. startWith\nExpression that specifies the value of the connectFromField with which to start the recursive search. Optionally, startWith may be array of values, each of which is individually followed through the traversal process.\nconnectFromField\nField name whose value $graphLookup uses to recursively match against the connectToField of other documents in the collection. If the value is an array, each element is individually followed through the traversal process.\nconnectToField\nField name in other documents against which to match the value of the field specified by the connectFromField parameter.\nas Name of the array field added to each output document. Contains the documents traversed in the $graphLookup stage to reach the document. \nNOTE Documents returned in the as field are not guaranteed to be in any order. maxDepth\nOptional. Non-negative integral number specifying the maximum recursion depth.\ndepthField\nOptional. Name of the field to add to each traversed document in the search path. The value of this field is the recursion depth for the document, represented as a NumberLong. Recursion depth value starts at zero, so the first lookup corresponds to zero depth.\nrestrictSearchWithMatch Optional. A document specifying additional conditions for the recursive search. The syntax is identical to query filter syntax. \nNOTE You cannot use any aggregation expression in this filter. For example, a query document such as { lastName: { $ne: \"$lastName\" } }  will not work in this context to find documents in which the lastName value is different from the lastName value of the input document, because \"$lastName\" will act as a string literal, not a field path. \nCONSIDERATIONS  SHARDED COLLECTIONS \nStarting in MongoDB 5.1, you can specify sharded collections in the from parameter of $graphLookup stages. \nMAX DEPTH \nSetting the maxDepth field to 0 is equivalent to a non-recursive $graphLookup search stage. \nMEMORY \nThe $graphLookup stage must stay within the 100 megabyte memory limit. If allowDiskUse: true is specified for the aggregate() operation, the $graphLookup stage ignores the option. If there are other stages in the aggregate() operation, allowDiskUse: true option is in effect for these other stages.\n\n \nVIEWS AND COLLATION \nIf performing an aggregation that involves multiple views, such as with $lookup or $graphLookup, the views must have the same collation. \nEXAMPLES  WITHIN A SINGLE COLLECTION \nA collection named employees has the following documents: { \"_id\" : 1, \"name\" : \"Dev\" }{ \"_id\" : 2, \"name\" : \"Eliot\", \"reportsTo\" : \"Dev\" }{ \"_id\" : 3, \"name\" : \"Ron\", \"reportsTo\" : \"Eliot\" }{ \"_id\" : 4, \"name\" : \"Andrew\", \"reportsTo\" : \"Eliot\" }{ \"_id\" : 5, \"name\" : \"Asya\", \"reportsTo\" : \"Ron\" }{ \"_id\" : 6, \"name\" : \"Dan\", \"reportsTo\" : \"Andrew\" }  The following $graphLookup operation recursively matches on the reportsTo and name fields in the employees collection, returning the reporting hierarchy for each person: db.employees.aggregate( [   {      $graphLookup: {         from: \"employees\",         startWith: \"$reportsTo\",         connectFromField: \"reportsTo\",         connectToField: \"name\",         as: \"reportingHierarchy\"      }   }] )  The operation returns the following: {   \"_id\" : 1,   \"name\" : \"Dev\",   \"reportingHierarchy\" : [ ]}{   \"_id\" : 2,   \"name\" : \"Eliot\",   \"reportsTo\" : \"Dev\",   \"reportingHierarchy\" : [      { \"_id\" : 1, \"name\" : \"Dev\" }   ]}{   \"_id\" : 3,   \"name\" : \"Ron\",   \"reportsTo\" : \"Eliot\",   \"reportingHierarchy\" : [      { \"_id\" : 1, \"name\" : \"Dev\" },      { \"_id\" : 2, \"name\" : \"Eliot\", \"reportsTo\" : \"Dev\" }   ]}{   \"_id\" : 4,   \"name\" : \"Andrew\",   \"reportsTo\" : \"Eliot\",   \"reportingHierarchy\" : [      { \"_id\" : 1, \"name\" : \"Dev\" },      { \"_id\" : 2, \"name\" : \"Eliot\", \"reportsTo\" : \"Dev\" }   ]}{   \"_id\" : 5,   \"name\" : \"Asya\",   \"reportsTo\" : \"Ron\",   \"reportingHierarchy\" : [      { \"_id\" : 1, \"name\" : \"Dev\" },      { \"_id\" : 2, \"name\" : \"Eliot\", \"reportsTo\" : \"Dev\" },      { \"_id\" : 3, \"name\" : \"Ron\", \"reportsTo\" : \"Eliot\" }   ]}{   \"_id\" : 6,   \"name\" : \"Dan\",   \"reportsTo\" : \"Andrew\",   \"reportingHierarchy\" : [      { \"_id\" : 1, \"name\" : \"Dev\" },      { \"_id\" : 2, \"name\" : \"Eliot\", \"reportsTo\" : \"Dev\" },      { \"_id\" : 4, \"name\" : \"Andrew\", \"reportsTo\" : \"Eliot\" }   ]}  The following table provides a traversal path for the document { \"_id\" : 5, \"name\" : \"Asya\", \"reportsTo\" : \"Ron\" }: Start value The reportsTo value of the document: { ... \"reportsTo\" : \"Ron\" } \nDepth 0 { \"_id\" : 3, \"name\" : \"Ron\", \"reportsTo\" : \"Eliot\" } \nDepth 1 { \"_id\" : 2, \"name\" : \"Eliot\", \"reportsTo\" : \"Dev\" } \nDepth 2\n\n  The output generates the hierarchy Asya -> Ron -> Eliot -> Dev. \nACROSS MULTIPLE COLLECTIONS \nLike $lookup, $graphLookup can access another collection in the same database. For example, create a database with two collections:  * An airports collection with the following documents:\n   \n   db.airports.insertMany( [   { \"_id\" : 0, \"airport\" : \"JFK\", \"connects\" : [ \"BOS\", \"ORD\" ] },   { \"_id\" : 1, \"airport\" : \"BOS\", \"connects\" : [ \"JFK\", \"PWM\" ] },   { \"_id\" : 2, \"airport\" : \"ORD\", \"connects\" : [ \"JFK\" ] },   { \"_id\" : 3, \"airport\" : \"PWM\", \"connects\" : [ \"BOS\", \"LHR\" ] },   { \"_id\" : 4, \"airport\" : \"LHR\", \"connects\" : [ \"PWM\" ] }] )\n   \n     * A travelers collection with the following documents:\n   \n   db.travelers.insertMany( [   { \"_id\" : 1, \"name\" : \"Dev\", \"nearestAirport\" : \"JFK\" },   { \"_id\" : 2, \"name\" : \"Eliot\", \"nearestAirport\" : \"JFK\" },   { \"_id\" : 3, \"name\" : \"Jeff\", \"nearestAirport\" : \"BOS\" }] )\n   \n    For each document in the travelers collection, the following aggregation operation looks up the nearestAirport value in the airports collection and recursively matches the connects field to the airport field. The operation specifies a maximum recursion depth of 2. db.travelers.aggregate( [   {      $graphLookup: {         from: \"airports\",         startWith: \"$nearestAirport\",         connectFromField: \"connects\",         connectToField: \"airport\",         maxDepth: 2,         depthField: \"numConnections\",         as: \"destinations\"      }   }] )  The operation returns the following results:\n\n  The following table provides a traversal path for the recursive search, up to depth 2, where the starting airport is JFK: Start value The nearestAirport value from the travelers collection: { ... \"nearestAirport\" : \"JFK\" } \nDepth 0 { \"_id\" : 0, \"airport\" : \"JFK\", \"connects\" : [ \"BOS\", \"ORD\" ] } \nDepth 1 { \"_id\" : 1, \"airport\" : \"BOS\", \"connects\" : [ \"JFK\", \"PWM\" ] }{ \"_id\" : 2, \"airport\" : \"ORD\", \"connects\" : [ \"JFK\" ] } \nDepth 2 { \"_id\" : 3, \"airport\" : \"PWM\", \"connects\" : [ \"BOS\", \"LHR\" ] }  \nWITH A QUERY FILTER \nThe following example uses a collection with a set of documents containing names of people along with arrays of their friends and their hobbies. An aggregation operation finds one particular person and traverses her network of connections to find people who list golf among their hobbies. A collection named people contains the following documents: {  \"_id\" : 1,  \"name\" : \"Tanya Jordan\",  \"friends\" : [ \"Shirley Soto\", \"Terry Hawkins\", \"Carole Hale\" ],  \"hobbies\" : [ \"tennis\", \"unicycling\", \"golf\" ]}{  \"_id\" : 2,  \"name\" : \"Carole Hale\",  \"friends\" : [ \"Joseph Dennis\", \"Tanya Jordan\", \"Terry Hawkins\" ],  \"hobbies\" : [ \"archery\", \"golf\", \"woodworking\" ]}{  \"_id\" : 3,  \"name\" : \"Terry Hawkins\",  \"friends\" : [ \"Tanya Jordan\", \"Carole Hale\", \"Angelo Ward\" ],  \"hobbies\" : [ \"knitting\", \"frisbee\" ]}{  \"_id\" : 4,  \"name\" : \"Joseph Dennis\",  \"friends\" : [ \"Angelo Ward\", \"Carole Hale\" ],  \"hobbies\" : [ \"tennis\", \"golf\", \"topiary\" ]}{  \"_id\" : 5,  \"name\" : \"Angelo Ward\",  \"friends\" : [ \"Terry Hawkins\", \"Shirley Soto\", \"Joseph Dennis\" ],  \"hobbies\" : [ \"travel\", \"ceramics\", \"golf\" ]}{   \"_id\" : 6,   \"name\" : \"Shirley Soto\",   \"friends\" : [ \"Angelo Ward\", \"Tanya Jordan\", \"Carole Hale\" ],   \"hobbies\" : [ \"frisbee\", \"set theory\" ] }  The following aggregation operation uses three stages:  * $match matches on documents with a name field containing the string \"Tanya Jordan\". Returns one output document.  * $graphLookup connects the output document's friends field with the name field of other documents in the collection to traverse Tanya Jordan's network of connections. This stage uses the restrictSearchWithMatch parameter to find only documents in which the hobbies array contains golf. Returns one output document.  * $project shapes the output document. The names listed in connections who play golf are taken from the name field of the documents listed in the input document's golfers array. db.people.aggregate( [  { $match: { \"name\": \"Tanya Jordan\" } },  { $graphLookup: {      from: \"people\",      startWith: \"$friends\",      connectFromField: \"friends\",      connectToField: \"name\",      as: \"golfers\",      restrictSearchWithMatch: { \"hobbies\" : \"golf\" }    }  },  { $project: {      \"name\": 1,      \"friends\": 1,      \"connections who play golf\": \"$golfers.name\"    }  }] )  The operation returns the following document:\n\n  \nADDITIONAL RESOURCE \nWebinar: Working with Graph Data in MongoDB ←  $geoNear (aggregation)$group (aggregation) → On this page  * Definition\n * Considerations\n * Examples\n * Additional Resource Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/log/": " Docs Home → MongoDB Manual \n$LOG (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$log \nCalculates the log of a number in the specified base and returns the result as a double. $log has the following syntax: { $log: [ <number>, <base> ] }  The <number> expression can be any valid expression as long as it resolves to a non-negative number. The <base> expression can be any valid expression as long as it resolves to a positive number greater than 1. For more information on expressions, see Expressions. \nBEHAVIOR \nIf either argument resolves to a value of null or refers to a field that is missing, $log returns null. If either argument resolves to NaN, $log returns NaN. Example\nResults\n{ $log: [ 100, 10 ] }\n2\n{ $log: [ 100, Math.E ] } where Math.E is a JavaScript representation for e.\n4.605170185988092 \nEXAMPLE \nA collection examples contains the following documents: { _id: 1, positiveInt: 5 }{ _id: 2, positiveInt: 2 }{ _id: 3, positiveInt: 23 }{ _id: 4, positiveInt: 10 }  The following example uses log 2 in its calculation to determine the number of bits required to represent the value of positiveInt. db.examples.aggregate([   { $project: { bitsNeeded:      {         $floor: { $add: [ 1, { $log: [ \"$positiveInt\", 2 ] } ] } } }      }])  The operation returns the following results: { \"_id\" : 1, \"bitsNeeded\" : 3 }{ \"_id\" : 2, \"bitsNeeded\" : 2 }{ \"_id\" : 3, \"bitsNeeded\" : 5 }{ \"_id\" : 4, \"bitsNeeded\" : 4 }  \nTIP \nSEE ALSO:      * $log10  * $ln ←  $locf (aggregation)$log10 (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/and/": " Docs Home → MongoDB Manual \n$AND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Error Handling\n * Example \nDEFINITION \n$and \nEvaluates one or more expressions and returns true if all of the expressions are true or if run with no argument expressions. Otherwise, $and returns false. $and syntax: { $and: [ <expression1>, <expression2>, ... ] }  For more information on expressions, see Expressions. \nBEHAVIOR \nIn addition to the false boolean value, $and evaluates as false the following: null, 0, and undefined values. The $and evaluates all other values as true, including non-zero numeric values and arrays. Example\nResult\n{ $and: [ 1, \"green\" ] }\ntrue\n{ $and: [ ] }\ntrue\n{ $and: [ [ null ], [ false ], [ 0 ] ] }\ntrue\n{ $and: [ null, true ] }\nfalse\n{ $and: [ 0, true ] }\nfalse \nERROR HANDLING \nTo allow the query engine to optimize queries, $and handles errors as follows:      * If any expression supplied to $and would cause an error when evaluated alone, the $and containing the expression may cause an error but an error is not guaranteed.  * An expression supplied after the first expression supplied to $and may cause an error even if the first expression evaluates to false. For example, the following query always produces an error if $x is 0: db.example.find( {   $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] }} )  The following query, which contains multiple expressions supplied to $and, may produce an error if there is any document where $x is 0: db.example.find( {   $and: [      { x: { $ne: 0 } },      { $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] } }   ]} )  \nEXAMPLE \nCreate an example inventory collection with these documents: db.inventory.insertMany([   { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 },   { \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 },   { \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 },   { \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 },   { \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }])  This operation uses the $and operator to determine if qty is greater than 100 and less than 250: db.inventory.aggregate(   [     {       $project:          {            item: 1,            qty: 1,            result: { $and: [ { $gt: [ \"$qty\", 100 ] }, { $lt: [ \"$qty\", 250 ] } ] }          }     }   ])  The operation returns these results: { \"_id\" : 1, \"item\" : \"abc1\", \"qty\" : 300, \"result\" : false }{ \"_id\" : 2, \"item\" : \"abc2\", \"qty\" : 200, \"result\" : true }{ \"_id\" : 3, \"item\" : \"xyz1\", \"qty\" : 250, \"result\" : false }{ \"_id\" : 4, \"item\" : \"VWZ1\", \"qty\" : 300, \"result\" : false }{ \"_id\" : 5, \"item\" : \"VWZ2\", \"qty\" : 180, \"result\" : true } \n←  $allElementsTrue (aggregation)$anyElementTrue (aggregation) → On this page  * Definition\n * Behavior\n * Error Handling\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/min/": " Docs Home → MongoDB Manual \n$MIN (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \nChanged in version 5.0. $min \nReturns the minimum value. $min compares both value and type, using the specified BSON comparison order for values of different types. $min is available in these stages:      * $addFields (Available starting in MongoDB 3.4)  * $bucket  * $bucketAuto  * $group  * $match stage that includes an $expr expression  * $project  * $replaceRoot (Available starting in MongoDB 3.4)  * $replaceWith (Available starting in MongoDB 4.2)  * $set (Available starting in MongoDB 4.2)  * $setWindowFields (Available starting in MongoDB 5.0) In MongoDB 3.2 and earlier, $min is available in the $group stage only. \nSYNTAX \nWhen used in the $bucket, $bucketAuto, $group, and $setWindowFields stages, $min has this syntax: { $min: <expression> } When used in other supported stages, $min has one of two syntaxes:  * $min has one specified expression as its operand:\n   \n   { $min: <expression> }  * $min has a list of specified expressions as its operand:\n   \n   { $min: [ <expression1>, <expression2> ... ]  } For more information on expressions, see Expressions. \nBEHAVIOR  NULL OR MISSING VALUES \nIf some, but not all, documents for the $min operation have either a null value for the field or are missing the field, the $min operator only considers the non-null and the non-missing values for the field. If all documents for the $min operation have null value for the field or are missing the field, the $min operator returns null for the minimum value. \nARRAY OPERAND \nIn the $group and $setWindowFields stages, if the expression resolves to an array, $min does not traverse the array and compares the array as a whole. In the other supported stages:  * With a single expression as its operand, if the expression resolves to an array, $min traverses into the array to operate on the numerical elements of the array to return a single value.  * With a list of expressions as its operand, if any of the expressions resolves to an array, $min does not traverse into the array but instead treats the array as a non-numerical value. \nEXAMPLES  USE IN $GROUP STAGE \nConsider a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 2, \"date\" : ISODate(\"2014-01-01T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"quantity\" : 1, \"date\" : ISODate(\"2014-02-03T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 5, \"date\" : ISODate(\"2014-02-03T09:05:00Z\") }{ \"_id\" : 4, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T08:00:00Z\") }{ \"_id\" : 5, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T09:05:00Z\") }  Grouping the documents by the item field, the following operation uses the $min accumulator to compute the minimum amount and minimum quantity for each grouping. db.sales.aggregate(   [     {       $group:         {           _id: \"$item\",           minQuantity: { $min: \"$quantity\" }         }     }   ])  The operation returns the following results:\n\n \nUSE IN $PROJECT STAGE \nA collection students contains the following documents: { \"_id\": 1, \"quizzes\": [ 10, 6, 7 ], \"labs\": [ 5, 8 ], \"final\": 80, \"midterm\": 75 }{ \"_id\": 2, \"quizzes\": [ 9, 10 ], \"labs\": [ 8, 8 ], \"final\": 95, \"midterm\": 80 }{ \"_id\": 3, \"quizzes\": [ 4, 5, 5 ], \"labs\": [ 6, 5 ], \"final\": 78, \"midterm\": 70 }  The following example uses the $min in the $project stage to calculate the minimum quiz scores, the minimum lab scores, and the minimum of the final and the midterm: db.students.aggregate([   { $project: { quizMin: { $min: \"$quizzes\"}, labMin: { $min: \"$labs\" }, examMin: { $min: [ \"$final\", \"$midterm\" ] } } }])  The operation results in the following documents: { \"_id\" : 1, \"quizMin\" : 6, \"labMin\" : 5, \"examMin\" : 75 }{ \"_id\" : 2, \"quizMin\" : 9, \"labMin\" : 8, \"examMin\" : 80 }{ \"_id\" : 3, \"quizMin\" : 4, \"labMin\" : 5, \"examMin\" : 70 } \nUSE IN $SETWINDOWFIELDS STAGE \nNew in version 5.0. Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  This example uses $min in the $setWindowFields stage to output the minimum quantity of cake sales for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            minimumQuantityForState: {               $min: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"current\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.\n\n In this output, the minimum quantity for CA and WA is shown in the minimumQuantityForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"minimumQuantityForState\" : 162 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"minimumQuantityForState\" : 120 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"minimumQuantityForState\" : 120 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"minimumQuantityForState\" : 134 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"minimumQuantityForState\" : 104 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"minimumQuantityForState\" : 104 } ←  $meta$minN (aggregation accumulator) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/linearFill/": " Docs Home → MongoDB Manual \n$LINEARFILL (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples\n * Restrictions \nDEFINITION \n$linearFill \nNew in version 5.3. Fills null and missing fields in a window using linear interpolation based on surrounding field values. $linearFill is only available in the $setWindowFields stage. \nSYNTAX \nThe $linearFill expression has this syntax: { $linearFill: <expression> }  For more information on expressions, see Expressions. \nBEHAVIOR \n$linearFill fills null and missing fields using linear interpolation based on surrounding non-null field values. The surrounding field values are determined by the sort order specified in $setWindowFields.      * $linearFill fills null and missing values proportionally spanning the value range between surrounding non-null values. To determine the values for missing fields, $linearFill uses:\n   \n   * The difference of surrounding non-null values.\n   \n   * The number of null fields to fill between the surrounding values.  * $linearFill can fill multiple consecutive null values if those values are preceded and followed by non-null values according to the sort order specified in $setWindowFields.\n   \n   \n   EXAMPLE\n   \n   If a collection contains these documents:\n   \n   { index: 0, value: 0 },{ index: 1, value: null },{ index: 2, value: null },{ index: 3, value: null },{ index: 4, value: 10 }\n   \n   \n   \n   After using $linearFill to fill the null values, the documents become:\n   \n   { index: 0, value: 0 },{ index: 1, value: 2.5 },{ index: 2, value: 5 },{ index: 3, value: 7.5 },{ index: 4, value: 10 }\n   \n   For a complete example, see Examples.  * null values that are not preceded and followed by non-null values remain null. \nCOMPARISON OF $FILL AND $LINEARFILL \nTo fill missing field values using linear interpolation, you can use:  * The $fill stage with { method: \"linear\" }.\n   \n   When you use the $fill stage, the field you specify in the output is the same field used as the source data. See Fill Missing Field Values with Linear Interpolation.  * The $linearFill operator inside of a $setWindowFields stage.\n   \n   When you use the $linearFill operator, you can set values for a different field than the field used as the source data. See Use Multiple Fill Methods in a Single Stage. \nEXAMPLES \nThe examples on this page use a stock collection that contains tracks a single company's stock price at hourly intervals: db.stock.insertMany( [   {      time: ISODate(\"2021-03-08T09:00:00.000Z\"),      price: 500   },   {      time: ISODate(\"2021-03-08T10:00:00.000Z\"),   },   {      time: ISODate(\"2021-03-08T11:00:00.000Z\"),      price: 515   },   {      time: ISODate(\"2021-03-08T12:00:00.000Z\")   },   {      time: ISODate(\"2021-03-08T13:00:00.000Z\")   },   {      time: ISODate(\"2021-03-08T14:00:00.000Z\"),      price: 485   }] )  The price field is missing for some of the documents in the collection. \nFILL MISSING VALUES WITH LINEAR INTERPOLATION \nTo populate the missing price values using linear interpolation, use $linearFill inside of a $setWindowFields stage:\n\n  In the example:  * sortBy: { time: 1 } sorts the documents by the time field in ascending order, from earliest to latest.  * output specifies:\n   \n   * price as the field for which to fill in missing values.\n   \n   * { $linearFill: \"$price\" } as the value for the missing field. $linearFill fills missing price values using linear interpolation based on the surrounding price values in the sequence. Example output: [   {      _id: ObjectId(\"620ad555394d47411658b5ef\"),      time: ISODate(\"2021-03-08T09:00:00.000Z\"),      price: 500   },   {      _id: ObjectId(\"620ad555394d47411658b5f0\"),      time: ISODate(\"2021-03-08T10:00:00.000Z\"),      price: 507.5   },   {      _id: ObjectId(\"620ad555394d47411658b5f1\"),      time: ISODate(\"2021-03-08T11:00:00.000Z\"),      price: 515   },   {      _id: ObjectId(\"620ad555394d47411658b5f2\"),      time: ISODate(\"2021-03-08T12:00:00.000Z\"),      price: 505   },   {      _id: ObjectId(\"620ad555394d47411658b5f3\"),      time: ISODate(\"2021-03-08T13:00:00.000Z\"),      price: 495   },   {      _id: ObjectId(\"620ad555394d47411658b5f4\"),      time: ISODate(\"2021-03-08T14:00:00.000Z\"),      price: 485   }] \nUSE MULTIPLE FILL METHODS IN A SINGLE STAGE \nWhen you use the $setWindowFields stage to fill missing values, you can set values for a different field than the field you fill from. As a result, you can use multiple fill methods in a single $setWindowFields stage and output the results in distinct fields. The following pipeline populates missing price fields using linear interpolation and the last-observation-carried-forward method: db.stock.aggregate( [   {      $setWindowFields:         {            sortBy: { time: 1 },            output:               {                  linearFillPrice: { $linearFill: \"$price\" },                  locfPrice: { $locf: \"$price\" }               }         }   }] )  In the example:  * sortBy: { time: 1 } sorts the documents by the time field in ascending order, from earliest to latest.  * output specifies:\n   \n   * linearFillPrice as a target field to be filled.\n     \n     * { $linearFill: \"$price\" } is the value for the linearFillPrice field. $linearFill fills missing price values using linear interpolation based on the surrounding price values in the sequence.\n   \n   * locfPrice as a target field to be filled.\n     \n     * { $locf: \"$price\" } is the value for the locfPrice field. locf stands for last observation carried forward. $locf fills missing price values with the value from the previous document in the sequence. Example output:\n\n \nRESTRICTIONS \n * To use $linearFill, you must use the sortBy field to sort your data.  * When using $linearFill window function, $setWindowFields returns an error if there are any repeated values in the sortBy field in a single partition. ←  $let (aggregation)$literal (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples\n * Restrictions Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/avg/": " Docs Home → MongoDB Manual \n$AVG (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \nChanged in version 5.0. $avg \nReturns the average value of the numeric values. $avg ignores non-numeric values. $avg is available in these stages:      * $addFields (Available starting in MongoDB 3.4)  * $bucket  * $bucketAuto  * $group  * $match stage that includes an $expr expression  * $project  * $replaceRoot (Available starting in MongoDB 3.4)  * $replaceWith (Available starting in MongoDB 4.2)  * $set (Available starting in MongoDB 4.2)  * $setWindowFields (Available starting in MongoDB 5.0) In MongoDB 3.2 and earlier, $avg is available in the $group stage only. \nSYNTAX \nWhen used in the $bucket, $bucketAuto, $group, and $setWindowFields stages, $avg has this syntax: { $avg: <expression> } When used in other supported stages, $avg has one of two syntaxes:  * $avg has one specified expression as its operand:\n   \n   { $avg: <expression> }  * $avg has a list of specified expressions as its operand:\n   \n   { $avg: [ <expression1>, <expression2> ... ]  } For more information on expressions, see Expressions. \nBEHAVIOR  NON-NUMERIC OR MISSING VALUES \n$avg ignores non-numeric values, including missing values. If all of the operands for the average are non-numeric, $avg returns null since the average of zero values is undefined. \nARRAY OPERAND \nIn the $group stage, if the expression resolves to an array, $avg treats the operand as a non-numerical value. In the other supported stages:  * With a single expression as its operand, if the expression resolves to an array, $avg traverses into the array to operate on the numerical elements of the array to return a single value.  * With a list of expressions as its operand, if any of the expressions resolves to an array, $avg does not traverse into the array but instead treats the array as a non-numerical value. \nEXAMPLES  USE IN $GROUP STAGE \nConsider a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 2, \"date\" : ISODate(\"2014-01-01T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"quantity\" : 1, \"date\" : ISODate(\"2014-02-03T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 5, \"date\" : ISODate(\"2014-02-03T09:05:00Z\") }{ \"_id\" : 4, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T08:00:00Z\") }{ \"_id\" : 5, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T09:12:00Z\") }  Grouping the documents by the item field, the following operation uses the $avg accumulator to compute the average amount and average quantity for each grouping. db.sales.aggregate(   [     {       $group:         {           _id: \"$item\",           avgAmount: { $avg: { $multiply: [ \"$price\", \"$quantity\" ] } },           avgQuantity: { $avg: \"$quantity\" }         }     }   ])  The operation returns the following results:\n\n \nUSE IN $PROJECT STAGE \nA collection students contains the following documents: { \"_id\": 1, \"quizzes\": [ 10, 6, 7 ], \"labs\": [ 5, 8 ], \"final\": 80, \"midterm\": 75 }{ \"_id\": 2, \"quizzes\": [ 9, 10 ], \"labs\": [ 8, 8 ], \"final\": 95, \"midterm\": 80 }{ \"_id\": 3, \"quizzes\": [ 4, 5, 5 ], \"labs\": [ 6, 5 ], \"final\": 78, \"midterm\": 70 }  The following example uses the $avg in the $project stage to calculate the average quiz scores, the average lab scores, and the average of the final and the midterm: db.students.aggregate([   { $project: { quizAvg: { $avg: \"$quizzes\"}, labAvg: { $avg: \"$labs\" }, examAvg: { $avg: [ \"$final\", \"$midterm\" ] } } }])  The operation results in the following documents: { \"_id\" : 1, \"quizAvg\" : 7.666666666666667, \"labAvg\" : 6.5, \"examAvg\" : 77.5 }{ \"_id\" : 2, \"quizAvg\" : 9.5, \"labAvg\" : 8, \"examAvg\" : 87.5 }{ \"_id\" : 3, \"quizAvg\" : 4.666666666666667, \"labAvg\" : 5.5, \"examAvg\" : 74 } \nUSE IN $SETWINDOWFIELDS STAGE \nNew in version 5.0. Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  This example uses $avg in the $setWindowFields stage to output the moving average for the cake sales quantity for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            averageQuantityForState: {               $avg: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"current\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.\n\n In this output, the moving average quantity for CA and WA is shown in the averageQuantityForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"averageQuantityForState\" : 162 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"averageQuantityForState\" : 141 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"averageQuantityForState\" : 142.33333333333334 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"averageQuantityForState\" : 134 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"averageQuantityForState\" : 119 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"averageQuantityForState\" : 126 } ←  $atanh (aggregation)$binarySize (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toDate/": " Docs Home → MongoDB Manual \n$TODATE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toDate \nConverts a value to a date. If the value cannot be converted to a date, $toDate errors. If the value is null or missing, $toDate returns null. $toDate has the following syntax: {   $toDate: <expression>}  The $toDate takes any valid expression. The $toDate is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"date\" } }  \nTIP \nSEE ALSO:      * $convert  * $dateFromString \nBEHAVIOR \nThe following table lists the input types that can be converted to a date: Input Type\nBehavior\nDouble Returns a date that corresponds to the number of milliseconds represented by the truncated double value. Positive number corresponds to the number of milliseconds since Jan 1, 1970. Negative number corresponds to the number of milliseconds before Jan 1, 1970. Decimal Returns a date that corresponds to the number of milliseconds represented by the truncated decimal value. Positive number corresponds to the number of milliseconds since Jan 1, 1970. Negative number corresponds to the number of milliseconds before Jan 1, 1970. Long Returns a date that corresponds to the number of milliseconds represented by the long value. Positive number corresponds to the number of milliseconds since Jan 1, 1970. Negative number corresponds to the number of milliseconds before Jan 1, 1970. String Returns a date that corresponds to the date string. The string must be a valid date string, such as:  * \"2018-03-20\"  * \"2018-03-20T12:00:00Z\"  * \"2018-03-20T12:00:00+0500\" ObjectId\nReturns a date that corresponds to the timestamp of the ObjectId.\nTimestamp\nReturns a date that corresponds to the timestamp. The following table lists some conversion to date examples: Example\nResults\n{$toDate: 120000000000.5}\nISODate(\"1973-10-20T21:20:00Z\")\n{$toDate: NumberDecimal(\"1253372036000.50\")}\nISODate(\"2009-09-19T14:53:56Z\")\n{$toDate: NumberLong(\"1100000000000\")}\nISODate(\"2004-11-19T11:33:20Z\")\n{$toDate: NumberLong(\"-1100000000000\")}\nISODate(\"1935-02-22T12:26:40Z\")\n{$toDate: ObjectId(\"5ab9c3da31c2ab715d421285\")}\nISODate(\"2018-03-27T04:08:58Z\")\n{$toDate: \"2018-03-20\"}\nISODate(\"2018-03-20T00:00:00Z\")\n{$toDate: \"2018-03-20 11:00:06 +0500\"}\nISODate(\"2018-03-20T06:00:06Z\")\n{$toDate: \"Friday\"}\nError\n{$toDate: Timestamp({ t: 1637688118, i: 1 })}\nISODate(\"2021-11-23T17:21:58.00Z\") \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: 1, item: \"apple\", qty: 5, price: 2, order_date: new Date( \"2018-03-20\" ) },   { _id: 2, item: \"pie\", qty: 10, price: 3, order_date: new Date( \"2018-03-22\" ) },   { _id: 3, item: \"ice cream\", qty: 2, price: 4, order_date: \"2018-03-15\" },   { _id: 4, item: \"almonds\" , qty: 5, price: 7, order_date: \"2018-03-15 +10:00\" }] )  The following aggregation operation on the orders collection converts the order_date to date before sorting by the date value:\n\n  The operation returns the following documents: {   _id: 4,   item: 'almonds',   qty: 5,   price: 7,   order_date: '2018-03-15 +10:00',   convertedDate: ISODate(\"2018-03-14T14:00:00.000Z\")},{   _id: 3,   item: 'ice cream',   qty: 2,   price: 4,   order_date: '2018-03-15',   convertedDate: ISODate(\"2018-03-15T00:00:00.000Z\")},{   _id: 1,   item: 'apple',   qty: 5,   price: 2,   order_date: ISODate(\"2018-03-20T00:00:00.000Z\"),   convertedDate: ISODate(\"2018-03-20T00:00:00.000Z\")},{   _id: 2,   item: 'pie',   qty: 10,   price: 3,   order_date: ISODate(\"2018-03-22T00:00:00.000Z\"),   convertedDate: ISODate(\"2018-03-22T00:00:00.000Z\")}  \nNOTE If the conversion operation encounters an error, the aggregation operation stops and throws an error. To override this behavior, use $convert instead. ←  $toBool (aggregation)$toDecimal (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/denseRank/": " Docs Home → MongoDB Manual \n$DENSERANK (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \nNew in version 5.0. $denseRank \nReturns the document position (known as the rank) relative to other documents in the $setWindowFields stage partition. The $setWindowFields stage sortBy field value determines the document rank. For more information on how MongoDB compares fields with different types, see BSON comparison order. If multiple documents occupy the same rank, $denseRank places the document with the subsequent value at the next rank without any gaps (see Behavior). $denseRank is only available in the $setWindowFields stage. $denseRank syntax: { $denseRank: { } } $denseRank does not accept any parameters. \nTIP \nSEE ALSO: $rank \nBEHAVIOR \n$rank and $denseRank differ in how they rank duplicate sortBy field values. For example, with sortBy field values of 7, 9, 9, and 10:      * $denseRank ranks the values as 1, 2, 2, and 3. The duplicate 9 values have a rank of 2, and 10 has a rank of 3. There is no gap in the ranks.  * $rank ranks the values as 1, 2, 2, and 4. The duplicate 9 values have a rank of 2, and 10 has a rank of 4. There is a gap in the ranks for 3. Documents with a null value for a sortBy field or documents missing the sortBy field are assigned a rank based on the BSON comparison order. See the example in Dense Rank for Duplicate, Null, and Missing Values. \nEXAMPLES  DENSE RANK PARTITIONS BY AN INTEGER FIELD \nCreate a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  This example uses $denseRank in the $setWindowFields stage to output the quantity dense rank of the cake sales for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { quantity: -1 },         output: {            denseRankQuantityForState: {               $denseRank: {}            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { quantity: -1 } sorts the documents in each partition by quantity in descending order (-1), so the highest quantity is first.  * output sets the denseRankOrderDateForState field to the orderDate dense rank using $denseRank, as shown in the following results.\n\n \nDENSE RANK PARTITIONS BY A DATE FIELD \nThis example shows how to use dates with $denseRank in the $setWindowFields stage to output the orderDate dense rank of the cake sales for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            denseRankOrderDateForState: {               $denseRank: {}            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output sets the denseRankOrderDateForState field to the orderDate rank using $denseRank, as shown in the following results. { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"denseRankOrderDateForState\" : 1 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"denseRankOrderDateForState\" : 2 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"denseRankOrderDateForState\" : 3 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"denseRankOrderDateForState\" : 1 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"denseRankOrderDateForState\" : 2 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"denseRankOrderDateForState\" : 3 } \nDENSE RANK FOR DUPLICATE, NULL, AND MISSING VALUES \nCreate a cakeSalesWithDuplicates collection where:  * Cake sales are placed in the state of California (CA) and Washington (WA).  * Documents 6 to 8 have the same quantity and state as document 5.  * Document 9 has the same quantity and state as document 4.  * Document 10 has a null quantity.  * Document 11 is missing the quantity.\n\n  This example uses $denseRank in the $setWindowFields stage to output the quantity dense rank from the cakeSalesWithDuplicates collection for each state: db.cakeSalesWithDuplicates.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { quantity: -1 },         output: {            denseRankQuantityForState: {               $denseRank: {}            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { quantity: -1 } sorts the documents in each partition by quantity in descending order (-1), so the highest quantity is first.  * output sets the denseRankQuantityForState field to the quantity dense rank using $denseRank. In the following example output:  * The documents with the same quantity and state have the same rank and there is no gap between the ranks. This differs from $rank that has a gap between the ranks (for an example, see Rank Partitions Containing Duplicate Values, Nulls, or Missing Data).  * The document with the null quantity and then the document with the missing quantity are ranked the lowest in the output for the CA partition. This sorting is the result of the BSON comparison order, which sorts null and missing values after number values in this example.\n\n ←  $degreesToRadians (aggregation)$derivative (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dayOfMonth/": " Docs Home → MongoDB Manual \n$DAYOFMONTH (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$dayOfMonth \nReturns the day of the month for a date as a number between 1 and 31. The $dayOfMonth expression has the following operator expression syntax: { $dayOfMonth: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $dayOfMonth: new Date(\"2016-01-01\") } 1 { $dayOfMonth: { date: new Date(\"Jan 7, 2003\") } } 7 { $dayOfMonth: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 14 { $dayOfMonth: ISODate(\"1998-11-07T00:00:00Z\") } 7 { $dayOfMonth: {    date: ISODate(\"1998-11-07T00:00:00Z\"),    timezone: \"-0400\"} } 6 { $dayOfMonth: \"March 28, 1976\" } error { $dayOfMonth: Date(\"2016-01-01\") } error { $dayOfMonth: \"2009-04-09\" } error \nNOTE \n$DAYOFMONTH CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $dayOfMonth and other date operators to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $dateTrunc (aggregation)$dayOfWeek (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sortArray/": " Docs Home → MongoDB Manual \n$SORTARRAY (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$sortArray \nNew in version 5.2. Sorts an array based on its elements. The sort order is user specified. $sortArray has the following syntax: $sortArray: {   input: <array>,   sortBy: <sort spec>} \nField\nType\nDescription\ninput\nexpression The array to be sorted. The result is null if the expression:      * is missing  * evaluates to null  * evaluates to undefined If the expression evaluates to any other non-array value, the document returns an error. sortBy\ndocument\nThe document specifies a sort ordering. \nBEHAVIOR \nThe $sortArray expression orders the input array according to the sortBy specification. The $sortArray syntax and semantics are the same as the behavior in a $push operation modified by $sort. \nSORT BY DOCUMENT FIELDS \nIf the array elements are documents, you can sort by a document field. Specify the field name and a sort direction, ascending (1), or descending (-1 ). {   input: <array-of-documents>,   sortBy: { <document-field>: {sort-direction> }}  \nSORT BY VALUE \nTo sort the whole array by value, or to sort by array elements that are not documents, identify the input array and specify 1 for an ascending sort or -1 for descending sort in the sortBy parameter. {   input: <array-of-documents>,   sortBy: { sort-direction> }}  \nCONSIDERATIONS \n * There is no implicit array traversal on the sort key.  * Positional operators are not supported. A field name like \"values.1\" denotes a sub-field called \"1\" in the values array. It does not refer to the item at index 1 in the values array.  * When a whole array is sorted, the sort is lexicographic. The aggregation $sort stage, behaves differently. See $sort for more details.  * When an array is sorted by a field, any documents or scalars that do not have the specified field are sorted equally. The resulting sort order is undefined.  * null values and missing values sort equally. \nSORT STABILITY \nThe stability of the sort is not specified. Users should not rely on $sortArray to use a particular sorting algorithm. \nEXAMPLES \nThe $sortArray examples in this section work on MongoDB 5.2 and later. Create the team collection: db.engineers.insertOne(   {      \"team\":         [            {              \"name\": \"pat\",              \"age\": 30,              \"address\": { \"street\": \"12 Baker St\", \"city\": \"London\" }            },            {              \"name\": \"dallas\",              \"age\": 36,              \"address\": { \"street\": \"12 Cowper St\", \"city\": \"Palo Alto\" }            },            {              \"name\": \"charlie\",              \"age\": 42,              \"address\": { \"street\": \"12 French St\", \"city\": \"New Brunswick\" }            }         ]   })  The team array has three elements. Each element of team has nested sub-elements: name, age, and address. The following examples show how to sort the team array using these sub-elements. \nSORT ON A FIELD \nSort on a field within an array element:\n\n  The name field is a sub-element in the team array. The operation returns the following results: {   result:   [      {        name: 'charlie',        age: 42,        address: { street: '12 French St', city: 'New Brunswick' }      },      {        name: 'dallas',        age: 36,        address: { street: '12 Cowper St', city: 'Palo Alto' }      },      {        name: 'pat',        age: 30,        address: { street: '12 Baker St', city: 'London' }      }   ]}  \nSORT ON A SUBFIELD \nThe address field is a document with subfields of its own. Use dot notation to sort the array based on a subfield: db.engineers.aggregate( [   {      $project:         {                _id: 0,                result:               {                  $sortArray:                     {                        input: \"$team\",                        sortBy: { \"address.city\": -1 }                     }               }             }   }] )  The sort direction is descending because the sortBy value is \"-1\". {  result: [    {      name: 'dallas',      age: 36,      address: { street: '12 Cowper St', city: 'Palo Alto' }    },    {      name: 'charlie',      age: 42,      address: { street: '12 French St', city: 'New Brunswick' }    },    {      name: 'pat',      age: 30,      address: { street: '12 Baker St', city: 'London' }    }  ]}  \nSORT ON MULTIPLE FIELDS \nSpecify multiple index fields to do a compound sort: db.engineers.aggregate( [   {      $project:         {            _id: 0,            result:               {                  $sortArray:                     {                        input: \"$team\",                        sortBy: { age: -1, name: 1 }                     }               }         }    } ] )  Example output:\n\n  \nSORT AN ARRAY OF INTEGERS \nThis example specifies an input array directly. The values are all the same type, Int32: db.engineers.aggregate( [   {      $project:         {            _id: 0,            result:               {                  $sortArray:                     {                         input: [ 1, 4, 1, 6, 12, 5 ],                         sortBy: 1                     }               }         }   }] )  Example output: [ { result: [ 1, 1, 4, 5, 6, 12 ] } ]  The input array has a \"1\" in position 0 and position 2. The ones are grouped together in the results, but their are no guarantees regarding how the group of ones is sorted relative to their original order. \nSORT ON MIXED TYPE FIELDS \nThis example specifies an input array directly. The values have different types: db.engineers.aggregate( [   {      $project:         {            _id: 0,            result:            {               $sortArray:                  {                     input: [ 20, 4, { a: \"Free\" }, 6, 21, 5, \"Gratis\",                             { a: null }, { a: { sale: true, price: 19 } },                             Decimal128( \"10.23\" ), { a: \"On sale\" } ],                     sortBy: 1                  }            }         }   }] )  Example output: { result: [      4,      5,      6,      Decimal128(\"10.23\"),      20,      21,      'Gratis',      { a: null },      { a: 'Free' },      { a: 'On sale' },      { a: { sale: true, price: 19 } }] } The results are ordered. In contrast, after changing the sortBy field to sort on the one of the document fields, sortBy: { a: 1 }, the sort order for the scalar and null values is undefined: { result: [     20,     4,     6,     21,     5,     'Gratis',     { a: null },     Decimal128(\"10.23\"),     { a: 'Free' },     { a: 'On sale' },     { a: { sale: true, price: 19 } }] } \n←  $slice (aggregation)$split (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/unset/": " Docs Home → MongoDB Manual \n$UNSET (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Considerations\n * Examples \nDEFINITION  NOTE \nDISAMBIGUATION The following page refers to the aggregation stage $unset. For the update operator $unset, see $unset. $unset \nNew in version 4.2. Removes/excludes fields from documents. \nSYNTAX \nThe $unset stage has the following syntax:      * To remove a single field, the $unset takes a string that specifies the field to remove:\n   \n   { $unset: \"<field>\" }\n   \n     * To remove multiple fields, the $unset takes an array of fields to remove.\n   \n   { $unset: [ \"<field1>\", \"<field2>\", ... ] }\n   \n    \nCONSIDERATIONS  $UNSET AND $PROJECT \nThe $unset is an alias for the $project stage that removes/excludes fields: { $project: { \"<field1>\": 0, \"<field2>\": 0, ... } }  \nEMBEDDED FIELDS \nTo remove/exclude a field or fields within an embedded document, you can use the dot notation, as in: { $unset: \"<field.nestedfield>\" }  or { $unset: [ \"<field1.nestedfield>\", ...] }  \nEXAMPLES \nCreate a sample books collection with the following documents: db.books.insertMany([   { \"_id\" : 1, title: \"Antelope Antics\", isbn: \"0001122223334\", author: { last:\"An\", first: \"Auntie\" }, copies: [ { warehouse: \"A\", qty: 5 }, { warehouse: \"B\", qty: 15 } ] },   { \"_id\" : 2, title: \"Bees Babble\", isbn: \"999999999333\", author: { last:\"Bumble\", first: \"Bee\" }, copies: [ { warehouse: \"A\", qty: 2 }, { warehouse: \"B\", qty: 5 } ] }])  \nREMOVE A SINGLE FIELD \nThe following example removes the top-level field copies: db.books.aggregate([ { $unset: \"copies\" } ])  Alternatively, you can also use the following syntax: db.books.aggregate([ { $unset: [ \"copies\" ] } ])  Either operation returns the following documents: { \"_id\" : 1, \"title\" : \"Antelope Antics\", \"isbn\" : \"0001122223334\", \"author\" : { \"last\" : \"An\", \"first\" : \"Auntie\" } }{ \"_id\" : 2, \"title\" : \"Bees Babble\", \"isbn\" : \"999999999333\", \"author\" : { \"last\" : \"Bumble\", \"first\" : \"Bee\" } }  \nREMOVE TOP-LEVEL FIELDS \nThe following example removes the top-level fields isbn and copies: db.books.aggregate([   { $unset: [ \"isbn\", \"copies\" ] }])  The $unset operation outputs the following documents: { \"_id\" : 1, \"title\" : \"Antelope Antics\", \"author\" : { \"last\" : \"An\", \"first\" : \"Auntie\" } }{ \"_id\" : 2, \"title\" : \"Bees Babble\", \"author\" : { \"last\" : \"Bumble\", \"first\" : \"Bee\" } }  \nREMOVE EMBEDDED FIELDS \nThe following example removes the top-level field isbn, the embedded field first (from the name document) and the embedded field warehouse (from the elements in the copies array): db.books.aggregate([   { $unset: [ \"isbn\", \"author.first\", \"copies.warehouse\" ] }])  The $unset operation outputs the following documents:\n\n \n←  $unionWith (aggregation)$unwind (aggregation) → On this page  * Definition\n * Syntax\n * Considerations\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/maxN/": " Docs Home → MongoDB Manual \n$MAXN (AGGREGATION ACCUMULATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Restrictions\n * Examples \nDEFINITION \n$maxN \nNew in version 5.2. Returns an aggregation of the maxmimum value n elements within a group. If the group contains fewer than n elements, $maxN returns all elements in the group. \nSYNTAX \n{   $maxN:      {         input: <expression>,         n: <expression>      }}      * input specifies an expression that is the input to $maxN. It is evaluated for each element in the group and $maxN preserves the maximum n values.  * n limits the number of results per group and n has to be a positive integral expression that is either a constant or depends on the _id value for $group. \nBEHAVIOR  NULL AND MISSING VALUES \n * $maxN filters out null and missing values. Consider the following aggregation that returns the maximum n documents from a group: db.aggregate( [   {      $documents: [         { playerId: \"PlayerA\", gameId: \"G1\", score: 1 },         { playerId: \"PlayerB\", gameId: \"G1\", score: 2 },         { playerId: \"PlayerC\", gameId: \"G1\", score: 3 },         { playerId: \"PlayerD\", gameId: \"G1\" },         { playerId: \"PlayerE\", gameId: \"G1\", score: null }      ]   },   {      $group:      {         _id: \"$gameId\",         maximumThreeScores:            {               $maxN:                  {                     input: \"$score\",                     n: 4                  }            }      }   }] )  In this example:  * $documents creates the literal documents that contain player scores.  * $group groups the documents by gameId. This example has only one gameId, G1.  * PlayerD has a missing score and PlayerE has a null score. These values are both considered as null.  * The maximumThreeScores field is specified as $maxN with input : \"$score\" and returned as an array.  * Since there are only 3 documents with scores maxN returns the maximum 3 score fields even though n = 4. [   {   _id: 'G1',   maximumThreeScores: [ 3, 2, 1 ]   }] \nCOMPARISON OF $MAXN AND $TOPN ACCUMULATORS \nBoth $maxN and $topN accumulators can accomplish similar results. In general:  * $maxN has the advantage of finding maximum values in no particular sort order. If you want to know the maximum values for n documents use $maxN.  * If guaranteing a particular sort order is a requirement use $topN.  * Use $topN if you don't intend on sorting on the output values. \nRESTRICTIONS  WINDOW FUNCTION AND AGGREGATION EXPRESSION SUPPORT \nYou can use $maxN as an accumulator. $maxN is supported as an aggregation expression. $maxN is supported as a window operator. \nMEMORY LIMIT CONSIDERATIONS \nAggregation pipelines which call $maxN are subject to the 100 MB limit. If this limit is exceeded for an individual group, the aggregation fails with an error. \nEXAMPLES \nConsider a gamescores collection with the following documents:\n\n  \nFIND THE MAXIMUM THREE SCORES FOR A SINGLE GAME \nYou can use the $maxN accumulator to find the maximum three scores in a single game. db.gamescores.aggregate( [   {      $match : { gameId : \"G1\" }   },   {      $group:         {            _id: \"$gameId\",            maxThreeScores:               {                  $maxN:                  {                     input: [\"$score\",\"$playerId\"],                     n:3                  }               }         }   }] )  The example pipeline:  * Uses $match to filter the results on a single gameId. In this case, G1.  * Uses $group to group the results by gameId. In this case, G1.  * Specifies the fields that are input for $maxN with input : [\"$score\",\"$playerId\"].  * Uses $maxN to return the maximum three score elements for the G1 game with n : 3. The operation returns the following results: [   {      _id: 'G1',      maxThreeScores: [ [ 99, 'PlayerC' ], [ 33, 'PlayerB' ], [ 31, 'PlayerA' ] ]   }] \nFINDING THE MAXIMUM THREE SCORES ACROSS MULTIPLE GAMES \nYou can use the $maxN accumulator to find the maximum n scores in each game. db.gamescores.aggregate( [   {      $group:      {         _id: \"$gameId\",         maxScores:            {               $maxN:                  {                     input: [\"$score\",\"$playerId\"],                     n: 3                  }            }      }   }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Uses $maxN to return the maximum three score elements for each game with n: 3.  * Specifies the fields that are input for $maxN with input: [\"$score\",\"$playerId\"]. The operation returns the following results: [   {      _id: 'G1',      maxScores: [ [ 99, 'PlayerC' ], [ 33, 'PlayerB' ], [ 31, 'PlayerA' ] ]   },   {      _id: 'G2',      maxScores: [ [ 80, 'PlayerD' ], [ 66, 'PlayerC' ], [ 14, 'PlayerB' ] ]   }] \nCOMPUTING N BASED ON THE GROUP KEY FOR $GROUP \nYou can also assign the value of n dynamically. In this example, the $cond expression is used on the gameId field.\n\n  The example pipeline:  * Uses $group to group the results by gameId.  * Specifies the fields that input for $maxN with input : [\"$score\",\"$playerId\"].  * If the gameId is G2 then n is 1, otherwise n is 3. The operation returns the following results: [   { _id: { gameId: 'G2' }, gamescores: [ [ 80, 'PlayerD' ] ] },   {      _id: { gameId: 'G1' },      gamescores: [ [ 99, 'PlayerC' ], [ 33, 'PlayerB' ], [ 31, 'PlayerA' ] ]   }] ←  $max (aggregation)$maxN (array operator) → On this page  * Definition\n * Syntax\n * Behavior\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/collStats/": " Docs Home → MongoDB Manual \n$COLLSTATS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior \nDEFINITION \n$collStats \nReturns statistics regarding a collection or view. The $collStats stage has the following prototype form: {  $collStats:    {      latencyStats: { histograms: <boolean> },      storageStats: { scale: <number> },      count: {},      queryExecStats: {}    }}  The $collStats stage accepts an argument document with the following optional fields: Field Name\nDescription\nlatencyStats\nAdds latency statistics to the return document.\nlatencyStats.histograms\nAdds latency histogram information to the embedded documents in latencyStats if true.\nstorageStats Adds storage statistics to the return document.      * Specify an empty document (i.e. storageStats: {}) to use the default scale factor of 1 for the various size data. Scale factor of 1 displays the returned sizes in bytes.  * Specify the scale factor (i.e. storageStats: { scale: <number> }) to use the specified scale factor for the various size data. For example, to display kilobytes rather than bytes, specify a scale value of 1024.\n   \n   If you specify a non-integer scale factor, MongoDB uses the integer part of the specified factor. For example, if you specify a scale factor of 1023.999, MongoDB uses 1023 as the scale factor.\n   \n   The scale factor does not affect those sizes that specify the unit of measurement in the field name, such as \"bytes currently in the cache\". count Adds the total number of documents in the collection to the return document. \nNOTE The count is based on the collection's metadata, which provides a fast but sometimes inaccurate count for sharded clusters. See count Field queryExecStats Adds query execution statistics to the return document. New in version 4.4. For a collection in a replica set or a non-sharded collection in a cluster, $collStats outputs a single document. For a sharded collection, $collStats outputs one document per shard. The output document includes the following fields: Field Name\nDescription\nns\nThe namespace of the requested collection or view.\nshard The name of the shard the output document corresponds to. Only present when $collStats runs on a sharded cluster. Both sharded and non-sharded collections will produce this field. host\nThe hostname and port of the mongod process which produced the output document.\nlocalTime\nThe current time on the MongoDB server, expressed as UTC milliseconds since the UNIX epoch.\nlatencyStats Statistics related to request latency for a collection or view. See latencyStats Document for details on this document. Only present when the latencyStats: {} option is specified. storageStats Statistics related to a collection's storage engine. See storageStats Document for details on this document. The various size data is scaled by the specified factor (with the exception of those sizes that specify the unit of measurement in the field name). Only present when the storageStats option is specified. Returns an error if applied to a view. count The total number of documents in the collection. This data is also available in storageStats.count. \nNOTE The count is based on the collection's metadata, which provides a fast but sometimes inaccurate count for sharded clusters. Only present when the count: {} option is specified. Returns an error if applied to a view. queryExecStats Statistics related to query execution for the collection. Only present when the queryExecStats: {} option is specified. Returns an error if applied to a view. \nBEHAVIOR \n$collStats must be the first stage in an aggregation pipeline, or else the pipeline returns an error. \nTRANSACTIONS \n$collStats is not allowed in transactions. \nLATENCYSTATS DOCUMENT \nThe latencyStats embedded document only exists in the output if you specify the latencyStats option. Field Name\nDescription\nreads\nLatency statistics for read requests.\nwrites\nLatency statistics for write requests.\ncommands\nLatency statistics for database commands. Each of these fields contains an embedded document bearing the following fields: Field Name\nDescription\nlatency\nA 64-bit integer giving the total combined latency in microseconds.\nops\nA 64-bit integer giving the total number of operations performed on the collection since startup.\nhistogram\n\n This field only exists given the latencyStats: { histograms: true } option. Empty ranges with a zero count are omitted from the output. Each document bears the following fields: Field Name\nDescription\nmicros A 64-bit integer giving the inclusive upper time bound of the current latency range in microseconds. The document's range spans between the previous document's micros value, exclusive, and this document's micros value, inclusive. count\nA 64-bit integer giving the number of operations with latency less than or equal to micros. For example, if collStats returns the following histogram: histogram: [  { micros: NumberLong(1), count: NumberLong(10) },  { micros: NumberLong(2), count: NumberLong(1) },  { micros: NumberLong(4096), count: NumberLong(1) },  { micros: NumberLong(16384), count: NumberLong(1000) },  { micros: NumberLong(49152), count: NumberLong(100) }]  This indicates that there were:  * 10 operations taking 1 microsecond or less,  * 1 operation in the range (1, 2] microseconds,  * 1 operation in the range (3072, 4096] microseconds,  * 1000 operations in the range (12288, 16384], and  * 100 operations in the range (32768, 49152]. For example, if you run $collStats with the latencyStats: {} option on a matrices collection: db.matrices.aggregate( [ { $collStats: { latencyStats: { histograms: true } } } ] )  This query returns a result similar to the following:\n\n  \nSTORAGESTATS DOCUMENT \nThe storageStats embedded document only exists in the output if you specify the storageStats option. The contents of this document are dependent on the storage engine in use. See Output for a reference on this document. For example, if you run $collStats with the storageStats: {} option on a matrices collection using the WiredTiger Storage Engine: db.matrices.aggregate( [ { $collStats: { storageStats: { } } } ] )  This query returns a result similar to the following: {  \"ns\" : \"test.matrices\",  \"host\" : mongo.example.net:27017\",  \"localTime\" : ISODate(\"2020-03-06T01:44:57.437Z\"),  \"storageStats\" : {    \"size\" : 608500363,    \"count\" : 1104369,    \"avgObjSize\" : 550,    \"storageSize\" : 352878592,    \"freeStorageSize\" : 2490380,  // Starting in MongoDB 4.4    \"capped\" : false,    \"wiredTiger\" : {      ...    },    \"nindexes\" : 2,    \"indexDetails\" : {      ...    },    \"indexBuilds\" : [    // Starting in MongoDB 4.2       \"_id_1_abc_1\"    ],    \"totalIndexSize\" : 260337664,    \"totalSize\" : 613216256,    // Starting in MongoDB 4.4    \"indexSizes\" : {      \"_id_\" : 9891840,      \"_id_1_abc_1\" : 250445824    },    \"scaleFactor\" : 1    // Starting in MongoDB 4.2  }}  See Output for a reference on this document. \nNOTE \nIN-PROGRESS INDEXES Starting in MongoDB 4.2, the returned storageStats includes information on indexes being built. For details, see:  * collStats.nindexes  * collStats.indexDetails  * collStats.indexBuilds  * collStats.totalIndexSize  * collStats.indexSizes Performing $collStats with the storageStats option on a view results in an error. \nCOUNT FIELD \nThe count field only exists in the output if you specify the count option. For example, if you run $collStats with the count: {} option on a matrices collection: db.matrices.aggregate( [ { $collStats: { count: { } } } ] )  The query returns a result similar to the following: {  \"ns\" : \"test.matrices\",  \"host\" : mongo.example.net:27017\",  \"localTime\" : ISODate(\"2017-10-06T19:43:56.599Z\"),  \"count\" : 1103869}  \nNOTE The count is based on the collection's metadata, which provides a fast but sometimes inaccurate count for sharded clusters. The total number of documents in the collection is also available as storageStats.count when storageStats: {} is specified. For more information, see storageStats Document. \nQUERYEXECSTATS DOCUMENT \nNew in version 4.4. The queryExecStats embedded document only exists in the output if you specify the queryExecStats option. The collectionScans field contains an embedded document bearing the following fields: Field Name\nDescription\ntotal\nA 64-bit integer giving the total number of queries that performed a collection scan. The total consists of queries that did and did not use a tailable cursor.\nnonTailable\nA 64-bit integer giving the number of queries that performed a collection scan that did not use a tailable cursor. For example, if you run $collStats with the queryExecStats: {} option on a matrices collection: db.matrices.aggregate( [ { $collStats: { queryExecStats: { } } } ] )  The query returns a result similar to the following:\n\n  \n$COLLSTATS ON SHARDED COLLECTIONS \n$collStats outputs one document per shard when run on sharded collections. Each output document contains a shard field with the name of the shard the document corresponds to. For example, if you run $collStats on a sharded collection with the count: {} option on a collection named matrices: db.matrices.aggregate( [ { $collStats: { count: { } } } ] )  The query returns a result similar to the following: {  \"ns\" : \"test.matrices\",  \"shard\" : \"s1\",  \"host\" : \"s1-mongo1.example.net:27017\",  \"localTime\" : ISODate(\"2017-10-06T15:14:21.258Z\"),  \"count\" : 661705}{  \"ns\" : \"test.matrices\",  \"shard\" : \"s2\",  \"host\" : \"s2-mongo1.example.net:27017\",  \"localTime\" : ISODate(\"2017-10-06T15:14:21.258Z\"),  \"count\" : 442164}  \nTIP \nSEE ALSO:  * collStats  * db.collection.stats() ←  $changeStream (aggregation)$count (aggregation) → On this page  * Definition\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/bucketAuto/": " Docs Home → MongoDB Manual \n$BUCKETAUTO (AGGREGATION) \nOn this page    \n * Definition\n   \n * Considerations\n * Behavior\n * Example \nDEFINITION \n$bucketAuto \nCategorizes incoming documents into a specific number of groups, called buckets, based on a specified expression. Bucket boundaries are automatically determined in an attempt to evenly distribute the documents into the specified number of buckets. Each bucket is represented as a document in the output. The document for each bucket contains:      * An _id object that specifies the bounds of the bucket.\n   \n   * The _id.min field specifies the inclusive lower bound for the bucket.\n   \n   * The _id.max field specifies the upper bound for the bucket. This bound is exclusive for all buckets except the final bucket in the series, where it is inclusive.  * A count field that contains the number of documents in the bucket. The count field is included by default when the output document is not specified. The $bucketAuto stage has the following form: {  $bucketAuto: {      groupBy: <expression>,      buckets: <number>,      output: {         <output1>: { <$accumulator expression> },         ...      }      granularity: <string>  }} \nField\nType\nDescription\ngroupBy\nexpression\nAn expression to group documents by. To specify a field path, prefix the field name with a dollar sign $ and enclose it in quotes.\nbuckets\ninteger\nA positive 32-bit integer that specifies the number of buckets into which input documents are grouped.\noutput\ndocument Optional. A document that specifies the fields to include in the output documents in addition to the _id field. To specify the field to include, you must use accumulator expressions: <outputfield1>: { <accumulator>: <expression1> },...  The default count field is not included in the output document when output is specified. Explicitly specify the count expression as part of the output document to include it: output: {  <outputfield1>: { <accumulator>: <expression1> },  ...  count: { $sum: 1 }} \ngranularity\nstring Optional. A string that specifies the preferred number series to use to ensure that the calculated boundary edges end on preferred round numbers or their powers of 10. Available only if the all groupBy values are numeric and none of them are NaN. The suppported values of granularity are:  * \"R5\"  * \"R10\"  * \"R20\"  * \"R40\"  * \"R80\"  * \"1-2-5\"  * \"E6\"  * \"E12\"  * \"E24\"  * \"E48\"  * \"E96\"  * \"E192\"  * \"POWERSOF2\" \nCONSIDERATIONS  $BUCKETAUTO AND MEMORY RESTRICTIONS \nThe $bucketAuto stage has a limit of 100 megabytes of RAM. By default, if the stage exceeds this limit, $bucketAuto returns an error. To allow more space for stage processing, use the allowDiskUse option to enable aggregation pipeline stages to write data to temporary files. \nTIP \nSEE ALSO: Aggregation Pipeline Limits \nBEHAVIOR \nThere may be less than the specified number of buckets if:  * The number of input documents is less than the specified number of buckets.  * The number of unique values of the groupBy expression is less than the specified number of buckets.  * The granularity has fewer intervals than the number of buckets.  * The granularity is not fine enough to evenly distribute documents into the specified number of buckets. If the groupBy expression refers to an array or document, the values are arranged using the same ordering as in $sort before determining the bucket boundaries. The even distribution of documents across buckets depends on the cardinality, or the number of unique values, of the groupBy field. If the cardinality is not high enough, the $bucketAuto stage may not evenly distribute the results across buckets. \nGRANULARITY \nThe $bucketAuto accepts an optional granularity parameter which ensures that the boundaries of all buckets adhere to a specified preferred number series. Using a preferred number series provides more control on where the bucket boundaries are set among the range of values in the groupBy expression. They may also be used to help logarithmically and evenly set bucket boundaries when the range of the groupBy expression scales exponentially. RENARD SERIES\n\n Set granularity to R5, R10, R20, R40, or R80 to restrict bucket boundaries to values in the series. The values of the series are multiplied by a power of 10 when the groupBy values are outside of the 1.0 to 10.0 (10.3 for R80) range. \nEXAMPLE The R5 series is based off of the fifth root of 10, which is 1.58, and includes various powers of this root (rounded) until 10 is reached. The R5 series is derived as follows:  * 10 0/5 = 1  * 10 1/5 = 1.584 ~ 1.6  * 10 2/5 = 2.511 ~ 2.5  * 10 3/5 = 3.981 ~ 4.0  * 10 4/5 = 6.309 ~ 6.3  * 10 5/5 = 10 The same approach is applied to the other Renard series to offer finer granularity, i.e., more intervals between 1.0 and 10.0 (10.3 for R80). E SERIES \nThe E number series are similar to the Renard series in that they subdivide the interval from 1.0 to 10.0 by the 6 th, 12 th, 24 th, 48 th, 96 th, or 192 nd root of ten with a particular relative error. Set granularity to E6, E12, E24, E48, E96, or E192 to restrict bucket boundaries to values in the series. The values of the series are multiplied by a power of 10 when the groupBy values are outside of the 1.0 to 10.0 range. To learn more about the E-series and their respective relative errors, see preferred number series. 1-2-5 SERIES \nThe 1-2-5 series behaves like a three-value Renard series, if such a series existed. Set granularity to 1-2-5 to restrict bucket boundaries to various powers of the third root of 10, rounded to one significant digit. \nEXAMPLE The following values are part of the 1-2-5 series: 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, and so on... POWERS OF TWO SERIES \nSet granularity to POWERSOF2 to restrict bucket boundaries to numbers that are a power of two. \nEXAMPLE The following numbers adhere to the power of two Series:  * 2 0 = 1  * 2 1 = 2  * 2 2 = 4  * 2 3 = 8  * 2 4 = 16  * 2 5 = 32  * and so on... A common implementation is how various computer components, like memory, often adhere to the POWERSOF2 set of preferred numbers: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, and so on.... COMPARING DIFFERENT GRANULARITIES \nThe following operation demonstrates how specifying different values for granularity affects how $bucketAuto determines bucket boundaries. A collection of things have an _id numbered from 1 to 100: { _id: 1 }{ _id: 2 }...{ _id: 100 }  Different values for granularity are substituted into the following operation: db.things.aggregate( [  {    $bucketAuto: {      groupBy: \"$_id\",      buckets: 5,      granularity: <granularity>    }  }] )  The results in the following table demonstrate how different values for granularity yield different bucket boundaries: Granularity\nResults\nNotes\nNo granularity\n{ \"_id\" : { \"min\" : 0, \"max\" : 20 }, \"count\" : 20 }\n{ \"_id\" : { \"min\" : 20, \"max\" : 40 }, \"count\" : 20 }\n{ \"_id\" : { \"min\" : 40, \"max\" : 60 }, \"count\" : 20 }\n{ \"_id\" : { \"min\" : 60, \"max\" : 80 }, \"count\" : 20 }\n{ \"_id\" : { \"min\" : 80, \"max\" : 99 }, \"count\" : 20 }\n\n E24\n{ \"_id\" : { \"min\" : 0, \"max\" : 20 }, \"count\" : 20 }\n{ \"_id\" : { \"min\" : 20, \"max\" : 43 }, \"count\" : 23 }\n{ \"_id\" : { \"min\" : 43, \"max\" : 68 }, \"count\" : 25 }\n{ \"_id\" : { \"min\" : 68, \"max\" : 91 }, \"count\" : 23 }\n{ \"_id\" : { \"min\" : 91, \"max\" : 100 }, \"count\" : 9 } 1-2-5\n{ \"_id\" : { \"min\" : 0, \"max\" : 20 }, \"count\" : 20 }\n{ \"_id\" : { \"min\" : 20, \"max\" : 50 }, \"count\" : 30 }\n{ \"_id\" : { \"min\" : 50, \"max\" : 100 }, \"count\" : 50 }\nThe specified number of buckets exceeds the number of intervals in the series.\nPOWERSOF2\n{ \"_id\" : { \"min\" : 0, \"max\" : 32 }, \"count\" : 32 }\n{ \"_id\" : { \"min\" : 32, \"max\" : 64 }, \"count\" : 32 }\n{ \"_id\" : { \"min\" : 64, \"max\" : 128 }, \"count\" : 36 }\nThe specified number of buckets exceeds the number of intervals in the series. \nEXAMPLE \nConsider a collection artwork with the following documents: { \"_id\" : 1, \"title\" : \"The Pillars of Society\", \"artist\" : \"Grosz\", \"year\" : 1926,    \"price\" : NumberDecimal(\"199.99\"),    \"dimensions\" : { \"height\" : 39, \"width\" : 21, \"units\" : \"in\" } }{ \"_id\" : 2, \"title\" : \"Melancholy III\", \"artist\" : \"Munch\", \"year\" : 1902,    \"price\" : NumberDecimal(\"280.00\"),    \"dimensions\" : { \"height\" : 49, \"width\" : 32, \"units\" : \"in\" } }{ \"_id\" : 3, \"title\" : \"Dancer\", \"artist\" : \"Miro\", \"year\" : 1925,    \"price\" : NumberDecimal(\"76.04\"),    \"dimensions\" : { \"height\" : 25, \"width\" : 20, \"units\" : \"in\" } }{ \"_id\" : 4, \"title\" : \"The Great Wave off Kanagawa\", \"artist\" : \"Hokusai\",    \"price\" : NumberDecimal(\"167.30\"),    \"dimensions\" : { \"height\" : 24, \"width\" : 36, \"units\" : \"in\" } }{ \"_id\" : 5, \"title\" : \"The Persistence of Memory\", \"artist\" : \"Dali\", \"year\" : 1931,    \"price\" : NumberDecimal(\"483.00\"),    \"dimensions\" : { \"height\" : 20, \"width\" : 24, \"units\" : \"in\" } }{ \"_id\" : 6, \"title\" : \"Composition VII\", \"artist\" : \"Kandinsky\", \"year\" : 1913,    \"price\" : NumberDecimal(\"385.00\"),    \"dimensions\" : { \"height\" : 30, \"width\" : 46, \"units\" : \"in\" } }{ \"_id\" : 7, \"title\" : \"The Scream\", \"artist\" : \"Munch\",    \"price\" : NumberDecimal(\"159.00\"),    \"dimensions\" : { \"height\" : 24, \"width\" : 18, \"units\" : \"in\" } }{ \"_id\" : 8, \"title\" : \"Blue Flower\", \"artist\" : \"O'Keefe\", \"year\" : 1918,    \"price\" : NumberDecimal(\"118.42\"),    \"dimensions\" : { \"height\" : 24, \"width\" : 20, \"units\" : \"in\" } }  \nSINGLE FACET AGGREGATION \nIn the following operation, input documents are grouped into four buckets according to the values in the price field:\n\n  The operation returns the following documents: {  \"_id\" : {    \"min\" : NumberDecimal(\"76.04\"),    \"max\" : NumberDecimal(\"159.00\")  },  \"count\" : 2}{  \"_id\" : {    \"min\" : NumberDecimal(\"159.00\"),    \"max\" : NumberDecimal(\"199.99\")  },  \"count\" : 2}{  \"_id\" : {    \"min\" : NumberDecimal(\"199.99\"),    \"max\" : NumberDecimal(\"385.00\")  },  \"count\" : 2}{  \"_id\" : {    \"min\" : NumberDecimal(\"385.00\"),    \"max\" : NumberDecimal(\"483.00\")  },  \"count\" : 2}  \nMULTI-FACETED AGGREGATION \nThe $bucketAuto stage can be used within the $facet stage to process multiple aggregation pipelines on the same set of input documents from artwork. The following aggregation pipeline groups the documents from the artwork collection into buckets based on price, year, and the calculated area: db.artwork.aggregate( [  {    $facet: {      \"price\": [        {          $bucketAuto: {            groupBy: \"$price\",            buckets: 4          }        }      ],      \"year\": [        {          $bucketAuto: {            groupBy: \"$year\",            buckets: 3,            output: {              \"count\": { $sum: 1 },              \"years\": { $push: \"$year\" }            }          }        }      ],      \"area\": [        {          $bucketAuto: {            groupBy: {              $multiply: [ \"$dimensions.height\", \"$dimensions.width\" ]            },            buckets: 4,            output: {              \"count\": { $sum: 1 },              \"titles\": { $push: \"$title\" }            }          }        }      ]    }  }] )  The operation returns the following document:\n\n \n←  $bucket (aggregation)$changeStream (aggregation) → On this page  * Definition\n * Considerations\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/set/": " Docs Home → MongoDB Manual \n$SET (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION  NOTE \nDISAMBIGUATION The following page refers to the aggregation stage $set, available starting in MongoDB 4.2 For the update operator $set, see $set. $set \nNew in version 4.2. Adds new fields to documents. $set outputs documents that contain all existing fields from the input documents and newly added fields. The $set stage is an alias for $addFields. Both stages are equivalent to a $project stage that explicitly specifies all existing fields in the input documents and adds the new fields. $set has the following form: { $set: { <newField>: <expression>, ... } }  Specify the name of each field to add and set its value to an aggregation expression. For more information on expressions, see Expressions. \nIMPORTANT If the name of the new field is the same as an existing field name (including _id), $set overwrites the existing value of that field with the value of the specified expression. \nBEHAVIOR \n$set appends new fields to existing documents. You can include one or more $set stages in an aggregation operation. To add field or fields to embedded documents (including documents in arrays) use the dot notation. See example. To add an element to an existing array field with $set, use with $concatArrays. See example. \nEXAMPLES  USING TWO $SET STAGES \nCreate a sample scores collection with the following: db.scores.insertMany([   { _id: 1, student: \"Maya\", homework: [ 10, 5, 10 ], quiz: [ 10, 8 ], extraCredit: 0 },   { _id: 2, student: \"Ryan\", homework: [ 5, 6, 5 ], quiz: [ 8, 8 ], extraCredit: 8 }])  The following operation uses two $set stages to include three new fields in the output documents: db.scores.aggregate( [   {     $set: {        totalHomework: { $sum: \"$homework\" },        totalQuiz: { $sum: \"$quiz\" }     }   },   {     $set: {        totalScore: { $add: [ \"$totalHomework\", \"$totalQuiz\", \"$extraCredit\" ] } }   }] )  The operation returns the following documents: {  \"_id\" : 1,  \"student\" : \"Maya\",  \"homework\" : [ 10, 5, 10 ],  \"quiz\" : [ 10, 8 ],  \"extraCredit\" : 0,  \"totalHomework\" : 25,  \"totalQuiz\" : 18,  \"totalScore\" : 43}{  \"_id\" : 2,  \"student\" : \"Ryan\",  \"homework\" : [ 5, 6, 5 ],  \"quiz\" : [ 8, 8 ],  \"extraCredit\" : 8,  \"totalHomework\" : 16,  \"totalQuiz\" : 16,  \"totalScore\" : 40}  \nADDING FIELDS TO AN EMBEDDED DOCUMENT \nUse dot notation to add new fields to embedded documents. Create a sample collection vehicles with the following: db.vehicles.insertMany([   { _id: 1, type: \"car\", specs: { doors: 4, wheels: 4 } },   { _id: 2, type: \"motorcycle\", specs: { doors: 0, wheels: 2 } },   { _id: 3, type: \"jet ski\" }])  The following aggregation operation adds a new field fuel_type to the embedded document specs. db.vehicles.aggregate( [   { $set: { \"specs.fuel_type\": \"unleaded\" } }] )  The operation returns the following results: { _id: 1, type: \"car\", specs: { doors: 4, wheels: 4, fuel_type: \"unleaded\" } }{ _id: 2, type: \"motorcycle\", specs: { doors: 0, wheels: 2, fuel_type: \"unleaded\" } }{ _id: 3, type: \"jet ski\", specs: { fuel_type: \"unleaded\" } }  \nOVERWRITING AN EXISTING FIELD\n\n Create a sample collection called animals with the following: db.animals.insertOne( { _id: 1, dogs: 10, cats: 15 } )  The following $set operation overrides the cats field: db.animals.aggregate( [  { $set: { \"cats\": 20 } }] )  The operation returns the following document: { _id: 1, dogs: 10, cats: 20 }  It is possible to replace one field with another. In the following example the item field substitutes for the _id field. Create a sample collection called fruits contains the following documents: db.fruits.insertMany([   { \"_id\" : 1, \"item\" : \"tangerine\", \"type\" : \"citrus\" },   { \"_id\" : 2, \"item\" : \"lemon\", \"type\" : \"citrus\" },   { \"_id\" : 3, \"item\" : \"grapefruit\", \"type\" : \"citrus\" }])  The following aggregration operation uses $set to replace the _id field of each document with the value of the item field, and replaces the item field with a string \"fruit\". db.fruits.aggregate( [  { $set: { _id : \"$item\", item: \"fruit\" } }] )  The operation returns the following: { \"_id\" : \"tangerine\", \"item\" : \"fruit\", \"type\" : \"citrus\" }{ \"_id\" : \"lemon\", \"item\" : \"fruit\", \"type\" : \"citrus\" }{ \"_id\" : \"grapefruit\", \"item\" : \"fruit\", \"type\" : \"citrus\" }  \nADD ELEMENT TO AN ARRAY \nCreate a sample scores collection with the following: db.scores.insertMany([   { _id: 1, student: \"Maya\", homework: [ 10, 5, 10 ], quiz: [ 10, 8 ], extraCredit: 0 },   { _id: 2, student: \"Ryan\", homework: [ 5, 6, 5 ], quiz: [ 8, 8 ], extraCredit: 8 }])  You can use $set with a $concatArrays expression to add an element to an existing array field. For example, the following operation uses $set to replace the homework field with a new array whose elements are the current homework array concatenated with another array containing a new score [ 7 ]. db.scores.aggregate([   { $match: { _id: 1 } },   { $set: { homework: { $concatArrays: [ \"$homework\", [ 7 ] ] } } }])  The operation returns the following: { \"_id\" : 1, \"student\" : \"Maya\", \"homework\" : [ 10, 5, 10, 7 ], \"quiz\" : [ 10, 8 ], \"extraCredit\" : 0 } \nCREATING A NEW FIELD WITH EXISTING FIELDS \nCreate a sample scores collection with the following: db.scores.insertMany([   { _id: 1, student: \"Maya\", homework: [ 10, 5, 10 ], quiz: [ 10, 8 ], extraCredit: 0 },   { _id: 2, student: \"Ryan\", homework: [ 5, 6, 5 ], quiz: [ 8, 8 ], extraCredit: 8 }])  The following aggregation operation adds a new field quizAverage to each document that contains the average of the quiz array. db.scores.aggregate( [   {      $set: {         quizAverage: { $avg: \"$quiz\" }      }   }] )  The operation returns the following documents: [   {      _id: 1,      student: 'Maya',      homework: [ 10, 5, 10 ],      quiz: [ 10, 8 ],      extraCredit: 0,      quizAverage: 9   },   {      _id: 2,      student: 'Ryan',      homework: [ 5, 6, 5 ],      quiz: [ 8, 8 ],      extraCredit: 8,      quizAverage: 8   }] ←  $searchMeta (aggregation)$setWindowFields (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/reduce/": " Docs Home → MongoDB Manual \n$REDUCE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$reduce \nApplies an expression to each element in an array and combines them into a single value. $reduce has the following syntax: {    $reduce: {        input: <array>,        initialValue: <expression>,        in: <expression>    }} \nField\nType\nDescription\ninput\narray Can be any valid expression that resolves to an array. For more information on expressions, see Expressions. If the argument resolves to a value of null or refers to a missing field, $reduce returns null. If the argument does not resolve to an array or null nor refers to a missing field, $reduce returns an error. initialValue\nexpression\nThe initial cumulative value set before in is applied to the first element of the input array.\nin\nexpression A valid expression that $reduce applies to each element in the input array in left-to-right order. Wrap the input value with $reverseArray to yield the equivalent of applying the combining expression from right-to-left. During evaluation of the in expression, two variables will be available:      * value is the variable that represents the cumulative value of the expression.  * this is the variable that refers to the element being processed. If input resolves to an empty array, $reduce returns initialValue. Example\nResults  {    $reduce: {       input: [\"a\", \"b\", \"c\"],       initialValue: \"\",       in: { $concat : [\"$$value\", \"$$this\"] }     } } \"abc\" {   $reduce: {      input: [ 1, 2, 3, 4 ],      initialValue: { sum: 5, product: 2 },      in: {         sum: { $add : [\"$$value.sum\", \"$$this\"] },         product: { $multiply: [ \"$$value.product\", \"$$this\" ] }      }   }} { \"sum\" : 15, \"product\" : 48 } {   $reduce: {      input: [ [ 3, 4 ], [ 5, 6 ] ],      initialValue: [ 1, 2 ],      in: { $concatArrays : [\"$$value\", \"$$this\"] }   }} [ 1, 2, 3, 4, 5, 6 ] \nEXAMPLES  MULTIPLICATION \nPROBABILITY \nA collection named events contains the events of a probability experiment. Each experiment can have multiple events, such as rolling a die several times or drawing several cards (without replacement) in succession to achieve a desired result. In order to obtain the overall probability of the experiment, we will need to multiply the probability of each event in the experiment.\n\n  Steps:  1. Use $group to group by the experimentId and use $push to create an array with the probability of each event.  2. Use $reduce with $multiply to multiply and combine the elements of probabilityArr into a single value and project it. db.probability.aggregate(  [    {      $group: {        _id: \"$experimentId\",        \"probabilityArr\": { $push: \"$probability\" }      }    },    {      $project: {        \"description\": 1,        \"results\": {          $reduce: {            input: \"$probabilityArr\",            initialValue: 1,            in: { $multiply: [ \"$$value\", \"$$this\" ] }          }        }      }    }  ])  The operation returns the following: { \"_id\" : \"dak\", \"results\" : 0.00603318250377101 }{ \"_id\" : \"r5\", \"results\" : 0.16666666666667 }{ \"_id\" : \"r16\", \"results\" : 0.027777777777778886 }{ \"_id\" : \"d3rc\", \"results\" : 0.11764705882352879 }  DISCOUNTED MERCHANDISE \nA collection named clothes contains the following documents: { \"_id\" : 1, \"productId\" : \"ts1\", \"description\" : \"T-Shirt\", \"color\" : \"black\", \"size\" : \"M\", \"price\" : 20, \"discounts\" : [ 0.5, 0.1 ] }{ \"_id\" : 2, \"productId\" : \"j1\", \"description\" : \"Jeans\", \"color\" : \"blue\", \"size\" : \"36\", \"price\" : 40, \"discounts\" : [ 0.25, 0.15, 0.05 ] }{ \"_id\" : 3, \"productId\" : \"s1\", \"description\" : \"Shorts\", \"color\" : \"beige\", \"size\" : \"32\", \"price\" : 30, \"discounts\" : [ 0.15, 0.05 ] }{ \"_id\" : 4, \"productId\" : \"ts2\", \"description\" : \"Cool T-Shirt\", \"color\" : \"White\", \"size\" : \"L\", \"price\" : 25, \"discounts\" : [ 0.3 ] }{ \"_id\" : 5, \"productId\" : \"j2\", \"description\" : \"Designer Jeans\", \"color\" : \"blue\", \"size\" : \"30\", \"price\" : 80, \"discounts\" : [ 0.1, 0.25 ] }  Each document contains a discounts array containing the currently available percent-off coupons for each item. If each discount can be applied to the product once, we can calculate the lowest price by using $reduce to apply the following formula for each element in the discounts array: (1 - discount) * price. db.clothes.aggregate(  [    {      $project: {        \"discountedPrice\": {          $reduce: {            input: \"$discounts\",            initialValue: \"$price\",            in: { $multiply: [ \"$$value\", { $subtract: [ 1, \"$$this\" ] } ] }          }        }      }    }  ])  The operation returns the following:\n\n  \nSTRING CONCATENATION \nA collection named people contains the following documents: { \"_id\" : 1, \"name\" : \"Melissa\", \"hobbies\" : [ \"softball\", \"drawing\", \"reading\" ] }{ \"_id\" : 2, \"name\" : \"Brad\", \"hobbies\" : [ \"gaming\", \"skateboarding\" ] }{ \"_id\" : 3, \"name\" : \"Scott\", \"hobbies\" : [ \"basketball\", \"music\", \"fishing\" ] }{ \"_id\" : 4, \"name\" : \"Tracey\", \"hobbies\" : [ \"acting\", \"yoga\" ] }{ \"_id\" : 5, \"name\" : \"Josh\", \"hobbies\" : [ \"programming\" ] }{ \"_id\" : 6, \"name\" : \"Claire\" }  The following example reduces the hobbies array of strings into a single string bio: db.people.aggregate(   [     // Filter to return only non-empty arrays     { $match: { \"hobbies\": { $gt: [ ] } } },     {       $project: {         \"name\": 1,         \"bio\": {           $reduce: {             input: \"$hobbies\",             initialValue: \"My hobbies include:\",             in: {               $concat: [                 \"$$value\",                 {                   $cond: {                     if: { $eq: [ \"$$value\", \"My hobbies include:\" ] },                     then: \" \",                     else: \", \"                   }                 },                 \"$$this\"               ]             }           }         }       }     }   ])  The operation returns the following: { \"_id\" : 1, \"name\" : \"Melissa\", \"bio\" : \"My hobbies include: softball, drawing, reading\" }{ \"_id\" : 2, \"name\" : \"Brad\", \"bio\" : \"My hobbies include: gaming, skateboarding\" }{ \"_id\" : 3, \"name\" : \"Scott\", \"bio\" : \"My hobbies include: basketball, music, fishing\" }{ \"_id\" : 4, \"name\" : \"Tracey\", \"bio\" : \"My hobbies include: acting, yoga\" }{ \"_id\" : 5, \"name\" : \"Josh\", \"bio\" : \"My hobbies include: programming\" }  \nARRAY CONCATENATION \nA collection named matrices contains the following documents: { \"_id\" : 1, \"arr\" : [ [ 24, 55, 79 ], [ 14, 78, 35 ], [ 84, 90, 3 ], [ 50, 89, 70 ] ] }{ \"_id\" : 2, \"arr\" : [ [ 39, 32, 43, 7 ], [ 62, 17, 80, 64 ], [ 17, 88, 11, 73 ] ] }{ \"_id\" : 3, \"arr\" : [ [ 42 ], [ 26, 59 ], [ 17 ], [ 72, 19, 35 ] ] }{ \"_id\" : 4 }  COMPUTING A SINGLE REDUCTION \nThe following example collapses the two dimensional arrays into a single array collapsed:\n\n  The operation returns the following: { \"_id\" : 1, \"collapsed\" : [ 24, 55, 79, 14, 78, 35, 84, 90, 3, 50, 89, 70 ] }{ \"_id\" : 2, \"collapsed\" : [ 39, 32, 43, 7, 62, 17, 80, 64, 17, 88, 11, 73 ] }{ \"_id\" : 3, \"collapsed\" : [ 42, 26, 59, 17, 72, 19, 35 ] }{ \"_id\" : 4, \"collapsed\" : null }  COMPUTING A MULTIPLE REDUCTIONS \nThe following example performs the same two dimensional array collapse as the example above, but also creates a new array containing only the first element of each array. db.arrayconcat.aggregate(  [    {      $project: {        \"results\": {          $reduce: {            input: \"$arr\",            initialValue: [ ],            in: {              \"collapsed\": {                $concatArrays: [ \"$$value.collapsed\", \"$$this\" ]              },              \"firstValues\": {                $concatArrays: [ \"$$value.firstValues\", { $slice: [ \"$$this\", 1 ] } ]              }            }          }        }      }    }  ])  The operation returns the following: { \"_id\" : 1, \"results\" : { \"collapsed\" : [ 24, 55, 79, 14, 78, 35, 84, 90, 3, 50, 89, 70 ], \"firstValues\" : [ 24, 14, 84, 50 ] } }{ \"_id\" : 2, \"results\" : { \"collapsed\" : [ 39, 32, 43, 7, 62, 17, 80, 64, 17, 88, 11, 73 ], \"firstValues\" : [ 39, 62, 17 ] } }{ \"_id\" : 3, \"results\" : { \"collapsed\" : [ 42, 26, 59, 17, 72, 19, 35 ], \"firstValues\" : [ 42, 26, 17, 72 ] } }{ \"_id\" : 4, \"results\" : null } \n←  $rank (aggregation)$regexFind (aggregation) → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/locf/": " Docs Home → MongoDB Manual \n$LOCF (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$locf \nNew in version 5.2. Last observation carried forward. Sets values for null and missing fields in a window to the last non-null value for the field. $locf is only available in the $setWindowFields stage. \nSYNTAX \nThe $locf expression has this syntax: { $locf: <expression> }  For more information on expressions, see Expressions. \nBEHAVIOR \nIf a field being filled contains both null and non-null values, $locf sets the null and missing values to the field's last known non-null value according to the sort order specified in $setWindowFields. null and missing field values that appear before non-null values in the sort order remain null. If a field being filled contains only null or missing values in a partition, $locf sets the field value to null for that partition. \nCOMPARISON OF $FILL AND $LOCF \nTo fill missing field values based on the last observed value in a sequence, you can use:      * The $fill stage with { method: \"locf\" }.\n   \n   When you use the $fill stage, the field you specify in the output is the same field used as the source data. See Fill Missing Field Values Based on the Last Observed Value.  * The $locf operator inside of a $setWindowFields stage.\n   \n   When you use the $locf operator, you can set values for a different field than the field used as the source data. See Use Multiple Fill Methods in a Single Stage. \nEXAMPLES \nThe examples on this page use a stock collection that contains tracks a single company's stock price at hourly intervals: db.stock.insertMany( [   {      time: ISODate(\"2021-03-08T09:00:00.000Z\"),      price: 500   },   {      time: ISODate(\"2021-03-08T10:00:00.000Z\"),   },   {      time: ISODate(\"2021-03-08T11:00:00.000Z\"),      price: 515   },   {      time: ISODate(\"2021-03-08T12:00:00.000Z\")   },   {      time: ISODate(\"2021-03-08T13:00:00.000Z\")   },   {      time: ISODate(\"2021-03-08T14:00:00.000Z\"),      price: 485   }] )  The price field is missing for some of the documents in the collection. \nFILL MISSING VALUES WITH THE LAST OBSERVED VALUE \nThe following example uses the $locf operator to set missing fields to the value from the last-observed non-null value: db.stock.aggregate( [   {      $setWindowFields: {        sortBy: { time: 1 },        output: {           price: { $locf: \"$price\" }        }      }    }] )  In the example:  * sortBy: { time: 1 } sorts the documents in each partition by time in ascending order (1), so the earliest time is first.  * For documents where the price field is missing, the $locf operator sets the price to the last-observed value in the sequence. Example output:\n\n \nUSE MULTIPLE FILL METHODS IN A SINGLE STAGE \nWhen you use the $setWindowFields stage to fill missing values, you can set values for a different field than the field you fill from. As a result, you can use multiple fill methods in a single $setWindowFields stage and output the results in distinct fields. The following pipeline populates missing price fields using linear interpolation and the last-observation-carried-forward method: db.stock.aggregate( [   {      $setWindowFields:         {            sortBy: { time: 1 },            output:               {                  linearFillPrice: { $linearFill: \"$price\" },                  locfPrice: { $locf: \"$price\" }               }         }   }] )  In the example:  * sortBy: { time: 1 } sorts the documents by the time field in ascending order, from earliest to latest.  * output specifies:\n   \n   * linearFillPrice as a target field to be filled.\n     \n     * { $linearFill: \"$price\" } is the value for the linearFillPrice field. $linearFill fills missing price values using linear interpolation based on the surrounding price values in the sequence.\n   \n   * locfPrice as a target field to be filled.\n     \n     * { $locf: \"$price\" } is the value for the locfPrice field. locf stands for last observation carried forward. $locf fills missing price values with the value from the previous document in the sequence. Example output: [  {    _id: ObjectId(\"620ad555394d47411658b5ef\"),    time: ISODate(\"2021-03-08T09:00:00.000Z\"),    price: 500,    linearFillPrice: 500,    locfPrice: 500  },  {    _id: ObjectId(\"620ad555394d47411658b5f0\"),    time: ISODate(\"2021-03-08T10:00:00.000Z\"),    linearFillPrice: 507.5,    locfPrice: 500  },  {    _id: ObjectId(\"620ad555394d47411658b5f1\"),    time: ISODate(\"2021-03-08T11:00:00.000Z\"),    price: 515,    linearFillPrice: 515,    locfPrice: 515  },  {    _id: ObjectId(\"620ad555394d47411658b5f2\"),    time: ISODate(\"2021-03-08T12:00:00.000Z\"),    linearFillPrice: 505,    locfPrice: 515  },  {    _id: ObjectId(\"620ad555394d47411658b5f3\"),    time: ISODate(\"2021-03-08T13:00:00.000Z\"),    linearFillPrice: 495,    locfPrice: 515  },  {    _id: ObjectId(\"620ad555394d47411658b5f4\"),    time: ISODate(\"2021-03-08T14:00:00.000Z\"),    price: 485,    linearFillPrice: 485,    locfPrice: 485  }] ←  $ln (aggregation)$log (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/integral/": " Docs Home → MongoDB Manual \n$INTEGRAL (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \nNew in version 5.0. $integral \nReturns the approximation of the area under a curve, which is calculated using the trapezoidal rule where each set of adjacent documents form a trapezoid using the:      * sortBy field values in the $setWindowFields stage for the integration intervals.  * input field expression result values in $integral for the y axis values. $integral is only available in the $setWindowFields stage. $integral syntax: {   $integral: {      input: <expression>,      unit: <time unit>   }} $integral takes a document with these fields: Field\nDescription\ninput Specifies the expression to evaluate. You must provide an expression that returns a number. unit A string that specifies the time unit. Use one of these strings:  * \"week\"  * \"day\"  * \"hour\"  * \"minute\"  * \"second\"  * \"millisecond\" If the sortBy field is not a date, you must omit a unit. If you specify a unit, you must specify a date in the sortBy field. \nBEHAVIOR \nIf you omit a window, a default window with unbounded upper and lower limits is used. \nEXAMPLE \nCreate a powerConsumption collection that contains electrical power usage in kilowatts measured by meter devices at 30 second intervals: db.powerConsumption.insertMany( [   { powerMeterID: \"1\", timeStamp: new Date( \"2020-05-18T14:10:30Z\" ),     kilowatts: 2.95 },   { powerMeterID: \"1\", timeStamp: new Date( \"2020-05-18T14:11:00Z\" ),     kilowatts: 2.7 },   { powerMeterID: \"1\", timeStamp: new Date( \"2020-05-18T14:11:30Z\" ),     kilowatts: 2.6 },   { powerMeterID: \"1\", timeStamp: new Date( \"2020-05-18T14:12:00Z\" ),     kilowatts: 2.98 },   { powerMeterID: \"2\", timeStamp: new Date( \"2020-05-18T14:10:30Z\" ),     kilowatts: 2.5 },   { powerMeterID: \"2\", timeStamp: new Date( \"2020-05-18T14:11:00Z\" ),     kilowatts: 2.25 },   { powerMeterID: \"2\", timeStamp: new Date( \"2020-05-18T14:11:30Z\" ),     kilowatts: 2.75 },   { powerMeterID: \"2\", timeStamp: new Date( \"2020-05-18T14:12:00Z\" ),     kilowatts: 2.82 }] )  This example uses $integral in the $setWindowFields stage to output the energy consumption in kilowatt-hours measured by each meter device:\n\n  In the example:  * partitionBy: \"$powerMeterID\" partitions the documents in the collection by powerMeterID.  * sortBy: { timeStamp: 1 } sorts the documents in each partition by timeStamp in ascending order (1), so the earliest timeStamp is first.  * output sets the kilowatts integral value in a new field called powerMeterKilowattHours using $integral that is run in a range window.\n   \n   * The input expression is set to \"$kilowatts\", which is used for the y axis values in the integral calculation.\n   \n   * The $integral unit is set to \"hour\" for the timeStamp field, which means $integral returns the kilowatt-hours energy consumption.\n   \n   * The window contains documents between an unbounded lower limit and the current document in the output. This means $integral returns the total kilowatt-hours energy consumption for the documents from the beginning of the partition, which is the first data point in the partition for each power meter, to the timestamp of the current document in the output. In this example output, the energy consumption measured by meters 1 and 2 are shown in the powerMeterKilowattHours field: { \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62863\"), \"powerMeterID\" : \"1\",  \"timeStamp\" : ISODate(\"2020-05-18T14:10:30Z\"), \"kilowatts\" : 2.95,  \"powerMeterKilowattHours\" : 0 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62864\"), \"powerMeterID\" : \"1\",  \"timeStamp\" : ISODate(\"2020-05-18T14:11:00Z\"), \"kilowatts\" : 2.7,  \"powerMeterKilowattHours\" : 0.023541666666666666 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62865\"), \"powerMeterID\" : \"1\",  \"timeStamp\" : ISODate(\"2020-05-18T14:11:30Z\"), \"kilowatts\" : 2.6,  \"powerMeterKilowattHours\" : 0.045625 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62866\"), \"powerMeterID\" : \"1\",  \"timeStamp\" : ISODate(\"2020-05-18T14:12:00Z\"), \"kilowatts\" : 2.98,  \"powerMeterKilowattHours\" : 0.068875 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62867\"), \"powerMeterID\" : \"2\",  \"timeStamp\" : ISODate(\"2020-05-18T14:10:30Z\"), \"kilowatts\" : 2.5,  \"powerMeterKilowattHours\" : 0 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62868\"), \"powerMeterID\" : \"2\",  \"timeStamp\" : ISODate(\"2020-05-18T14:11:00Z\"), \"kilowatts\" : 2.25,  \"powerMeterKilowattHours\" : 0.019791666666666666 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e62869\"), \"powerMeterID\" : \"2\",  \"timeStamp\" : ISODate(\"2020-05-18T14:11:30Z\"), \"kilowatts\" : 2.75,  \"powerMeterKilowattHours\" : 0.040625 }{ \"_id\" : ObjectId(\"60cbdc3f833dfeadc8e6286a\"), \"powerMeterID\" : \"2\",  \"timeStamp\" : ISODate(\"2020-05-18T14:12:00Z\"), \"kilowatts\" : 2.82,  \"powerMeterKilowattHours\" : 0.06383333333333334 } \nTIP \nSEE ALSO: For an additional example about IOT Power Consumption, see the Practical MongoDB Aggregations e-book.\n\n On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/shardedDataDistribution/": " Docs Home → MongoDB Manual \n$SHARDEDDATADISTRIBUTION (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Output Fields\n * Examples \nDEFINITION \n$shardedDataDistribution \nNew in version 6.0.3. Returns information on the distribution of data in sharded collections. \nNOTE This aggregation stage is only available on mongos. This aggregation stage must be run on the admin database. The user must have the shardedDataDistribution privilege action. \nSYNTAX \nThe shardedDataDistribution stage has the following syntax: db.aggregate( [   { $shardedDataDistribution: { } }] )  \nOUTPUT FIELDS \nThe $shardedDataDistribution stage outputs an array of documents for each sharded collection in the database. These documents contain the following fields: Field Name\nData Type\nDescription\nns\nstring\nThe namespace of the sharded collection.\nshards\narray\nLists the shards in the collection with data distribution information on each shard.\nshards.numOrphanedDocs\ninteger\nNumber of orphaned documents in the shard.\nshards.numOwnedDocuments\ninteger\nNumber of documents owned by the shard.\nshards.orphanedSizeBytes\ninteger\nAmount of storage in bytes used by orphaned documents in the shard.\nshards.ownedSizeBytes\ninteger\nAmount of storage in bytes used by owned documents in the shard. \nEXAMPLES \ndb.aggregate( [   { $shardedDataDistribution: { } }] )  Example output: [  {    \"ns\": \"test.names\",    \"shards\": [      {        \"shardName\": \"shard-1\",        \"numOrphanedDocs\": 0,        \"numOwnedDocuments\": 6,        \"ownedSizeBytes\": 366,        \"orphanedSizeBytes\": 0      },      {        \"shardName\": \"shard-2\",        \"numOrphanedDocs\": 0,        \"numOwnedDocuments\": 6,        \"ownedSizeBytes\": 366,        \"orphanedSizeBytes\": 0      }    ]  }] \n←  $setWindowFields (aggregation)$skip (aggregation) → On this page  * Definition\n * Syntax\n * Output Fields\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/limit/": " Docs Home → MongoDB Manual \n$LIMIT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$limit \nLimits the number of documents passed to the next stage in the pipeline. The $limit stage has the following prototype form: { $limit: <positive 64-bit integer> }  $limit takes a positive integer that specifies the maximum number of documents to pass along. \nNOTE Starting in MongoDB 5.0, the $limit pipeline aggregation has a 64-bit integer limit. Values passed to the pipeline which exceed this limit will return a invalid argument error. \nBEHAVIOR  USING $LIMIT WITH SORTED RESULTS \nIf using the $limit stage with any of:      * the $sort aggregation stage,  * the sort() method, or  * the sort field to the findAndModify command or the findAndModify() shell method, be sure to include at least one field in your sort that contains unique values, before passing results to the $limit stage. Sorting on fields that contain duplicate values may return an inconsistent sort order for those duplicate fields over multiple executions, especially when the collection is actively receiving writes. The easiest way to guarantee sort consistency is to include the _id field in your sort query. See the following for more information on each:  * Consistent sorting with $sort (aggregation)  * Consistent sorting with the sort() shell method  * Consistent sorting with the findAndModify command  * Consistent sorting with the findAndModify() shell method \nEXAMPLE \nConsider the following example: db.article.aggregate([   { $limit : 5 }]);  This operation returns only the first 5 documents passed to it by the pipeline. $limit has no effect on the content of the documents it passes. \nNOTE When a $sort precedes a $limit and there are no intervening stages that modify the number of documents, the optimizer can coalesce the $limit into the $sort. This allows the $sort operation to only maintain the top n results as it progresses, where n is the specified limit, and ensures that MongoDB only needs to store n items in memory. This optimization still applies when allowDiskUse is true and the n items exceed the aggregation memory limit. \nTIP \nSEE ALSO:  * Aggregation with the Zip Code Data Set,  * Aggregation with User Preference Data ←  $indexStats (aggregation)$listLocalSessions → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/regexMatch/": " Docs Home → MongoDB Manual \n$REGEXMATCH (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$regexMatch \nNew in version 4.2. Performs a regular expression (regex) pattern matching and returns:      * true if a match exists.  * false if a match doesn't exist. MongoDB uses Perl compatible regular expressions (i.e. \"PCRE\" ) version 8.41 with UTF-8 support. Prior to MongoDB 4.2, aggregation pipeline can only use the query operator $regex in the $match stage. For more information on using regex in a query, see $regex. \nSYNTAX \nThe $regexMatch operator has the following syntax: { $regexMatch: { input: <expression> , regex: <expression>, options: <expression> } } \nField\nDescription\ninput The string on which you wish to apply the regex pattern. Can be a string or any valid expression that resolves to a string. regex The regex pattern to apply. Can be any valid expression that resolves to either a string or regex pattern /<pattern>/. When using the regex /<pattern>/, you can also specify the regex options i and m (but not the s or x options):  * \"pattern\"  * /<pattern>/  * /<pattern>/<options> Alternatively, you can also specify the regex options with the options field. To specify the s or x options, you must use the options field. You cannot specify options in both the regex and the options field. options Optional. The following <options> are available for use with regular expression. \nNOTE You cannot specify options in both the regex and the options field. Option\nDescription\ni\nCase insensitivity to match both upper and lower cases. You can specify the option in the options field or as part of the regex field.\nm For patterns that include anchors (i.e. ^ for the start, $ for the end), match at the beginning or end of each line for strings with multiline values. Without this option, these anchors match at beginning or end of the string. If the pattern contains no anchors or if the string value has no newline characters (e.g. \\n), the m option has no effect. x \"Extended\" capability to ignore all white space characters in the pattern unless escaped or included in a character class. Additionally, it ignores characters in-between and including an un-escaped hash/pound (#) character and the next new line, so that you may include comments in complicated patterns. This only applies to data characters; white space characters may never appear within special character sequences in a pattern. The x option does not affect the handling of the VT character (i.e. code 11). You can specify the option only in the options field. s Allows the dot character (i.e. .) to match all characters including newline characters. You can specify the option only in the options field. \nRETURNS \nThe operator returns a boolean:  * true if a match exists.  * false if a match doesn't exist. \nTIP \nSEE ALSO:  * $regexFind  * $regexFindAll \nBEHAVIOR  $REGEXMATCH AND COLLATION \n$regexMatch ignores the collation specified for the collection, db.collection.aggregate(), and the index, if used. For example, the create a sample collection with collation strength 1 (i.e. compare base character only and ignore other differences such as case and diacritics): db.createCollection( \"myColl\", { collation: { locale: \"fr\", strength: 1 } } )  Insert the following documents: db.myColl.insertMany([   { _id: 1, category: \"café\" },   { _id: 2, category: \"cafe\" },   { _id: 3, category: \"cafE\" }])  Using the collection's collation, the following operation performs a case-insensitive and diacritic-insensitive match: db.myColl.aggregate( [ { $match: { category: \"cafe\" } } ] )  The operation returns the following 3 documents: { \"_id\" : 1, \"category\" : \"café\" }{ \"_id\" : 2, \"category\" : \"cafe\" }{ \"_id\" : 3, \"category\" : \"cafE\" }  However, the aggregation expression $regexMatch ignores collation; that is, the following regular expression pattern matching examples are case-sensitive and diacritic sensitive:\n\n  Both operations return the following: { \"_id\" : 1, \"category\" : \"café\", \"results\" : false }{ \"_id\" : 2, \"category\" : \"cafe\", \"results\" : true }{ \"_id\" : 3, \"category\" : \"cafE\", \"results\" : false }  To perform a case-insensitive regex pattern matching, use the i Option instead. See i Option for an example. \nEXAMPLES  $REGEXMATCH AND ITS OPTIONS \nTo illustrate the behavior of the $regexMatch operator as discussed in this example, create a sample collection products with the following documents: db.products.insertMany([   { _id: 1, description: \"Single LINE description.\" },   { _id: 2, description: \"First lines\\nsecond line\" },   { _id: 3, description: \"Many spaces before     line\" },   { _id: 4, description: \"Multiple\\nline descriptions\" },   { _id: 5, description: \"anchors, links and hyperlinks\" },   { _id: 6, description: \"métier work vocation\" }])  By default, $regexMatch performs a case-sensitive match. For example, the following aggregation performs a case-sensitive $regexMatch on the description field. The regex pattern /line/ does not specify any grouping: db.products.aggregate([   { $addFields: { result: { $regexMatch: { input: \"$description\", regex: /line/ } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"result\" : false }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"result\" : true }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"result\" : true }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"result\" : true }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"result\" : false }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"result\" : false } The following regex pattern /lin(e|k)/ specifies a grouping (e|k) in the pattern: db.products.aggregate([   { $addFields: { result: { $regexMatch: { input: \"$description\", regex: /lin(e|k)/ } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"result\" : false }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"result\" : true }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"result\" : true }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"result\" : true }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"result\" : true }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"result\" : false } I OPTION  NOTE You cannot specify options in both the regex and the options field. To perform case-insensitive pattern matching, include the i option as part of the regex field or in the options field: // Specify i as part of the regex field{ $regexMatch: { input: \"$description\", regex: /line/i } }\n// Specify i in the options field{ $regexMatch: { input: \"$description\", regex: /line/, options: \"i\" } }{ $regexMatch: { input: \"$description\", regex: \"line\", options: \"i\" } } For example, the following aggregation performs a case-insensitive $regexMatch on the description field. The regex pattern /line/ does not specify any grouping: db.products.aggregate([   { $addFields: { result: { $regexMatch: { input: \"$description\", regex: /line/i } } } }])  The operation returns the following documents:\n\n M OPTION  NOTE You cannot specify options in both the regex and the options field. To match the specified anchors (e.g. ^, $) for each line of a multiline string, include the m option as part of the regex field or in the options field: // Specify m as part of the regex field{ $regexMatch: { input: \"$description\", regex: /line/m } }\n// Specify m in the options field{ $regexMatch: { input: \"$description\", regex: /line/, options: \"m\" } }{ $regexMatch: { input: \"$description\", regex: \"line\", options: \"m\" } } The following example includes both the i and the m options to match lines starting with either the letter s or S for multiline strings: db.products.aggregate([   { $addFields: { result: { $regexMatch: { input: \"$description\", regex: /^s/im } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"result\" : true }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"result\" : true }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"result\" : false }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"result\" : false }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"result\" : false }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"result\" : false } X OPTION  NOTE You cannot specify options in both the regex and the options field. To ignore all unescaped white space characters and comments (denoted by the un-escaped hash # character and the next new-line character) in the pattern, include the s option in the options field: // Specify x in the options field{ $regexMatch: { input: \"$description\", regex: /line/, options: \"x\" } }{ $regexMatch: { input: \"$description\", regex: \"line\", options: \"x\" } } The following example includes the x option to skip unescaped white spaces and comments: db.products.aggregate([   { $addFields: { returns: { $regexMatch: { input: \"$description\", regex: /lin(e|k) # matches line or link/, options:\"x\" } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"returns\" : false }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"returns\" : true }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"returns\" : true }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"returns\" : true }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"returns\" : true }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"returns\" : false } S OPTION  NOTE You cannot specify options in both the regex and the options field. To allow the dot character (i.e. .) in the pattern to match all characters including the new line character, include the s option in the options field: // Specify s in the options field{ $regexMatch: { input: \"$description\", regex: /m.*line/, options: \"s\" } }{ $regexMatch: { input: \"$description\", regex: \"m.*line\", options: \"s\" } } The following example includes the s option to allow the dot character (i.e. .) to match all characters including new line as well as the i option to perform a case-insensitive match: db.products.aggregate([   { $addFields: { returns: { $regexMatch: { input: \"$description\", regex:/m.*line/, options: \"si\"  } } } }])  The operation returns the following:\n\n \nUSE $REGEXMATCH TO CHECK EMAIL ADDRESS \nCreate a sample collection feedback with the following documents: db.feedback.insertMany([   { \"_id\" : 1, comment: \"Hi, I'm just reading about MongoDB -- aunt.arc.tica@example.com\"  },   { \"_id\" : 2, comment: \"I wanted to concatenate a string\" },   { \"_id\" : 3, comment: \"How do I convert a date to string? Contact me at either cam@mongodb.com or c.dia@mongodb.com\" },   { \"_id\" : 4, comment: \"It's just me. I'm testing.  fred@MongoDB.com\" }])  The following aggregation uses the $regexMatch to check if the comment field contains an email address with @mongodb.com and categorize the feedback as Employee or External. db.feedback.aggregate( [    { $addFields: {       \"category\": { $cond: { if:  { $regexMatch: { input: \"$comment\", regex: /[a-z0-9_.+-]+@mongodb.com/i } },                              then: \"Employee\",                              else: \"External\" } }    } },  The operation returns the following documents: { \"_id\" : 1, \"comment\" : \"Hi, I'm just reading about MongoDB -- aunt.arc.tica@example.com\", \"category\" : \"External\" }{ \"_id\" : 2, \"comment\" : \"I wanted to concatenate a string\", \"category\" : \"External\" }{ \"_id\" : 3, \"comment\" : \"How do I convert a date to string? Contact me at either cam@mongodb.com or c.dia@mongodb.com\", \"category\" : \"Employee\" }{ \"_id\" : 4, \"comment\" : \"It's just me. I'm testing.  fred@MongoDB.com\", \"category\" : \"Employee\" } ←  $regexFindAll (aggregation)$replaceOne (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/geoNear/": " Docs Home → MongoDB Manual \n$GEONEAR (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$geoNear \nOutputs documents in order of nearest to farthest from a specified point. \nNOTE Starting in version 4.2, MongoDB removes the limit and num options for the $geoNear stage as well as the default limit of 100 documents. To limit the results of $geoNear, use the $geoNear stage with the $limit stage. The $geoNear stage has the following prototype form: { $geoNear: { <geoNear options> } }  The $geoNear operator accepts a document that contains the following $geoNear options. Specify all distances in the same units as those of the processed documents' coordinate system: Field\nType\nDescription\ndistanceField\nstring\nThe output field that contains the calculated distance. To specify a field within an embedded document, use dot notation.\ndistanceMultiplier\nnumber\nOptional. The factor to multiply all distances returned by the query. For example, use the distanceMultiplier to convert radians, as returned by a spherical query, to kilometers by multiplying by the radius of the Earth.\nincludeLocs\nstring\nOptional. This specifies the output field that identifies the location used to calculate the distance. This option is useful when a location field contains multiple locations. To specify a field within an embedded document, use dot notation.\nkey \nOptional. Specify the geospatial indexed field to use when calculating the distance. If your collection has multiple 2d and/or multiple 2dsphere indexes, you must use the key option to specify the indexed field path to use. Specify Which Geospatial Index to Use provides a full example. If there is more than one 2d index or more than one 2dsphere index and you do not specify a key, MongoDB will return an error. If you do not specify the key, and you have at most only one 2d index and/or only one 2dsphere index, MongoDB looks first for a 2d index to use. If a 2d index does not exists, then MongoDB looks for a 2dsphere index to use. maxDistance\nnumber Optional. The maximum distance from the center point that the documents can be. MongoDB limits the results to those documents that fall within the specified distance from the center point. Specify the distance in meters if the specified point is GeoJSON and in radians if the specified point is legacy coordinate pairs. minDistance\nnumber Optional. The minimum distance from the center point that the documents can be. MongoDB limits the results to those documents that fall outside the specified distance from the center point. Specify the distance in meters for GeoJSON data and in radians for legacy coordinate pairs. near\nGeoJSON point or legacy coordinate pair The point for which to find the closest documents. If using a 2dsphere index, you can specify the point as either a GeoJSON point or legacy coordinate pair. If using a 2d index, specify the point as a legacy coordinate pair. query\ndocument Optional. Limits the results to the documents that match the query. The query syntax is the usual MongoDB read operation query syntax. You cannot specify a $near predicate in the query field of the $geoNear stage. spherical\nboolean Optional. Determines how MongoDB calculates the distance between two points:      * When true, MongoDB uses $nearSphere semantics and calculates distances using spherical geometry.  * When false, MongoDB uses $near semantics: spherical geometry for 2dsphere indexes and planar geometry for 2d indexes. Default: false. \nBEHAVIOR \nWhen using $geoNear, consider that:  * You can only use $geoNear as the first stage of a pipeline.  * You must include the distanceField option. The distanceField option specifies the field that will contain the calculated distance.  * $geoNear requires a geospatial index.\n   \n   If you have more than one geospatial index on the collection, use the keys parameter to specify which field to use in the calculation. If you have only one geospatial index, $geoNear implicitly uses the indexed field for the calculation.  * You cannot specify a $near predicate in the query field of the $geoNear stage.  * Views do not support geoNear operations (i.e. $geoNear pipeline stage).  * Starting in version 4.2, $geoNear no longer has a default limit of 100 documents.  * Starting in MongoDB 5.1, the near parameter supports the let option and bound let option.\n\n  * Starting in MongoDB 6.0, you can create partial and 2dsphere indexes on any field in a time series collection. \nEXAMPLES \nCreate a collection places with the following documents: db.places.insertMany( [   {      name: \"Central Park\",      location: { type: \"Point\", coordinates: [ -73.97, 40.77 ] },      category: \"Parks\"   },   {      name: \"Sara D. Roosevelt Park\",      location: { type: \"Point\", coordinates: [ -73.9928, 40.7193 ] },      category: \"Parks\"   },   {      name: \"Polo Grounds\",      location: { type: \"Point\", coordinates: [ -73.9375, 40.8303 ] },      category: \"Stadiums\"   }] )  The following operation creates a 2dsphere index on the location field: db.places.createIndex( { location: \"2dsphere\" } )  \nMAXIMUM DISTANCE  NOTE Starting in version 4.2, MongoDB removes the limit and num options for the $geoNear stage as well as the default limit of 100 documents. To limit the results of $geoNear, use the $geoNear stage with the $limit stage. The places collection above has a 2dsphere index. The following aggregation uses $geoNear to find documents with a location at most 2 meters from the center [ -73.99279 , 40.719296 ] and category equal to Parks. db.places.aggregate([   {     $geoNear: {        near: { type: \"Point\", coordinates: [ -73.99279 , 40.719296 ] },        distanceField: \"dist.calculated\",        maxDistance: 2,        query: { category: \"Parks\" },        includeLocs: \"dist.location\",        spherical: true     }   }])  The aggregation returns the following: {   \"_id\" : 8,   \"name\" : \"Sara D. Roosevelt Park\",   \"category\" : \"Parks\",   \"location\" : {      \"type\" : \"Point\",      \"coordinates\" : [ -73.9928, 40.7193 ]   },   \"dist\" : {      \"calculated\" : 0.9539931676365992,      \"location\" : {         \"type\" : \"Point\",         \"coordinates\" : [ -73.9928, 40.7193 ]      }   }}  The matching document contains two new fields:  * dist.calculated field that contains the calculated distance, and  * dist.location field that contains the location used in the calculation. \nMINIMUM DISTANCE  NOTE Starting in version 4.2, MongoDB removes the limit and num options for the $geoNear stage as well as the default limit of 100 documents. To limit the results of $geoNear, use the $geoNear stage with the $limit stage. The following example uses the option minDistance to specify the minimum distance from the center point that the documents can be. The following aggregation finds all documents with a location at least 2 meters from the center [ -73.99279 , 40.719296 ] and category equal to Parks. db.places.aggregate([   {     $geoNear: {        near: { type: \"Point\", coordinates: [ -73.99279 , 40.719296 ] },        distanceField: \"dist.calculated\",        minDistance: 2,        query: { category: \"Parks\" },        includeLocs: \"dist.location\",        spherical: true     }   }])  \n$GEONEAR WITH THE LET OPTION \nIn this example:\n\n  * $pt is specified as a let option to the near parameter in the $geoNear stage. db.places.aggregate([   {      \"$geoNear\":      {         \"near\":\"$$pt\",         \"distanceField\":\"distance\",         \"maxDistance\":2,         \"query\":{\"category\":\"Parks\"},         \"includeLocs\":\"dist.location\",         \"spherical\":true      }   }],{   \"let\":{ \"pt\": [ -73.99279, 40.719296 ] }})  The aggregation returns all documents with:  * A location at most 2 meters from the point defined in the let variable  * A category equal to Parks. {   _id: ObjectId(\"61715cf9b0c1d171bb498fd7\"),   name: 'Sara D. Roosevelt Park',   location: { type: 'Point', coordinates: [ -73.9928, 40.7193 ] },   category: 'Parks',   distance: 1.4957325341976439e-7,   dist: { location: { type: 'Point', coordinates: [ -73.9928, 40.7193 ] } }},{   _id: ObjectId(\"61715cf9b0c1d171bb498fd6\"),   name: 'Central Park',   location: { type: 'Point', coordinates: [ -73.97, 40.77 ] },   category: 'Parks',   distance: 0.0009348548688841822,   dist: { location: { type: 'Point', coordinates: [ -73.97, 40.77 ] } }} \n$GEONEAR WITH BOUND LET OPTION \nThe let option can bind a variable which can be used in a $geoNear query. In this example, $lookup uses:  * let to define $pt.  * $geoNear in the pipeline.  * $pt to define near in the $geoNear pipeline stage. db.places.aggregate( [   {      $lookup: {         from: \"places\",         let: { pt: \"$location\" },         pipeline: [            {               $geoNear: {                  near: \"$$pt\",                  distanceField: \"distance\"               }            }         ],         as: \"joinedField\"      }   },   {      $match: { name: \"Sara D. Roosevelt Park\" }   }] );  The aggregation returns a document with:  * The 'Sara D. Roosevelt Park' document as the main document.  * Every document in the places collection as subDocuments using the $pt variable for calculating distance.\n\n \nSPECIFY WHICH GEOSPATIAL INDEX TO USE \nConsider a places collection that has a 2dsphere index on the location field and a 2d index on the legacy field. A document in the places collection resembles the following: {   \"_id\" : 3,   \"name\" : \"Polo Grounds\",   \"location\": {      \"type\" : \"Point\",      \"coordinates\" : [ -73.9375, 40.8303 ]   },   \"legacy\" : [ -73.9375, 40.8303 ],   \"category\" : \"Stadiums\"}  The following example uses the key option to specify that the aggregation should use the location field values for the $geoNear operation rather than the legacy field values. The pipeline also uses $limit to return at most 5 documents. \nNOTE Starting in version 4.2, MongoDB removes the limit and num options for the $geoNear stage as well as the default limit of 100 documents. To limit the results of $geoNear, use the $geoNear stage with the $limit stage. db.places.aggregate([   {     $geoNear: {        near: { type: \"Point\", coordinates: [ -73.98142 , 40.71782 ] },        key: \"location\",        distanceField: \"dist.calculated\",        query: { \"category\": \"Parks\" }     }   },   { $limit: 5 }])  The aggregation returns the following: {   \"_id\" : 8,   \"name\" : \"Sara D. Roosevelt Park\",   \"location\" : {      \"type\" : \"Point\",      \"coordinates\" : [         -73.9928,         40.7193      ]   },   \"category\" : \"Parks\",   \"dist\" : {      \"calculated\" : 974.175764916902   }}{   \"_id\" : 1,   \"name\" : \"Central Park\",   \"location\" : {      \"type\" : \"Point\",      \"coordinates\" : [         -73.97,         40.77      ]   },   \"legacy\" : [      -73.97,      40.77   ],   \"category\" : \"Parks\",   \"dist\" : {      \"calculated\" : 5887.92792958097   }} \n←  $fill (aggregation)$graphLookup (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/lte/": " Docs Home → MongoDB Manual \n$LTE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$lte \nCompares two values and returns:      * true when the first value is less than or equivalent to the second value.  * false when the first value is greater than the second value. The $lte compares both value and type, using the specified BSON comparison order for values of different types. $lte has the following syntax: { $lte: [ <expression1>, <expression2> ] }  For more information on expressions, see Expressions. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 }{ \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 }{ \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }  The following operation uses the $lte operator to determine if qty is less than or equal to 250: db.inventory.aggregate(   [     {       $project:          {            item: 1,            qty: 1,            qtyLte250: { $lte: [ \"$qty\", 250 ] },            _id: 0          }     }   ])  The operation returns the following results: { \"item\" : \"abc1\", \"qty\" : 300, \"qtyLte250\" : false }{ \"item\" : \"abc2\", \"qty\" : 200, \"qtyLte250\" : true }{ \"item\" : \"xyz1\", \"qty\" : 250, \"qtyLte250\" : true }{ \"item\" : \"VWZ1\", \"qty\" : 300, \"qtyLte250\" : false }{ \"item\" : \"VWZ2\", \"qty\" : 180, \"qtyLte250\" : true } \n←  $lt (aggregation)$ltrim (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/merge/": " Docs Home → MongoDB Manual \n$MERGE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Considerations\n * Restrictions\n * Examples \nDEFINITION  NOTE This page describes the $merge stage, which outputs the aggregation pipeline results to a collection. For the $mergeObjects operator, which merges documents into a single document, see $mergeObjects. $merge \nWrites the results of the aggregation pipeline to a specified collection. The $merge operator must be the last stage in the pipeline. The $merge stage:      * Can output to a collection in the same or different database.  * Starting in MongoDB 4.4:\n   \n   * $merge can output to the same collection that is being aggregated. For more information, see Output to the Same Collection that is Being Aggregated.\n   \n   * Pipelines with the $merge stage can run on replica set secondary nodes if all the nodes in cluster have featureCompatibilityVersion set to 4.4 or higher and the Read Preference allows secondary reads.\n     \n     * Read operations of the $merge statement are sent to secondary nodes, while the write operations occur only on the primary node.\n     \n     * Not all driver versions support targeting of $merge operations to replica set secondary nodes. Check your driver documentation to see when your driver added support for $merge read operations running on secondary nodes.  * Creates a new collection if the output collection does not already exist.  * Can incorporate results (insert new documents, merge documents, replace documents, keep existing documents, fail the operation, process documents with a custom update pipeline) into an existing collection.  * Can output to a sharded collection. Input collection can also be sharded. For a comparison with the $out stage which also outputs the aggregation results to a collection, see $merge and $out Comparison. \nNOTE \nON-DEMAND MATERIALIZED VIEWS $merge can incorporate the pipeline results into an existing output collection rather than perform a full replacement of the collection. This functionality allows users to create on-demand materialized views, where the content of the output collection is incrementally updated when the pipeline is run. For more information on this use case, see On-Demand Materialized Views as well as the examples on this page. Materialized views are separate from read-only views. For information on creating read-only views, see read-only views. \nSYNTAX \n$merge has the following syntax: { $merge: {     into: <collection> -or- { db: <db>, coll: <collection> },     on: <identifier field> -or- [ <identifier field1>, ...],  // Optional     let: <variables>,                                         // Optional     whenMatched: <replace|keepExisting|merge|fail|pipeline>,  // Optional     whenNotMatched: <insert|discard|fail>                     // Optional} }  For example: { $merge: { into: \"myOutput\", on: \"_id\", whenMatched: \"replace\", whenNotMatched: \"insert\" } }  If using all default options for $merge, including writing to a collection in the same database, you can use the simplified form: { $merge: <collection> } // Output collection is in the same database  The $merge takes a document with the following fields: Field\nDescription\ninto The output collection. Specify either:  * The collection name as a string to output to a collection in the same database where the aggregation is run. For example:\n   \n   into: \"myOutput\"\n   \n     * The database and collection name in a document to output to a collection in the specified database. For example:\n   \n   into: { db:\"myDB\", coll:\"myOutput\" } \nNOTE\n\n  * The output collection can be a sharded collection. on Optional. Field or fields that act as a unique identifier for a document. The identifier determines if a results document matches an existing document in the output collection. Specify either:  * A single field name as a string. For example:\n   \n   on: \"_id\"\n   \n     * A combination of fields in an array. For example:\n   \n   on: [ \"date\", \"customerId\" ]\n   The order of the fields in the array does not matter, and you cannot specify the same field multiple times. For the specified field or fields:  * The aggregation results documents must contain the field(s) specified in the on, unless the on field is the _id field. If the _id field is missing from a results document, MongoDB adds it automatically.  * The specified field or fields cannot contain a null or an array value. $merge requires a unique, index with keys that correspond to the on identifier fields. Although the order of the index key specification does not matter, the unique index must only contain the on fields as its keys.  * The index must also have the same collation as the aggregation's collation.  * The unique index can be a sparse index.  * The unique index cannot be a partial index.  * For output collections that already exist, the corresponding index must already exist. The default value for on depends on the output collection:  * If the output collection does not exist, the on identifier must be and defaults to the _id field. The corresponding unique _id index is automatically created.\n   \n   \n   TIP\n   \n   To use a different on identifier field(s) for a collection that does not exist, you can create the collection first by creating a unique index on the desired field(s). See the section on non-existent output collection for an example.  * If the existing output collection is unsharded, the on identifier defaults to the _id field.  * If the existing output collection is a sharded collection, the on identifier defaults to all the shard key fields and the _id field. If specifying a different on identifier, the on must contain all the shard key fields. whenMatched Optional. The behavior of $merge if a result document and an existing document in the collection have the same value for the specified on field(s). You can specify either:\n\n  * An aggregation pipeline to update the document in the collection.\n   \n   [ <stage1>, <stage2> ... ]\n   \n   The pipeline can only consist of the following stages:\n   \n   * $addFields and its alias $set\n   \n   * $project and its alias $unset\n   \n   * $replaceRoot and its alias $replaceWith\n   \n   The pipeline cannot modify the on field's value. For example, if you are matching on the field month, the pipeline cannot modify the month field.\n   \n   The whenMatched pipeline can directly access the fields of the existing documents in the output collection using $<field>.\n   \n   To access the fields from the aggregation results documents, use either:\n   \n   * The built-in $$new variable to access the field. Specifically, $$new.<field>. The $$new variable is only available if the let specification is omitted.\n     \n     \n     NOTE\n     \n     Starting in MongoDB 4.2.2, the $$new variable is reserved, and cannot be overridden.\n   \n   * The user-defined variables in the let field.\n     \n     Specify the double dollar sign ($$) prefix together with the variable name in the form $$<variable_name>. For example, $$year. If the variable is set to a document, you can also include a document field in the form $$<variable_name>.<field>. For example, $$year.month.\n     \n     For more examples, see Use Variables to Customize the Merge. let Optional. Specifies variables for use in the whenMatched pipeline. Specify a document with the variable names and value expressions: { <variable_name_1>: <expression_1>,  ...,  <variable_name_n>: <expression_n> } If unspecified, defaults to { new: \"$$ROOT\" } (see ROOT). The whenMatched pipeline can access the $$new variable. \nNOTE Starting in MongoDB 4.2.2, the $$new variable is reserved, and cannot be overridden. To access the variables in the whenMatched pipeline: Specify the double dollar sign ($$) prefix together with the variable name in the form $$<variable_name>. For example, $$year. If the variable is set to a document, you can also include a document field in the form $$<variable_name>.<field>. For example, $$year.month. For examples, see Use Variables to Customize the Merge. whenNotMatched Optional. The behavior of $merge if a result document does not match an existing document in the out collection. You can specify one of the pre-defined action strings: Action\nDescription\n\"insert\" (Default) Insert the document into the output collection. \"discard\" Discard the document. Specifically, $merge does not insert the document into the output collection. \"fail\" Stop and fail the aggregation operation. Any changes already written to the output collection are not reverted. \nCONSIDERATIONS  _ID FIELD GENERATION \nIf the _id field is not present in a document from the aggregation pipeline results, the $merge stage generates it automatically. For example, in the following aggregation pipeline, $project excludes the _id field from the documents passed into $merge. When $merge writes these documents to the \"newCollection\", $merge generates a new _id field and value. db.sales.aggregate( [   { $project: { _id: 0 } },   { $merge : { into : \"newCollection\" } }] )  \nCREATE A NEW COLLECTION IF OUTPUT COLLECTION IS NON-EXISTENT \nThe $merge operation creates a new collection if the specified output collection does not exist.  * The output collection is created when $merge writes the first document into the collection and is immediately visible.  * If the aggregation fails, any writes completed by the $merge before the error will not be rolled back. \nNOTE For a replica set or a standalone, if the output database does not exist, $merge also creates the database. For a sharded cluster, the specified output database must already exist.\n\n db.newDailySales201905.createIndex( { salesDate: 1 }, { unique: true } )\ndb.sales.aggregate( [   { $match: { date: { $gte: new Date(\"2019-05-01\"), $lt: new Date(\"2019-06-01\") } } },   { $group: { _id: { $dateToString: { format: \"%Y-%m-%d\", date: \"$date\" } }, totalqty: { $sum: \"$quantity\" } } },   { $project: { _id: 0, salesDate: { $toDate: \"$_id\" }, totalqty: 1 } },   { $merge : { into : \"newDailySales201905\", on: \"salesDate\" } }] )  \nOUTPUT TO A SHARDED COLLECTION \nThe $merge stage can output to a sharded collection. When the output collection is sharded, $merge uses the _id field and all the shard key fields as the default on identifier. If you override the default, the on identifier must include all the shard key fields: { $merge: {   into: \"<shardedColl>\" or { db:\"<sharding enabled db>\", coll: \"<shardedColl>\" },   on: [ \"<shardkeyfield1>\", \"<shardkeyfield2>\",... ], // Shard key fields and any additional fields   let: <variables>,                                         // Optional   whenMatched: <replace|keepExisting|merge|fail|pipeline>,  // Optional   whenNotMatched: <insert|discard|fail>                     // Optional} }  For example, use the sh.shardCollection() method to create a new sharded collection newrestaurants with the postcode field as the shard key. sh.shardCollection(   \"exampledb.newrestaurants\",      // Namespace of the collection to shard   { postcode: 1 },                 // Shard key);  The newrestaurants collection will contain documents with information on new restaurant openings by month (date field) and postcode (shard key); specifically, the on identifier is [\"date\", \"postcode\"] (the ordering of the fields does not matter). Because $merge requires a unique index with keys that correspond to the on identifier fields, create the unique index (the ordering of the fields do not matter): [1] use exampledbdb.newrestaurants.createIndex( { postcode: 1, date: 1 }, { unique: true } )  With the sharded collection restaurants and the unique index created, you can use $merge to output the aggregation results to this collection, matching on [ \"date\", \"postcode\" ] as in this example: use exampledb\ndb.openings.aggregate([   { $group: {      _id: { date: { $dateToString: { format: \"%Y-%m\", date: \"$date\" } }, postcode: \"$postcode\" },      restaurants: { $push: \"$restaurantName\" } } },   { $project: { _id: 0, postcode: \"$_id.postcode\", date: \"$_id.date\", restaurants: 1 } },   { $merge: { into: \"newrestaurants\", \"on\": [ \"date\", \"postcode\" ], whenMatched: \"replace\", whenNotMatched: \"insert\" } }])  [1] The sh.shardCollection() method can also create a unique index on the shard key when passed the { unique: true } option if: the shard key is range-based, the collection is empty, and a unique index on the shard key doesn't already exist.In the previous example, because the on identifier is the shard key and another field, a separate operation to create the corresponding index is required. \nREPLACE DOCUMENTS ($MERGE) VS REPLACE COLLECTION ($OUT)\n\n However, to replace an existing collection regardless of the aggregation results, use $out instead. \nEXISTING DOCUMENTS AND _ID AND SHARD KEY VALUES \nThe $merge errors if the $merge results in a change to an existing document's _id value. \nTIP To avoid this error, if the on field does not include the _id field, remove the _id field in the aggregation results to avoid the error, such as with a preceding $unset stage, and so on. Additionally, for a sharded collection, $merge also generates an error if it results in a change to the shard key value of an exising document. Any writes completed by the $merge before the error will not be rolled back. \nUNIQUE INDEX CONSTRAINTS \nIf the unique index used by $merge for on field(s) is dropped mid-aggregation, there is no guarantee that the aggregation will be killed. If the aggregation continues, there is no guarantee that documents do not have duplicate on field values. If the $merge attempts to write a document that violates any unique index on the output collection, the operation generates an error. For example:  * Insert a non-matching document that violates a unique index other than the index on the on field(s).  * Fail if there is a matching document in the collection. Specifically, the operation attempts to insert the matching document which violates the unique index on the on field(s).  * Replace an existing document with a new document that violates a unique index other than the index on the on field(s).  * Merge the matching documents that results in a document that violates a unique index other than the index on the on field(s). \nWHENMATCHED PIPELINE BEHAVIOR \nStarting in MongoDB 4.2.2, if all of the following are true for a $merge stage:  * The value of whenMatched is an aggregation pipeline,  * The value of whenNotMatched is insert, and  * There is no match for a document in the output collection, $merge inserts the document directly into the output collection. Prior to MongoDB 4.2.2, when these conditions for a $merge stage are met, the pipeline specified in the whenMatched field is executed with an empty input document. The resulting document from the pipeline is inserted into the output collection. \nTIP \nSEE ALSO: 4.2.2 Release Notes \n$MERGE AND $OUT COMPARISON \nWith the introduction of $merge, MongoDB provides two stages, $merge and $out, for writing the results of the aggregation pipeline to a collection: $merge\n$out  * Can output to a collection in the same or different database.  * Can output to a collection in the same or, starting in MongoDB 4.4, different database.  * Creates a new collection if the output collection does not already exist.  * Creates a new collection if the output collection does not already exist.  * Can incorporate results (insert new documents, merge documents, replace documents, keep existing documents, fail the operation, process documents with a custom update pipeline) into an existing collection.\n   \n   See also Replace Documents ($merge) vs Replace Collection ($out).  * Replaces the output collection completely if it already exists.  * Can output to a sharded collection. Input collection can also be sharded.  * Cannot output to a sharded collection. Input collection, however, can be sharded.  * Corresponds to SQL statements:\n   \n   * MERGE.\n   \n   * INSERT INTO T2 SELECT FROM T1.\n   \n   * SELECT INTO T2 FROM T1.\n   \n   * Create/Refresh Materialized Views.  * Corresponds to SQL statement:\n   \n   * INSERT INTO T2 SELECT FROM T1.\n   \n   * SELECT INTO T2 FROM T1. \nOUTPUT TO THE SAME COLLECTION THAT IS BEING AGGREGATED  WARNING When $merge outputs to the same collection that is being aggregated, documents may get updated multiple times or the operation may result in an infinite loop. This behavior occurs when the update performed by $merge changes the physical location of documents stored on disk. When the physical location of a document changes, $merge may view it as an entirely new document, resulting in additional updates. For more information on this behavior, see Halloween Problem. Starting in MongoDB 4.4, $merge can output to the same collection that is being aggregated. You can also output to a collection which appears in other stages of the pipeline, such as $lookup.\n\n \nRESTRICTIONS \nRestrictions\nDescription\nTransactions\nAn aggregation pipeline cannot use $merge inside a transaction.\nTime Series Collections\nAn aggregation pipeline cannot use $merge to output to a time series collection.\nView definition\nSeparate from materialized view\nView definition cannot include the $merge stage. If the view definition includes nested pipeline (for example, the view definition includes $facet stage), this $merge stage restriction applies to the nested pipelines as well.\n$lookup stage\n$lookup stage's nested pipeline cannot include the $merge stage.\n$facet stage\n$facet stage's nested pipeline cannot include the $merge stage.\n$unionWith stage\n$unionWith stage's nested pipeline cannot include the $merge stage.\n\"linearizable\" read concern The $merge stage cannot be used in conjunction with read concern \"linearizable\". That is, if you specify \"linearizable\" read concern for db.collection.aggregate(), you cannot include the $merge stage in the pipeline. \nEXAMPLES  ON-DEMAND MATERIALIZED VIEW: INITIAL CREATION \nIf the output collection does not exist, the $merge creates the collection. For example, a collection named salaries in the zoo database is populated with the employee salary and department history: db.getSiblingDB(\"zoo\").salaries.insertMany([   { \"_id\" : 1, employee: \"Ant\", dept: \"A\", salary: 100000, fiscal_year: 2017 },   { \"_id\" : 2, employee: \"Bee\", dept: \"A\", salary: 120000, fiscal_year: 2017 },   { \"_id\" : 3, employee: \"Cat\", dept: \"Z\", salary: 115000, fiscal_year: 2017 },   { \"_id\" : 4, employee: \"Ant\", dept: \"A\", salary: 115000, fiscal_year: 2018 },   { \"_id\" : 5, employee: \"Bee\", dept: \"Z\", salary: 145000, fiscal_year: 2018 },   { \"_id\" : 6, employee: \"Cat\", dept: \"Z\", salary: 135000, fiscal_year: 2018 },   { \"_id\" : 7, employee: \"Gecko\", dept: \"A\", salary: 100000, fiscal_year: 2018 },   { \"_id\" : 8, employee: \"Ant\", dept: \"A\", salary: 125000, fiscal_year: 2019 },   { \"_id\" : 9, employee: \"Bee\", dept: \"Z\", salary: 160000, fiscal_year: 2019 },   { \"_id\" : 10, employee: \"Cat\", dept: \"Z\", salary: 150000, fiscal_year: 2019 }])  You can use the $group and $merge stages to initially create a collection named budgets (in the reporting database) from the data currently in the salaries collection: \nNOTE For a replica set or a standalone deployment, if the output database does not exist, $merge also creates the database. For a sharded cluster deployment, the specified output database must already exist. db.getSiblingDB(\"zoo\").salaries.aggregate( [   { $group: { _id: { fiscal_year: \"$fiscal_year\", dept: \"$dept\" }, salaries: { $sum: \"$salary\" } } },   { $merge : { into: { db: \"reporting\", coll: \"budgets\" }, on: \"_id\",  whenMatched: \"replace\", whenNotMatched: \"insert\" } }] )   * $group stage to group the salaries by the fiscal_year and dept.  * $merge stage writes the output of the preceding $group stage to the budgets collection in the reporting database. To view the documents in the new budgets collection: db.getSiblingDB(\"reporting\").budgets.find().sort( { _id: 1 } )  The budgets collection contains the following documents:\n\n  \nTIP \nSEE ALSO: On-Demand Materialized Views \nON-DEMAND MATERIALIZED VIEW: UPDATE/REPLACE DATA \nThe following example uses the collections in the previous example. The example salaries collection contains the employee salary and department history: { \"_id\" : 1, employee: \"Ant\", dept: \"A\", salary: 100000, fiscal_year: 2017 },{ \"_id\" : 2, employee: \"Bee\", dept: \"A\", salary: 120000, fiscal_year: 2017 },{ \"_id\" : 3, employee: \"Cat\", dept: \"Z\", salary: 115000, fiscal_year: 2017 },{ \"_id\" : 4, employee: \"Ant\", dept: \"A\", salary: 115000, fiscal_year: 2018 },{ \"_id\" : 5, employee: \"Bee\", dept: \"Z\", salary: 145000, fiscal_year: 2018 },{ \"_id\" : 6, employee: \"Cat\", dept: \"Z\", salary: 135000, fiscal_year: 2018 },{ \"_id\" : 7, employee: \"Gecko\", dept: \"A\", salary: 100000, fiscal_year: 2018 },{ \"_id\" : 8, employee: \"Ant\", dept: \"A\", salary: 125000, fiscal_year: 2019 },{ \"_id\" : 9, employee: \"Bee\", dept: \"Z\", salary: 160000, fiscal_year: 2019 },{ \"_id\" : 10, employee: \"Cat\", dept: \"Z\", salary: 150000, fiscal_year: 2019 }  The example budgets collection contains the cumulative yearly budgets: { \"_id\" : { \"fiscal_year\" : 2017, \"dept\" : \"A\" }, \"salaries\" : 220000 }{ \"_id\" : { \"fiscal_year\" : 2017, \"dept\" : \"Z\" }, \"salaries\" : 115000 }{ \"_id\" : { \"fiscal_year\" : 2018, \"dept\" : \"A\" }, \"salaries\" : 215000 }{ \"_id\" : { \"fiscal_year\" : 2018, \"dept\" : \"Z\" }, \"salaries\" : 280000 }{ \"_id\" : { \"fiscal_year\" : 2019, \"dept\" : \"A\" }, \"salaries\" : 125000 }{ \"_id\" : { \"fiscal_year\" : 2019, \"dept\" : \"Z\" }, \"salaries\" : 310000 }  During the current fiscal year (2019 in this example), new employees are added to the salaries collection and new head counts are pre-allocated for the next year: db.getSiblingDB(\"zoo\").salaries.insertMany([   { \"_id\" : 11,  employee: \"Wren\", dept: \"Z\", salary: 100000, fiscal_year: 2019 },   { \"_id\" : 12,  employee: \"Zebra\", dept: \"A\", salary: 150000, fiscal_year: 2019 },   { \"_id\" : 13,  employee: \"headcount1\", dept: \"Z\", salary: 120000, fiscal_year: 2020 },   { \"_id\" : 14,  employee: \"headcount2\", dept: \"Z\", salary: 120000, fiscal_year: 2020 }])  To update the budgets collection to reflect the new salary information, the following aggregation pipeline uses:  * $match stage to find all documents with fiscal_year greater than or equal to 2019.  * $group stage to group the salaries by the fiscal_year and dept.  * $merge to write the result set to the budgets collection, replacing documents with the same _id value (in this example, a document with the fiscal year and dept). For documents that do not have matches in the collection, $merge inserts the new documents. db.getSiblingDB(\"zoo\").salaries.aggregate( [   { $match : { fiscal_year:  { $gte : 2019 } } },   { $group: { _id: { fiscal_year: \"$fiscal_year\", dept: \"$dept\" }, salaries: { $sum: \"$salary\" } } },   { $merge : { into: { db: \"reporting\", coll: \"budgets\" }, on: \"_id\",  whenMatched: \"replace\", whenNotMatched: \"insert\" } }] )  After the aggregation is run, view the documents in the budgets collection:\n\n  The budgets collection incorporates the new salary data for fiscal year 2019 and adds new documents for fiscal year 2020: { \"_id\" : { \"fiscal_year\" : 2017, \"dept\" : \"A\" }, \"salaries\" : 220000 }{ \"_id\" : { \"fiscal_year\" : 2017, \"dept\" : \"Z\" }, \"salaries\" : 115000 }{ \"_id\" : { \"fiscal_year\" : 2018, \"dept\" : \"A\" }, \"salaries\" : 215000 }{ \"_id\" : { \"fiscal_year\" : 2018, \"dept\" : \"Z\" }, \"salaries\" : 280000 }{ \"_id\" : { \"fiscal_year\" : 2019, \"dept\" : \"A\" }, \"salaries\" : 275000 }{ \"_id\" : { \"fiscal_year\" : 2019, \"dept\" : \"Z\" }, \"salaries\" : 410000 }{ \"_id\" : { \"fiscal_year\" : 2020, \"dept\" : \"Z\" }, \"salaries\" : 240000 }  \nTIP \nSEE ALSO: On-Demand Materialized Views \nONLY INSERT NEW DATA \nTo ensure that the $merge does not overwrite existing data in the collection, set whenMatched to keepExisting or fail. The example salaries collection in the zoo database contains the employee salary and department history: { \"_id\" : 1, employee: \"Ant\", dept: \"A\", salary: 100000, fiscal_year: 2017 },{ \"_id\" : 2, employee: \"Bee\", dept: \"A\", salary: 120000, fiscal_year: 2017 },{ \"_id\" : 3, employee: \"Cat\", dept: \"Z\", salary: 115000, fiscal_year: 2017 },{ \"_id\" : 4, employee: \"Ant\", dept: \"A\", salary: 115000, fiscal_year: 2018 },{ \"_id\" : 5, employee: \"Bee\", dept: \"Z\", salary: 145000, fiscal_year: 2018 },{ \"_id\" : 6, employee: \"Cat\", dept: \"Z\", salary: 135000, fiscal_year: 2018 },{ \"_id\" : 7, employee: \"Gecko\", dept: \"A\", salary: 100000, fiscal_year: 2018 },{ \"_id\" : 8, employee: \"Ant\", dept: \"A\", salary: 125000, fiscal_year: 2019 },{ \"_id\" : 9, employee: \"Bee\", dept: \"Z\", salary: 160000, fiscal_year: 2019 },{ \"_id\" : 10, employee: \"Cat\", dept: \"Z\", salary: 150000, fiscal_year: 2019 }  A collection orgArchive in the reporting database contains historical departmental organization records for the past fiscal years. Archived records should not be modified. { \"_id\" : ObjectId(\"5cd8c68261baa09e9f3622be\"), \"employees\" : [ \"Ant\", \"Gecko\" ], \"dept\" : \"A\", \"fiscal_year\" : 2018 }{ \"_id\" : ObjectId(\"5cd8c68261baa09e9f3622bf\"), \"employees\" : [ \"Ant\", \"Bee\" ], \"dept\" : \"A\", \"fiscal_year\" : 2017 }{ \"_id\" : ObjectId(\"5cd8c68261baa09e9f3622c0\"), \"employees\" : [ \"Bee\", \"Cat\" ], \"dept\" : \"Z\", \"fiscal_year\" : 2018 }{ \"_id\" : ObjectId(\"5cd8c68261baa09e9f3622c1\"), \"employees\" : [ \"Cat\" ], \"dept\" : \"Z\", \"fiscal_year\" : 2017 }  The orgArchive collection has a unique compound index on the fiscal_year and dept fields. Specifically, there should be at most one record for the same fiscal year and department combination: db.getSiblingDB(\"reporting\").orgArchive.createIndex ( { fiscal_year: 1, dept: 1 }, { unique: true } )  At the end of current fiscal year (2019 in this example), the salaries collection contain the following documents:\n\n  To update the orgArchive collection to include the fiscal year 2019 that has just ended, the following aggregation pipeline uses:  * $match stage to find all documents with fiscal_year equal to 2019.  * $group stage to group the employees by the fiscal_year and dept.  * $project stage to suppress the _id field and add separate dept and fiscal_year field. When the documents are passed to $merge, $merge automatically generates a new _id field for the documents.  * $merge to write the result set to orgArchive.\n   \n   The $merge stage matches documents on the dept and fiscal_year fields and fails when matched. That is, if a document already exists for the same department and fiscal year, the $merge errors. db.getSiblingDB(\"zoo\").salaries.aggregate( [    { $match: { fiscal_year: 2019 }},    { $group: { _id: { fiscal_year: \"$fiscal_year\", dept: \"$dept\" }, employees: { $push: \"$employee\" } } },    { $project: { _id: 0, dept: \"$_id.dept\", fiscal_year: \"$_id.fiscal_year\", employees: 1 } },    { $merge : { into : { db: \"reporting\", coll: \"orgArchive\" }, on: [ \"dept\", \"fiscal_year\" ], whenMatched: \"fail\" } }] )  After the operation, the orgArchive collection contains the following documents: { \"_id\" : ObjectId(\"5caccc6a66b22dd8a8cc419f\"), \"employees\" : [ \"Ahn\", \"Bess\" ], \"dept\" : \"A\", \"fiscal_year\" : 2017 }{ \"_id\" : ObjectId(\"5caccc6a66b22dd8a8cc419e\"), \"employees\" : [ \"Ahn\", \"Gee\" ], \"dept\" : \"A\", \"fiscal_year\" : 2018 }{ \"_id\" : ObjectId(\"5caccd0b66b22dd8a8cc438e\"), \"employees\" : [ \"Ahn\", \"Zeb\" ], \"dept\" : \"A\", \"fiscal_year\" : 2019 }{ \"_id\" : ObjectId(\"5caccc6a66b22dd8a8cc41a0\"), \"employees\" : [ \"Carl\" ], \"dept\" : \"Z\", \"fiscal_year\" : 2017 }{ \"_id\" : ObjectId(\"5caccc6a66b22dd8a8cc41a1\"), \"employees\" : [ \"Bess\", \"Carl\" ], \"dept\" : \"Z\", \"fiscal_year\" : 2018 }{ \"_id\" : ObjectId(\"5caccd0b66b22dd8a8cc438d\"), \"employees\" : [ \"Bess\", \"Carl\", \"Wen\" ], \"dept\" : \"Z\", \"fiscal_year\" : 2019 }  If the orgArchive collection already contained a document for 2019 for department \"A\" and/or \"B\", the aggregation fails because of the duplicate key error. However, any document inserted before the error will not be rolled back. If you specify keepExisting for the matching document, the aggregation does not affect the matching document and does not error with duplicate key error. Similarly, if you specify replace, the operation would not fail; however, the operation would replace the existing document. \nMERGE RESULTS FROM MULTIPLE COLLECTIONS \nBy default, if a document in the aggregation results matches a document in the collection, the $merge stage merges the documents. An example collection purchaseorders is populated with the purchase order information by quarter and regions:\n\n  Another example collection reportedsales is populated with the reported sales information by quarter and regions: db.reportedsales.insertMany( [   { _id: 1, quarter: \"2019Q1\", region: \"A\", qty: 400, reportDate: new Date(\"2019-04-02\") },   { _id: 2, quarter: \"2019Q1\", region: \"B\", qty: 550, reportDate: new Date(\"2019-04-02\") },   { _id: 3, quarter: \"2019Q1\", region: \"C\", qty: 1000, reportDate: new Date(\"2019-04-05\") },   { _id: 4, quarter: \"2019Q2\", region: \"B\", qty: 500, reportDate: new Date(\"2019-07-02\") },] )  Assume that, for reporting purposes, you want to view the data by quarter in the following format: { \"_id\" : \"2019Q1\", \"sales\" : 1950, \"purchased\" : 1200 }{ \"_id\" : \"2019Q2\", \"sales\" : 500, \"purchased\" : 1700 }  You can use the $merge to merge in results from the purchaseorders collection and the reportedsales collection to create a new collection quarterlyreport. To create the quarterlyreport collection, you can use the following pipeline: db.purchaseorders.aggregate( [   { $group: { _id: \"$quarter\", purchased: { $sum: \"$qty\" } } },  // group purchase orders by quarter   { $merge : { into: \"quarterlyreport\", on: \"_id\",  whenMatched: \"merge\", whenNotMatched: \"insert\" } }]) \nFirst stage: The $group stage groups by the quarter and uses $sum to add the qty fields into a new purchased field. For example: To create the quarterlyreport collection, you can use this pipeline: { \"_id\" : \"2019Q2\", \"purchased\" : 1700 }{ \"_id\" : \"2019Q1\", \"purchased\" : 1200 } Second stage:The $merge stage writes the documents to the quarterlyreport collection in the same database. If the stage finds an existing document in the collection that matches on the _id field, the stage merges the matching documents. Otherwise, the stage inserts the document. For the initial creation, no documents should match. To view the documents in the collection, run the following operation: db.quarterlyreport.find().sort( { _id: 1 } )  The collection contains the following documents: { \"_id\" : \"2019Q1\", \"sales\" : 1200, \"purchased\" : 1200 }{ \"_id\" : \"2019Q2\", \"sales\" : 1700, \"purchased\" : 1700 } Similarly, run the following aggregation pipeline against the reportedsales collection to merge the sales results into the quarterlyreport collection. db.reportedsales.aggregate( [   { $group: { _id: \"$quarter\", sales: { $sum: \"$qty\" } } },  // group sales by quarter   { $merge : { into: \"quarterlyreport\", on: \"_id\",  whenMatched: \"merge\", whenNotMatched: \"insert\" } }]) \nFirst stage: The $group stage groups by the quarter and uses $sum to add the qty fields into a new sales field. For example: { \"_id\" : \"2019Q2\", \"sales\" : 500 }{ \"_id\" : \"2019Q1\", \"sales\" : 1950 } Second stage:The $merge stage writes the documents to the quarterlyreport collection in the same database. If the stage finds an existing document in the collection that matches on the _id field (the quarter), the stage merges the matching documents. Otherwise, the stage inserts the document. To view the documents in the quarterlyreport collection after the data has been merged, run the following operation: db.quarterlyreport.find().sort( { _id: 1 } )  The collection contains the following documents: { \"_id\" : \"2019Q1\", \"sales\" : 1950, \"purchased\" : 1200 }{ \"_id\" : \"2019Q2\", \"sales\" : 500, \"purchased\" : 1700 } \nUSE THE PIPELINE TO CUSTOMIZE THE MERGE \nThe $merge can use a custom update pipeline when documents match. The whenMatched pipeline can have the following stages:  * $addFields and its alias $set  * $project and its alias $unset\n\n An example collection votes is populated with the daily vote tally. Create the collection with the following documents:s db.votes.insertMany( [   { date: new Date(\"2019-05-01\"), \"thumbsup\" : 1, \"thumbsdown\" : 1 },   { date: new Date(\"2019-05-02\"), \"thumbsup\" : 3, \"thumbsdown\" : 1 },   { date: new Date(\"2019-05-03\"), \"thumbsup\" : 1, \"thumbsdown\" : 1 },   { date: new Date(\"2019-05-04\"), \"thumbsup\" : 2, \"thumbsdown\" : 2 },   { date: new Date(\"2019-05-05\"), \"thumbsup\" : 6, \"thumbsdown\" : 10 },   { date: new Date(\"2019-05-06\"), \"thumbsup\" : 13, \"thumbsdown\" : 16 }] )  Another example collection monthlytotals has the up-to-date monthly vote totals. Create the collection with the following document: db.monthlytotals.insertOne(   { \"_id\" : \"2019-05\", \"thumbsup\" : 26, \"thumbsdown\" : 31 })  At the end of each day, that day's votes is inserted into the votes collection: db.votes.insertOne(   { date: new Date(\"2019-05-07\"), \"thumbsup\" : 14, \"thumbsdown\" : 10 })  You can use $merge with an custom pipeline to update the existing document in the collection monthlytotals: db.votes.aggregate([   { $match: { date: { $gte: new Date(\"2019-05-07\"), $lt: new Date(\"2019-05-08\") } } },   { $project: { _id: { $dateToString: { format: \"%Y-%m\", date: \"$date\" } }, thumbsup: 1, thumbsdown: 1 } },   { $merge: {         into: \"monthlytotals\",         on: \"_id\",         whenMatched:  [            { $addFields: {                thumbsup: { $add:[ \"$thumbsup\", \"$$new.thumbsup\" ] },                thumbsdown: { $add: [ \"$thumbsdown\", \"$$new.thumbsdown\" ] }            } } ],         whenNotMatched: \"insert\"   } }]) \nFirst stage: The $match stage finds the specific day's votes. For example: { \"_id\" : ObjectId(\"5ce6097c436eb7e1203064a6\"), \"date\" : ISODate(\"2019-05-07T00:00:00Z\"), \"thumbsup\" : 14, \"thumbsdown\" : 10 } Second stage: The $project stage sets the _id field to a year-month string. For example: { \"thumbsup\" : 14, \"thumbsdown\" : 10, \"_id\" : \"2019-05\" } Third stage: The $merge stage writes the documents to the monthlytotals collection in the same database. If the stage finds an existing document in the collection that matches on the _id field, the stage uses a pipeline to add the thumbsup votes and the thumbsdown votes.  * This pipeline cannot directly accesses the fields from the results document. To access the thumbsup field and the thumbsdown field in the results document, the pipeline uses the $$new variable; i.e. $$new.thumbsup and $new.thumbsdown.  * This pipeline can directly accesses the thumbsup field and the thumbsdown field in the existing document in the collection; i.e. $thumbsup and $thumbsdown. The resulting document replaces the existing document. To view documents in the monthlytotals collection after the merge operation, run the following operation: db.monthlytotals.find()  The collection contains the following document: { \"_id\" : \"2019-05\", \"thumbsup\" : 40, \"thumbsdown\" : 41 } \nUSE VARIABLES TO CUSTOMIZE THE MERGE\n\n Define variables in one or both of the following:  * $merge stage let  * aggregate command let (starting in MongoDB 5.0) To use variables in whenMatched: Specify the double dollar sign ($$) prefix together with the variable name in the form $$<variable_name>. For example, $$year. If the variable is set to a document, you can also include a document field in the form $$<variable_name>.<field>. For example, $$year.month. The tabs below demonstrate behavior when variables are defined in the merge stage, the aggregate command, or both.  ←  $match (aggregation)$out (aggregation) → On this page  * Definition\n * Syntax\n * Considerations\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/substrBytes/": " Docs Home → MongoDB Manual \n$SUBSTRBYTES (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$substrBytes \nReturns the substring of a string. The substring starts with the character at the specified UTF-8 byte index (zero-based) in the string and continues for the number of bytes specified. $substrBytes has the following operator expression syntax: { $substrBytes: [ <string expression>, <byte index>, <byte count> ] } \nField\nType\nDescription\nstring expression\nstring The string from which the substring will be extracted. string expression can be any valid expression as long as it resolves to a string. For more information on expressions, see Expressions. If the argument resolves to a value of null or refers to a field that is missing, $substrBytes returns an empty string. If the argument does not resolve to a string or null nor refers to a missing field, $substrBytes returns an error. byte index\nnumber Indicates the starting point of the substring. byte index can be any valid expression as long as it resolves to a non-negative integer or number that can be represented as an integer (such as 2.0). byte index cannot refer to a starting index located in the middle of a multi-byte UTF-8 character. byte count\nnumber Can be any valid expression as long as it resolves to a non-negative integer or number that can be represented as an integer (such as 2.0). byte count can not result in an ending index that is in the middle of a UTF-8 character. \nBEHAVIOR \nThe $substrBytes operator uses the indexes of UTF-8 encoded bytes where each code point, or character, may use between one and four bytes to encode. For example, US-ASCII characters are encoded using one byte. Characters with diacritic markings and additional Latin alphabetical characters (i.e. Latin characters outside of the English alphabet) are encoded using two bytes. Chinese, Japanese and Korean characters typically require three bytes, and other planes of unicode (emoji, mathematical symbols, etc.) require four bytes. It is important to be mindful of the content in the string expression because providing a byte index or byte count located in the middle of a UTF-8 character will result in an error. $substrBytes differs from $substrCP in that $substrBytes counts the bytes of each character, whereas $substrCP counts the code points, or characters, regardless of how many bytes a character uses. Example\nResults { $substrBytes: [ \"abcde\", 1, 2 ] } \"bc\" { $substrBytes: [ \"Hello World!\", 6, 5 ] }  \"World\" { $substrBytes: [ \"cafétéria\", 0, 5 ] } \"café\" { $substrBytes: [ \"cafétéria\", 5, 4 ] } \"tér\" { $substrBytes: [ \"cafétéria\", 7, 3 ] } Errors with message: \"Error: Invalid range, starting index is a UTF-8 continuation byte.\" { $substrBytes: [ \"cafétéria\", 3, 1 ] } Errors with message: \"Error: Invalid range, ending index is in the middle of a UTF-8 character.\" \nEXAMPLE  SINGLE-BYTE CHARACTER SET \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \"product 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", quarter: \"13Q4\", \"description\" : \"product 2\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $substrBytes operator separate the quarter value (containing only single byte US-ASCII characters) into a yearSubstring and a quarterSubstring. The quarterSubstring field represents the rest of the string from the specified byte index following the yearSubstring. It is calculated by subtracting the byte index from the length of the string using $strLenBytes.\n\n  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"yearSubstring\" : \"13\", \"quarterSubtring\" : \"Q1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", \"yearSubstring\" : \"13\", \"quarterSubtring\" : \"Q4\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"yearSubstring\" : \"14\", \"quarterSubtring\" : \"Q2\" }  \nSINGLE-BYTE AND MULTIBYTE CHARACTER SET \nCreate a food collection with the following documents: db.food.insertMany( [    { \"_id\" : 1, \"name\" : \"apple\" },    { \"_id\" : 2, \"name\" : \"banana\" },    { \"_id\" : 3, \"name\" : \"éclair\" },    { \"_id\" : 4, \"name\" : \"hamburger\" },    { \"_id\" : 5, \"name\" : \"jalapeño\" },    { \"_id\" : 6, \"name\" : \"pizza\" },    { \"_id\" : 7, \"name\" : \"tacos\" },    { \"_id\" : 8, \"name\" : \"寿司sushi\" } ])  The following operation uses the $substrBytes operator to create a three byte menuCode from the name value: db.food.aggregate(  [    {      $project: {        \"name\": 1,        \"menuCode\": { $substrBytes: [ \"$name\", 0, 3 ] }      }    }  ])  The operation returns the following results: { \"_id\" : 1, \"name\" : \"apple\", \"menuCode\" : \"app\" }{ \"_id\" : 2, \"name\" : \"banana\", \"menuCode\" : \"ban\" }{ \"_id\" : 3, \"name\" : \"éclair\", \"menuCode\" : \"éc\" }{ \"_id\" : 4, \"name\" : \"hamburger\", \"menuCode\" : \"ham\" }{ \"_id\" : 5, \"name\" : \"jalapeño\", \"menuCode\" : \"jal\" }{ \"_id\" : 6, \"name\" : \"pizza\", \"menuCode\" : \"piz\" }{ \"_id\" : 7, \"name\" : \"tacos\", \"menuCode\" : \"tac\" }{ \"_id\" : 8, \"name\" : \"寿司sushi\", \"menuCode\" : \"寿\" }  \nTIP \nSEE ALSO: $substrCP ←  $substr (aggregation)$substrCP (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/cmp/": " Docs Home → MongoDB Manual \n$CMP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$cmp \nCompares two values and returns:      * -1 if the first value is less than the second.  * 1 if the first value is greater than the second.  * 0 if the two values are equivalent. The $cmp compares both value and type, using the specified BSON comparison order for values of different types. $cmp has the following syntax: { $cmp: [ <expression1>, <expression2> ] }  For more information on expressions, see Expressions. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 }{ \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 }{ \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }  The following operation uses the $cmp operator to compare the qty value with 250: db.inventory.aggregate(   [     {       $project:          {            item: 1,            qty: 1,            cmpTo250: { $cmp: [ \"$qty\", 250 ] },            _id: 0          }     }   ])  The operation returns the following results: { \"item\" : \"abc1\", \"qty\" : 300, \"cmpTo250\" : 1 }{ \"item\" : \"abc2\", \"qty\" : 200, \"cmpTo250\" : -1 }{ \"item\" : \"xyz1\", \"qty\" : 250, \"cmpTo250\" : 0 }{ \"item\" : \"VWZ1\", \"qty\" : 300, \"cmpTo250\" : 1 }{ \"item\" : \"VWZ2\", \"qty\" : 180, \"cmpTo250\" : -1 } \n←  $ceil (aggregation)$concat (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/ne/": " Docs Home → MongoDB Manual \n$NE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$ne \nCompares two values and returns:      * true when the values are not equivalent.  * false when the values are equivalent. The $ne compares both value and type, using the specified BSON comparison order for values of different types. $ne has the following syntax: { $ne: [ <expression1>, <expression2> ] }  For more information on expressions, see Expressions. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 }{ \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 }{ \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }  The following operation uses the $ne operator to determine if qty does not equal 250: db.inventory.aggregate(   [     {       $project:          {            item: 1,            qty: 1,            qtyNe250: { $ne: [ \"$qty\", 250 ] },            _id: 0          }     }   ])  The operation returns the following results: { \"item\" : \"abc1\", \"qty\" : 300, \"qtyNe250\" : true }{ \"item\" : \"abc2\", \"qty\" : 200, \"qtyNe250\" : true }{ \"item\" : \"xyz1\", \"qty\" : 250, \"qtyNe250\" : false }{ \"item\" : \"VWZ1\", \"qty\" : 300, \"qtyNe250\" : true }{ \"item\" : \"VWZ2\", \"qty\" : 180, \"qtyNe250\" : true } \n←  $multiply (aggregation)$not (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/firstN/": " Docs Home → MongoDB Manual \n$FIRSTN (AGGREGATION ACCUMULATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Restrictions\n * Examples \nDEFINITION \n$firstN \nNew in version 5.2. Returns an aggregation of the first n elements within a group. The elements returned are meaningful only if in a specified sort order. If the group contains fewer than n elements, $firstN returns all elements in the group. \nSYNTAX \n{   $firstN:      {         input: <expression>,         n: <expression>      }}      * input specifies the field(s) from the document to take the first n of. Input can be any expression.  * n has to be a positive integral expression that is either a constant or depends on the _id value for $group. For details see group key example. \nBEHAVIOR  NULL AND MISSING VALUES \n * $firstN does not filter out null values.  * $firstN converts missing values to null. Consider the following aggregation that returns the first five documents from a group: db.aggregate( [   {      $documents: [         { playerId: \"PlayerA\", gameId: \"G1\", score: 1 },         { playerId: \"PlayerB\", gameId: \"G1\", score: 2 },         { playerId: \"PlayerC\", gameId: \"G1\", score: 3 },         { playerId: \"PlayerD\", gameId: \"G1\"},         { playerId: \"PlayerE\", gameId: \"G1\", score: null }      ]   },   {      $group:      {         _id: \"$gameId\",         firstFiveScores:            {               $firstN:                  {                     input: \"$score\",                     n: 5                  }            }      }   }] )  In this example:  * $documents creates the literal documents that contain player scores.  * $group groups the documents by gameId. This example has only one gameId, G1.  * PlayerD has a missing score and PlayerE has a null score. These values are both considered as null.  * The firstFiveScores field is specified using input : \"$score\" and returned as an array.  * Since there is no sort criteria the first 5 score fields are returned. [   {      _id: 'G1',      firstFiveScores: [ 1, 2, 3, null, null ]   }] \nCOMPARISON OF $FIRSTN AND $TOPN ACCUMULATORS \nBoth $firstN and $topN accumulators can accomplish similar results. In general:  * If the documents coming into $group are already ordered, you should use $firstN.  * If you're sorting and selecting the top n elements then you can use $topN to accomplish both tasks with one accumulator.  * $firstN can be used as an aggregation expression, $topN cannot. \nRESTRICTIONS  WINDOW FUNCTION AND AGGREGATION EXPRESSION SUPPORT \n$firstN is supported as an aggregation expression. For details on aggregation expression usage see Using $firstN as an Aggregation Expression. $firstN is supported as a window operator. \nMEMORY LIMIT CONSIDERATIONS \nAggregation pipelines which call $firstN are subject to the 100 MB limit. If this limit is exceeded for an individual group, the aggregation fails with an error. \nEXAMPLES \nConsider a gamescores collection with the following documents:\n\n  \nFIND THE FIRST THREE PLAYER SCORES FOR A SINGLE GAME \nYou can use the $firstN accumulator to find the first three scores in a single game. db.gamescores.aggregate( [   {      $match : { gameId : \"G1\" }   },   {      $group:         {            _id: \"$gameId\",            firstThreeScores:               {                  $firstN:                  {                     input: [\"$playerId\", \"$score\"],                     n:3                  }               }         }   }] )  The example pipeline:  * Uses $match to filter the results on a single gameId. In this case, G1.  * Uses $group to group the results by gameId. In this case, G1.  * Specifies the fields that are input for $firstN with input : [\"$playerId\",\" $score\"].  * Uses $firstN to return the first three documents for the G1 game with n : 3. The operation returns the following results: [   {      _id: 'G1',      firstThreeScores: [ [ 'PlayerA', 31 ], [ 'PlayerB', 33 ], [ 'PlayerC', 99 ] ]   }] \nFINDING THE FIRST THREE PLAYER SCORES ACROSS MULTIPLE GAMES \nYou can use the $firstN accumulator to find the first n input fields in each game. db.gamescores.aggregate( [   {      $group:      {      _id: \"$gameId\", playerId:         {            $firstN:               {                  input: [ \"$playerId\",\"$score\" ],                  n: 3               }         }      }   }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Uses $firstN to return the first three documents for each game with n: 3.  * Specifies the fields that are input for $firstN with input : [\"$playerId\", \"$score\"]. The operation returns the following results: [   {      _id: 'G1',      playerId: [ [ 'PlayerA', 31 ], [ 'PlayerB', 33 ], [ 'PlayerC', 99 ] ]   },   {      _id: 'G2',      playerId: [ [ 'PlayerA', 10 ], [ 'PlayerB', 14 ], [ 'PlayerC', 66 ] ]   }] \nUSING $SORT WITH $FIRSTN \nUsing a $sort stage earlier in the pipeline can influence the results of the $firstN accumulator. In this example:  * {$sort : { score : -1 } } sorts the highest scores to the back of each group.  * firstN returns the three highest scores from front of each group.\n\n  The operation returns the following results: [   {      _id: 'G2',      playerId: [ [ 'PlayerD', 80 ], [ 'PlayerC', 66 ], [ 'PlayerB', 14 ] ]   },   {      _id: 'G1',      playerId: [ [ 'PlayerC', 99 ], [ 'PlayerB', 33 ], [ 'PlayerA', 31 ] ]   }] \nCOMPUTING N BASED ON THE GROUP KEY FOR $GROUP \nYou can also assign the value of n dynamically. In this example, the $cond expression is used on the gameId field. db.gamescores.aggregate([   {      $group:      {         _id: {\"gameId\": \"$gameId\"},         gamescores:            {               $firstN:                  {                     input: \"$score\",                     n: { $cond: { if: {$eq: [\"$gameId\",\"G2\"] }, then: 1, else: 3 } }                  }            }      }   }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Specifies the fields that input for $firstN with input : \"$score\".  * If the gameId is G2 then n is 1, otherwise n is 3. The operation returns the following results: [   { _id: { gameId: 'G1' }, gamescores: [ 31, 33, 99 ] },   { _id: { gameId: 'G2' }, gamescores: [ 10 ] }] \nUSING $FIRSTN AS AN AGGREGATION EXPRESSION \nYou can also use $firstN as an aggregation expression. In this example:  * $documents creates the literal document that contains an array of values.  * $project is used to return the output of $firstN.  * _id is omited from the output with _id : 0.  * $firstN uses the input array of [10, 20, 30, 40].  * The first three elements of the array are returned for the input document. db.aggregate( [   {      $documents: [         { array: [10, 20, 30, 40] } ]   },   { $project: {      firstThreeElements:{                           $firstN:                           {                              input: \"$array\",                              n: 3                           }                        }               }   }] )  The operation returns the following results: [   { firstThreeElements: [ 10, 20, 30 ] }] ←  $first (aggregation accumulator)$first (array operator) → On this page  * Definition\n * Syntax\n * Behavior\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/top/": " Docs Home → MongoDB Manual \n$TOP (AGGREGATION ACCUMULATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Restrictions\n * Examples \nDEFINITION \n$top \nNew in version 5.2. Returns the top element within a group according to the specified sort order. \nSYNTAX \n{   $top:      {         sortBy: { <field1>: <sort order>, <field2>: <sort order> ... },         output: <expression>      }} Field\nNecessity\nDescription\nsortBy\nRequired\nSpecifies the order of results, with syntax similar to $sort.\noutput\nRequired\nRepresents the output for each element in the group and can be any expression. \nBEHAVIOR  NULL AND MISSING VALUES \nConsider the following aggregation that returns the top document from a group of scores:      * $top does not filter out null values.  * $top converts missing values to null. db.aggregate( [   {      $documents: [         { playerId: \"PlayerA\", gameId: \"G1\", score: 1 },         { playerId: \"PlayerB\", gameId: \"G1\", score: 2 },         { playerId: \"PlayerC\", gameId: \"G1\", score: 3 },         { playerId: \"PlayerD\", gameId: \"G1\"},         { playerId: \"PlayerE\", gameId: \"G1\", score: null }      ]   },   {      $group:      {         _id: \"$gameId\",         playerId:            {               $top:                  {                     output: [ \"$playerId\", \"$score\" ],                     sortBy: { \"score\": 1 }                  }            }      }   }] )  In this example:  * $documents creates the literal documents that contain player scores.  * $group groups the documents by gameId. This example has only one gameId, G1.  * PlayerD has a missing score and PlayerE has a null score. These values are both considered as null.  * The playerId and score fields are specified as output : [\"$playerId\",\" $score\"] and returned as array values.  * Specify the sort order with sortBy: { \"score\": 1 }.  * PlayerD and PlayerE tied for the top element. PlayerD is returned as the top score.  * To have more deterministic tie breaking behavior for multiple null values, add more fields to``sortBy``. [   {      _id: 'G1',      playerId: [ [ 'PlayerD', null ] ]   }] \nRESTRICTIONS  WINDOW FUNCTION AND AGGREGATION EXPRESSION SUPPORT \n$top is not supported as a aggregation expression. $top is supported as a window operator. \nMEMORY LIMIT CONSIDERATIONS \nAggregation pipelines which call $top are subject to the 100 MB limit. If this limit is exceeded for an individual group, the aggregation fails with an error. \nEXAMPLES \nConsider a gamescores collection with the following documents:\n\n  \nFIND THE TOP SCORE \nYou can use the $top accumulator to find the top score in a single game. db.gamescores.aggregate( [   {      $match : { gameId : \"G1\" }   },   {      $group:         {            _id: \"$gameId\",            playerId:               {                  $top:                  {                     output: [ \"$playerId\", \"$score\" ],                     sortBy: { \"score\": -1 }                  }               }         }   }] )  The example pipeline:  * Uses $match to filter the results on a single gameId. In this case, G1.  * Uses $group to group the results by gameId. In this case, G1.  * Specifies the fields that are output for $top with output : [\"$playerId\",\" $score\"].  * Uses sortBy: { \"score\": -1 } to sort the scores in descending order.  * Uses $top to return the top score in the game. The operation returns the following results: [ { _id: 'G1', playerId: [ 'PlayerC', 99 ] } ] \nFIND THE TOP SCORE ACROSS MULTIPLE GAMES \nYou can use the $top accumulator to find the top score in each game. db.gamescores.aggregate( [      {         $group:         { _id: \"$gameId\", playerId:            {               $top:                  {                     output: [ \"$playerId\", \"$score\" ],                     sortBy: { \"score\": -1 }                  }            }         }      }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Uses $top to return top score for each game.  * Specifies the fields that are output for $top with output : [\"$playerId\", \"$score\"].  * Uses sortBy: { \"score\": -1 } to sort the scores in descending order. The operation returns the following results: [   { _id: 'G2', playerId: [ 'PlayerD', 80 ] },   { _id: 'G1', playerId: [ 'PlayerC', 99 ] }] ←  $toObjectId (aggregation)$topN (aggregation accumulator) → On this page  * Definition\n * Syntax\n * Behavior\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/last-array-element/": " Docs Home → MongoDB Manual \n$LAST (ARRAY OPERATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example\n * See Also \nDEFINITION \n$last \nNew in version 4.4. Returns the last element in an array. \nNOTE \nDISAMBIGUATION This page describes the $last array operator. For the $last aggregation accumulator, see $last (aggregation accumulator). \nSYNTAX \nThe $last operator has the following syntax: { $last: <expression> }  The <expression> can be any valid expression as long as it resolves to an array, null, or missing. For more information on expressions, see Expressions. The $last operator is an alias for the following $arrayElemAt expression: { $arrayElemAt: [ <array expression>, -1 ] }  \nBEHAVIOR  VALID OPERANDS \nValid operand for $last must resolve to an array, null, or missing.      * If the operand resolves to a non-empty array, $last returns the last element in the array:  * If the operand resolves to an empty array [], $last does not return a value.  * If the operand is null or missing, $last returns null. For example, create a test collection example1 with the following documents: db.example1.insertMany([     { \"_id\" : 1, \"x\" : [ 1, 2, 3 ] },      // Non-empty array     { \"_id\" : 2, \"x\" : [ [ ] ] },          // Non-empty array     { \"_id\" : 3, \"x\" : [ null ] },         // Non-empty array     { \"_id\" : 4, \"x\" : [ ] },              // Empty array     { \"_id\" : 5, \"x\" : null },             // Is null     { \"_id\" : 6 }                          // Is Missing])  Then, the following adds a new field lastElem whose value is derived from applying the $last operator to the x field: db.example1.aggregate([   { $addFields: { lastElem: { $last: \"$x\" } } }])  The operator returns the following documents: { \"_id\" : 1, \"x\" : [ 1, 2, 3 ], \"lastElem\" : 3 }{ \"_id\" : 2, \"x\" : [ [ ] ], \"lastElem\" : [ ] }{ \"_id\" : 3, \"x\" : [ null ], \"lastElem\" : null }{ \"_id\" : 4, \"x\" : [ ] }                         // No output{ \"_id\" : 5, \"x\" : null, \"lastElem\" : null }{ \"_id\" : 6, \"lastElem\" : null } \nINVALID OPERANDS \nIf the operand does not resolve to an array, null, or missing, the aggregation operation as a whole errors. For example, create a test collection example2 with the following documents: db.example2.insertMany([   { \"_id\" : 1, \"x\" : [ 1, 2, 3 ] },   { \"_id\" : 2, \"x\" : 2 },             // x is not an array/null or missing])  Then, the following aggregation operation returns an error because of the { \"_id\" : 2, \"x\" : 2 } document: db.example2.aggregate( { $addFields: { lastElem: { $last: \"$x\" } } } )  That is, the operation returns the following: 2020-01-20T21:12:26.713-05:00 E  QUERY    [js] uncaught exception: Error: command failed: {   \"ok\" : 0,   \"errmsg\" : \"$last's argument must be an array, but is double\",   \"code\" : 28689,   \"codeName\" : \"Location28689\"} : aggregate failed : \nEXAMPLE\n\n db.runninglog.insertMany([   { \"_id\" : 1, \"team\" : \"Anteater\", log: [ { run: 1, distance: 8 }, { run2: 2, distance: 7.5 }, { run: 3, distance: 9.2 } ] },   { \"_id\" : 2, \"team\" : \"Bears\", log: [ { run: 1, distance: 18 }, { run2: 2, distance: 17 }, { run: 3, distance: 16 } ] },   { \"_id\" : 3, \"team\" : \"Cobras\", log: [ { run: 1, distance: 2 } ] }])  The following aggregation uses the $first and $last operator on the log array to retrieve the information for the first run and the last run: db.runninglog.aggregate([   { $addFields: { firstrun: { $first: \"$log\" }, lastrun: { $last: \"$log\" } } }])  The operation returns the following results: { \"_id\" : 1, \"team\" : \"Anteater\", \"log\" : [ { \"run\" : 1, \"distance\" : 8 }, { \"run2\" : 2, \"distance\" : 7.5 }, { \"run\" : 3, \"distance\" : 9.2 } ],      \"firstrun\" : { \"run\" : 1, \"distance\" : 8 }, \"lastrun\" : { \"run\" : 3, \"distance\" : 9.2 } }{ \"_id\" : 2, \"team\" : \"Bears\", \"log\" : [ { \"run\" : 1, \"distance\" : 18 }, { \"run2\" : 2, \"distance\" : 17 }, { \"run\" : 3, \"distance\" : 16 } ],      \"firstrun\" : { \"run\" : 1, \"distance\" : 18 }, \"lastrun\" : { \"run\" : 3, \"distance\" : 16 } }{ \"_id\" : 3, \"team\" : \"Cobras\", \"log\" : [ { \"run\" : 1, \"distance\" : 2 } ],      \"firstrun\" : { \"run\" : 1, \"distance\" : 2 }, \"lastrun\" : { \"run\" : 1, \"distance\" : 2 } }  To calculate the change between the first and the last distances, the following operation uses $cond and $size operators to calculate the difference (i.e. $subtract) the two distances if there are two or more elements in the log array: db.runninglog.aggregate([  { $addFields: { firstrun: { $first: \"$log\" }, lastrun: { $last: \"$log\" } } },  { $project: { team: 1, progress:      {        $cond: {           if: { $gt: [ { $size:\"$log\" }, 1 ] } ,           then: { $subtract: [ \"$lastrun.distance\", \"$firstrun.distance\"] },           else: \"Not enough data.\" }      }\n  }}])  The operation returns the following documents: { \"_id\" : 1, \"team\" : \"Anteater\", \"progress\" : 1.1999999999999993 }{ \"_id\" : 2, \"team\" : \"Bears\", \"progress\" : -2 }{ \"_id\" : 3, \"team\" : \"Cobras\", \"progress\" : \"Not enough data.\" } By default, mongosh uses the 64-bit floating-point double for numbers. To improve precision, you can use Decimal128 instead. \nSEE ALSO \n * $first  * $arrayElemAt  * $slice  * Array Expression Operators ←  $lastN (aggregation accumulator)$lastN (array operator) → On this page  * Definition\n * Syntax\n * Behavior\n * Example\n * See Also Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/strLenBytes/": " Docs Home → MongoDB Manual \n$STRLENBYTES (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$strLenBytes \nReturns the number of UTF-8 encoded bytes in the specified string. $strLenBytes has the following operator expression syntax: { $strLenBytes: <string expression> }  The argument can be any valid expression as long as it resolves to a string. For more information on expressions, see Expressions. If the argument resolves to a value of null or refers to a missing field, $strLenBytes returns an error. \nBEHAVIOR \nThe $strLenBytes operator counts the number of UTF-8 encoded bytes in a string where each character may use between one and four bytes. For example, US-ASCII characters are encoded using one byte. Characters with diacritic markings and additional Latin alphabetical characters (i.e. Latin characters outside of the English alphabet) are encoded using two bytes. Chinese, Japanese and Korean characters typically require three bytes, and other planes of unicode (emoji, mathematical symbols, etc.) require four bytes. The $strLenBytes operator differs from $strLenCP operator which counts the code points in the specified string regardless of how many bytes each character uses. Example\nResults\nNotes { $strLenBytes: \"abcde\" } 5\nEach character is encoded using one byte. { $strLenBytes: \"Hello World!\" } 12\nEach character is encoded using one byte. { $strLenBytes: \"cafeteria\" } 9\nEach character is encoded using one byte. { $strLenBytes: \"cafétéria\" } 11\né is encoded using two bytes. { $strLenBytes: \"\" } 0\nEmpty strings return 0. { $strLenBytes: \"$€λG\" } 7\n€ is encoded using three bytes. λ is encoded using two bytes. { $strLenBytes: \"寿司\" } 6\nEach character is encoded using three bytes. \nEXAMPLE  SINGLE-BYTE AND MULTIBYTE CHARACTER SET \nCreate a food collection with the following documents: db.food.insertMany( [    { \"_id\" : 1, \"name\" : \"apple\" },    { \"_id\" : 2, \"name\" : \"banana\" },    { \"_id\" : 3, \"name\" : \"éclair\" },    { \"_id\" : 4, \"name\" : \"hamburger\" },    { \"_id\" : 5, \"name\" : \"jalapeño\" },    { \"_id\" : 6, \"name\" : \"pizza\" },    { \"_id\" : 7, \"name\" : \"tacos\" },    { \"_id\" : 8, \"name\" : \"寿司\" } ])  The following operation uses the $strLenBytes operator to calculate the length of each name value: db.food.aggregate(  [    {      $project: {        \"name\": 1,        \"length\": { $strLenBytes: \"$name\" }      }    }  ])  The operation returns the following results: { \"_id\" : 1, \"name\" : \"apple\", \"length\" : 5 }{ \"_id\" : 2, \"name\" : \"banana\", \"length\" : 6 }{ \"_id\" : 3, \"name\" : \"éclair\", \"length\" : 7 }{ \"_id\" : 4, \"name\" : \"hamburger\", \"length\" : 9 }{ \"_id\" : 5, \"name\" : \"jalapeño\", \"length\" : 9 }{ \"_id\" : 6, \"name\" : \"pizza\", \"length\" : 5 }{ \"_id\" : 7, \"name\" : \"tacos\", \"length\" : 5 }{ \"_id\" : 8, \"name\" : \"寿司\", \"length\" : 6 }  The documents with _id: 3 and _id: 5 each contain a diacritic character (é and ñ respectively) that requires two bytes to encode. The document with _id: 8 contains two Japanese characters that are encoded using three bytes each. This makes the length greater than the number of characters in name for the documents with _id: 3, _id: 5 and _id: 8. \nTIP \nSEE ALSO:      * $strLenCP\n\n ←  $strcasecmp (aggregation)$strLenCP (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/ltrim/": " Docs Home → MongoDB Manual \n$LTRIM (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$ltrim \nRemoves whitespace characters, including null, or the specified characters from the beginning of a string. $ltrim has the following syntax: { $ltrim: { input: <string>,  chars: <string> } }  The $ltrim takes a document with the following fields: Field\nDescription\ninput\nThe string to trim. The argument can be any valid expression that resolves to a string. For more information on expressions, see Expressions.\nchars Optional. The character(s) to trim from the beginning of the input. The argument can be any valid expression that resolves to a string. The $ltrim operator breaks down the string into individual UTF code point to trim from input. If unspecified, $ltrim removes whitespace characters, including the null character. For the list of whitespace characters, see Whitespace Characters. \nTIP \nSEE ALSO:      * $trim  * $rtrim \nBEHAVIOR \n * By default, $ltrim removes whitespace characters, including the null character, from the beginning of the input string:\n   \n   Example\n   Results\n   { $ltrim: { input: \" \\n good bye \\t \" } }\n   \"good bye \\t \"  * You can override the default characters to trim using the chars field.\n   \n   For example, the following trims any g and e from the start of the input string. Since the input starts with a whitespace, neither character can be trimmed from the start of the string.\n   \n   Example\n   Results\n   { $ltrim: { input: \" ggggoodbyeeeee\", chars: \"ge\" } }\n   \" ggggoodbyeeeee\"  * If overriding the default characters to trim, you can explicitly include the whitespace character(s) to trim in the chars field.\n   \n   For example, the following trims any space, g, or d from the start of the input string.\n   \n   Example\n   Results\n   { $ltrim: { input: \" ggggoodbyeeeee \", chars: \" gd\" } }\n   \"oodbyeeeee \" \nWHITESPACE CHARACTERS \nBy default, $ltrim removes the following characters: Unicode\nEscape sequence\nDescription\nU+0000\n'0'\nNull character\nU+0020\n' '\nSpace\nU+0009\n't'\nHorizontal tab\nU+000A\n'n'\nLine feed/new line\nU+000B\n'v'\nVertical tab\nU+000C\n'f'\nForm feed\nU+000D\n'r'\nCarriage return\nU+00A0 Non-breaking space\nU+1680 Ogham space mark\nU+2000 En quad\nU+2001 Em quad\nU+2002 En space\nU+2003 Em space\nU+2004 Three-per-em space\nU+2005 Four-per-em space\nU+2006 Six-per-em space\nU+2007 Figure space\nU+2008 Punctuation space\nU+2009 Thin space\nU+200A Hair space \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \" product 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", quarter: \"13Q4\", \"description\" : \"product 2 \\n The product is in stock.  \\n\\n  \" }{ \"_id\" : 3, \"item\" : \"XYZ1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $ltrim operator to remove leading whitespaces from the description field: db.inventory.aggregate([   { $project: { item: 1, description: { $ltrim: { input: \"$description\" } } } }])  The operation returns the following results:\n\n \n←  $lte (aggregation)$map (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/last/": " Docs Home → MongoDB Manual \n$LAST (AGGREGATION ACCUMULATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$last \nChanged in version 5.0. Returns the value that results from applying an expression to the last document in a group of documents. Only meaningful when documents are in a defined order. $last is available in these stages:      * $bucket  * $bucketAuto  * $group  * $setWindowFields (Available starting in MongoDB 5.0) \nNOTE \nDISAMBIGUATION This page describes the $last aggregation accumulator. For the $last array operator, see $last (array operator). \nSYNTAX \n$last syntax: { $last: <expression> }  For more information on expressions, see Expressions. \nBEHAVIOR \nTo define the document order for $last with the:  * $group stage, add a $sort stage before the $group stage.  * $setWindowFields stage, set the sortBy field. \nNOTE Although the $sort stage passes ordered documents as input to the $group and $setWindowFields stages, those stages are not guaranteed to maintain the sort order in their own output. When used with $setWindowFields, $last returns null for empty windows. An example empty window is a { documents: [ -1, -1 ] } documents window on the first document of a partition. \nEXAMPLES  USE IN $GROUP STAGE \nConsider a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"date\" : ISODate(\"2014-01-01T08:00:00Z\"), \"price\" : 10, \"quantity\" : 2 }{ \"_id\" : 2, \"item\" : \"jkl\", \"date\" : ISODate(\"2014-02-03T09:00:00Z\"), \"price\" : 20, \"quantity\" : 1 }{ \"_id\" : 3, \"item\" : \"xyz\", \"date\" : ISODate(\"2014-02-03T09:05:00Z\"), \"price\" : 5, \"quantity\" : 5 }{ \"_id\" : 4, \"item\" : \"abc\", \"date\" : ISODate(\"2014-02-15T08:00:00Z\"), \"price\" : 10, \"quantity\" : 10 }{ \"_id\" : 5, \"item\" : \"xyz\", \"date\" : ISODate(\"2014-02-15T09:05:00Z\"), \"price\" : 5, \"quantity\" : 10 }{ \"_id\" : 6, \"item\" : \"xyz\", \"date\" : ISODate(\"2014-02-15T12:05:10Z\"), \"price\" : 5, \"quantity\" : 5 }{ \"_id\" : 7, \"item\" : \"xyz\", \"date\" : ISODate(\"2014-02-15T14:12:12Z\"), \"price\" : 5, \"quantity\" : 10 }  The following operation first sorts the documents by item and date, and then in the following $group stage, groups the now sorted documents by the item field and uses the $last accumulator to compute the last sales date for each item: db.sales.aggregate(   [     { $sort: { item: 1, date: 1 } },     {       $group:         {           _id: \"$item\",           lastSalesDate: { $last: \"$date\" }         }     }   ])  The operation returns the following results: { \"_id\" : \"xyz\", \"lastSalesDate\" : ISODate(\"2014-02-15T14:12:12Z\") }{ \"_id\" : \"jkl\", \"lastSalesDate\" : ISODate(\"2014-02-03T09:00:00Z\") }{ \"_id\" : \"abc\", \"lastSalesDate\" : ISODate(\"2014-02-15T08:00:00Z\") }  \nUSE IN $SETWINDOWFIELDS STAGE \nNew in version 5.0. Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA):\n\n  This example uses $last in the $setWindowFields stage to output the last cake sales order type for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            lastOrderTypeForState: {               $last: \"$type\",               window: {                  documents: [ \"current\", \"unbounded\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output sets the lastOrderTypeForState field to the last order type from the documents window.\n   \n   The window contains documents between the current lower limit, which is the current document in the output, and the unbounded upper limit. This means $last returns the last order type for the documents between the current document and the end of the partition. In this output, the last order type value for CA and WA is shown in the lastOrderTypeForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"lastOrderTypeForState\" : \"vanilla\" }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"lastOrderTypeForState\" : \"vanilla\" }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"lastOrderTypeForState\" : \"vanilla\" }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"lastOrderTypeForState\" : \"chocolate\" }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"lastOrderTypeForState\" : \"chocolate\" }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"lastOrderTypeForState\" : \"chocolate\" } ←  $isoWeekYear (aggregation)$lastN (aggregation accumulator) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dateFromParts/": " Docs Home → MongoDB Manual \n$DATEFROMPARTS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$dateFromParts \nConstructs and returns a Date object given the date's constituent properties. The $dateFromParts expression has the following syntax: {    $dateFromParts : {        'year': <year>, 'month': <month>, 'day': <day>,        'hour': <hour>, 'minute': <minute>, 'second': <second>,        'millisecond': <ms>, 'timezone': <tzExpression>    }}  You can also specify your constituent date fields in ISO week date format using the following syntax: {    $dateFromParts : {        'isoWeekYear': <year>, 'isoWeek': <week>, 'isoDayOfWeek': <day>,        'hour': <hour>, 'minute': <minute>, 'second': <second>,        'millisecond': <ms>, 'timezone': <tzExpression>    }}  The $dateFromParts takes a document with the following fields: \nIMPORTANT You cannot combine the use of calendar dates and ISO week date fields when constructing your $dateFromParts input document. Field\nRequired/Optional\nDescription\nyear\nRequired if not using isoWeekYear Calendar year. Can be any expression that evaluates to a number. Value range: 1-9999 If the number specified is outside this range, $dateFromParts errors. Starting in MongoDB 4.4, the lower bound for this value is 1. In previous versions of MongoDB, the lower bound was 0.\nisoWeekYear\nRequired if not using year ISO Week Date Year. Can be any expression that evaluates to a number. Value range: 1-9999 If the number specified is outside this range, $dateFromParts errors. Starting in MongoDB 4.4, the lower bound for this value is 1. In previous versions of MongoDB, the lower bound was 0.\nmonth\nOptional. Can only be used with year. Month. Can be any expression that evaluates to a number. Defaults to 1. Value range: 1-12 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\nisoWeek\nOptional. Can only be used with isoWeekYear. Week of year. Can be any expression that evaluates to a number. Defaults to 1. Value range: 1-53 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\nday\nOptional. Can only be used with year. Day of month. Can be any expression that evaluates to a number. Defaults to 1. Value range: 1-31 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\nisoDayOfWeek\nOptional. Can only be used with isoWeekYear. Day of week (Monday 1 - Sunday 7). Can be any expression that evaluates to a number. Defaults to 1. Value range: 1-7 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\nhour\nOptional Hour. Can be any expression that evaluates to a number. Defaults to 0. Value range: 0-23 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\nminute\nOptional Minute. Can be any expression that evaluates to a number. Defaults to 0. Value range: 0-59 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples. second\nOptional Second. Can be any expression that evaluates to a number. Defaults to 0. Value range: 0-59 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\nmillisecond\nOptional Millisecond. Can be any expression that evaluates to a number. Defaults to 0. Value range: 0-999 If the number specified is outside this range, $dateFromParts incorporates the difference in the date calculation. See Value Range for examples.\ntimezone\nOptional <timezone> can be any expression that evaluates to a string whose value is either:    \n\n  * a UTC offset in the form:\n   \n   * +/-[hh]:[mm], e.g. \"+04:45\", or\n   \n   * +/-[hh][mm], e.g. \"-0530\", or\n   \n   * +/-[hh], e.g. \"+03\". For more information on expressions, see Expressions. \nBEHAVIOR  VALUE RANGE \nStarting in MongoDB 4.4, the supported value range for year and isoWeekYear is 1-9999. In prior versions of MongoDB, the lower bound for these values was 0 and the supported value range was 0-9999. If the value specified for fields other than year, isoWeekYear, and timezone is outside the valid range, $dateFromParts carries or subtracts the difference from other date parts to calculate the date. \nVALUE IS GREATER THAN THE RANGE \nConsider the following $dateFromParts expression where the month field value is 14, which is 2 months greater than the maximum value of 12 months(or 1 year): { $dateFromParts: { 'year' : 2017, 'month' : 14, 'day': 1, 'hour' : 12  } }  The expression calculates the date by increasing the year by 1 and setting the month to 2 to return: ISODate(\"2018-02-01T12:00:00Z\")  \nVALUE IS LESS THAN THE RANGE \nConsider the following $dateFromParts expression where the month field value is 0, which is 1 month less than the minimum value of 1 month: { $dateFromParts: { 'year' : 2017, 'month' : 0, 'day': 1, 'hour' : 12  } }  The expression calculates the date by decreasing the year by 1 and setting the month to 12 to return: ISODate(\"2016-12-01T12:00:00Z\")  \nTIME ZONE \nWhen using an Olson Timezone Identifier in the <timezone> field, MongoDB applies the DST offset if applicable for the specified timezone. For example, consider a sales collection with the following document: {   \"_id\" : 1,   \"item\" : \"abc\",   \"price\" : 20,   \"quantity\" : 5,   \"date\" : ISODate(\"2017-05-20T10:24:51.303Z\")}  The following aggregation illustrates how MongoDB handles the DST offset for the Olson Timezone Identifier. The example uses the $hour and $minute operators to return the corresponding portions of the date field: db.sales.aggregate([{   $project: {      \"nycHour\": {         $hour: { date: \"$date\", timezone: \"-05:00\" }       },       \"nycMinute\": {          $minute: { date: \"$date\", timezone: \"-05:00\" }       },       \"gmtHour\": {          $hour: { date: \"$date\", timezone: \"GMT\" }       },       \"gmtMinute\": {          $minute: { date: \"$date\", timezone: \"GMT\" } },       \"nycOlsonHour\": {          $hour: { date: \"$date\", timezone: \"America/New_York\" }       },       \"nycOlsonMinute\": {          $minute: { date: \"$date\", timezone: \"America/New_York\" }       }   }}])  The operation returns the following result: {   \"_id\": 1,   \"nycHour\" : 5,   \"nycMinute\" : 24,   \"gmtHour\" : 10,   \"gmtMinute\" : 24,   \"nycOlsonHour\" : 6,   \"nycOlsonMinute\" : 24}  \nEXAMPLE \nThe following aggregation uses $dateFromParts to construct three date objects from the provided input fields:\n\n  The operation returns the following result: {  \"_id\" : 1,  \"date\" : ISODate(\"2017-02-08T12:00:00Z\"),  \"date_iso\" : ISODate(\"2017-02-08T12:00:00Z\"),  \"date_timezone\" : ISODate(\"2017-01-01T04:46:12Z\")} \n←  $dateDiff (aggregation)$dateFromString (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/multiply/": " Docs Home → MongoDB Manual \n$MULTIPLY (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$multiply \nMultiplies numbers together and returns the result. Pass the arguments to $multiply in an array. The $multiply expression has the following syntax: { $multiply: [ <expression1>, <expression2>, ... ] }  The arguments can be any valid expression as long as they resolve to numbers. For more information on expressions, see Expressions. \nEXAMPLE \nConsider a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"quantity\": 2, date: ISODate(\"2014-03-01T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"quantity\": 1, date: ISODate(\"2014-03-01T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"price\" : 5, \"quantity\": 10, date: ISODate(\"2014-03-15T09:00:00Z\") }  The following aggregation uses the $multiply expression in the $project pipeline to multiply the price and the quantity fields: db.sales.aggregate(   [     { $project: { date: 1, item: 1, total: { $multiply: [ \"$price\", \"$quantity\" ] } } }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc\", \"date\" : ISODate(\"2014-03-01T08:00:00Z\"), \"total\" : 20 }{ \"_id\" : 2, \"item\" : \"jkl\", \"date\" : ISODate(\"2014-03-01T09:00:00Z\"), \"total\" : 20 }{ \"_id\" : 3, \"item\" : \"xyz\", \"date\" : ISODate(\"2014-03-15T09:00:00Z\"), \"total\" : 50 } \n←  $month (aggregation)$ne (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/size/": " Docs Home → MongoDB Manual \n$SIZE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$size \nCounts and returns the total number of items in an array. $size has the following syntax: { $size: <expression> }  The argument for $size can be any expression as long as it resolves to an array. For more information on expressions, see Expressions. \nBEHAVIOR \nThe argument for $size must resolve to an array. If the argument for $size is missing or does not resolve to an array, $size errors. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", \"description\" : \"product 1\", colors: [ \"blue\", \"black\", \"red\" ] }{ \"_id\" : 2, \"item\" : \"ABC2\", \"description\" : \"product 2\", colors: [ \"purple\" ] }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"description\" : \"product 3\", colors: [ ] }{ \"_id\" : 4, \"item\" : \"ZZZ1\", \"description\" : \"product 4 - missing colors\" }{ \"_id\" : 5, \"item\" : \"ZZZ2\", \"description\" : \"product 5 - colors is string\", colors: \"blue,red\" }  The following aggregation pipeline operation uses the $size operator to return the number of elements in the colors array: db.inventory.aggregate([   {      $project: {         item: 1,         numberOfColors: { $cond: { if: { $isArray: \"$colors\" }, then: { $size: \"$colors\" }, else: \"NA\"} }      }   }] )  The operation returns the following: { \"_id\" : 1, \"item\" : \"ABC1\", \"numberOfColors\" : 3 }{ \"_id\" : 2, \"item\" : \"ABC2\", \"numberOfColors\" : 1 }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"numberOfColors\" : 0 }{ \"_id\" : 4, \"item\" : \"ZZZ1\", \"numberOfColors\" : \"NA\" }{ \"_id\" : 5, \"item\" : \"ZZZ2\", \"numberOfColors\" : \"NA\" } \n←  $shift (aggregation)$sin (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/currentOp/": " Docs Home → MongoDB Manual \n$CURRENTOP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Constraints\n * Example\n * Output Fields \nDEFINITION \n$currentOp \nReturns a stream of documents containing information on active and/or dormant operations as well as inactive sessions that are holding locks as part of a transaction. The stage returns a document for each operation or session. To run $currentOp, use the db.aggregate() helper on the admin database. The $currentOp aggregation stage is preferred over the currentOp command and its mongosh helper method db.currentOp(). Because the currentOp command and db.currentOp() helper method return the results in a single document, the total size of the currentOp result set is subject to the maximum 16MB BSON size limit for documents. The $currentOp stage returns a cursor over a stream of documents, each of which reports a single operation. Each operation document is subject to the 16MB BSON limit, but unlike the currentOp command, there is no limit on the overall size of the result set. $currentOp also enables you to perform arbitrary transformations of the results as the documents pass through the pipeline. \nSYNTAX \nChanged in version 4.2. { $currentOp: { allUsers: <boolean>, idleConnections: <boolean>, idleCursors: <boolean>, idleSessions: <boolean>, localOps: <boolean> } }  $currentOp takes an options document as its operand: Option\nDescription\nallUsers Boolean.      * If set to false, $currentOp will only report on operations/idle connections/idle cursors/idle sessions belonging to the user who ran the command.  * If set to true, $currentOp will report operations belonging to all users. \nNOTE For standalone and replica sets that enforce access control, inprog privilege is required if allUsers: true. For sharded clusters that enforce access control, the inprog privilege is required to run $currentOp. Defaults to false. idleConnections Boolean. If set to false, $currentOp will only report active operations. If set to true, all operations including idle connections will be returned. Defaults to false. idleCursors Boolean. If set to true, $currentOp will report on cursors that are \"idle\"; i.e. open but not currently active in a getMore operation. Information on idle cursors have the type set to \"idleCursor\". Information on cursors currently active in a getMore operation information have the type set to \"op\" and op set to getmore. Defaults to false. New in version 4.2. idleSessions Boolean.  * If set to true, in addition to active/dormant operations, $currentOp will report on:\n   \n   * Inactive sessions that are holding locks as part of a transaction. Each inactive session will appear as a separate document in the $currentOp stream.\n     \n     The document for a session includes information on the session ID in the lsid field and the transaction in the transaction field.\n     \n     Starting in MongoDB 4.2, information on idle sessions have the type set to \"idleSession\".\n   \n   * $currentOp.twoPhaseCommitCoordinator in inactive state  * If set to false, $currentOp will not report on:\n   \n   * Inactive sessions\n   \n   * $currentOp.twoPhaseCommitCoordinator information in inactive state Defaults to true. localOps Boolean. If set to true for an aggregation running on mongos, $currentOp reports only those operations running locally on that mongos. If false, then the $currentOp will instead report operations running on the shards. The localOps parameter has no effect for $currentOp aggregations running on mongod. Defaults to false. backtrace Boolean. Determines whether callstack information is returned as part of the waitingForLatch output field.  * If set to true, $currentOp includes waitingForLatch.backtrace field that contains the callstack information, if available. If unavailable, the field contains an empty array.  * If set to false, $currentOp omits the waitingForLatch.backtrace field. Defaults to false. New in version 4.2.2.\n\n { $currentOp: { } }  \nCONSTRAINTS  PIPELINE \n * $currentOp must be the first stage in the pipeline.  * Pipelines that start with $currentOp can only be run on the admin database. \nACCESS CONTROL \n * For standalone and replica sets that enforce access control, inprog privilege is required to run $currentOp if allUsers: true.  * For sharded clusters that enforce access control, the inprog privilege is required to run $currentOp. \nTRANSACTIONS \n * $currentOp is not allowed in transactions. \nEXAMPLE \nThe following example returns information on inactive sessions that are holding locks as part of a transaction. Specifically:  * The first stage returns documents for all active operations as well as inactive sessions that are holding locks as part of a transaction.  * The second stage filters for just those documents related to inactive sessions that are holding locks as part of a transaction. db.getSiblingDB(\"admin\").aggregate( [   { $currentOp : { allUsers: true, idleSessions: true } },   { $match : { active: false, transaction : { $exists: true } } }] )  Starting in version 4.2, you can use $currentOp.type to specify an equivalent filter: db.getSiblingDB(\"admin\").aggregate( [   { $currentOp : { allUsers: true, idleSessions: true } },   { $match : { type: \"idleSession\" } }] )  \nTIP For transactions on a sharded cluster, starting in version 4.2.1, include localOps:true in the aforementioned examples for a composite view of the transactions. Both operations return documents of the form:   OUTPUT FIELDS \nEach output document may contain a subset of the following fields as relevant for the operation: $currentOp.type \nNew in version 4.2. The type of operation. Values are either:  * op  * idleSession  * idleCursor If the $currentOp.type is op, $currentOp.op provides details on the specific operation. $currentOp.host \nThe name of the host against which the operation is run. $currentOp.shard \nThe name of the shard where the operation is running. Only present for sharded clusters. $currentOp.desc \nA description of the operation. $currentOp.connectionId \nAn identifier for the connection where the specific operation originated. $currentOp.client \nThe IP address (or hostname) and the ephemeral port of the client connection where the operation originates. For multi-document transactions, $currentOp.client stores information about the most recent client to run an operation inside the transaction. For standalones and replica sets only $currentOp.client_s \nThe IP address (or hostname) and the ephemeral port of the mongos where the operation originates. For sharded clusters only $currentOp.clientMetadata \nAdditional information on the client. For multi-document transactions, $currentOp.client stores information about the most recent client to run an operation inside the transaction. $currentOp.appName \nThe identifier of the client application which ran the operation. Use the appName connection string option to set a custom value for the appName field. $currentOp.active \nA boolean value specifying whether the operation has started. Value is true if the operation has started or false if the operation is idle, such as an idle connection, an inactive session, or an internal thread that is currently idle. An operation can be active even if the operation has yielded to another operation. $currentOp.twoPhaseCommitCoordinator \nInformation on either:  * The commit coordination metrics for a transaction whose write operations span multiple shards.\n   \n   Commit coordination is handled by a shard, and $currentOp (run either on a mongos or a shard member) returns a shard's coordination information only for those transactions currently being coordinated by that shard.\n   \n   To filter for just the commit coordination metrics:\n   \n   db.getSiblingDB(\"admin\").aggregate( [    { $currentOp: { allUsers: true, idleSessions: true } },    { $match: { desc: \"transaction coordinator\" } }] )\n   \n     * A specific commit coordination operation (i.e. type is op and desc is \"TransactionCoordinator\") spawned by the transaction coordinator. \nNOTE  * If run with idleSessions: false, $currentOp does not return the $currentOp.twoPhaseCommitCoordinator information in inactive state\n\n New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.lsid \nThe session identifier for the multi-shard transaction. The combination of the lsid and txnNumber identifies the transaction. Available for both the commit coordination metrics and for specific coordination operation. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.txnNumber \nThe transaction number for the multi-shard transaction. The combination of the txnNumber and lsid identifies the transaction. Available for both the commit coordination metrics and for specific coordination operation. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.action \nThe specific commit coordination operation spawned by the transaction coordinator:  * \"sendingPrepare\"  * \"sendingCommit\"  * \"sendingAbort\"  * \"writingParticipantList\"  * \"writingDecision\"  * \"deletingCoordinatorDoc\" Only available for specific coordination operation. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.startTime \nThe start date and time of the action. Only available for specific coordination operation. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.numParticipants \nNumber of shards participating in this commit. Only available for the commit coordination metrics. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.state \nThe current step/state of the commit coordination process. Step/stage\nDescription\ninactive\nNot actively part of a commit.\nwritingParticipantList\nWriting a local record of the list of shards that are part of this multi-shard transaction.\nwaitingForVotes\nWaiting for the participants to respond with vote to commit or abort.\nwritingDecision\nWriting a local record of the coordinator's decision to commit or abort based on votes.\nwaitingForDecisionAck\nWaiting for participants to acknowledge the coordinator's decision to commit or abort.\ndeletingCoordinatorDoc\nDeleting the local record of commit decision. Only available for the commit coordination metrics. See also $currentOp.twoPhaseCommitCoordinator.stepDurations. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.commitStartTime \nThe date and time when the commit started. Only available for the commit coordination metrics. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.hasRecoveredFromFailover \nA boolean that indicates whether the commit coordination was restarted due to failover on the shard that is coordinating the commit. If hasRecoveredFromFailover is true, then the times specified in $currentOp.twoPhaseCommitCoordinator.stepDurations may not be accurate for all steps. Only available for the commit coordination metrics. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.stepDurations \nA document that contains the duration, in microseconds, of the completed or in-progress steps/state of the active process as well as the cumulative total duration; for example: \"stepDurations\" : {   \"writingParticipantListMicros\" : NumberLong(17801),   \"totalCommitDurationMicros\" : NumberLong(42488463),   \"waitingForVotesMicros\" : NumberLong(30378502),   \"writingDecisionMicros\" : NumberLong(15015),   \"waitingForDecisionAcksMicros\" : NumberLong(12077145),   \"deletingCoordinatorDocMicros\" : NumberLong(6009)},  If $currentOp.twoPhaseCommitCoordinator.hasRecoveredFromFailover is true, then the times specified in stepDurations may not be accurate for all steps. For a coordinator in an inactive state, the document is empty: \"stepDurations\" : {}  Only available for the commit coordination metrics. See $currentOp.twoPhaseCommitCoordinator.state. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.decision \nA document that contains the commit/abort decision, for example:  * For a commmit decision:\n   \n   \"decision\" : {   \"decision\" : \"commit\",   \"commitTimestamp\" : Timestamp(1572034669, 3)}\n   \n   \n\n Only available for the commit coordination metrics. New in version 4.2.1. $currentOp.twoPhaseCommitCoordinator.deadline \nThe date and time by which the commit must finish. Only available for the commit coordination metrics. New in version 4.2.1. $currentOp.currentOpTime \nThe start time of the operation. $currentOp.effectiveUsers \nAn array that contains a document for each user associated with the operation. Each user document contains the user name and the authentication db. New in version 4.2. \nTIP \nSEE ALSO: $currentOp.runBy $currentOp.runBy \nAn array that contains a document for each user who is impersonating the effectiveUser(s) for the operation. The runBy document contains the user name and the authentication db. In general, the impersonating user is the __system user; e.g. \"runBy\" : [   {      \"user\" : \"__system\",      \"db\" : \"local\"   }]  New in version 4.2. $currentOp.opid \nThe identifier for the operation. You can pass this value to db.killOp() in mongosh to terminate the operation. \nWARNING Terminate running operations with extreme caution. Only use db.killOp() to terminate operations initiated by clients and do not terminate internal database operations. $currentOp.secs_running \nThe duration of the operation in seconds. MongoDB calculates this value by subtracting the current time from the start time of the operation. Only present if the operation is running; i.e. if active is true. $currentOp.microsecs_running \nThe duration of the operation in microseconds. MongoDB calculates this value by subtracting the current time from the start time of the operation. Only present if the operation is running; i.e. if active is true. $currentOp.lsid \nThe session identifier. Only present if the operation is associated with a session. $currentOp.transaction \nA document that contains multi-document transaction information. Only present if the operation is part of a transaction:  * Present starting in 4.0 for transactions on a replica set.  * Present starting in 4.2 for transactions on a sharded cluster if $currentOp is run without localOps: true. The transaction information is per shard.  * Present starting in 4.2.1 for transactions on a sharded cluster if $currentOp is run with localOps: true. The transaction information is a composite view rather than per shard. $currentOp.transaction.parameters \nA document that contains information on multi-document transaction. Only present if the operation is part of a multi-document transaction. $currentOp.transaction.parameters.txnNumber \nThe transaction number. Only present if the operation is part of a multi-document transaction. $currentOp.transaction.parameters.autocommit \nA boolean flag that indicates if autocommit is on for the transaction. Only present if the operation is part of a multi-document transaction. $currentOp.transaction.parameters.readConcern \nThe read concern for the transaction. Multi-document transactions support read concern \"snapshot\", \"local\", and \"majority\". Only present if the operation is part of a multi-document transaction. $currentOp.transaction.globalReadTimestamp \nThe timestamp of the snapshot read by the operations in the sharded cluster transaction that uses \"snapshot\" read concern. For transactions on sharded clusters, the read concern \"snapshot\" of the data is synchronized across shards; i.e. other read concerns cannot guarantee that the data is from the same snapshot view across the shards. Only present when run with localOps: true for sharded cluster transactions. New in version 4.2.1. $currentOp.transaction.readTimestamp \nThe timestamp of the snapshot being read by the operations in this transaction Only present if the operation is part of a multi-document transaction. However, the field is not returned if:  * the transaction is on a sharded cluster and uses \"snapshot\" read concern, and  * $currentOp is run with localOps: true. Instead, $currentOp.transaction.globalReadTimestamp is returned. $currentOp.transaction.startWallClockTime \nThe date and time (with time zone) of the transaction start. Only present if the operation is part of a multi-document transaction. $currentOp.transaction.timeOpenMicros \nThe duration, in microseconds, for the transaction. The timeActiveMicros value added to the timeInactiveMicros should equal the timeOpenMicros.\n\n $currentOp.transaction.timeActiveMicros \nThe total amount of time that the transaction has been active; i.e. when the transaction had operations running. The timeActiveMicros value added to the timeInactiveMicros should equal the timeOpenMicros. Only present if the operation is part of a multi-document transaction. $currentOp.transaction.timeInactiveMicros \nThe total amount of time that the transaction has been inactive; i.e. when the transaction had no operations running. The timeInactiveMicros value added to the timeActiveMicros should equal the timeOpenMicros. Only present if the operation is part of a multi-document transaction. $currentOp.transaction.numParticipants \nNumber of shards participating in this transaction. Only present if the operation is part of a transaction on a sharded cluster and $currentOp is run with localOps: true New in version 4.2.1. $currentOp.transaction.participants \nAn array of documents detailing the participating shardas in this transaction. Each document contains the name, a flag indicating if the shard acts as the commit coordinator, and a flag indicating if the shard is only involved in read operations for the transaction. {   \"name\" : \"shardA\",   \"coordinator\" : false,   \"readOnly\" : false} Only present if the operation is part of a transaction on a sharded cluster and $currentOp is run with localOps: true New in version 4.2.1. $currentOp.transaction.numReadOnlyParticipants \nNumber of shards only affected by read operations in this transaction. Only present if the operation is part of a transaction on a sharded cluster and $currentOp is run with localOps: true New in version 4.2.1. $currentOp.transaction.numNonReadOnlyParticipants \nNumber of shards affected by operations other than reads in this transaction. Only present if the operation is part of a transaction on a sharded cluster and $currentOp is run with localOps: true New in version 4.2.1. $currentOp.transaction.expiryTime \nThe date and time (with time zone) when the transaction will time out and abort. The $currentOp.transaction.expiryTime equals the $currentOp.transaction.startWallClockTime + the transactionLifetimeLimitSeconds. For more information, seee Runtime Limit for transactions. Only present if the operation is part of a multi-document transaction. $currentOp.op \nA string that identifies the specific operation type. Only present if $currentOp.type is op. The possible values are:  * \"none\"  * \"update\"  * \"insert\"  * \"query\"  * \"command\"  * \"getmore\"  * \"remove\"  * \"killcursors\" \"command\" operations include most commands such as the createIndexes, aggregate, and findAndModify. \"query\" operations include find operations and OP_QUERY operations. $currentOp.ns \nThe namespace the operation targets. A namespace consists of the database name and the collection name concatenated with a dot (.); that is, \"<database>.<collection>\". $currentOp.command \nA document containing the full command object associated with this operation. For example, the following output contains the command object for a find operation on a collection named items in a database named test: \"command\" : {  \"find\" : \"items\",  \"filter\" : {    \"sku\" : 1403978  },  ...  \"$db\" : \"test\"}  The following example output contains the command object for a getMore operation generated by a command with cursor ID 19234103609 on a collection named items in a database named test: \"command\" : {    \"getMore\" : NumberLong(\"19234103609\"),    \"collection\" : \"items\",    \"batchSize\" : 10,    ...    \"$db\" : \"test\"},  If the command document exceeds 1 kilobyte, the document has the following form: \"command\" : {  \"$truncated\": <string>,  \"comment\": <string>}  The $truncated field contains a string summary of the document excluding the document's comment field if present. If the summary still exceeds 1 kilobyte then it is further truncated, denoted by an ellipsis (...) at the end of the string. The comment field is present if a comment was passed to the operation. Starting in MongoDB 4.4, a comment may be attached to any database command. $currentOp.cursor \nNew in version 4.2.\n\n If reporting on a getmore operation before the getmore has accessed its cursor information, the cursor field is not available. $currentOp.cursor.cursorId \nNew in version 4.2. The ID of the cursor. $currentOp.cursor.createdDate \nNew in version 4.2. The date and time when the cursor was created. $currentOp.cursor.lastAccessDate \nNew in version 4.2. The date and time when the cursor was last used. If the cursor is actively in use (i.e. op is getmore and the type is not idleCursor), then lastAccessDate reports either the time the previous getmore ended or the time the cursor was created if this is the first getmore. $currentOp.cursor.nDocsReturned \nNew in version 4.2. The cumulative number of documents returned by the cursor. $currentOp.cursor.nBatchesReturned \nNew in version 4.2. The curmulative number of batches returned by the cursor. $currentOp.cursor.noCursorTimeout \nNew in version 4.2. The flag that indicates that the cursor will not timeout when idle; i.e. if the cursor has the noTimeout option set.  * If true, the cursor does not time out when idle.  * If false, the cursor will time out when idle. \nTIP \nSEE ALSO: cursor.addOption() $currentOp.cursor.tailable \nNew in version 4.2. The flag that indicates if the cursor is a tailable cursor for a capped collection. Tailable cursors remain open after the client exhausts the results in the initial cursor. \nTIP \nSEE ALSO:  * find  * cursor.tailable()  * cursor.addOption() $currentOp.cursor.awaitData \nNew in version 4.2. The flag that indicates whether the tailable cursor should temporarily block a getMore command on the cursor while waiting for new data rather than returning no data. For non-tailable cursors, the value is always false. \nTIP \nSEE ALSO:  * find  * cursor.tailable()  * cursor.addOption() $currentOp.cursor.originatingCommand \nNew in version 4.2. The originatingCommand field contains the full command object (e.g. find or aggregate) which originally created the cursor. \nNOTE Starting in version 4.2, MongoDB now returns originatingCommand field as a nested field in the new cursor field. In previous versions, the originatingCommand was a top-level field for the associated \"getmore\" document. $currentOp.cursor.planSummary \nNew in version 4.2. A string that specifies whether the cursor uses a collection scan (COLLSCAN) or an index scan (IXSCAN { ... }). The IXSCAN also includes the specification document of the index used. Not available when running with localOps: true on mongos or when reporting on idleCursors. $currentOp.cursor.operationUsingCursorId \nNew in version 4.2. The opid of the operation using the cursor. Only present if the cursor is not idle. $currentOp.planSummary \nA string that specifies whether the cursor uses a collection scan (COLLSCAN) or an index scan (IXSCAN { ... }). Not available when running with localOps: true on mongos. $currentOp.prepareReadConflicts \nThe number of times the current operation had to wait for a prepared transaction with a write to commit or abort. While waiting, the operation continues to hold any necessary locks and storage engine resources. New in version 4.2. $currentOp.writeConflicts \nThe number of times the current operation conflicted with another write operation on the same document. New in version 4.2. $currentOp.numYields \nnumYields is a counter that reports the number of times the operation has yielded to allow other operations to complete. Typically, operations yield when they need access to data that MongoDB has not yet fully read into memory. This allows other operations that have data in memory to complete quickly while MongoDB reads in data for the yielding operation. $currentOp.dataThroughputLastSecond \nAmount of data (in MiB) processed by the validate operation in the last second. Only available for a validate operation that is currently scanning documents. For example:\n\n  New in version 4.4. $currentOp.dataThroughputAverage \nThe average amount of data (in MiB) processed by the validate operation. Only available for a validate operation that is currently scanning documents. For example: \"msg\" : \"Validate: scanning documents Validate: scanning documents: 7258/24000 30%\",\"progress\" : {   \"done\" : 7258,   \"total\" : 24000},\"numYields\" : 0,\"dataThroughputLastSecond\" : 15.576952934265137,\"dataThroughputAverage\" : 15.375944137573242,  New in version 4.4. $currentOp.waitingForLatch \nThe waitingForLatch document is only available if the operation is waiting to acquire an internal locking primitive (a.k.a. a latch) or for an internal condition to be met. For example, \"waitingForLatch\" : {   \"timestamp\" : ISODate(\"2020-03-19T23:25:58.412Z\"),   \"captureName\" : \"FutureResolution\",   \"backtrace\" : [ ]   // Only if backtrace: true}, \nOutput Field\nDescription\ntimestamp\nThe date and time at which the operation started to wait.\ncaptureName\nThe internal name of the section where the operation is currently blocked.\nbacktrace\nThe callstack, if available. The field is only included if backtrace: true. New in version 4.2.2. $currentOp.locks \nThe locks document reports the type and mode of locks the operation currently holds. The possible lock types are as follows: Lock Type\nDescription\nParallelBatchWriterMode Represents a lock for parallel batch writer mode. In earlier versions, PBWM information was reported as part of the Global lock information. New in version 4.2. ReplicationStateTransition Represents lock taken for replica set member state transitions. New in version 4.2. Global\nRepresents global lock.\nDatabase\nRepresents database lock.\nCollection\nRepresents collection lock.\nMutex\nRepresents mutex.\nMetadata\nRepresents metadata lock.\noplog\nRepresents lock on the oplog. The possible modes are as follows: Lock Mode\nDescription\nR\nRepresents Shared (S) lock.\nW\nRepresents Exclusive (X) lock.\nr\nRepresents Intent Shared (IS) lock.\nw\nRepresents Intent Exclusive (IX) lock. $currentOp.lockStats \nFor each lock type and mode (see locks for descriptions of lock types and modes), returns the following information: $currentOp.lockStats.acquireCount \nNumber of times the operation acquired the lock in the specified mode. $currentOp.lockStats.acquireWaitCount \nNumber of times the operation had to wait for the acquireCount lock acquisitions because the locks were held in a conflicting mode. acquireWaitCount is less than or equal to acquireCount. $currentOp.lockStats.timeAcquiringMicros \nCumulative time in microseconds that the operation had to wait to acquire the locks. timeAcquiringMicros divided by acquireWaitCount gives an approximate average wait time for the particular lock mode. $currentOp.lockStats.deadlockCount \nNumber of times the operation encountered deadlocks while waiting for lock acquisitions. $currentOp.waitingForLock \nReturns a boolean value. waitingForLock is true if the operation is waiting for a lock and false if the operation has the required lock. $currentOp.msg \nThe msg provides a message that describes the status and progress of the operation. In the case of indexing or mapReduce operations, the field reports the completion percentage. $currentOp.progress \nReports on the progress of mapReduce or indexing operations. The progress fields corresponds to the completion percentage in the msg field. The progress specifies the following information: $currentOp.progress.done \nReports the number of work items completed. $currentOp.progress.total \nReports the total number of work items. $currentOp.killPending \nReturns true if the operation is currently flagged for termination. When the operation encounters its next safe termination point, the operation will terminate. $currentOp.waitingForFlowControl \nA boolean that indicates if the operation had to wait because of flow control. New in version 4.2. $currentOp.flowControlStats \nThe flow control statistics for this operation. New in version 4.2. $currentOp.flowControlStats.acquireCount \nThe number of times this operation acquired a ticket. New in version 4.2. $currentOp.flowControlStats.acquireWaitCount\n\n New in version 4.2. $currentOp.flowControlStats.timeAcquiringMicros \nThe total time this operation has waited to acquire a ticket. New in version 4.2. $currentOp.totalOperationTimeElapsed \nThe total time elapsed, in seconds, for the current resharding operation. The time is set to 0 when a new resharding operation starts. Only present if a resharding operation is taking place. New in version 5.0. $currentOp.remainingOperationTimeEstimated \nThe estimated time remaining in seconds for the current resharding operation. The time is set to -1 when a new resharding operation starts. Only present when a resharding operation is taking place. New in version 5.0. $currentOp.approxDocumentsToCopy \nThe approximate number of documents to be copied from the donor shards to the recipient shards during the resharding operation. This number is an estimate that is set at the beginning of the resharding operation and does not change after it has been set. The number is set to 0 when a new resharding operation starts. It is possible for $currentOp.documentsCopied and $currentOp.bytesCopied to end up exceeding $currentOp.approxDocumentsToCopy and $currentOp.approxBytesToCopy, respectively, if the post-resharding data distribution is not perfectly uniform. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.documentsCopied \nThe number of documents copied form donor shards to recipient shards during the resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.approxBytesToCopy \nThe approximate number of bytes to be copied from the donor shards to the recipient shards during the resharding operation. This number is an estimate that is set at the beginning of the resharding operation and does not change after it has been set. The number is set to 0 when a new resharding operation starts. It is possible for $currentOp.documentsCopied and $currentOp.bytesCopied to end up exceeding $currentOp.approxDocumentsToCopy and $currentOp.approxBytesToCopy, respectively, if the post-resharding data distribution is not perfectly uniform. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.bytesCopied \nThe number of bytes copied from donor shards to recipient shards during the resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.totalCopyTimeElapsed \nThe total elapsed time, in seconds, for ongoing data copy tasks from donor shards to recipient shards for the current resharding operation. The time is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.oplogEntriesFetched \nThe number of entries fetched from the oplog for the current resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.oplogEntriesApplied \nThe number of entries applied to the oplog for the current resharding operation. The number is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.totalApplyTimeElapsed \nThe total elapsed time, in seconds, for the apply step of the current resharding operation. In the apply step, recipient shards apply oplog entries to modify their data based on new incoming writes from donor shards. The time is set to 0 when a new resharding operation starts. Only present on a recipient shard when a resharding operation is taking place. New in version 5.0. $currentOp.countWritesDuringCriticalSection \nThe number of writes perfomed in the critical section for the current resharding operation. The critical section prevents new incoming writes to the collection currently being resharded. The number is set to 0 when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place.\n\n $currentOp.totalCriticalSectionTimeElapsed \nThe total elapsed time, in seconds, for the critical section of the current resharding operation. The critical section prevents new incoming writes to the collection currently being resharded. The time is set to 0 when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place. New in version 5.0. $currentOp.donorState \nThe current state of a donor shard for the resharding operation. The state is set to unused when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place. State\nDescription\nunused\nThe resharding operation is about to start or recovering from a primary failover.\npreparing-to-donate\nThe donor shard is preparing to donate data to the recipient shards.\ndonating-initial-data\nThe donor shard is donating data to the recipient shards.\ndonating-oplog-entries\nThe donor shard is donating oplog entries to the recipient shards.\npreparing-to-block-writes\nThe donor shard is about to prevent new incoming write operations to the collection that is being resharded.\nerror\nAn error occurred during the resharding operation.\nblocking-writes\nThe donor shard is preventing new incoming write operations and the donor shard has notified all recipient shards that new incoming writes are prevented.\ndone\nThe donor shard has dropped the old sharded collection and the resharding operation is complete. New in version 5.0. $currentOp.recipientState \nThe current state of a recipient shard for a resharding operation. The state is set to unused when a new resharding operation starts. Only present on a donor shard when a resharding operation is taking place. State\nDescription\nunused\nThe resharding operation is about to start or recovering from a primary failover.\nawaiting-fetch-timestamp\nThe recipient shard is waiting for the donor shards to be prepared to donate their data.\ncreating-collection\nThe recipient shard is creating the new sharded collection.\ncloning\nThe recipient shard is receiving data from the donor shards.\napplying\nThe recipient shard is applying oplog entries to modify its copy of the data based on the new incoming writes from donor shards.\nerror\nAn error occurred during the resharding operation.\nstrict-consistency\nThe recipient shard has all data changes stored in a temporary collection.\ndone\nThe resharding operation is complete. New in version 5.0. $currentOp.coordinatorState \nThe state of the resharding coordinator for the current resharding operation. The resharding coordinator is an operation that runs on the config server primary. The state is set to unused when a new resharding operation starts. Only present on the coordinating config server. State\nDescription\nunused\nThe resharding operation is about to start or recovering from a primary failover.\ninitializing\nThe resharding coordinator has inserted the coordinator document into config.reshardingOperations and has added the reshardingFields to the config.collections entry for the original collection.\npreparing-to-donate The resharding coordinator  * has created a config.collections entry for the temporary resharding collection.  * has inserted entries into config.chunks for ranges based on the new shard key.  * has inserted entries into config.tags for any zones associated with the new shard key. The coordinator informs participant shards to begin the resharding operation. The coordinator then waits until all donor shards have picked a minFetchTimestamp and are ready to donate. cloning\nThe resharding coordinator informs donor shards to donate data to recipient shards. The coordinator waits for all recipients to finish cloning the data from the donor.\napplying\nThe resharding coordinator informs recipient shards to modify their copies of data based on new incoming writes from donor shards. The coordinator waits for all recipients to finish applying oplog entries.\nblocking-writes\nThe resharding coordinator informs donor shards to prevent new incoming write operations to the collection being resharded. The coordinator then waits for all recipients to have all data changes.\naborting\nAn unrecoverable error occurred during the resharding operation or the abortReshardCollection command (or the sh.abortReshardCollection() method) was run.\ncommitting\nThe resharding coordinator removes the config.collections entry for the temporary resharding collection. The coordinator then adds the recipientFields to the source collection's entry. New in version 5.0.\n\n \nThe current state of a resharding operation. Only present if a resharding operation is taking place. Once the operation has completed, the operation is removed from currentOp output. State\nDescription\nactively running\nThe resharding operation is actively running.\nsuccess\nThe resharding operation has succeeded.\nfailure\nThe resharding operation has failed.\ncanceled\nThe resharding operation was canceled. New in version 5.0. ←  $count (aggregation)$densify (aggregation) → On this page  * Definition\n * Constraints\n * Example\n * Output Fields Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/bsonSize/": " Docs Home → MongoDB Manual \n$BSONSIZE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$bsonSize \nNew in version 4.4. Returns the size in bytes of a given document (i.e. bsontype Object) when encoded as BSON. You can use $bsonSize as an alternative to the bsonSize() method. $bsonSize has the following syntax: { $bsonSize: <object> }  The argument can be any valid expression as long as it resolves to either an object or null. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the argument is an object, the expression returns the size of the object in bytes when the object is encoded as BSON. If the argument is null, the expression returns null. If the argument resolves to a data type other than an object or null, $bsonSize errors. \nEXAMPLES  RETURN SIZES OF DOCUMENTS \nIn mongosh, create a sample collection named employees with the following documents:  db.employees.insertMany([   {     \"_id\": 1,     \"name\": \"Alice\", \"email\": \"alice@company.com\", \"position\": \"Software Developer\",     \"current_task\": {       \"project_id\": 1,       \"project_name\": \"Aggregation Improvements\",       \"project_duration\": 5,       \"hours\": 20     }   },   {     \"_id\": 2,     \"name\": \"Bob\", \"email\": \"bob@company.com\", \"position\": \"Sales\",     \"current_task\": {       \"project_id\": 2,       \"project_name\": \"Write Blog Posts\",       \"project_duration\": 2,       \"hours\": 10,       \"notes\": \"Progress is slow. Waiting for feedback.\"     }   },   {     \"_id\": 3,     \"name\": \"Charlie\", \"email\": \"charlie@company.com\", \"position\": \"HR (On Leave)\",     \"current_task\": null   },   {     \"_id\": 4,     \"name\": \"Dianne\", \"email\": \"diane@company.com\", \"position\": \"Web Designer\",     \"current_task\": {       \"project_id\": 3,       \"project_name\": \"Update Home Page\",       \"notes\": \"Need to scope this project.\"     }   }]);  The following aggregation projects:      * The name field  * The object_size field, which uses $bsonSize to return the size of the document in bytes. The $$ROOT variable references the document currently being processed by the pipeline. To learn more about variables in the aggregation pipeline, see Variables in Aggregation Expressions. db.employees.aggregate([  {    \"$project\": {      \"name\": 1,      \"object_size\": { $bsonSize: \"$$ROOT\" }    }  }])  The operation returns the following result: { \"_id\" : 1, \"name\" : \"Alice\", \"object_size\" : 222 }{ \"_id\" : 2, \"name\" : \"Bob\", \"object_size\" : 248 }{ \"_id\" : 3, \"name\" : \"Charlie\", \"object_size\" : 112 }{ \"_id\" : 4, \"name\" : \"Dianne\", \"object_size\" : 207 }  \nRETURN COMBINED SIZE OF ALL DOCUMENTS IN A COLLECTION \nThe following pipeline returns the combined size of all of the documents in the employees collection: db.employees.aggregate([  {    \"$group\": {      \"_id\": null,      \"combined_object_size\": { $sum: { $bsonSize: \"$$ROOT\" } }    }  }]) \n\n The operation uses the $sum operator to calculate the combined $bsonSize of each document in the collection. The $$ROOT variable references the document currently being processed by the pipeline. To learn more about variables in the aggregation pipeline, see Variables in Aggregation Expressions. The operation returns the following result: { \"_id\" : null, \"combined_object_size\" : 789 } \nTIP \nSEE ALSO:  * $group  * $sum \nRETURN DOCUMENT WITH LARGEST SPECIFIED FIELD \nThe following pipeline returns the document with the largest current_task field in bytes: db.employees.aggregate([   // First Stage   { $project: { name: \"$name\", task_object_size: { $bsonSize: \"$current_task\" } }  },   // Second Stage   { $sort: { \"task_object_size\" : -1 } },   // Third Stage   { $limit: 1 }]) \nFirst Stage The first stage of the pipeline projects:  * The name field  * The task_object_size field, which uses $bsonSize to return the size of the document's current_task field in bytes. This stage outputs the following documents to the next stage: { \"_id\" : 1, \"name\" : \"Alice\", \"task_object_size\" : 109 }{ \"_id\" : 2, \"name\" : \"Bob\", \"task_object_size\" : 152 }{ \"_id\" : 3, \"name\" : \"Charlie\", \"task_object_size\" : null }{ \"_id\" : 4, \"name\" : \"Dianne\", \"task_object_size\" : 99 } Second Stage The second stage sorts the documents by task_object_size in descending order. This stage outputs the following documents to the next stage: { \"_id\" : 2, \"name\" : \"Bob\", \"task_object_size\" : 152 }{ \"_id\" : 1, \"name\" : \"Alice\", \"task_object_size\" : 109 }{ \"_id\" : 4, \"name\" : \"Dianne\", \"task_object_size\" : 99 }{ \"_id\" : 3, \"name\" : \"Charlie\", \"task_object_size\" : null } Third Stage The third stage limits the output documents to only return the document appearing first in the sort order: { \"_id\" : 2, \"name\" : \"Bob\", \"task_object_size\" : 152 } \nTIP \nSEE ALSO:  * $project  * $sort  * $limit  * $strLenBytes  * $binarySize ←  $bottomN (aggregation accumulator)$ceil (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/setIntersection/": " Docs Home → MongoDB Manual \n$SETINTERSECTION (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$setIntersection \nTakes two or more arrays and returns an array that contains the elements that appear in every input array. $setIntersection has the following syntax: { $setIntersection: [ <array1>, <array2>, ... ] }  The arguments can be any valid expression as long as they each resolve to an array. For more information on expressions, see Expressions. \nBEHAVIOR \n$setIntersection performs set operation on arrays, treating arrays as sets. If an array contains duplicate entries, $setIntersection ignores the duplicate entries. $setIntersection ignores the order of the elements. $setIntersection filters out duplicates in its result to output an array that contain only unique entries. The order of the elements in the output array is unspecified. If no intersections are found (i.e. the input arrays contain no common elements), $setIntersection returns an empty array. If a set contains a nested array element, $setIntersection does not descend into the nested array but evaluates the array at top-level. Example\nResult { $setIntersection: [ [ \"a\", \"b\", \"a\" ], [ \"b\", \"a\" ] ] } [ \"b\", \"a\" ] { $setIntersection: [ [ \"a\", \"b\" ], [ [ \"a\", \"b\" ] ] ] } [ ] \nEXAMPLE \nConsider an experiments collection with the following documents: { \"_id\" : 1, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"red\", \"blue\" ] }{ \"_id\" : 2, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"blue\", \"red\", \"blue\" ] }{ \"_id\" : 3, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"red\", \"blue\", \"green\" ] }{ \"_id\" : 4, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"green\", \"red\" ] }{ \"_id\" : 5, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ ] }{ \"_id\" : 6, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ [ \"red\" ], [ \"blue\" ] ] }{ \"_id\" : 7, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ [ \"red\", \"blue\" ] ] }{ \"_id\" : 8, \"A\" : [ ], \"B\" : [ ] }{ \"_id\" : 9, \"A\" : [ ], \"B\" : [ \"red\" ] }  The following operation uses the $setIntersection operator to return an array of elements common to both the A array and the B array: db.experiments.aggregate(   [     { $project: { A: 1, B: 1, commonToBoth: { $setIntersection: [ \"$A\", \"$B\" ] }, _id: 0 } }   ])  The operation returns the following results:\n\n \n←  $setField (aggregation)$setIsSubset (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/isoDayOfWeek/": " Docs Home → MongoDB Manual \n$ISODAYOFWEEK (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$isoDayOfWeek \nReturns the weekday number in ISO 8601 format, ranging from 1 (for Monday) to 7 (for Sunday). The $isoDayOfWeek expression has the following operator expression syntax: { $isoDayOfWeek: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $isoDayOfWeek: new Date(\"2016-01-01\") } 5 { $isoDayOfWeek: { date: new Date(\"Jan 7, 2003\") } } 2 { $isoDayOfWeek: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 7 { $isoDayOfWeek: ISODate(\"1998-11-07T00:00:00Z\") } 6 { $isoDayOfWeek: {    date: ISODate(\"1998-11-07T00:00:00Z\"),    timezone: \"-0400\"} } 5 { $isoDayOfWeek: \"March 28, 1976\" } error { $isoDayOfWeek: Date(\"2016-01-01\") } error { $isoDayOfWeek: \"2009-04-09\" } error \nNOTE \n$ISODAYOFWEEK CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nA collection called birthdays contains the following documents: { \"_id\" : 1, \"name\" : \"Betty\", \"birthday\" : ISODate(\"1993-09-21T00:00:00Z\") }{ \"_id\" : 2, \"name\" : \"Veronica\", \"birthday\" : ISODate(\"1981-11-07T00:00:00Z\") }  The following operation returns the weekday number for each birthday field. db.dates.aggregate( [  {    $project: {      _id: 0,      name: \"$name\",      dayOfWeek: { $isoDayOfWeek: \"$birthday\" }    }  }] )  The operation returns the following results: { \"name\" : \"Betty\", \"dayOfWeek\" : 2 }{ \"name\" : \"Veronica\", \"dayOfWeek\" : 6 }  \nTIP \nSEE ALSO:  * $isoWeekYear (aggregation)  * $isoWeek (aggregation) ←  $isNumber (aggregation)$isoWeek (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/substr/": " Docs Home → MongoDB Manual \n$SUBSTR (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$substr \nDeprecated since version 3.4: $substr is now an alias for $substrBytes. Returns a substring of a string, starting at a specified index position and including the specified number of characters. The index is zero-based. $substr has the following syntax: { $substr: [ <string>, <start>, <length> ] }  The arguments can be any valid expression as long as the first argument resolves to a string, and the second and third arguments resolve to integers. For more information on expressions, see Expressions. \nBEHAVIOR \nIf <start> is a negative number, $substr returns an empty string \"\". If <length> is a negative number, $substr returns a substring that starts at the specified index and includes the rest of the string. $substr only has a well-defined behavior for strings of ASCII characters. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \"product 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", quarter: \"13Q4\", \"description\" : \"product 2\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $substr operator to separate the quarter value into a yearSubstring and a quarterSubstring: db.inventory.aggregate(   [     {       $project:          {            item: 1,            yearSubstring: { $substr: [ \"$quarter\", 0, 2 ] },            quarterSubtring: { $substr: [ \"$quarter\", 2, -1 ] }          }      }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"yearSubstring\" : \"13\", \"quarterSubtring\" : \"Q1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", \"yearSubstring\" : \"13\", \"quarterSubtring\" : \"Q4\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"yearSubstring\" : \"14\", \"quarterSubtring\" : \"Q2\" } \n←  $strLenCP (aggregation)$substrBytes (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/add/": " Docs Home → MongoDB Manual \n$ADD (AGGREGATION) \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$add \nAdds numbers together or adds numbers and a date. If one of the arguments is a date, $add treats the other arguments as milliseconds to add to the date. The $add expression has the following syntax: { $add: [ <expression1>, <expression2>, ... ] }  The arguments can be any valid expression as long as they resolve to either all numbers or to numbers and a date. For more information on expressions, see Expressions. \nEXAMPLES \nThe following examples use a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"fee\" : 2, date: ISODate(\"2014-03-01T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"fee\" : 1, date: ISODate(\"2014-03-01T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"price\" : 5,  \"fee\" : 0, date: ISODate(\"2014-03-15T09:00:00Z\") }  \nADD NUMBERS \nThe following aggregation uses the $add expression in the $project pipeline to calculate the total cost: db.sales.aggregate(   [     { $project: { item: 1, total: { $add: [ \"$price\", \"$fee\" ] } } }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc\", \"total\" : 12 }{ \"_id\" : 2, \"item\" : \"jkl\", \"total\" : 21 }{ \"_id\" : 3, \"item\" : \"xyz\", \"total\" : 5 }  \nPERFORM ADDITION ON A DATE \nThe following aggregation uses the $add expression to compute the billing_date by adding 3*24*60*60000 milliseconds (i.e. 3 days) to the date field : db.sales.aggregate(   [     { $project: { item: 1, billing_date: { $add: [ \"$date\", 3*24*60*60000 ] } } }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc\", \"billing_date\" : ISODate(\"2014-03-04T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"billing_date\" : ISODate(\"2014-03-04T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"billing_date\" : ISODate(\"2014-03-18T09:00:00Z\") } \n←  $acosh (aggregation)$addToSet (aggregation) → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/trunc/": " Docs Home → MongoDB Manual \n$TRUNC (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$trunc \nChanged in version 4.2.. $trunc truncates a number to a whole integer or to a specified decimal place. MongoDB 4.2 adds the following syntax for $trunc: { $trunc : [ <number>, <place> ] } \nField\nType\nDescription\n<number>\nnumber Can be any valid expression that resolves to a number. Specifically, the expression must resolve to an integer, double, decimal, or long. $trunc returns an error if the expression resolves to a non-numeric data type. <place>\ninteger Optional Can be any valid expression that resolves to an integer between -20 and 100, exclusive. e.g. -20 < place < 100. Defaults to 0 if unspecified.      * If <place> resolves to a positive integer, $trunc truncates to <place> decimal places.\n   \n   For example, $trunc : [1234.5678, 2] truncates to two decimal places and returns 1234.56.  * If <place> resolves to a negative integer, $trunc replaces <place> digits left of the decimal with 0.\n   \n   For example, $trunc : [1234.5678, -2] replaces to two digits left of the decimal with 0 and returns 1200.\n   \n   If the absolute value of <place> exceeds the number of digits to the left of the decimal, $trunc returns 0.\n   \n   For example, $trunc : [ 1234.5678, -5] specifies the fifth digit left of the decimal. This exceeds the number of digits left of the decimal and returns 0.  * If <place> resolves to 0, $trunc truncates all digits to the right of the decimal and returns the whole integer value.\n   \n   For example, $trunc : [1234.5678, 0] returns 1234 Prior to MongoDB 4.2, $trunc truncated the input value to the whole integer. MongoDB 4.2 continues supporting the pre-4.2 syntax and behavior: { $trunc: <number> }  The <number> expression can be any valid expression as long as it resolves to a number. For more information on expressions, see Expressions. \nBEHAVIOR \n$trunc does not round the truncated data. To round input values to a specified place, use the $round expression. \nRETURNED DATA TYPE \nIf truncating to a specific decimal place, the data type returned by $trunc matches the data type of the input expression or value. If truncating to a whole integer value, $trunc returns an integer. \nNULL, NAN, AND +/- INFINITY \n * If the argument resolves to a value of null or refers to a field that is missing, $trunc returns null.  * If the argument resolves to NaN, $trunc returns NaN.  * If the argument resolves to negative or positive infinity, $trunc returns negative or positive infinity respectively. Example\nResults\n{ $trunc: [ NaN, 1] }\nNaN\n{ $trunc: [ null, 1] }\nnull\n{ $trunc : [ Infinity, 1 ] }\nInfinity\n{ $trunc : [ -Infinity, 1 ] }\n-Infinity \nEXAMPLE \nCreate a collection named samples with the following documents: db.samples.insertMany(   [      { _id: 1, value: 19.25 },      { _id: 2, value: 28.73 },      { _id: 3, value: 34.32 },      { _id: 4, value: -45.34 }   ]) \n\n  * The following aggregation returns value truncated to the first place:\n   \n   db.samples.aggregate([   { $project: { truncatedValue: { $trunc: [ \"$value\", -1 ] } } }])\n   \n   \n   \n   The operation returns the following results:\n   \n   { \"_id\" : 1, \"truncatedValue\" : 10 }{ \"_id\" : 2, \"truncatedValue\" : 20 }{ \"_id\" : 3, \"truncatedValue\" : 30 }{ \"_id\" : 4, \"truncatedValue\" : -40 }\n   \n     * The following aggregation returns``value`` truncated to the whole integer:\n   \n   db.samples.aggregate([   { $project: { truncatedValue: { $trunc: [ \"$value\", 0 ] } } }])\n   \n   \n   \n   The operation returns the following results:\n   \n   { \"_id\" : 1, \"truncatedValue\" : 19 }{ \"_id\" : 2, \"truncatedValue\" : 28 }{ \"_id\" : 3, \"truncatedValue\" : 34 }{ \"_id\" : 4, \"truncatedValue\" : -45 }\n   \n    ←  $trim (aggregation)$type (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/mod/": " Docs Home → MongoDB Manual \n$MOD (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$mod \nDivides one number by another and returns the remainder. The $mod expression has the following syntax: { $mod: [ <expression1>, <expression2> ] }  The first argument is the dividend, and the second argument is the divisor; i.e. first argument is divided by the second argument. The arguments can be any valid expression as long as they resolve to numbers. For more information on expressions, see Expressions. \nEXAMPLE \nConsider a planning collection with the following documents: { \"_id\" : 1, \"project\" : \"A\", \"hours\" : 80, \"tasks\" : 7 }{ \"_id\" : 2, \"project\" : \"B\", \"hours\" : 40, \"tasks\" : 4 }  The following aggregation uses the $mod expression to return the remainder of the hours field divided by the tasks field: db.planning.aggregate(   [     { $project: { remainder: { $mod: [ \"$hours\", \"$tasks\" ] } } }   ])  The operation returns the following results: { \"_id\" : 1, \"remainder\" : 3 }{ \"_id\" : 2, \"remainder\" : 0 } \n←  $minute (aggregation)$month (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/arrayToObject/": " Docs Home → MongoDB Manual \n$ARRAYTOOBJECT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$arrayToObject \nConverts an array into a single document; the array must be either:      * An array of two-element arrays where the first element is the field name, and the second element is the field value:\n   \n   [ [ [ \"item\", \"abc123\" ], [ \"qty\", 25 ] ] ]\n   \n    - OR -  * An array of documents that contains two fields, k and v where:\n   \n   * The k field contains the field name.\n   \n   * The v field contains the value of the field.\n   \n   [ [ { \"k\": \"item\", \"v\": \"abc123\" }, { \"k\": \"qty\", \"v\": 25 } ] ]\n   \n    $arrayToObject has the following syntax: { $arrayToObject: <expression> }  The <expression> can be any valid expression that resolves to an array of two-element arrays or array of documents that contains \"k\" and \"v\" fields. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the name of a field repeats in the array,  * Starting in 4.0.5, $arrayToObject uses the last value for that field. For 4.0.0-4.0.4, the value used depends on the driver.  * Starting in 3.6.10, $arrayToObject uses the last value for that field. For 3.6.0-3.6.9, the value used depends on the driver.  * Starting in 3.4.19, $arrayToObject uses the last value for that field. For 3.4.0-3.4.19, the value uses depends on the driver. Example\nResults { $arrayToObject: [   [ { \"k\": \"item\", \"v\": \"abc123\" },     { \"k\": \"qty\", \"v\": \"$qty\" } ]] } Given a document with the field qty and value 25, $arrayToObject returns: { \"item\" : \"abc123\", \"qty\" : 25 } { $arrayToObject: { $literal:  [   [ \"item\", \"abc123\" ], [ \"qty\", 25 ]] } } { \"item\" : \"abc123\", \"qty\" : 25 } { $arrayToObject: { $literal: [   { \"k\": \"item\", \"v\": \"123abc\" },   { \"k\": \"item\", \"v\": \"abc123\" }] } } { \"item\" : \"abc123\" } Starting in versions 4.0.5+ (3.6.10+ and 3.4.19+), if the name of a field repeats in the array, $arrayToObject uses the last value for that field. \nEXAMPLES  $ARRAYTOOBJECT EXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\",  dimensions: [ { \"k\": \"l\", \"v\": 25} , { \"k\": \"w\", \"v\": 10 }, { \"k\": \"uom\", \"v\": \"cm\" } ] }{ \"_id\" : 2, \"item\" : \"ABC2\",  dimensions: [ [ \"l\", 50 ], [ \"w\",  25 ], [ \"uom\", \"cm\" ] ] }{ \"_id\" : 3, \"item\" : \"ABC3\",  dimensions: [ [ \"l\", 25 ], [ \"l\",  \"cm\" ], [ \"l\", 50 ] ] }  The following aggregation pipeline operation use the $arrayToObject to return the dimensions field as a document: db.inventory.aggregate(   [      {         $project: {            item: 1,            dimensions: { $arrayToObject: \"$dimensions\" }         }      }   ])  The operation returns the following:\n\n  Starting in versions 4.0.5+ (3.6.10+ and 3.4.19+), if the name of a field repeats in the array, $arrayToObject uses the last value for that field. \n$OBJECTTOARRAY + $ARRAYTOOBJECT EXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", instock: { warehouse1: 2500, warehouse2: 500 } }{ \"_id\" : 2, \"item\" : \"ABC2\", instock: { warehouse2: 500, warehouse3: 200} }  The following aggregation pipeline operation calculates the total in stock for each item and adds to the instock document: db.inventory.aggregate( [   { $addFields: { instock: { $objectToArray: \"$instock\" } } },   { $addFields: { instock: { $concatArrays: [ \"$instock\", [ { \"k\": \"total\", \"v\": { $sum: \"$instock.v\" } } ] ] } } } ,   { $addFields: { instock: { $arrayToObject: \"$instock\" } } }] )  The operation returns the following: { \"_id\" : 1, \"item\" : \"ABC1\", \"instock\" : { \"warehouse1\" : 2500, \"warehouse2\" : 500, \"total\" : 3000 } }{ \"_id\" : 2, \"item\" : \"ABC2\", \"instock\" : { \"warehouse2\" : 500, \"warehouse3\" : 200, \"total\" : 700 } }  \nTIP \nSEE ALSO: $objectToArray ←  $arrayElemAt (aggregation)$asin (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/first-array-element/": " Docs Home → MongoDB Manual \n$FIRST (ARRAY OPERATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example\n * See Also \nDEFINITION \n$first \nNew in version 4.4. Returns the first element in an array. \nNOTE \nDISAMBIGUATION This page describes the $first array operator. For the $first aggregation accumulator, see $first (aggregation accumulator). \nTIP \nSEE ALSO: $last \nSYNTAX \n$first has the following syntax: { $first: <expression> }  The <expression> can be any valid expression as long as it resolves to an array, null or missing. For more information on expressions, see Expressions. The $first operator is an alias for the following $arrayElemAt expression: { $arrayElemAt: [ <array expression>, 0 ] }  \nBEHAVIOR  VALID OPERANDS \nValid operand for $first must resolve to an array, null, or missing      * If the operand resolves to a non-empty array, $first returns the first element in the array:  * If the operand resolves to an empty array [], $first does not return a value.  * If the operand is null or missing, $first returns null. For example, create a test collection example1 with the following documents: db.example1.insertMany([     { \"_id\" : 1, \"x\" : [ 1, 2, 3 ] },      // Non-empty array     { \"_id\" : 2, \"x\" : [ [ ] ] },          // Non-empty array     { \"_id\" : 3, \"x\" : [ null ] },         // Non-empty array     { \"_id\" : 4, \"x\" : [ ] },              // Empty array     { \"_id\" : 5, \"x\" : null },             // Is null     { \"_id\" : 6 }                          // Is Missing])  Then, the following adds a new field firstElem whose value is derived from applying the $first operator to the x field: db.example1.aggregate([   { $addFields: { firstElem: { $first: \"$x\" } } }])  The operator returns the following documents: { \"_id\" : 1, \"x\" : [ 1, 2, 3 ], \"firstElem\" : 1 }{ \"_id\" : 2, \"x\" : [ [ ] ], \"firstElem\" : [ ] }{ \"_id\" : 3, \"x\" : [ null ], \"firstElem\" : null }{ \"_id\" : 4, \"x\" : [ ] }                          // No output{ \"_id\" : 5, \"x\" : null, \"firstElem\" : null }{ \"_id\" : 6, \"firstElem\" : null } \nINVALID OPERANDS \nIf the operand does not resolve to an array, null, or missing, the aggregation operation as a whole errors. For example, create a test collection example2 with the following documents: db.example2.insertMany([   { \"_id\" : 1, \"x\" : [ 1, 2, 3 ] },   { \"_id\" : 2, \"x\" : 2 },             // x is not an array/null or missing])  Then, the following aggregation operation returns an error because of the { \"_id\" : 2, \"x\" : 2 } document: db.example2.aggregate( { $addFields: { firstElem: { $first: \"$x\" } } } )  That is, the operation returns the following: 2020-01-20T18:31:13.431-05:00 E  QUERY    [js] uncaught exception: Error: command failed: {   \"ok\" : 0,   \"errmsg\" : \"$first's argument must be an array, but is double\",   \"code\" : 28689,   \"codeName\" : \"Location28689\"} : aggregate failed : \nEXAMPLE\n\n db.runninglog.insertMany([   { \"_id\" : 1, \"team\" : \"Anteater\", log: [ { run: 1, distance: 8 }, { run2: 2, distance: 7.5 }, { run: 3, distance: 9.2 } ] },   { \"_id\" : 2, \"team\" : \"Bears\", log: [ { run: 1, distance: 18 }, { run2: 2, distance: 17 }, { run: 3, distance: 16 } ] },   { \"_id\" : 3, \"team\" : \"Cobras\", log: [ { run: 1, distance: 2 } ] }])  The following aggregation uses the $first and $last operator on the log array to retrieve the information for the first run and the last run: db.runninglog.aggregate([   { $addFields: { firstrun: { $first: \"$log\" }, lastrun: { $last: \"$log\" } } }])  The operation returns the following results: { \"_id\" : 1, \"team\" : \"Anteater\", \"log\" : [ { \"run\" : 1, \"distance\" : 8 }, { \"run2\" : 2, \"distance\" : 7.5 }, { \"run\" : 3, \"distance\" : 9.2 } ],      \"firstrun\" : { \"run\" : 1, \"distance\" : 8 }, \"lastrun\" : { \"run\" : 3, \"distance\" : 9.2 } }{ \"_id\" : 2, \"team\" : \"Bears\", \"log\" : [ { \"run\" : 1, \"distance\" : 18 }, { \"run2\" : 2, \"distance\" : 17 }, { \"run\" : 3, \"distance\" : 16 } ],      \"firstrun\" : { \"run\" : 1, \"distance\" : 18 }, \"lastrun\" : { \"run\" : 3, \"distance\" : 16 } }{ \"_id\" : 3, \"team\" : \"Cobras\", \"log\" : [ { \"run\" : 1, \"distance\" : 2 } ],      \"firstrun\" : { \"run\" : 1, \"distance\" : 2 }, \"lastrun\" : { \"run\" : 1, \"distance\" : 2 } }  To calculate the change between the first and the last distances, the following operation uses $cond and $size operators to calculate the difference (i.e. $subtract) the two distances if there are two or more elements in the log array: db.runninglog.aggregate([  { $addFields: { firstrun: { $first: \"$log\" }, lastrun: { $last: \"$log\" } } },  { $project: { team: 1, progress:      {        $cond: {           if: { $gt: [ { $size:\"$log\" }, 1 ] } ,           then: { $subtract: [ \"$lastrun.distance\", \"$firstrun.distance\"] },           else: \"Not enough data.\" }      }\n  }}])  The operation returns the following documents: { \"_id\" : 1, \"team\" : \"Anteater\", \"progress\" : 1.1999999999999993 }{ \"_id\" : 2, \"team\" : \"Bears\", \"progress\" : -2 }{ \"_id\" : 3, \"team\" : \"Cobras\", \"progress\" : \"Not enough data.\" } By default, mongosh uses the 64-bit floating-point double for numbers. To improve precision, you can use Decimal128 instead. \nSEE ALSO \n * $last  * $arrayElemAt  * $slice  * Array Expression Operators ←  $firstN (aggregation accumulator)$firstN (array operator) → On this page  * Definition\n * Syntax\n * Behavior\n * Example\n * See Also Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/documentNumber/": " Docs Home → MongoDB Manual \n$DOCUMENTNUMBER (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \nNew in version 5.0. $documentNumber \nReturns the position of a document (known as the document number) in the $setWindowFields stage partition. The $setWindowFields stage sortBy field determines the document number. For more information on how MongoDB compares fields with different types, see BSON comparison order. $documentNumber returns a unique number for each document in a partition, even if multiple documents have identical sortBy field values in the partition. $documentNumber is only available in the $setWindowFields stage. $documentNumber syntax: { $documentNumber: { } } $documentNumber does not accept any parameters. \nBEHAVIOR \n$documentNumber includes documents that have a sortBy field that is null or missing. $documentNumber, $rank, and $denseRank return the position of the documents based on the sortBy field values. $documentNumber differs from $rank and $denseRank in how documents with identical sortBy field values in a partition are treated:      * $rank and $denseRank return the same position (known as the rank) for those documents.  * $documentNumber returns a unique position (known as the document number) for those documents. See the example in Document Number for Duplicate, Null, and Missing Values. \nEXAMPLES  DOCUMENT NUMBER FOR EACH STATE \nCreate a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  This example uses $documentNumber in the $setWindowFields stage to output the cake sales document number for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { quantity: -1 },         output: {            documentNumberForState: {               $documentNumber: {}            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { quantity: -1 } sorts the documents in each partition by quantity in descending order (-1), so the highest quantity is first.  * output sets the document number in a new field called documentNumberForState shown in the following results. documentNumberForState is unique within each state partition.\n\n \nDOCUMENT NUMBER FOR DUPLICATE, NULL, AND MISSING VALUES \nCreate a cakeSalesWithDuplicates collection where:  * Cake sales are placed in the state of California (CA) and Washington (WA).  * Documents 6 to 8 have the same quantity and state as document 5.  * Document 9 has the same quantity and state as document 4.  * Document 10 has a null quantity.  * Document 11 is missing the quantity. db.cakeSalesWithDuplicates.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 },   { _id: 6, type: \"strawberry\", orderDate: new Date(\"2020-01-08T06:12:03Z\"),     state: \"WA\", price: 41, quantity: 134 },   { _id: 7, type: \"strawberry\", orderDate: new Date(\"2020-01-01T06:12:03Z\"),     state: \"WA\", price: 34, quantity: 134 },   { _id: 8, type: \"strawberry\", orderDate: new Date(\"2020-01-02T06:12:03Z\"),     state: \"WA\", price: 40, quantity: 134 },   { _id: 9, type: \"strawberry\", orderDate: new Date(\"2020-05-11T16:09:01Z\"),     state: \"CA\", price: 39, quantity: 162 },   { _id: 10, type: \"strawberry\", orderDate: new Date(\"2020-05-11T16:09:01Z\"),     state: \"CA\", price: 39, quantity: null },   { _id: 11, type: \"strawberry\", orderDate: new Date(\"2020-05-11T16:09:01Z\"),     state: \"CA\", price: 39 }] )  This example uses $documentNumber in the $setWindowFields stage to output the cakeSalesWithDuplicates document number for each state: db.cakeSalesWithDuplicates.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { quantity: -1 },         output: {            documentNumberForState: {               $documentNumber: {}            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { quantity: -1 } sorts the documents in each partition by quantity in descending order (-1), so the highest quantity is first.  * output sets the document number in a new field called documentNumberForState shown in the following results. documentNumberForState is unique within each state partition, and there are documentNumberForState values for documents with null quantity and missing quantity values.\n\n ←  $divide (aggregation)$eq (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/convert/": " Docs Home → MongoDB Manual \n$CONVERT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$convert \nConverts a value to a specified type. $convert has the following syntax: {   $convert:      {         input: <expression>,         to: <type expression>,         onError: <expression>,  // Optional.         onNull: <expression>    // Optional.      }}  The $convert takes a document with the following fields: Field\nDescription\ninput\nThe argument can be any valid expression. For more information on expressions, see Expressions.\nto The argument can be any valid expression that resolves to one of the following numeric or string identifiers: String Identifier\nNumeric Identifier\nNotes\n\"double\"\n1\nFor more information on the conversion to double, see Converting to a Double.\n\"string\"\n2\nFor more information on the conversion to string, see Converting to a String.\n\"objectId\"\n7\nFor more information on the conversion to objectId, see Converting to an ObjectId.\n\"bool\"\n8\nFor more information on the conversion to boolean, see Converting to a Boolean.\n\"date\"\n9\nFor more information on the conversion to date, see Converting to a Date.\n\"int\"\n16\nFor more information on the conversion to integer, see Converting to an Integer.\n\"long\"\n18\nFor more information on the conversion to long, see Converting to a Long.\n\"decimal\"\n19\nFor more information on the conversion to decimal, see Converting to a Decimal. onError Optional. The value to return on encountering an error during conversion, including unsupported type conversions. The arguments can be any valid expression. If unspecified, the operation throws an error upon encountering an error and stops. onNull Optional. The value to return if the input is null or missing. The arguments can be any valid expression. If unspecified, $convert returns null if the input is null or missing. In addition to $convert, MongoDB provides the following aggregation operators as shorthand when the default \"onError\" and \"onNull\" behavior is acceptable:      * $toBool  * $toDate  * $toDecimal  * $toDouble  * $toInt  * $toLong  * $toObjectId  * $toString \nBEHAVIOR  CONVERTING TO A BOOLEAN \nThe following table lists the input types that can be converted to a boolean: Input Type\nBehavior\nBoolean\nNo-op. Returns the boolean value.\nDouble\nReturns true if not zero.\nReturn false if zero.\nDecimal\nReturns true if not zero.\nReturn false if zero.\nInteger\nReturns true if not zero.\nReturn false if zero.\nLong\nReturns true if not zero.\nReturn false if zero.\nObjectId\nReturns true.\nString\nReturns true.\nDate\nReturns true.\nTimestamp\nReturns true. The following table lists some conversion to boolean examples: Example\nResults { input: true, to: \"bool\" } true { input: false, to: \"bool\" } false { input: 1.99999, to: \"bool\" } true { input: Decimal128( \"5\" ), to: \"bool\" } true { input: Decimal128( \"0\" ), to: \"bool\" } false { input: 100, to: \"bool\" } true {   input: ISODate( \"2018-03-26T04:38:28.044Z\" ),   to: \"bool\"} true { input: \"hello\", to: \"bool\" } true { input: \"false\", to: \"bool\" } true { input: \"\", to: \"bool\" } true { input: null, to: \"bool\" } null \nTIP \nSEE ALSO: $toBool \nCONVERTING TO AN INTEGER \nThe following table lists the input types that can be converted to an integer: Input Type\nBehavior\nBoolean\nReturns 0 for false.\nReturns 1 for true.\nDouble Returns truncated value. The truncated double value must fall within the minimum and maximum value for an integer. You cannot convert a double value whose truncated value is less than the minimum integer value or is greater than the maximum integer value. Decimal Returns truncated value.\n\n You cannot convert a decimal value whose truncated value is less than the minimum integer value or is greater than the maximum integer value. Integer\nNo-op. Returns the integer value.\nLong Returns the long value as an integer. The long value must fall within the minimum and maximum value for an integer. You cannot convert a long value that is less than the minimum integer value or is greater than the maximum integer value. String Returns the numerical value of the string as an integer. The string value must be a base 10 integer (e.g. \"-5\", \"123456\") and fall within the minimum and maximum value for an integer. You cannot convert a string value of a float or decimal or non-base 10 number (e.g. \"-5.0\", \"0x6400\") or a value that falls outside the minimum and maximum value for an integer. The following table lists some conversion to integer examples: Example\nResults { input: true, to: \"int\" } 1 { input: false, to: \"int\" } 0 { input: 1.99999, to: \"int\" } 1 { input: Decimal128( \"5.5000\" ), to: \"int\" } 5 {   input: Decimal128( \"9223372036000.000\" ),   to: \"int\"} Error { input: Long( \"5000\" ), to: \"int\" } 5000 { input: Long( \"922337203600\" ), to: \"int\" } Error { input: \"-2\", to: \"int\" } -2 { input: \"2.5\", to: \"int\" } Error { input: null, to: \"int\" } null \nTIP \nSEE ALSO: $toInt operator. \nCONVERTING TO A DECIMAL \nThe following table lists the input types that can be converted to a decimal: Input Type\nBehavior\nBoolean\nReturns Decimal128( \"0\" ) for false.\nReturns Decimal128( \"1\" ) for true.\nDouble\nReturns double value as a decimal.\nDecimal\nNo-op. Returns the decimal.\nInteger\nReturns the int value as a decimal.\nLong\nReturns the long value as a decimal.\nString Returns the numerical value of the string as a decimal. The string value must be of a base 10 numeric value (e.g. \"-5.5\", \"123456\"). You cannot convert a string value of a non-base 10 number (e.g. \"0x6400\") Date\nReturns the number of milliseconds since the epoch that corresponds to the date value. The following table lists some conversion to decimal examples: Example\nResults { input: true, to: \"decimal\" } Decimal128(\"1\") { input: false, to: \"decimal\" } Decimal128(\"0\") { input: 2.5, to: \"decimal\" } Decimal128( \"2.50000000000000\" ) { input: Int32( 5 ), to: \"decimal\" } Decimal128(\"5\") { input: Long( 10000 ), to: \"decimal\" } Decimal128(\"10000\") { input: \"-5.5\", to: \"decimal\" } Decimal128(\"-5.5\") {   input: ISODate( \"2018-03-26T04:38:28.044Z\" ),   to: \"decimal\"} Decimal128(\"1522039108044\") \nTIP \nSEE ALSO: $toDecimal \nCONVERTING TO A DOUBLE \nThe following table lists the input types that can be converted to a double: Input Type\nBehavior\nBoolean\nReturns NumberDouble(0) for false.\nReturns NumberDouble(1) for true.\nDouble\nNo-op. Returns the double.\nDecimal Returns the decimal value as a double. The decimal value must fall within the minimum and maximum value for a double. You cannot convert a decimal value whose value is less than the minimum double value or is greater than the maximum double value. Integer\nReturns the int value as a double.\nLong\nReturns the long value as a double.\nString Returns the numerical value of the string as a double. The string value must be of a base 10 numeric value (e.g. \"-5.5\", \"123456\") and fall within the minimum and maximum value for a double. You cannot convert a string value of a non-base 10 number (e.g. \"0x6400\") or a value that falls outside the minimum and maximum value for a double. Date\nReturns the number of milliseconds since the epoch that corresponds to the date value. The following table lists some conversion to double examples: Example\nResults\n\n 1 { input: false, to: \"double\" } 0 { input: 2.5, to: \"double\" } 2.5 { input: Int32( 5 ), to: \"double\" } 5 { input: Long( \"10000\" ), to: \"double\" } 10000 { input: \"-5.5\", to: \"double\" } -5.5 { input: \"5e10\", to: \"double\" } 50000000000 {   input: ISODate( \"2018-03-26T04:38:28.044Z\" ),   to: \"double\"} 1522039108044 \nTIP \nSEE ALSO: $toDouble \nCONVERTING TO A LONG \nThe following table lists the input types that can be converted to a long: Input Type\nBehavior\nBoolean\nReturns 0 for false.\nReturns 1 for true.\nDouble Returns truncated value. The truncated double value must fall within the minimum and maximum value for a long. You cannot convert a double value whose truncated value is less than the minimum long value or is greater than the maximum long value. Decimal Returns truncated value. The truncated decimal value must fall within the minimum and maximum value for a long. You cannot convert a decimal value whose truncated value is less than the minimum long value or is greater than the maximum long value. Integer\nReturns the int value as a long.\nLong\nNo-op. Returns the long value.\nString Returns the numerical value of the string. The string value must be of a base 10 long (e.g. \"-5\", \"123456\") and fall within the minimum and maximum value for a long. You cannot convert a string value of a float or decimal or non-base 10 number (e.g. \"-5.0\", \"0x6400\") or a value that falls outside the minimum and maximum value for a long. Date\nConverts the Date into the number of milliseconds since the epoch. The following table lists some conversion to long examples: Example\nResults { input: true, to: \"long\" } Long(\"1\") { input: false, to: \"long\"  } Long(\"0\") { input: 2.5, to: \"long\"  } Long(\"2\") { input: Decimal128( \"5.5000\" ), to: \"long\" } Long(\"5\") {   input: Decimal128( \"9223372036854775808.0\" ),   to: \"long\"} Error { input: Int32( 8 ), to: \"long\" } Long(\"8\") {   input: ISODate( \"2018-03-26T04:38:28.044Z\" ),   to: \"long\"} Long(\"1522039108044\") { input: \"-2\", to: \"long\" } Long(\"-2\") { input: \"2.5\", to: \"long\" } Error { input: null, to: \"long\" } null \nTIP \nSEE ALSO: $toLong \nCONVERTING TO A DATE \nThe following table lists the input types that can be converted to a date: Input Type\nBehavior\nDouble Returns a date that corresponds to the number of milliseconds represented by the truncated double value. Positive number corresponds to the number of milliseconds since Jan 1, 1970. Negative number corresponds to the number of milliseconds before Jan 1, 1970. Decimal Returns a date that corresponds to the number of milliseconds represented by the truncated decimal value. Positive number corresponds to the number of milliseconds since Jan 1, 1970. Negative number corresponds to the number of milliseconds before Jan 1, 1970. Long Returns a date that corresponds to the number of milliseconds represented by the long value. Positive number corresponds to the number of milliseconds since Jan 1, 1970. Negative number corresponds to the number of milliseconds before Jan 1, 1970. String Returns a date that corresponds to the date string. The string must be a valid date string, such as:  * \"2018-03-03\"  * \"2018-03-03T12:00:00Z\"  * \"2018-03-03T12:00:00+0500\" ObjectId\nReturns a date that corresponds to the timestamp of the ObjectId.\nTimestamp\nReturns a date that corresponds to the timestamp. The following table lists some conversion to date examples: Example\nResults {   input: 120000000000.5,   to: \"date\"} ISODate(\"1973-10-20T21:20:00.000Z\") {   input: Decimal128( \"1253372036000.50\" ),   to: \"date\"}\n\n {   input: Long( \"1100000000000\" ),   to: \"date} ISODate(\"2004-11-09T11:33:20.000Z\") {   input:  Long( \"-1100000000000\" ),   to: \"date\"} ISODate(\"1935-02-22T12:26:40.000Z\") {   input: ObjectId( \"5ab9c3da31c2ab715d421285\" ),   to: \"date\"} ISODate(\"2018-03-27T04:08:58.000Z\") { input:  \"2018-03-03\", to: \"date\" } ISODate(\"2018-03-03T00:00:00.000Z\") {   input: \"2018-03-20 11:00:06 +0500\",   to: \"date\"} ISODate(\"2018-03-20T06:00:06.000Z\") { input: \"Friday\", to: \"date\" } Error {   input: Timestamp( { t: 1637688118, i: 1 } ),   to: \"date\"} ISODate(\"2021-11-23T17:21:58.000Z\") \nTIP \nSEE ALSO:  * $toDate operator  * $dateFromString \nCONVERTING TO AN OBJECTID \nThe following table lists the input types that can be converted to an ObjectId: Input Type\nBehavior\nString Returns an ObjectId for the hexadecimal string of length 24. You cannot convert a string value that is not a hexadecimal string of length 24. The following table lists some conversion to date examples: Example\nResults {   input: \"5ab9cbfa31c2ab715d42129e\",   to: \"objectId\"} ObjectId(\"5ab9cbfa31c2ab715d42129e\") {   input: \"5ab9cbfa31c2ab715d42129\",   to: \"objectId\"} Error \nTIP \nSEE ALSO: $toObjectId operator. \nCONVERTING TO A STRING \nThe following table lists the input types that can be converted to a string: Input Type\nBehavior\nBoolean\nReturns the boolean value as a string.\nDouble\nReturns the double value as a string.\nDecimal\nReturns the decimal value as a string.\nInteger\nReturns the integer value as a string.\nLong\nReturns the long value as a string.\nObjectId\nReturns the ObjectId value as a hexadecimal string..\nString\nNo-op. Returns the string value.\nDate\nReturns the date as a string. The following table lists some conversion to string examples: Example\nResults { input: true, to: \"string\" } \"true\" { input: false, to: \"string\" } \"false\" { input: 2.5, to: \"string\" } \"2.5\" { input: Int32( 2 ), to: \"string\" } \"2\" { input:  Long( 1000 ), to: \"string\" } \"1000\" {   input: ObjectId( \"5ab9c3da31c2ab715d421285\" ),   to: \"string\"} \"5ab9c3da31c2ab715d421285\" {   input:  ISODate( \"2018-03-27T16:58:51.538Z\" ),   to: \"string\"} \"2018-03-27T16:58:51.538Z\" \nTIP \nSEE ALSO:  * $toString operator  * $dateToString \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: 1, item: \"apple\", qty: 5, price: 10 },   { _id: 2, item: \"pie\", qty: 10, price: Decimal128(\"20.0\") },   { _id: 3, item: \"ice cream\", qty: 2, price: \"4.99\" },   { _id: 4, item: \"almonds\" },   { _id: 5, item: \"bananas\", qty: 5000000000, price: Decimal128(\"1.25\") }] )  The following aggregation operation on the orders collection converts the price to a decimal:\n\n  The operation returns the following documents: { _id: 1, totalPrice: Decimal128(\"50\") },{ _id: 2, totalPrice: Decimal128(\"200.0\") },{ _id: 3, totalPrice: Decimal128(\"9.98\") },{ _id: 4, totalPrice: Decimal128(\"0\") },{ _id: 5, totalPrice: 'NaN' }  \nNOTE These examples use mongosh. The default types are different in the legacy mongo shell. ←  $cond (aggregation)$cos (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/pow/": " Docs Home → MongoDB Manual \n$POW (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$pow \nRaises a number to the specified exponent and returns the result. $pow has the following syntax: { $pow: [ <number>, <exponent> ] }  The <number> expression can be any valid expression as long as it resolves to a number. The <exponent> expression can be any valid expression as long as it resolves to a number. You cannot raise 0 to a negative exponent. \nBEHAVIOR \nThe result will have the same type as the input except when it cannot be represented accurately in that type. In these cases:      * A 32-bit integer will be converted to a 64-bit integer if the result is representable as a 64-bit integer.  * A 32-bit integer will be converted to a double if the result is not representable as a 64-bit integer.  * A 64-bit integer will be converted to double if the result is not representable as a 64-bit integer. If either argument resolves to a value of null or refers to a field that is missing, $pow returns null. If either argument resolves to NaN, $pow returns NaN. Example\nResults\n{ $pow: [ 5, 0 ] }\n1\n{ $pow: [ 5, 2 ] }\n25\n{ $pow: [ 5, -2 ] }\n0.04\n{ $pow: [ -5, 0.5 ] }\nNaN \nEXAMPLE \nA collection named quizzes contains the following documents: {   \"_id\" : 1,   \"scores\" : [      {         \"name\" : \"dave123\",         \"score\" : 85      },      {         \"name\" : \"dave2\",         \"score\" : 90      },      {         \"name\" : \"ahn\",         \"score\" : 71      }   ]}{   \"_id\" : 2,   \"scores\" : [      {         \"name\" : \"li\",         \"quiz\" : 2,         \"score\" : 96      },      {         \"name\" : \"annT\",         \"score\" : 77      },      {         \"name\" : \"ty\",         \"score\" : 82      }   ]}  The following example calculates the variance for each quiz: db.quizzes.aggregate([   { $project: { variance: { $pow: [ { $stdDevPop: \"$scores.score\" }, 2 ] } } }])  The operation returns the following results: { \"_id\" : 1, \"variance\" : 64.66666666666667 }{ \"_id\" : 2, \"variance\" : 64.66666666666667 } \n←  $or (aggregation)$push (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/in/": " Docs Home → MongoDB Manual \n$IN (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$in \nReturns a boolean indicating whether a specified value is in an array. \nNOTE This document describes the $in aggregation operator. For the $in query operator, see $in. $in has the following operator expression syntax: { $in: [ <expression>, <array expression> ] } \nOperand\nDescription\n<expression>\nAny valid expression expression.\n<array expression>\nAny valid expression that resolves to an array. Unlike the $in query operator, the aggregation $in operator does not support matching by regular expressions. Example\nResults\n{ $in: [ 2, [ 1, 2, 3 ] ] }\ntrue\n{ $in: [ \"abc\", [ \"xyz\", \"abc\" ] ] }\ntrue\n{ $in: [ \"xy\", [ \"xyz\", \"abc\" ] ] }\nfalse\n{ $in: [ [ \"a\" ], [ \"a\" ] ] }\nfalse\n{ $in: [ [ \"a\" ], [ [ \"a\" ] ] ] }\ntrue\n{ $in: [ /^a/, [ \"a\" ] ] }\nfalse\n{ $in: [ /^a/, [ /^a/ ] ] }\ntrue \nBEHAVIOR \n$in fails with an error in either of the following cases: if the $in expression is not given exactly two arguments, or if the second argument does not resolve to an array. \nEXAMPLE \nA collection named fruit has the following documents: { \"_id\" : 1, \"location\" : \"24th Street\",  \"in_stock\" : [ \"apples\", \"oranges\", \"bananas\" ] }{ \"_id\" : 2, \"location\" : \"36th Street\",  \"in_stock\" : [ \"bananas\", \"pears\", \"grapes\" ] }{ \"_id\" : 3, \"location\" : \"82nd Street\",  \"in_stock\" : [ \"cantaloupes\", \"watermelons\", \"apples\" ] }  The following aggregation operation looks at the in_stock array in each document and determines whether the string bananas is present. db.fruit.aggregate([  {    $project: {      \"store location\" : \"$location\",      \"has bananas\" : {        $in: [ \"bananas\", \"$in_stock\" ]      }    }  }])  The operation returns the following results: { \"_id\" : 1, \"store location\" : \"24th Street\", \"has bananas\" : true }{ \"_id\" : 2, \"store location\" : \"36th Street\", \"has bananas\" : true }{ \"_id\" : 3, \"store location\" : \"82nd Street\", \"has bananas\" : false } \n←  $ifNull (aggregation)$indexOfArray (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/tsIncrement/": " Docs Home → MongoDB Manual \n$TSINCREMENT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$tsIncrement \nNew in version 5.1. Returns the incrementing ordinal from a timestamp as a long. When multiple events happen within the same second, the incrementing ordinal uniquely identifies each event. $tsIncrement syntax: { $tsIncrement: <expression> } The expression must resolve to a timestamp. \nTIP \nSEE ALSO:      * Expressions  * BSON Types  * $tsSecond \nBEHAVIOR \n$tsIncrement returns:  * Null if the input expression evaluates to null or refers to a field that is missing.  * An error if the input expression does not evaluate to a timestamp. \nEXAMPLES  OBTAIN THE INCREMENTING ORDINAL FROM A TIMESTAMP FIELD \nCreate a stockSales collection that contains company stock financial market sales: db.stockSales.insertMany( [   { _id: 0, symbol: \"ABC\", saleTimestamp: Timestamp(1622731060, 1) },   { _id: 1, symbol: \"ABC\", saleTimestamp: Timestamp(1622731060, 2) },   { _id: 2, symbol: \"DEF\", saleTimestamp: Timestamp(1714124193, 1) },   { _id: 3, symbol: \"DEF\", saleTimestamp: Timestamp(1714124193, 2) },   { _id: 4, symbol: \"DEF\", saleTimestamp: Timestamp(1714124193, 3) }] )  In the timestamp constructor, the:  * First value is the number of seconds after the Unix epoch.  * Second value is the incrementing ordinal. When multiple events happen within the same second, the incrementing ordinal uniquely identifies each event. The following example uses $tsIncrement in a $project stage to return the incrementing ordinal from the stock sales saleTimestamp field: db.stockSales.aggregate( [   {      $project:      {         _id: 0, saleTimestamp: 1, saleIncrement: { $tsIncrement: \"$saleTimestamp\" }      }   }] )  In the example, $project only includes the saleTimestamp and saleIncrement fields as shown in the following output: {  saleTimestamp: Timestamp({ t: 1622731060, i: 1 }),  saleIncrement: Long(\"1\")},{  saleTimestamp: Timestamp({ t: 1622731060, i: 2 }),  saleIncrement: Long(\"2\")},{  saleTimestamp: Timestamp({ t: 1714124193, i: 1 }),  saleIncrement: Long(\"1\")},{  saleTimestamp: Timestamp({ t: 1714124193, i: 2 }),  saleIncrement: Long(\"2\")},{  saleTimestamp: Timestamp({ t: 1714124193, i: 3 }),  saleIncrement: Long(\"3\")} \nUSE $TSINCREMENT IN A CHANGE STREAM CURSOR TO MONITOR COLLECTION CHANGES \nThe example in this section uses $tsIncrement in a change stream cursor to return every other change made to a collection in the same second of time. Create a change stream cursor on a collection named cakeSales that you will see later in this section: cakeSalesCursor = db.cakeSales.watch( [   {      $match: {         $expr: {            $eq: [               { $mod: [ { $tsIncrement: \"$clusterTime\" } , 2 ] },               0            ]         }      }   }] )  In the example, the:  * db.collection.watch() method creates a change stream cursor for the cakeSales collection and stores the cursor in cakeSalesCursor.  * $match stage filters the documents to those returned by the $expr operator.\n\n Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  To monitor the cakeSales collection changes, use cakeSalesCursor. For example, to obtain the next document from cakeSalesCursor, use the next() method: cakeSalesCursor.next()  Depending on the second when the documents were added to cakeSales, the output from cakeSalesCursor.next() varies. For example, the document additions might span more than one second. The following cakeSalesCursor.next() example output shows the insert details for the first document added to the cakeSales collection. Notice the incrementing ordinal i is 2 in the clusterTime field. _id: {  _data: '82613A4F25000000022B022C0100296E5A100454C5BFAF538C47AB950614F43889BE00461E5F696400290004'},operationType: 'insert',clusterTime: Timestamp({ t: 1631211301, i: 2 }),fullDocument: {  _id: 0,  type: 'chocolate',  orderDate: ISODate(\"2020-05-18T14:10:30.000Z\"),  state: 'CA',  price: 13,  quantity: 120},ns: { db: 'test', coll: 'cakeSales' },documentKey: { _id: 0 } Running cakeSalesCursor.next() again returns the cakeSales document for which the clusterTime incrementing ordinal i is 4, omitting the document where i is 3. ←  $toUpper (aggregation)$tsSecond (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/subtract/": " Docs Home → MongoDB Manual \n$SUBTRACT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$subtract \nSubtracts two numbers to return the difference, or two dates to return the difference in milliseconds, or a date and a number in milliseconds to return the resulting date. The $subtract expression has the following syntax: { $subtract: [ <expression1>, <expression2> ] }  The second argument is subtracted from the first argument. The arguments can be any valid expression as long as they resolve to numbers and/or dates. To subtract a number from a date, the date must be the first argument. For more information on expressions, see Expressions. \nBEHAVIOR \nStarting in MongoDB 5.0, the result will have the same type as the input except when it cannot be represented accurately in that type. In these cases:      * A 32-bit integer will be converted to a 64-bit integer if the result is representable as a 64-bit integer.  * A 32-bit integer will be converted to a double if the result is not representable as a 64-bit integer.  * A 64-bit integer will be converted to double if the result is not representable as a 64-bit integer. \nEXAMPLES \nConsider a sales collection with the following documents: db.sales.insertMany([   { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"fee\" : 2, \"discount\" : 5, \"date\" : ISODate(\"2014-03-01T08:00:00Z\") },   { \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"fee\" : 1, \"discount\" : 2, \"date\" : ISODate(\"2014-03-01T09:00:00Z\") }])  \nSUBTRACT NUMBERS \nThe following aggregation uses the $subtract expression to compute the total by subtracting the discount from the subtotal of price and fee. db.sales.aggregate( [ { $project: { item: 1, total: { $subtract: [ { $add: [ \"$price\", \"$fee\" ] }, \"$discount\" ] } } } ] )  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc\", \"total\" : 7 }{ \"_id\" : 2, \"item\" : \"jkl\", \"total\" : 19 }  \nSUBTRACT TWO DATES \nThe following aggregation uses the $subtract expression to subtract $date from the current date, using the system NOW (available starting in 4.2) and returns the difference in milliseconds: db.sales.aggregate( [ { $project: { item: 1, dateDifference: { $subtract: [ \"$$NOW\", \"$date\" ] } } } ] )  Alternatively, you can use the Date() for the current date:s db.sales.aggregate( [ { $project: { item: 1, dateDifference: { $subtract: [ new Date(), \"$date\" ] } } } ] )  Both operations return documents that resemble the following: { \"_id\" : 1, \"item\" : \"abc\", \"dateDifference\" : NumberLong(\"186136746187\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"dateDifference\" : NumberLong(\"186133146187\") }  \nSUBTRACT MILLISECONDS FROM A DATE \nThe following aggregation uses the $subtract expression to subtract 5 * 60 * 1000 milliseconds (5 minutes) from the \"$date\" field: db.sales.aggregate( [ { $project: { item: 1, dateDifference: { $subtract: [ \"$date\", 5 * 60 * 1000 ] } } } ] )  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc\", \"dateDifference\" : ISODate(\"2014-03-01T07:55:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"dateDifference\" : ISODate(\"2014-03-01T08:55:00Z\") } \n←  $substrCP (aggregation)$sum (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sort/": " Docs Home → MongoDB Manual \n$SORT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples\n * $sort Operator and Memory\n * $sort Operator and Performance \nDEFINITION \n$sort \nSorts all input documents and returns them to the pipeline in sorted order. The $sort stage has the following prototype form: { $sort: { <field1>: <sort order>, <field2>: <sort order> ... } }  $sort takes a document that specifies the field(s) to sort by and the respective sort order. <sort order> can have one of the following values: Value\nDescription\n1\nSort ascending.\n-1\nSort descending.\n{ $meta: \"textScore\" }\nSort by the computed textScore metadata in descending order. See Text Score Metadata Sort for an example. If sorting on multiple fields, sort order is evaluated from left to right. For example, in the form above, documents are first sorted by <field1>. Then documents with the same <field1> values are further sorted by <field2>. \nBEHAVIOR  LIMITS \nYou can sort on a maximum of 32 keys. \nSORT CONSISTENCY \nMongoDB does not store documents in a collection in a particular order. When sorting on a field which contains duplicate values, documents containing those values may be returned in any order. If consistent sort order is desired, include at least one field in your sort that contains unique values. The easiest way to guarantee this is to include the _id field in your sort query. Consider the following restaurant collection: db.restaurants.insertMany( [   { \"_id\" : 1, \"name\" : \"Central Park Cafe\", \"borough\" : \"Manhattan\"},   { \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"borough\" : \"Queens\"},   { \"_id\" : 3, \"name\" : \"Empire State Pub\", \"borough\" : \"Brooklyn\"},   { \"_id\" : 4, \"name\" : \"Stan's Pizzaria\", \"borough\" : \"Manhattan\"},   { \"_id\" : 5, \"name\" : \"Jane's Deli\", \"borough\" : \"Brooklyn\"},] );  The following command uses the $sort stage to sort on the borough field: db.restaurants.aggregate(   [     { $sort : { borough : 1 } }   ])  In this example, sort order may be inconsistent, since the borough field contains duplicate values for both Manhattan and Brooklyn. Documents are returned in alphabetical order by borough, but the order of those documents with duplicate values for borough might not the be the same across multiple executions of the same sort. For example, here are the results from two different executions of the above command: { \"_id\" : 3, \"name\" : \"Empire State Pub\", \"borough\" : \"Brooklyn\" }{ \"_id\" : 5, \"name\" : \"Jane's Deli\", \"borough\" : \"Brooklyn\" }{ \"_id\" : 1, \"name\" : \"Central Park Cafe\", \"borough\" : \"Manhattan\" }{ \"_id\" : 4, \"name\" : \"Stan's Pizzaria\", \"borough\" : \"Manhattan\" }{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"borough\" : \"Queens\" }\n{ \"_id\" : 5, \"name\" : \"Jane's Deli\", \"borough\" : \"Brooklyn\" }{ \"_id\" : 3, \"name\" : \"Empire State Pub\", \"borough\" : \"Brooklyn\" }{ \"_id\" : 4, \"name\" : \"Stan's Pizzaria\", \"borough\" : \"Manhattan\" }{ \"_id\" : 1, \"name\" : \"Central Park Cafe\", \"borough\" : \"Manhattan\" }{ \"_id\" : 2, \"name\" : \"Rock A Feller Bar and Grill\", \"borough\" : \"Queens\" } While the values for borough are still sorted in alphabetical order, the order of the documents containing duplicate values for borough (i.e. Manhattan and Brooklyn) is not the same. To achieve a consistent sort, add a field which contains exclusively unique values to the sort. The following command uses the $sort stage to sort on both the borough field and the _id field: db.restaurants.aggregate(   [     { $sort : { borough : 1, _id: 1 } }   ]) \n\n \nEXAMPLES  ASCENDING/DESCENDING SORT \nFor the field or fields to sort by, set the sort order to 1 or -1 to specify an ascending or descending sort respectively, as in the following example: db.users.aggregate(   [     { $sort : { age : -1, posts: 1 } }   ])  This operation sorts the documents in the users collection, in descending order according by the age field and then in ascending order according to the value in the posts field. When comparing values of different BSON types, MongoDB uses the following comparison order, from lowest to highest:        1.  MinKey (internal type)  2.  Null  3.  Numbers (ints, longs, doubles, decimals)  4.  Symbol, String  5.  Object  6.  Array  7.  BinData  8.  ObjectId  9.  Boolean  10. Date  11. Timestamp  12. Regular Expression  13. MaxKey (internal type) For details on the comparison/sort order for specific types, see Comparison/Sort Order. \nTEXT SCORE METADATA SORT \nFor a pipeline that includes a $text search, you can sort by descending relevance score using the { $meta: \"textScore\" } expression. In the { <sort-key> } document, set the { $meta: \"textScore\" } expression to an arbitrary field name. The field name is ignored by the query system. For example: db.users.aggregate(   [     { $match: { $text: { $search: \"operating\" } } },     { $sort: { score: { $meta: \"textScore\" }, posts: -1 } }   ])  This operation uses the $text operator to match the documents, and then sorts first by the \"textScore\" metadata in descending order, and then by the posts field in descending order. The score field name in the sort document is ignored by the query system. In this pipeline, the \"textScore\" metadata is not included in the projection and is not returned as part of the matching documents. See $meta for more information. \n$SORT OPERATOR AND MEMORY  $SORT + $LIMIT MEMORY OPTIMIZATION \nWhen a $sort precedes a $limit and there are no intervening stages that modify the number of documents, the optimizer can coalesce the $limit into the $sort. This allows the $sort operation to only maintain the top n results as it progresses, where n is the specified limit, and ensures that MongoDB only needs to store n items in memory. This optimization still applies when allowDiskUse is true and the n items exceed the aggregation memory limit. Optimizations are subject to change between releases. \n$SORT AND MEMORY RESTRICTIONS \n$sort is subject to the 100 megabyte memory usage limit, but is able to write temporary files to disk if additional space is required. Starting in MongoDB 6.0, pipeline stages that require more than 100 megabytes of memory to execute write temporary files to disk by default. In earlier verisons of MongoDB, you must pass { allowDiskUse: true } to individual find and aggregate commands to enable this behavior. Individual find and aggregate commands may override the allowDiskUseByDefault parameter by either:  * Using { allowDiskUse: true } to allow writing temporary files out to disk when allowDiskUseByDefault is set to false  * Using { allowDiskUse: false } to prohibit writing temporary files out to disk when allowDiskUseByDefault is set to true \nTIP \nSEE ALSO: Aggregation Pipeline Limits \n$SORT OPERATOR AND PERFORMANCE \nThe $sort operator can take advantage of an index if it's used in the first stage of a pipeline or if it's only preceeded by a $match stage. When you use the $sort on a sharded cluster, each shard sorts its result documents using an index where available. Then the mongos or one of the shards performs a streamed merge sort. \nTIP \nSEE ALSO:  * Aggregation with the Zip Code Data Set  * Aggregation with User Preference Data ←  $skip (aggregation)$sortByCount (aggregation) → On this page  * Definition\n * Behavior\n * Examples\n * $sort Operator and Memory\n * $sort Operator and Performance Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/ceil/": " Docs Home → MongoDB Manual \n$CEIL (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$ceil \nReturns the smallest integer greater than or equal to the specified number. $ceil has the following syntax: { $ceil: <number> }  The <number> expression can be any valid expression as long as it resolves to a number. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the argument resolves to a value of null or refers to a field that is missing, $ceil returns null. If the argument resolves to NaN, $ceil returns NaN. Example\nResults\n{ $ceil: 1 }\n1\n{ $ceil: 7.80 }\n8\n{ $ceil: -2.8 }\n-2 \nEXAMPLE \nCreate a collection named samples with the following documents: db.samples.insertMany(   [      { _id: 1, value: 9.25 },      { _id: 2, value: 8.73 },      { _id: 3, value: 4.32 },      { _id: 4, value: -5.34 }   ])  The following example returns both the original value and the ceiling value: db.samples.aggregate([   { $project: { value: 1, ceilingValue: { $ceil: \"$value\" } } }])  The operation returns the following results: { \"_id\" : 1, \"value\" : 9.25, \"ceilingValue\" : 10 }{ \"_id\" : 2, \"value\" : 8.73, \"ceilingValue\" : 9 }{ \"_id\" : 3, \"value\" : 4.32, \"ceilingValue\" : 5 }{ \"_id\" : 4, \"value\" : -5.34, \"ceilingValue\" : -5 } \n←  $bsonSize (aggregation)$cmp (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/objectToArray/": " Docs Home → MongoDB Manual \n$OBJECTTOARRAY (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$objectToArray \nConverts a document to an array. The return array contains an element for each field/value pair in the original document. Each element in the return array is a document that contains two fields k and v:      * The k field contains the field name in the original document.  * The v field contains the value of the field in the original document. $objectToArray has the following syntax: { $objectToArray: <object> }  The <object> expression can be any valid expression as long as it resolves to a document object. $objectToArray applies to the top-level fields of its argument. If the argument is a document that itself contains embedded document fields, the $objectToArray does not recursively apply to the embedded document fields. For more information on expressions, see Expressions. \nBEHAVIOR \nFor more information on expressions, see Expressions. Example\nResults { $objectToArray: { item: \"foo\", qty: 25 } } [   {      \"k\" : \"item\",      \"v\" : \"foo\"   },   {      \"k\" : \"qty\",      \"v\" : 25   }] { $objectToArray: {    item: \"foo\",    qty: 25,    size: { len: 25, w: 10, uom: \"cm\" } } } [   {      \"k\" : \"item\",      \"v\" : \"foo\"   },   {      \"k\" : \"qty\",      \"v\" : 25   },   {      \"k\" : \"size\",      \"v\" : {         \"len\" : 25,         \"w\" : 10,         \"uom\" : \"cm\"      }   }] \nEXAMPLES  $OBJECTTOARRAY EXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\",  dimensions: { l: 25, w: 10, uom: \"cm\" } }{ \"_id\" : 2, \"item\" : \"ABC2\",  dimensions: { l: 50, w: 25, uom: \"cm\" } }{ \"_id\" : 3, \"item\" : \"XYZ1\",  dimensions: { l: 70, w: 75, uom: \"cm\" } }  The following aggregation pipeline operation use the $objectToArray to return the dimensions field as an array: db.inventory.aggregate(   [      {         $project: {            item: 1,            dimensions: { $objectToArray: \"$dimensions\" }         }      }   ])  The operation returns the following: { \"_id\" : 1, \"item\" : \"ABC1\", \"dimensions\" : [ { \"k\" : \"l\", \"v\" : 25 }, { \"k\" : \"w\", \"v\" : 10 }, { \"k\" : \"uom\", \"v\" : \"cm\" } ] }{ \"_id\" : 2, \"item\" : \"ABC2\", \"dimensions\" : [ { \"k\" : \"l\", \"v\" : 50 }, { \"k\" : \"w\", \"v\" : 25 }, { \"k\" : \"uom\", \"v\" : \"cm\" } ] }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"dimensions\" : [ { \"k\" : \"l\", \"v\" : 70 }, { \"k\" : \"w\", \"v\" : 75 }, { \"k\" : \"uom\", \"v\" : \"cm\" } ] }  \n$OBJECTTOARRAY TO SUM NESTED FIELDS \nConsider a inventory collection with the following documents:\n\n  The following aggregation pipeline operation uses the $objectToArray along with $unwind and $group to calculate the total items in stock per warehouse. db.inventory.aggregate([   { $project: { warehouses: { $objectToArray: \"$instock\" } } },   { $unwind: \"$warehouses\" },   { $group: { _id: \"$warehouses.k\", total: { $sum: \"$warehouses.v\" } } }])  The operation returns the following: { \"_id\" : \"warehouse3\", \"total\" : 200 }{ \"_id\" : \"warehouse2\", \"total\" : 1000 }{ \"_id\" : \"warehouse1\", \"total\" : 2500 }  \n$OBJECTTOARRAY + $ARRAYTOOBJECT EXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", instock: { warehouse1: 2500, warehouse2: 500 } }{ \"_id\" : 2, \"item\" : \"ABC2\", instock: { warehouse2: 500, warehouse3: 200} }  The following aggregation pipeline operation calculates the total in stock for each item and adds to the instock document: db.inventory.aggregate( [   { $addFields: { instock: { $objectToArray: \"$instock\" } } },   { $addFields: { instock: { $concatArrays: [ \"$instock\", [ { \"k\": \"total\", \"v\": { $sum: \"$instock.v\" } } ] ] } } } ,   { $addFields: { instock: { $arrayToObject: \"$instock\" } } }] )  The operation returns the following: { \"_id\" : 1, \"item\" : \"ABC1\", \"instock\" : { \"warehouse1\" : 2500, \"warehouse2\" : 500, \"total\" : 3000 } }{ \"_id\" : 2, \"item\" : \"ABC2\", \"instock\" : { \"warehouse2\" : 500, \"warehouse3\" : 200, \"total\" : 700 } }  \nTIP \nSEE ALSO: $arrayToObject ←  $not (aggregation)$or (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/degreesToRadians/": " Docs Home → MongoDB Manual \n$DEGREESTORADIANS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$degreesToRadians \nNew in version 4.2. Converts an input value measured in degrees to radians. $degreesToRadians has the following syntax: { $degreesToRadians: <expression> }  $degreesToRadians takes any valid expression that resolves to a number. By default $degreesToRadians returns values as a double. $degreesToRadians can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $degreesToRadians returns null. If the argument resolves to NaN, $degreesToRadians returns NaN. If the argument resolves to negative or positive infinity, $degreesToRadians negative or positive infinity respectively. Example\nResults\n{ $degreesToRadians: NaN }\nNaN\n{ $degreesToRadians: null }\nnull\n{ $degreesToRadians : Infinity}\nInfinity\n{ $degreesToRadians : -Infinity }\n-Infinity \nEXAMPLE \nThe trigonometry collection contains a document that contains three angles measured in degrees: {  \"angle_a\" : NumberDecimal(\"53.13010235415597870314438744090659\"),  \"angle_b\" : NumberDecimal(\"36.86989764584402129685561255909341\"),  \"angle_c\" : NumberDecimal(\"90\")}  The following aggregation operation uses the $degreesToRadians expression to convert each value to its radian equivalent and add them to the input document using the $addFields pipeline stage. db.trigonometry.aggregate([  {    $addFields: {      \"angle_a_rad\" : { $degreesToRadians : \"$angle_a\"},      \"angle_b_rad\" : { $degreesToRadians : \"$angle_b\"},      \"angle_c_rad\" : { $degreesToRadians : \"$angle_c\"}    }  }])  The operation returns the following document: {  \"_id\" : ObjectId(\"5c50aec71c75c59232b3ede4\"),  \"angle_a\" : NumberDecimal(\"53.13010235415597870314438744090660\"),  \"angle_b\" : NumberDecimal(\"36.86989764584402129685561255909341\"),  \"angle_c\" : NumberDecimal(\"90\"),  \"angle_a_rad\" : NumberDecimal(\"0.9272952180016122324285124629224290\"),  \"angle_b_rad\" : NumberDecimal(\"0.6435011087932843868028092287173227\"),  \"angle_c_rad\" : NumberDecimal(\"1.570796326794896619231321691639752\")} Since angle_a, angle_b, and angle_c are stored as 128-bit decimals, the output of $degreesToRadians is a 128-bit decimal. ←  $dayOfYear (aggregation)$denseRank (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toLong/": " Docs Home → MongoDB Manual \n$TOLONG (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toLong \nConverts a value to a long. If the value cannot be converted to a long, $toLong errors. If the value is null or missing, $toLong returns null. $toLong has the following syntax: {   $toLong: <expression>}  The $toLong takes any valid expression. The $toLong is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"long\" } }  \nTIP \nSEE ALSO: $convert \nBEHAVIOR \nThe following table lists the input types that can be converted to a decimal: The following table lists the input types that can be converted to a long: Input Type\nBehavior\nBoolean\nReturns Long(0) for false.\nReturns Long(1) for true.\nDouble Returns truncated value. The truncated double value must fall within the minimum and maximum value for a long. You cannot convert a double value whose truncated value is less than the minimum long value or is greater than the maximum long value. Decimal Returns truncated value. The truncated decimal value must fall within the minimum and maximum value for a long. You cannot convert a decimal value whose truncated value is less than the minimum long value or is greater than the maximum long value. Integer\nReturns the int value as a long.\nLong\nNo-op. Returns the long value.\nString Returns the numerical value of the string. The string value must be of a base 10 long (e.g. \"-5\", \"123456\"). You cannot convert a string value of a float or decimal or non-base 10 number (e.g. \"-5.0\", \"0x6400\") Date\nConverts the Date into the number of milliseconds since the epoch. The following table lists some conversion to long examples: Example\nResults\n{ $toLong: true }\nLong(\"1\")\n{ $toLong: false }\nLong(\"0\")\n{ $toLong: 1.99999 }\nLong(\"1\")\n{ $toLong: NumberDecimal(\"5.5000\") }\nLong(\"5\")\n{ $toLong: NumberDecimal(\"9223372036854775808.0\") }\nError\n{ $toLong: NumberInt(8) }\nLong(8)\n{ $toLong: ISODate(\"2018-03-26T04:38:28.044Z\") }\nLong(\"1522039108044\")\n{ $toLong: \"-2\" }\nLong(\"-2\")\n{ $toLong: \"2.5\" }\nError\n{ $toLong: null }\nnull \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: 1, item: \"apple\", qty: NumberInt(5) },   { _id: 2, item: \"pie\", qty: \"100\" },   { _id: 3, item: \"ice cream\", qty: NumberLong(\"500\") },   { _id: 4, item: \"almonds\", qty: \"50\" },] )  The following aggregation operation on the orders collection converts the qty to long before sorting by the value: // Define stage to add convertedQty field with converted qty value\nqtyConversionStage = {   $addFields: {      convertedQty: { $toLong: \"$qty\" }   }};\n// Define stage to sort documents by the converted qty values\nsortStage = {   $sort: { \"convertedQty\": -1 }}; db.orders.aggregate( [   qtyConversionStage,   sortStage])  The operation returns the following documents: { _id: 3, item: 'ice cream', qty: Long(\"500\"), convertedQty: Long(\"500\") },{ _id: 2, item: 'pie', qty: '100', convertedQty: Long(\"100\") },{ _id: 4, item: 'almonds', qty: '50', convertedQty: Long(\"50\") },{ _id: 1, item: 'apple', qty: 5, convertedQty: Long(\"5\") }  \nNOTE If the conversion operation encounters an error, the aggregation operation stops and throws an error. To override this behavior, use $convert instead.\n\n On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sin/": " Docs Home → MongoDB Manual \n$SIN (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$sin \nNew in version 4.2. Returns the sine of a value that is measured in radians. $sin has the following syntax: { $sin: <expression> }  $sin takes any valid expression that resolves to a number. If the expression returns a value in degrees, use the $degreesToRadians operator to convert the result to radians. By default $sin returns values as a double. $sin can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $sin returns null. If the argument resolves to NaN, $sin returns NaN. If the argument resolves to negative or positive infinity, $sin throws an error. Example\nResults\n{ $sin: NaN }\nNaN\n{ $sin: null }\nnull { $sin : Infinity} or { $sin : -Infinity } Throws an error message resembling the following formatted output: \"errmsg\" :  \"Failed to optimize pipeline :: caused by :: cannot  apply $sin to -inf, value must in (-inf,inf)\" \nEXAMPLE  \n←  $size (aggregation)$sinh (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/acosh/": " Docs Home → MongoDB Manual \n$ACOSH (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $acosh \nNew in version 4.2. Returns the inverse hyperbolic cosine (hyperbolic arc cosine) of a value. $acosh has the following syntax: { $acosh: <expression> }  $acosh takes any valid expression that resolves to a number between 1 and +Infinity, e.g. 1 <= value <= +Infinity. $acosh returns values in radians. Use $radiansToDegrees operator to convert the output value from radians to degrees. By default $acosh returns values as a double. $acosh can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $acosh returns null. If the argument resolves to NaN, $acosh returns NaN. If the argument resolves to negative infinity, $acosh throws an error. If the argument resolves to Infinity, $acosh returns Infinity. If the argument resolves to a value outside the bounds of [-1, Infinity] inclusive, $acosh throws an error. Example\nResults\n{ $acosh: NaN }\nNaN\n{ $acosh: null }\nnull\n{ $acosh : Infinity}\nInfinity\n{ $acosh : 0 } Throws an error message resembling the following formatted output: \"errmsg\" :  \"Failed to optimize pipeline :: caused by :: cannot  apply $acosh to -inf, value must in (1,inf)\" \nEXAMPLE  \n←  $acos (aggregation)$add (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/allElementsTrue/": " Docs Home → MongoDB Manual \n$ALLELEMENTSTRUE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$allElementsTrue \nEvaluates an array as a set and returns true if no element in the array is false. Otherwise, returns false. An empty array returns true. $allElementsTrue has the following syntax: { $allElementsTrue: [ <expression> ] }  The <expression> itself must resolve to an array, separate from the outer array that denotes the argument list. For more information on expressions, see Expressions. \nBEHAVIOR \nIf a set contains a nested array element, $allElementsTrue does not descend into the nested array but evaluates the array at top-level. In addition to the false boolean value, $allElementsTrue evaluates as false the following: null, 0, and undefined values. The $allElementsTrue evaluates all other values as true, including non-zero numeric values and arrays. Example\nResult\n{ $allElementsTrue: [ [ true, 1, \"someString\" ] ] }\ntrue\n{ $allElementsTrue: [ [ [ false ] ] ] }\ntrue\n{ $allElementsTrue: [ [ ] ] }\ntrue\n{ $allElementsTrue: [ [ null, false, 0 ] ] }\nfalse \nEXAMPLE \nCreate an example collection named survey with the following documents: db.survey.insertMany([   { \"_id\" : 1, \"responses\" : [ true ] },   { \"_id\" : 2, \"responses\" : [ true, false ] },   { \"_id\" : 3, \"responses\" : [ ] },   { \"_id\" : 4, \"responses\" : [ 1, true, \"seven\" ] },   { \"_id\" : 5, \"responses\" : [ 0 ] },   { \"_id\" : 6, \"responses\" : [ [ ] ] },   { \"_id\" : 7, \"responses\" : [ [ 0 ] ] },   { \"_id\" : 8, \"responses\" : [ [ false ] ] },   { \"_id\" : 9, \"responses\" : [ null ] },   { \"_id\" : 10, \"responses\" : [ undefined ] }])  The following operation uses the $allElementsTrue operator to determine if the responses array only contains values that evaluate to true: db.survey.aggregate(   [     { $project: { responses: 1, isAllTrue: { $allElementsTrue: [ \"$responses\" ] }, _id: 0 } }   ])  The operation returns the following results: { \"responses\" : [ true ], \"isAllTrue\" : true }{ \"responses\" : [ true, false ], \"isAllTrue\" : false }{ \"responses\" : [ ], \"isAllTrue\" : true }{ \"responses\" : [ 1, true, \"seven\" ], \"isAllTrue\" : true }{ \"responses\" : [ 0 ], \"isAllTrue\" : false }{ \"responses\" : [ [ ] ], \"isAllTrue\" : true }{ \"responses\" : [ [ 0 ] ], \"isAllTrue\" : true }{ \"responses\" : [ [ false ] ], \"isAllTrue\" : true }{ \"responses\" : [ null ], \"isAllTrue\" : false }{ \"responses\" : [ undefined ], \"isAllTrue\" : false } \n←  $addToSet (aggregation)$and (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/getField/": " Docs Home → MongoDB Manual \n$GETFIELD (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$getField \nNew in version 5.0. Returns the value of a specified field from a document. If you don't specify an object, $getField returns the value of the field from $$CURRENT. You can use $getField to retrieve the value of fields with names that contain periods (.) or start with dollar signs ($). \nTIP Use $setField to add or update fields with names that contain dollar signs ($) or periods (.). \nSYNTAX \n$getField has the following syntax: {  $getField: {    field: <String>,    input: <Object>  }} \nField\nType\nDescription\nfield\nString Field in the input object for which you want to return a value. field can be any valid expression that resolves to a string constant. If field begins with a dollar sign ($), place the field name inside of a $literal expression to return its value. input\nObject Default: $$CURRENT A valid expression that contains the field for which you want to return a value. input must resolve to an object, missing, null, or undefined. If omitted, defaults to the document currently being processed in the pipeline ($$CURRENT). $getField has the following shorthand syntax for retrieving field values from $$CURRENT: {  $getField: <String>}  For this syntax, the argument is equivalent to the value of field described above. \nBEHAVIOR \n     * If field resolves to anything other than a string constant, $getField returns an error.  * If the field that you specify is not present in the input object, or in $$CURRENT if you don't specify an input object, $getField returns missing.  * If input evaluates to missing, undefined, or null, $getField returns null.  * If input evaluates to anything other than an object, missing, undefined, or null, $getField returns an error.  * $getField doesn't implicitly traverse objects or arrays. For example, $getField evaluates a field value of a.b.c as a top-level field a.b.c instead of a nested field { a: { b: { c: } } }. \nTIP \nSEE ALSO: Considerations for field names \nEXAMPLES  QUERY FIELDS THAT CONTAIN PERIODS (.) \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"sweatshirt\", \"price.usd\": 45.99, qty: 300 }{ \"_id\" : 2, \"item\" : \"winter coat\", \"price.usd\": 499.99, qty: 200 }{ \"_id\" : 3, \"item\" : \"sun dress\", \"price.usd\": 199.99, qty: 250 }{ \"_id\" : 4, \"item\" : \"leather boots\", \"price.usd\": 249.99, qty: 300 }{ \"_id\" : 5, \"item\" : \"bow tie\", \"price.usd\": 9.99, qty: 180 }  The following operation uses the $getField and $gt operators to find which products have a price.usd greater than 200: db.inventory.aggregate( [  {    $match:      { $expr:        { $gt: [ { $getField: \"price.usd\" }, 200 ] }      }   }] )  The operation returns the following results: [  { _id: 2, item: 'winter coat', qty: 200, 'price.usd': 499.99 },  { _id: 4, item: 'leather boots', qty: 300, 'price.usd': 249.99 }] \nQUERY FIELDS THAT START WITH A DOLLAR SIGN ($) \nConsider an inventory collection with the following documents:\n\n  The following operation uses the $getField, $gt, and $literal operators to find which products have a $price greater than 200: db.inventory.aggregate( [  {    $match:      { $expr:        { $gt: [ { $getField: {$literal: \"$price\" } }, 200 ] }      }   }] )  The operation returns the following results: [  { _id: 2, item: 'winter coat', qty: 200, '$price': 499.99 },  { _id: 4, item: 'leather boots', qty: 300, '$price': 249.99 }] \nQUERY A FIELD IN A SUB-DOCUMENT \nCreate an inventory collection with the following documents: db.inventory.insertMany( [   { \"_id\" : 1, \"item\" : \"sweatshirt\",  \"price.usd\": 45.99,     \"quantity\": { \"$large\": 50, \"$medium\": 50, \"$small\": 25 }   },   { \"_id\" : 2, \"item\" : \"winter coat\", \"price.usd\": 499.99,     \"quantity\": { \"$large\": 35, \"$medium\": 35, \"$small\": 35 }   },   { \"_id\" : 3, \"item\" : \"sun dress\", \"price.usd\": 199.99,     \"quantity\": { \"$large\": 45, \"$medium\": 40, \"$small\": 5 }   },   { \"_id\" : 4, \"item\" : \"leather boots\", \"price.usd\": 249.99,     \"quantity\": { \"$large\": 20, \"$medium\": 30, \"$small\": 40 }   },   { \"_id\" : 5, \"item\" : \"bow tie\", \"price.usd\": 9.99,     \"quantity\": { \"$large\": 0, \"$medium\": 10, \"$small\": 75 }   }] )  The following operation returns documents where the number of $small items is less than or equal to 20. db.inventory.aggregate( [   { $match:      { $expr:         { $lte:            [               { $getField:                  { field: { $literal: \"$small\" },                    input: \"$quantity\"                  }               },               20            ]         }      }   }] )  Use these operators to query the collection:  * The $lte operator finds values less than or equal to 20.  * $getField requires explicit field and input parameters because the $small field is part of a sub-document.  * $getField uses $literal to evaluate \"$small\", because the field name has a dollar sign ($) in it. Example output: [  {    _id: 3,    item: 'sun dress',    'price.usd': 199.99,    quantity: { '$large': 45, '$medium': 40, '$small': 5 }  }] ←  $function (aggregation)$gt (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/shift/": " Docs Home → MongoDB Manual \n$SHIFT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \nNew in version 5.0. $shift \nReturns the value from an expression applied to a document in a specified position relative to the current document in the $setWindowFields stage partition. The $setWindowFields stage sortBy field value determines the document order. For more information on how MongoDB compares fields with different types, see BSON comparison order. $shift is only available in the $setWindowFields stage. $shift syntax: {   $shift: {      output: <output expression>,      by: <integer>,      default: <default expression>   }} $shift takes a document with these fields: Field\nDescription\noutput Specifies an expression to evaluate and return in the output. by Specifies an integer with a numeric document position relative to the current document in the output. For example:      * 1 specifies the document position after the current document.  * -1 specifies the document position before the current document.  * -2 specifies the document position that is two positions before the current document. default Specifies an optional default expression to evaluate if the document position is outside of the implicit $setWindowFields stage window. The implicit window contains all the documents in the partition. The default expression must evaluate to a constant value. If you do not specify a default expression, $shift returns null for documents whose positions are outside of the implicit $setWindowFields stage window. \nBEHAVIOR \n$shift returns an error if you specify a window in the $setWindowFields stage. \nEXAMPLES \nCreate a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  The cakeSales collection is used in the following examples. \nSHIFT USING A POSITIVE INTEGER \nThis example uses $shift in the $setWindowFields stage to output the quantity of the cake sales from each document following the current document for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { quantity: -1 },         output: {            shiftQuantityForState: {               $shift: {                  output: \"$quantity\",                  by: 1,                  default: \"Not available\"               }            }         }      }   }] )  In the example:\n\n  * sortBy: { quantity: -1 } sorts the documents in each partition by quantity in descending order (-1), so the highest quantity is first.  * output after sortBy:\n   \n   * Sets the shiftQuantityForState field to the quantity value from the documents in each state.\n   \n   * Uses $shift to return the quantity value from the document that follows the current document in the output.\n     \n     * The document position is specified using the $shift by integer set to 1.\n     \n     * For documents outside of the implicit window, $shift returns \"Not available\", which is specified using the default expression. In this example output, the shifted quantity value is shown in the shiftQuantityForState field for each returned document: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"shiftQuantityForState\" : 145 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"shiftQuantityForState\" : 120 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"shiftQuantityForState\" : \"Not available\" }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"shiftQuantityForState\" : 134 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"shiftQuantityForState\" : 104 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"shiftQuantityForState\" : \"Not available\" } \nSHIFT USING A NEGATIVE INTEGER \nThis example uses $shift in the $setWindowFields stage to output the quantity of the cake sales from each document before the current document for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { quantity: -1 },         output: {            shiftQuantityForState: {               $shift: {                  output: \"$quantity\",                  by: -1,                  default: \"Not available\"               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { quantity: -1 } sorts the documents in each partition by quantity in descending order (-1), so the highest quantity is first.\n\n In this example output, the shifted quantity value is shown in the shiftQuantityForState field for each returned document: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"shiftQuantityForState\" : \"Not available\" }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"shiftQuantityForState\" : 162 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"shiftQuantityForState\" : 145 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"shiftQuantityForState\" : \"Not available\" }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"shiftQuantityForState\" : 140 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"shiftQuantityForState\" : 134 } ←  $setUnion (aggregation)$size (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/type/": " Docs Home → MongoDB Manual \n$TYPE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$type \nReturns a string that specifies the BSON type of the argument. $type has the following operator expression syntax: { $type: <expression> }  The argument can be any valid expression. \nTIP \nSEE ALSO:      * $isNumber - checks if the argument is a number. New in MongoDB 4.4  * $type (Query) - filters fields based on BSON type. \nBEHAVIOR  $TYPE \nUnlike the $type query operator, which matches array elements based on their BSON type, the $type aggregation operator does not examine array elements. Instead, when passed an array as its argument, the $type aggregation operator returns the type of the argument, i.e. \"array\". If the argument is a field that is missing in the input document, $type returns the string \"missing\". The following table shows the $type output for several common types of expressions: Example\nResults\n{ $type: \"a\" }\n\"string\"\n{ $type: /a/ }\n\"regex\"\n{ $type: 1 }\n\"double\"\n{ $type: NumberLong(627) }\n\"long\"\n{ $type: { x: 1 } }\n\"object\"\n{ $type: [ [ 1, 2, 3 ] ] }\n\"array\" \nNOTE In the case of a literal array such as [ 1, 2, 3 ], enclose the expression in an outer set of array brackets to prevent MongoDB from parsing [ 1, 2, 3 ] as an argument list with three arguments (1, 2, 3). Wrapping the array [ 1, 2, 3 ] in a $literal expression achieves the same result. See operator expression syntax forms for more information. \nAVAILABLE TYPES \nType\nNumber\nAlias\nNotes\nDouble\n1\n\"double\" String\n2\n\"string\" Object\n3\n\"object\" Array\n4\n\"array\" Binary data\n5\n\"binData\" Undefined\n6\n\"undefined\"\nDeprecated.\nObjectId\n7\n\"objectId\" Boolean\n8\n\"bool\" Date\n9\n\"date\" Null\n10\n\"null\" Regular Expression\n11\n\"regex\" DBPointer\n12\n\"dbPointer\"\nDeprecated.\nJavaScript\n13\n\"javascript\" Symbol\n14\n\"symbol\"\nDeprecated.\nJavaScript code with scope\n15\n\"javascriptWithScope\"\nDeprecated in MongoDB 4.4.\n32-bit integer\n16\n\"int\" Timestamp\n17\n\"timestamp\" 64-bit integer\n18\n\"long\" Decimal128\n19\n\"decimal\" Min key\n-1\n\"minKey\" Max key\n127\n\"maxKey\" \nIf the argument is a field that is missing in the input document, $type returns the string \"missing\". \nEXAMPLE \nThis example uses a collection named coll with the following documents: { _id: 0, a : 8 }{ _id: 1, a : [ 41.63, 88.19 ] }{ _id: 2, a : { a : \"apple\", b : \"banana\", c: \"carrot\" } }{ _id: 3, a :  \"caribou\" }{ _id: 4, a : NumberLong(71) }{ _id: 5 }  The following aggregation operation uses the $type operator to display the type of field a for all documents as part of the $project stage. db.coll.aggregate([{    $project: {       a : { $type: \"$a\" }    }}])  The operation returns the following: { _id: 0, \"a\" : \"double\" }{ _id: 1, \"a\" : \"array\" }{ _id: 2, \"a\" : \"object\" }{ _id: 3, \"a\" : \"string\" }{ _id: 4, \"a\" : \"long\" }{ _id: 5, \"a\" : \"missing\" } \n←  $trunc (aggregation)$unsetField (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/strcasecmp/": " Docs Home → MongoDB Manual \n$STRCASECMP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$strcasecmp \nPerforms case-insensitive comparison of two strings. Returns      * 1 if first string is \"greater than\" the second string.  * 0 if the two strings are equal.  * -1 if the first string is \"less than\" the second string. $strcasecmp has the following syntax: { $strcasecmp: [ <expression1>, <expression2> ] }  The arguments can be any valid expression as long as they resolve to strings. For more information on expressions, see Expressions. \nBEHAVIOR \n$strcasecmp only has a well-defined behavior for strings of ASCII characters. For a case sensitive comparison, see $cmp. \nEXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \"product 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", quarter: \"13Q4\", \"description\" : \"product 2\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $strcasecmp operator to perform case-insensitive comparison of the quarter field value to the string \"13q4\": db.inventory.aggregate(   [     {       $project:          {            item: 1,            comparisonResult: { $strcasecmp: [ \"$quarter\", \"13q4\" ] }          }      }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"comparisonResult\" : -1 }{ \"_id\" : 2, \"item\" : \"ABC2\", \"comparisonResult\" : 0 }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"comparisonResult\" : 1 } \n←  $stdDevSamp (aggregation)$strLenBytes (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/cosh/": " Docs Home → MongoDB Manual \n$COSH (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $cosh \nNew in version 4.2. Returns the hyperbolic cosine of a value that is measured in radians. $cosh has the following syntax: { $cosh: <expression> }  $cosh takes any valid expression that resolves to a number, measured in radians. If the expression returns a value in degrees, use the $degreesToRadians operator to convert the value to radians. By default $cosh returns values as a double. $cosh can also return values as a 128-bit decimal if the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the input argument resolves to a value of null or refers to a field that is missing, $cosh returns null. If the argument resolves to NaN, $cosh returns NaN. If the argument resolves to negative or positive Infinity, $cosh returns positive Infinity. Example\nResults\n{ $cosh: NaN }\nNaN\n{ $cosh: null }\nnull\n{ $cosh: -Infinity }\nInfinity\n{ $cosh: Infinity }\nInfinity \nEXAMPLE  \n←  $cos (aggregation)$count (aggregation accumulator) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/substrCP/": " Docs Home → MongoDB Manual \n$SUBSTRCP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$substrCP \nReturns the substring of a string. The substring starts with the character at the specified UTF-8 code point (CP) index (zero-based) in the string for the number of code points specified. $substrCP has the following operator expression syntax: { $substrCP: [ <string expression>, <code point index>, <code point count> ] } \nField\nType\nDescription\nstring expression\nstring The string from which the substring will be extracted. string expression can be any valid expression as long as it resolves to a string. For more information on expressions, see Expressions. If the argument resolves to a value of null or refers to a field that is missing, $substrCP returns an empty string. If the argument does not resolve to a string or null nor refers to a missing field, $substrCP returns an error. code point index\nnumber\nIndicates the starting point of the substring. code point index can be any valid expression as long as it resolves to a non-negative integer.\ncode point count\nnumber\nCan be any valid expression as long as it resolves to a non-negative integer or number that can be represented as an integer (such as 2.0). Example\nResults\n{ $substrCP: [ \"abcde\", 1, 2 ] }\n\"bc\"\n{ $substrCP: [ \"Hello World!\", 6, 5 ] }\n\"World\"\n{ $substrCP: [ \"cafétéria\", 0, 5 ] }\n\"cafét\"\n{ $substrCP: [ \"cafétéria\", 5, 4 ] }\n\"tér\"\n{ $substrCP: [ \"cafétéria\", 7, 3 ] }\n\"ia\"\n{ $substrCP: [ \"cafétéria\", 3, 1 ] }\n\"é\" \nBEHAVIOR \nThe $substrCP operator uses the code points to extract the substring. This behavior differs from the $substrBytes operator which extracts the substring by the number of bytes, where each character uses between one and four bytes. \nEXAMPLE  SINGLE-BYTE CHARACTER SET \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \"product 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", quarter: \"13Q4\", \"description\" : \"product 2\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $substrCP operator to separate the quarter value into a yearSubstring and a quarterSubstring. The quarterSubstring field represents the rest of the string from the specified byte index following the yearSubstring. It is calculated by subtracting the byte index from the length of the string using $strLenCP. db.inventory.aggregate(  [    {      $project: {        item: 1,        yearSubstring: { $substrCP: [ \"$quarter\", 0, 2 ] },        quarterSubtring: {          $substrCP: [            \"$quarter\", 2, { $subtract: [ { $strLenCP: \"$quarter\" }, 2 ] }          ]        }      }    }  ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"yearSubstring\" : \"13\", \"quarterSubtring\" : \"Q1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", \"yearSubstring\" : \"13\", \"quarterSubtring\" : \"Q4\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"yearSubstring\" : \"14\", \"quarterSubtring\" : \"Q2\" }  \nSINGLE-BYTE AND MULTIBYTE CHARACTER SET \nCreate a food collection with the following documents:\n\n  The following example uses the $substrCP operator to create a three byte menuCode from the name value: db.food.aggregate(  [    {      $project: {          \"name\": 1,          \"menuCode\": { $substrCP: [ \"$name\", 0, 3 ] }      }    }  ])  The operation returns the following results: { \"_id\" : 1, \"name\" : \"apple\", \"menuCode\" : \"app\" }{ \"_id\" : 2, \"name\" : \"banana\", \"menuCode\" : \"ban\" }{ \"_id\" : 3, \"name\" : \"éclair\", \"menuCode\" : \"écl\" }{ \"_id\" : 4, \"name\" : \"hamburger\", \"menuCode\" : \"ham\" }{ \"_id\" : 5, \"name\" : \"jalapeño\", \"menuCode\" : \"jal\" }{ \"_id\" : 6, \"name\" : \"pizza\", \"menuCode\" : \"piz\" }{ \"_id\" : 7, \"name\" : \"tacos\", \"menuCode\" : \"tac\" }{ \"_id\" : 8, \"name\" : \"寿司sushi\", \"menuCode\" : \"寿司s\" }  \nTIP \nSEE ALSO: $substrBytes ←  $substrBytes (aggregation)$subtract (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/setIsSubset/": " Docs Home → MongoDB Manual \n$SETISSUBSET (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$setIsSubset \nTakes two arrays and returns true when the first array is a subset of the second, including when the first array equals the second array, and false otherwise. $setIsSubset has the following syntax: { $setIsSubset: [ <expression1>, <expression2> ] }  The arguments can be any valid expression as long as they each resolve to an array. For more information on expressions, see Expressions. \nBEHAVIOR \n$setIsSubset performs set operation on arrays, treating arrays as sets. If an array contains duplicate entries, $setIsSubset ignores the duplicate entries. $setIsSubset ignores the order of the elements. If a set contains a nested array element, $setIsSubset does not descend into the nested array but evaluates the array at top-level. Example\nResult { $setIsSubset: [ [ \"a\", \"b\", \"a\" ], [ \"b\", \"a\" ] ] } true { $setIsSubset: [ [ \"a\", \"b\" ], [ [ \"a\", \"b\" ] ] ] } false \nEXAMPLE \nConsider an experiments collection with the following documents: { \"_id\" : 1, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"red\", \"blue\" ] }{ \"_id\" : 2, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"blue\", \"red\", \"blue\" ] }{ \"_id\" : 3, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"red\", \"blue\", \"green\" ] }{ \"_id\" : 4, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"green\", \"red\" ] }{ \"_id\" : 5, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ ] }{ \"_id\" : 6, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ [ \"red\" ], [ \"blue\" ] ] }{ \"_id\" : 7, \"A\" : [ \"red\", \"blue\" ], \"B\" : [ [ \"red\", \"blue\" ] ] }{ \"_id\" : 8, \"A\" : [ ], \"B\" : [ ] }{ \"_id\" : 9, \"A\" : [ ], \"B\" : [ \"red\" ] }  The following operation uses the $setIsSubset operator to determine if the A array is a subset of the B array: db.experiments.aggregate(   [     { $project: { A:1, B: 1, AisSubset: { $setIsSubset: [ \"$A\", \"$B\" ] }, _id:0 } }   ])  The operation returns the following results: { \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"red\", \"blue\" ], \"AisSubset\" : true }{ \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"blue\", \"red\", \"blue\" ], \"AisSubset\" : true }{ \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"red\", \"blue\", \"green\" ], \"AisSubset\" : true }{ \"A\" : [ \"red\", \"blue\" ], \"B\" : [ \"green\", \"red\" ], \"AisSubset\" : false }{ \"A\" : [ \"red\", \"blue\" ], \"B\" : [ ], \"AisSubset\" : false }{ \"A\" : [ \"red\", \"blue\" ], \"B\" : [ [ \"red\" ], [ \"blue\" ] ], \"AisSubset\" : false }{ \"A\" : [ \"red\", \"blue\" ], \"B\" : [ [ \"red\", \"blue\" ] ], \"AisSubset\" : false }{ \"A\" : [ ], \"B\" : [ ], \"AisSubset\" : true }{ \"A\" : [ ], \"B\" : [ \"red\" ], \"AisSubset\" : true } \n←  $setIntersection (aggregation)$setUnion (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/max/": " Docs Home → MongoDB Manual \n$MAX (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \nChanged in version 5.0. $max \nReturns the maximum value. $max compares both value and type, using the specified BSON comparison order for values of different types. $max is available in these stages:      * $addFields (Available starting in MongoDB 3.4)  * $bucket  * $bucketAuto  * $group  * $match stage that includes an $expr expression  * $project  * $replaceRoot (Available starting in MongoDB 3.4)  * $replaceWith (Available starting in MongoDB 4.2)  * $set (Available starting in MongoDB 4.2)  * $setWindowFields (Available starting in MongoDB 5.0) In MongoDB 3.2 and earlier, $max is available in the $group stage only. \nSYNTAX \nWhen used in the $bucket, $bucketAuto, $group, and $setWindowFields stages, $max has this syntax: { $max: <expression> } When used in other supported stages, $max has one of two syntaxes:  * $max has one specified expression as its operand:\n   \n   { $max: <expression> }  * $max has a list of specified expressions as its operand:\n   \n   { $max: [ <expression1>, <expression2> ... ]  } For more information on expressions, see Expressions. \nBEHAVIOR  NULL OR MISSING VALUES \nIf some, but not all, documents for the $max operation have either a null value for the field or are missing the field, the $max operator only considers the non-null and the non-missing values for the field. If all documents for the $max operation have null value for the field or are missing the field, the $max operator returns null for the maximum value. \nARRAY OPERAND \nIn the $group and $setWindowFields stages, if the expression resolves to an array, $max does not traverse the array and compares the array as a whole. In the other supported stages:  * With a single expression as its operand, if the expression resolves to an array, $max traverses into the array to operate on the numerical elements of the array to return a single value.  * With a list of expressions as its operand, if any of the expressions resolves to an array, $max does not traverse into the array but instead treats the array as a non-numerical value. \nEXAMPLES  USE IN $GROUP STAGE \nConsider a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 2, \"date\" : ISODate(\"2014-01-01T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"quantity\" : 1, \"date\" : ISODate(\"2014-02-03T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 5, \"date\" : ISODate(\"2014-02-03T09:05:00Z\") }{ \"_id\" : 4, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T08:00:00Z\") }{ \"_id\" : 5, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T09:05:00Z\") }  Grouping the documents by the item field, the following operation uses the $max accumulator to compute the maximum total amount and maximum quantity for each group of documents. db.sales.aggregate(   [     {       $group:         {           _id: \"$item\",           maxTotalAmount: { $max: { $multiply: [ \"$price\", \"$quantity\" ] } },           maxQuantity: { $max: \"$quantity\" }         }     }   ])  The operation returns the following results:\n\n \nUSE IN $PROJECT STAGE \nA collection students contains the following documents: { \"_id\": 1, \"quizzes\": [ 10, 6, 7 ], \"labs\": [ 5, 8 ], \"final\": 80, \"midterm\": 75 }{ \"_id\": 2, \"quizzes\": [ 9, 10 ], \"labs\": [ 8, 8 ], \"final\": 95, \"midterm\": 80 }{ \"_id\": 3, \"quizzes\": [ 4, 5, 5 ], \"labs\": [ 6, 5 ], \"final\": 78, \"midterm\": 70 }  The following example uses the $max in the $project stage to calculate the maximum quiz scores, the maximum lab scores, and the maximum of the final and the midterm: db.students.aggregate([   { $project: { quizMax: { $max: \"$quizzes\"}, labMax: { $max: \"$labs\" }, examMax: { $max: [ \"$final\", \"$midterm\" ] } } }])  The operation results in the following documents: { \"_id\" : 1, \"quizMax\" : 10, \"labMax\" : 8, \"examMax\" : 80 }{ \"_id\" : 2, \"quizMax\" : 10, \"labMax\" : 8, \"examMax\" : 95 }{ \"_id\" : 3, \"quizMax\" : 5, \"labMax\" : 6, \"examMax\" : 78 } \nUSE IN $SETWINDOWFIELDS STAGE \nNew in version 5.0. Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  This example uses $max in the $setWindowFields stage to output the maximum quantity of cake sales for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            maximumQuantityForState: {               $max: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"current\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.\n\n In this output, the maximum quantity for CA and WA is shown in the maximumQuantityForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"maximumQuantityForState\" : 162 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"maximumQuantityForState\" : 162 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"maximumQuantityForState\" : 162 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"maximumQuantityForState\" : 134 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"maximumQuantityForState\" : 134 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"maximumQuantityForState\" : 140 } ←  $map (aggregation)$maxN (aggregation accumulator) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/search/": " Docs Home → MongoDB Manual \n$SEARCH (AGGREGATION) \n$search aggregation pipleline stage performs a full-text search of the field or fields in an Atlas collection. The fields must be covered by an Atlas Search index. See:      * Index Definitions to learn more about creating and managing Atlas Search indexes.  * Atlas Search Aggregation Pipeline Stages to learn more about the $search pipeline stage syntax and usage.  * $search Operators to learn more about the $search aggregation pipeline stage operators. \nIMPORTANT The $search aggregation pipeline stage is only available for collections hosted on MongoDB Atlas cluster tiers running MongoDB version 4.2 or later. To learn more, see Atlas Search. ←  $sample (aggregation)$searchMeta (aggregation) → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toUpper/": " Docs Home → MongoDB Manual \n$TOUPPER (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toUpper \nConverts a string to uppercase, returning the result. $toUpper has the following syntax: { $toUpper: <expression> }  The argument can be any expression as long as it resolves to a string. For more information on expressions, see Expressions. If the argument resolves to null, $toUpper returns an empty string \"\". \nBEHAVIOR \n$toUpper only has a well-defined behavior for strings of ASCII characters. \nEXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \"PRODUCT 1\" }{ \"_id\" : 2, \"item\" : \"abc2\", quarter: \"13Q4\", \"description\" : \"Product 2\" }{ \"_id\" : 3, \"item\" : \"xyz1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $toUpper operator return uppercase item and uppercase description values: db.inventory.aggregate(   [     {       $project:         {           item: { $toUpper: \"$item\" },           description: { $toUpper: \"$description\" }         }     }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"description\" : \"PRODUCT 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", \"description\" : \"PRODUCT 2\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"description\" : \"\" } \n←  $toLower (aggregation)$tsIncrement (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/atan/": " Docs Home → MongoDB Manual \n$ATAN (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $atan \nNew in version 4.2. Returns the inverse tangent (arc tangent) of a value. $atan has the following syntax: { $atan: <expression> }  $atan takes any valid expression that resolves to a number. $atan returns values in radians. Use $radiansToDegrees operator to convert the output value from radians to degrees. By default $atan returns values as a double. $atan can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL AND NAN \nIf the argument resolves to a value of null or refers to a field that is missing, $atan returns null. If the argument resolves to NaN, $tan returns NaN. Example\nResults\n{ $atan: NaN }\nNaN\n{ $atan: null }\nnull \nEXAMPLE  \n←  $asinh (aggregation)$atan2 (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/tanh/": " Docs Home → MongoDB Manual \n$TANH (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $tanh \nNew in version 4.2. Returns the hyperbolic tangent of a value that is measured in radians. $tanh has the following syntax: { $tanh: <expression> }  $tanh takes any valid expression that resolves to a number, measured in radians. If the expression returns a value in degrees, use the $degreesToRadians operator to convert the value to radians. By default $tanh returns values as a double. $tanh can also return values as a 128-bit decimal if the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the input argument resolves to a value of null or refers to a field that is missing, $tanh returns null. If the argument resolves to NaN, $tanh returns NaN. If the argument resolves to negative or positive Infinity, $tanh returns -1 or 1 respectively. Example\nResults\n{ $tanh: NaN }\nNaN\n{ $tanh: null }\nnull\n{ $tanh: -Infinity }\n-1\n{ $tanh: Infinity }\n1 \nEXAMPLE  \n←  $tan (aggregation)$toBool (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/setField/": " Docs Home → MongoDB Manual \n$SETFIELD (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$setField \nNew in version 5.0. Adds, updates, or removes a specified field in a document. You can use $setField to add, update, or remove fields with names that contain periods (.) or start with dollar signs ($). \nTIP Use $getField to retrieve the values of fields that contain dollar signs ($) or periods (.) that you add or update with $setField. \nSYNTAX \n$setField has the following syntax: {  $setField: {    field: <String>,    input: <Object>,    value: <Expression>  }}  You must provide the following fields: Field\nType\nDescription\nfield\nString\nField in the input object that you want to add, update, or remove. field can be any valid expression that resolves to a string constant.\ninput\nObject\nDocument that contains the field that you want to add or update. input must resolve to an object, missing, null, or undefined.\nvalue\nExpression The value that you want to assign to field. value can be any valid expression. Set to $$REMOVE to remove field from the input document. \nBEHAVIOR \n     * If input evaluates to missing, undefined, or null, $setField returns null and does not update input.  * If input evaluates to anything other than an object, missing, undefined, or null, $setField returns an error.  * If field resolves to anything other than a string constant, $setField returns an error.  * If field doesn't exist in input, $setField adds it.  * $setField doesn't implicitly traverse objects or arrays. For example, $setField evaluates a field value of \"a.b.c\" as a top-level field \"a.b.c\" instead of as a nested field, { \"a\": { \"b\": { \"c\": } } }.  * $unsetField is an alias for $setField with an input value of $$REMOVE. The following expressions are equivalent:\n   \n   {   $setField: {      field: <field name>,      input: “$$ROOT”,      value: \"$$REMOVE\"   }}\n   {   $unsetField: {      field: <field name>,      input: “$$ROOT”   }}\n   \n    \nTIP \nSEE ALSO: Considerations for field names \nEXAMPLES  ADD FIELDS THAT CONTAIN PERIODS (.) \nConsider an inventory collection with the following documents: db.inventory.insertMany( [   { \"_id\" : 1, \"item\" : \"sweatshirt\", price: 45.99, qty: 300 }   { \"_id\" : 2, \"item\" : \"winter coat\", price: 499.99, qty: 200 }   { \"_id\" : 3, \"item\" : \"sun dress\", price: 199.99, qty: 250 }   { \"_id\" : 4, \"item\" : \"leather boots\", price: 249.99, qty: 300 }   { \"_id\" : 5, \"item\" : \"bow tie\", price: 9.99, qty: 180 }] )  The following operation uses the $replaceWith pipeline stage and the $setField operator to add a new field to each document, \"price.usd\". The value of \"price.usd\" will equal the value of \"price\" in each document. Finally, the operation uses the $unset pipeline stage to remove the \"price\" field. db.inventory.aggregate( [   { $replaceWith: {        $setField: {           field: \"price.usd\",           input: \"$$ROOT\",           value: \"$price\"   } } },   { $unset: \"price\" }] )  The operation returns the following results:\n\n \nADD FIELDS THAT START WITH A DOLLAR SIGN ($) \nConsider an inventory collection with the following documents: db.inventory.insertMany( [   { \"_id\" : 1, \"item\" : \"sweatshirt\", price: 45.99, qty: 300 }   { \"_id\" : 2, \"item\" : \"winter coat\", price: 499.99, qty: 200 }   { \"_id\" : 3, \"item\" : \"sun dress\", price: 199.99, qty: 250 }   { \"_id\" : 4, \"item\" : \"leather boots\", price: 249.99, qty: 300 }   { \"_id\" : 5, \"item\" : \"bow tie\", price: 9.99, qty: 180 }] )  The following operation uses the $replaceWith pipeline stage and the $setField and $literal operators to add a new field to each document, \"$price\". The value of \"$price\" will equal the value of \"price\" in each document. Finally, the operation uses the $unset pipeline stage to remove the \"price\" field. db.inventory.aggregate( [   { $replaceWith: {        $setField: {           field: { $literal: \"$price\" },           input: \"$$ROOT\",           value: \"$price\"   } } },   { $unset: \"price\" }] )  The operation returns the following results: [  { _id: 1, item: 'sweatshirt', qty: 300, '$price': 45.99 },  { _id: 2, item: 'winter coat', qty: 200, '$price': 499.99 },  { _id: 3, item: 'sun dress', qty: 250, '$price': 199.99 },  { _id: 4, item: 'leather boots', qty: 300, '$price': 249.99 },  { _id: 5, item: 'bow tie', qty: 180, '$price': 9.99 }] \nUPDATE FIELDS THAT CONTAIN PERIODS (.) \nConsider an inventory collection with the following documents: db.inventory.insertMany( [   { _id: 1, item: 'sweatshirt', qty: 300, 'price.usd': 45.99 },   { _id: 2, item: 'winter coat', qty: 200, 'price.usd': 499.99 },   { _id: 3, item: 'sun dress', qty: 250, 'price.usd': 199.99 },   { _id: 4, item: 'leather boots', qty: 300, 'price.usd': 249.99 },   { _id: 5, item: 'bow tie', qty: 180, 'price.usd': 9.99 }] )  The following operation uses the $match pipeline stage to find a specific document and the $replaceWith pipeline stage and the $setField operator to update the \"price.usd\" field in the matching document: db.inventory.aggregate( [   { $match: { _id: 1 } },   { $replaceWith: {        $setField: {           field: \"price.usd\",           input: \"$$ROOT\",           value: 49.99    } } }] )  The operation returns the following results: [  { _id: 1, item: 'sweatshirt', qty: 300, 'price.usd': 49.99 }] \nUPDATE FIELDS THAT START WITH A DOLLAR SIGN ($) \nConsider an inventory collection with the following documents:\n\n  The following operation uses the $match pipeline stage to find a specific document and the $replaceWith pipeline stage and the $setField and $literal operators to update the \"$price\" field in the matching document: db.inventory.aggregate( [   { $match: { _id: 1 } },   { $replaceWith: {        $setField: {           field: { $literal: \"$price\" },           input: \"$$ROOT\",           value: 49.99   } } }] )  The operation returns the following results: [  { _id: 1, item: 'sweatshirt', qty: 300, '$price': 49.99 }] \nREMOVE FIELDS THAT CONTAIN PERIODS (.) \nConsider an inventory collection with the following documents: db.inventory.insertMany([   { _id: 1, item: 'sweatshirt', qty: 300, 'price.usd': 45.99 },   { _id: 2, item: 'winter coat', qty: 200, 'price.usd': 499.99 },   { _id: 3, item: 'sun dress', qty: 250, 'price.usd': 199.99 },   { _id: 4, item: 'leather boots', qty: 300, 'price.usd': 249.99 },   { _id: 5, item: 'bow tie', qty: 180, 'price.usd': 9.99 }] )  The following operation uses the $replaceWith pipeline stage and the $setField operator and $$REMOVE to remove the \"price.usd\" field from each document: db.inventory.aggregate( [   { $replaceWith:  {        $setField: {           field: \"price.usd\",           input: \"$$ROOT\",           value: \"$$REMOVE\"   } } }] )  The operation returns the following results: [  { _id: 1, item: 'sweatshirt', qty: 300 },  { _id: 2, item: 'winter coat', qty: 200 },  { _id: 3, item: 'sun dress', qty: 250 },  { _id: 4, item: 'leather boots', qty: 300 },  { _id: 5, item: 'bow tie', qty: 180 }] A similar query written using the $unsetField alias returns the same results: db.inventory.aggregate( [   { $replaceWith:  {        $unsetField: {           field: \"price.usd\",           input: \"$$ROOT\"   } } }] )  \nREMOVE FIELDS THAT START WITH A DOLLAR SIGN ($) \nConsider an inventory collection with the following documents: db.inventory.insertMany( [   { _id: 1, item: 'sweatshirt', qty: 300, '$price': 45.99 },   { _id: 2, item: 'winter coat', qty: 200, '$price': 499.99 },   { _id: 3, item: 'sun dress', qty: 250, '$price': 199.99 },   { _id: 4, item: 'leather boots', qty: 300, '$price': 249.99 },   { _id: 5, item: 'bow tie', qty: 180, '$price': 9.99 }} )  The following operation uses the $replaceWith pipeline stage, the $setField and $literal operators, and $$REMOVE to remove the \"$price\" field from each document: db.inventory.aggregate( [   { $replaceWith: {        $setField: {           field: { $literal: \"$price\" },           input: \"$$ROOT\",           value: \"$$REMOVE\"   } } }] )  The operation returns the following results:\n\n A similar query written using the $unsetField alias returns the same results: db.inventory.aggregate( [  { $replaceWith: {       $unsetField: {          field: { $literal: \"$price\" },          input: \"$$ROOT\"  } } }] )  \nTIP \nSEE ALSO: $unsetField ←  $setEquals (aggregation)$setIntersection (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/filter/": " Docs Home → MongoDB Manual \n$FILTER (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$filter \nSelects a subset of an array to return based on the specified condition. Returns an array with only those elements that match the condition. The returned elements are in the original order. $filter has the following syntax: {   $filter:      {         input: <array>,         cond: <expression>,         as: <string>,         limit: <number expression>      }} \nField\nSpecification\ninput\nAn expression that resolves to an array.\ncond\nAn expression that resolves to a boolean value used to determine if an element should be included in the output array. The expression references each element of the input array individually with the variable name specified in as.\nas\nOptional. A name for the variable that represents each individual element of the input array. If no name is specified, the variable name defaults to this.\nlimit Optional. A number expression that restricts the number of matching array elements that $filter returns. You cannot specify a limit less than 1. The matching array elements are returned in the order they appear in the input array. If the specified limit is greater than the number of matching array elements, $filter returns all matching array elements. If the limit is null, $filter returns all matching array elements. For more information on expressions, see Expressions. \nBEHAVIOR \nExample\nResults {  $filter: {     input: [ 1, \"a\", 2, null, 3.1, NumberLong(4), \"5\" ],     as: \"num\",     cond: { $and: [        { $gte: [ \"$$num\", NumberLong(\"-9223372036854775807\") ] },        { $lte: [ \"$$num\", NumberLong(\"9223372036854775807\") ] }      ] }  }} [ 1, 2, 3.1, NumberLong(4) ] {  $filter: {     input: [ 1, \"a\", 2, null, 3.1, NumberLong(4), \"5\" ],     as: \"num\",     cond: { $and:[        { $gte: [ \"$$num\", NumberLong(\"-9223372036854775807\") ] },        { $gte: [ \"$$num\", NumberLong(\"9223372036854775807\") ] }     ] }     limit: 2  }} [ 1, 2 ] {  $filter: {     input: [ 1, \"a\", 2, null, 3.1, NumberLong(4), \"5\" ],     as: \"num\",     cond: { $and:[        { $gte: [ \"$$num\", NumberLong(\"-9223372036854775807\") ] },        { $gte: [ \"$$num\", NumberLong(\"9223372036854775807\") ] }     ] }     limit: { $add: [ 0, 1 ]}  }} [ 1 ] \nEXAMPLES \nA collection sales has the following documents:\n\n  The following example filters the items array to only include documents that have a price greater than or equal to 100: db.sales.aggregate( [   {      $project: {         items: {            $filter: {               input: \"$items\",               as: \"item\",               cond: { $gte: [ \"$$item.price\", 100 ] }            }         }      }   }] )  The operation produces the following results: {   \"_id\" : 0,   \"items\" : [      { \"item_id\" : 2, \"quantity\" : 1, \"price\" : 240 }   ]}{   \"_id\" : 1,   \"items\" : [      { \"item_id\" : 23, \"quantity\" : 3, \"price\" : 110 },      { \"item_id\" : 38, \"quantity\" : 1, \"price\" : 300 }   ]}{ \"_id\" : 2, \"items\" : [ ] }  \nUSING THE LIMIT FIELD \nThis example uses the sales collection from the previous example. The example uses the limit field to specifiy the number of matching elements returned in each items array. db.sales.aggregate( [   {      $project: {         items: {            $filter: {               input: \"$items\",               cond: { $gte: [ \"$$item.price\", 100 ] },               as: \"item\",               limit: 1            }         }      }   }] )  The operation produces the following results: {   \"_id\" : 0,   \"items\" : [      { \"item_id\" : 2, \"quantity\" : 1, \"price\" : 240 }   ]}{   \"_id\" : 1,   \"items\" : [      { \"item_id\" : 23, \"quantity\" : 3, \"price\" : 110 }   ]}{ \"_id\" : 2, \"items\" : [ ] }  \nLIMIT AS A NUMERIC EXPRESSION \nThis example uses the sales collection from the previous example. The following example uses a numeric expression for the limit field to specifiy the number of matching elements returned in each items array. db.sales.aggregate( [   {      $project: {         items: {            $filter: {               input: \"$items\",               cond: { $lte: [ \"$$item.price\", 150] },               as: \"item\",               limit: 2.000            }         }      }   }] )  The operation produces the following results:\n\n  \nLIMIT GREATER THAN POSSIBLE MATCHES \nThis example uses the sales collection from the previous example. The example uses a limit field value that is larger than the possible number of matching elements that can be returned. db.sales.aggregate( [   {      $project: {         items: {            $filter: {               input: \"$items\",               cond: { $gte: [ \"$$item.price\", 100] },               as: \"item\",               limit: 5            }         }      }   }] )  The operation produces the following results: [   {      \"_id\": 0,      \"items\": [      { \"item_id\": 2, \"quantity\": 1, \"price\": 240 }      ]   },   {      \"_id\": 1,      \"items\": [         { \"item_id\": 23, \"quantity\": 3, \"price\": 110 },         { \"item_id\": 38, \"quantity\": 1, \"price\": 300 }      ]   },   {      \"_id\": 2,      \"items\": []   }] \n←  $expMovingAvg (aggregation)$first (aggregation accumulator) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dateAdd/": " Docs Home → MongoDB Manual \n$DATEADD (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$dateAdd \nNew in version 5.0. Increments a Date object by a specified number of time units. The $dateAdd expression has the following syntax: {   $dateAdd: {      startDate: <Expression>,      unit: <Expression>,      amount: <Expression>,      timezone: <tzExpression>   }}  Returns a Date. The startDate can be any expression that resolves to type Date, Timestamp or ObjectId. No matter which data type is used as input, the value returned will be a Date object. Field\nRequired/Optional\nDescription\nstartDate\nRequired\nThe beginning date, in UTC, for the addition operation. The startDate can be any expression that resolves to a Date, a Timestamp, or an ObjectID.\nunit\nRequired The unit used to measure the amount of time added to the startDate. The unit is an expression that resolves to one of the following strings:      * year  * quarter  * week  * month  * day  * hour  * minute  * second  * millisecond amount\nRequired\nThe number of units added to the startDate. The amount is an expression that resolves to an integer or long. The amount can also resolve to an integral decimal or a double if that value can be converted to a long without loss of precision.\ntimezone\nOptional The timezone to carry out the operation. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC. Format\nExamples\nOlson Timezone Identifier \"America/New_York\"\"Europe/London\"\"GMT\" \nUTC Offset +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"  For more information on expressions and types see Expressions and BSON Types. \nBEHAVIOR  TIME MEASUREMENT \nMongoDB follows prevaling database usage and works with time in UTC. The dateAdd expression always takes a startDate in UTC and returns a result in UTC. If the timezone is specified, the calculation will be done using the specified timezone. The timezone is especially important when a calculation involves Daylight Savings Time (DST). If the unit is a month, or larger the operation adjusts to account for the last day of the month. Adding one month on the last day of October, for example, demonstrates the \"last-day-of-the-month\" adjustment. {   $dateAdd:      {         startDate: ISODate(\"2020-10-31T12:10:05Z\"),         unit: \"month\",         amount: 1      }}  Notice that the date returned, ISODate(\"2020-11-30T12:10:05Z\"), is the 30th and not the 31st since November has fewer days than October. \nTIME ZONE \nWhen using an Olson Timezone Identifier in the <timezone> field, MongoDB applies the DST offset if applicable for the specified timezone. For example, consider a sales collection with the following document: {   \"_id\" : 1,   \"item\" : \"abc\",   \"price\" : 20,   \"quantity\" : 5,   \"date\" : ISODate(\"2017-05-20T10:24:51.303Z\")}  The following aggregation illustrates how MongoDB handles the DST offset for the Olson Timezone Identifier. The example uses the $hour and $minute operators to return the corresponding portions of the date field:\n\n  The operation returns the following result: {   \"_id\": 1,   \"nycHour\" : 5,   \"nycMinute\" : 24,   \"gmtHour\" : 10,   \"gmtMinute\" : 24,   \"nycOlsonHour\" : 6,   \"nycOlsonMinute\" : 24}  \nEXAMPLES  ADD A FUTURE DATE \nConsider a collection of customer orders with these documents: db.shipping.insertMany(  [     { custId: 456, purchaseDate: ISODate(\"2020-12-31\") },     { custId: 457, purchaseDate: ISODate(\"2021-02-28\") },     { custId: 458, purchaseDate: ISODate(\"2021-02-26\") }  ])  The normal shipping time is 3 days. You can use $dateAdd in an aggregation pipeline to set an expectedDeliveryDate 3 days in the future. db.shipping.aggregate(   [      {         $project:            {               expectedDeliveryDate:                  {                     $dateAdd:                        {                           startDate: \"$purchaseDate\",                           unit: \"day\",                           amount: 3                        }                  }            }       },       {          $merge: \"shipping\"       }    ] )  After adding 3 days to the purchaseDate with $dateAdd in the $project stage, the $merge stage updates the original documents with the expectedDeliveryDate. The resulting documents look like this: {   \"_id\" : ObjectId(\"603dd4b2044b995ad331c0b2\"),   \"custId\" : 456,   \"purchaseDate\" : ISODate(\"2020-12-31T00:00:00Z\"),   \"expectedDeliveryDate\" : ISODate(\"2021-01-03T00:00:00Z\")}{   \"_id\" : ObjectId(\"603dd4b2044b995ad331c0b3\"),   \"custId\" : 457,   \"purchaseDate\" : ISODate(\"2021-02-28T00:00:00Z\"),   \"expectedDeliveryDate\" : ISODate(\"2021-03-03T00:00:00Z\")}{    \"_id\" : ObjectId(\"603dd4b2044b995ad331c0b4\"),   \"custId\" : 458,   \"purchaseDate\" : ISODate(\"2021-02-26T00:00:00Z\"),   \"expectedDeliveryDate\" : ISODate(\"2021-03-01T00:00:00Z\")} \nFILTER ON A DATE RANGE \nUpdate the shipping collection from the last example with this code to add delivery dates to the documents: db.shipping.updateOne(   { custId: 456 },   { $set: { deliveryDate: ISODate( \"2021-01-10\" ) } })\ndb.shipping.updateOne(  { custId: 457 },  { $set: { deliveryDate:  ISODate( \"2021-03-01\" ) } })\ndb.shipping.updateOne(   { custId: 458 },   { $set: { deliveryDate:  ISODate( \"2021-03-02\" ) } }) \n\n db.shipping.aggregate(   [      {         $match:            {               $expr:                  {                     $gt:                        [ \"$deliveryDate\",                          {                             $dateAdd:                                {                                   startDate: \"$purchaseDate\",                                   unit: \"day\",                                   amount: 5                                }                           }                        ]                  }            }       },       {          $project:             {                _id: 0,                custId: 1,                purchased:                   {                       $dateToString:                          {                             format: \"%Y-%m-%d\",                             date: \"$purchaseDate\"                          }                   },                delivery:                   {                      $dateToString:                         {                            format: \"%Y-%m-%d\",                            date: \"$deliveryDate\"                         }                   }             }       }   ]) \n\n The $project stage uses the $dateToString expression to convert the dates to a more readable format. Without the conversion, MongoDB would return the date in ISODate format. In this example only one record is returned: { \"custId\" : 456, \"purchased\" : \"2020-12-31\", \"delivery\" : \"2021-01-10\" } \nADJUST FOR DAYLIGHT SAVINGS TIME \nAll dates are stored internally in UTC time. When a timezone is specified, $dateAdd uses local time to carry out the calculations. The results are displayed in UTC. You have customers in several timezones and you want to see what effect daylight savings time might have on your billing periods if you bill by day or by hour. Create this collection of connection times: db.billing.insertMany(   [      {         location: \"America/New_York\",         login: ISODate(\"2021-03-13T10:00:00-0500\"),         logout: ISODate(\"2021-03-14T18:00:00-0500\")      },      {         location: \"America/Mexico_City\",         login: ISODate(\"2021-03-13T10:00:00-00:00\"),         logout: ISODate(\"2021-03-14T08:00:00-0500\")      }   ])  First add 1 day, then add 24 hours to the login dates in each document.\n\n  The $dateToString expression reformats the output for readability. Results are summarized here: Field\nNew York\nMexico City\nStart\n2021-03-13 15:00\n2021-03-13 10:00\nStart, TZ Info\n2021-03-13 10:00\n2021-03-13 04:00\n1 Day\n2021-03-14 14:00\n2021-03-14 10:00\n1 Day, TZ Info\n2021-03-14 10:00\n2021-03-14 04:00\n24 Hours\n2021-03-14 15:00\n2021-03-14 10:00\n24 Hours, TZ Info\n2021-03-14 11:00\n2021-03-14 04:00 The chart highlights several points:  * Unformatted dates are returned in UTC. The $login for New York is UTC -5, however the start, days, and hours rows display the time in UTC.  * March 14th is the start of DST in New York, but not in Mexico. The calculated time is adjusted when a location switches to DST and crosses from one day to the next.  * DST modifies the length of the day, not the hour. There is no DST change for hours. There is an only an adjustment for DST when the measurement unit is day or larger and the computation crosses a clock change in the specified timezone. \nTIP \nSEE ALSO: $dateSubtract, $dateDiff ←  $covarianceSamp (aggregation)$dateDiff (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/replaceWith/": " Docs Home → MongoDB Manual \n$REPLACEWITH (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$replaceWith \nNew in version 4.2. Replaces the input document with the specified document. The operation replaces all existing fields in the input document, including the _id field. With $replaceWith, you can promote an embedded document to the top-level. You can also specify a new document as the replacement. The $replaceWith is an alias for $replaceRoot. The $replaceWith stage has the following form: { $replaceWith: <replacementDocument> }  The replacement document can be any valid expression that resolves to a document. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the <replacementDocument> is not a document, $replaceWith errors and fails. If the <replacementDocument> resolves to a missing document (i.e. the document does not exist), $replaceWith errors and fails. For example, create a collection with the following documents: db.collection.insertMany([   { \"_id\": 1, \"name\" : { \"first\" : \"John\", \"last\" : \"Backus\" } },   { \"_id\": 2, \"name\" : { \"first\" : \"John\", \"last\" : \"McCarthy\" } },   { \"_id\": 3, \"name\": { \"first\" : \"Grace\", \"last\" : \"Hopper\" } },   { \"_id\": 4, \"firstname\": \"Ole-Johan\", \"lastname\" : \"Dahl\" },])  Then the following $replaceWith operation fails because one of the document does not have the name field: db.collection.aggregate([   { $replaceWith: \"$name\" }])  To avoid the error, you can use $mergeObjects to merge the name document with some default document; for example: db.collection.aggregate([   { $replaceWith: { $mergeObjects: [ { _id: \"$_id\", first: \"\", last: \"\" }, \"$name\" ] } }])  Alternatively, you can skip the documents that are missing the name field by including a $match stage to check for existence of the document field before passing documents to the $replaceWith stage: db.collection.aggregate([   { $match: { name : { $exists: true, $not: { $type: \"array\" }, $type: \"object\" } } },   { $replaceWith: \"$name\" }])  Or, you can use $ifNull expression to specify some other document to be root; for example: db.collection.aggregate([   { $replaceWith: { $ifNull: [ \"$name\", { _id: \"$_id\", missingName: true} ] } }])  \nEXAMPLES  $REPLACEWITH AN EMBEDDED DOCUMENT FIELD \nCreate a collection named people with the following documents: db.people.insertMany([   { \"_id\" : 1, \"name\" : \"Arlene\", \"age\" : 34, \"pets\" : { \"dogs\" : 2, \"cats\" : 1 } },   { \"_id\" : 2, \"name\" : \"Sam\", \"age\" : 41, \"pets\" : { \"cats\" : 1, \"fish\" : 3 } },   { \"_id\" : 3, \"name\" : \"Maria\", \"age\" : 25 }])  The following operation uses the $replaceWith stage to replace each input document with the result of a $mergeObjects operation. The $mergeObjects expression merges the specified default document with the pets document. db.people.aggregate( [   { $replaceWith: { $mergeObjects:  [ { dogs: 0, cats: 0, birds: 0, fish: 0 }, \"$pets\" ] } }] )  The operation returns the following results: { \"dogs\" : 2, \"cats\" : 1, \"birds\" : 0, \"fish\" : 0 }{ \"dogs\" : 0, \"cats\" : 1, \"birds\" : 0, \"fish\" : 3 }{ \"dogs\" : 0, \"cats\" : 0, \"birds\" : 0, \"fish\" : 0 } \n$REPLACEWITH A DOCUMENT NESTED IN AN ARRAY \nA collection named students contains the following documents:\n\n  The following operation promotes the embedded document(s) with the grade field greater than or equal to 90 to the top level: db.students.aggregate( [   { $unwind: \"$grades\" },   { $match: { \"grades.grade\" : { $gte: 90 } } },   { $replaceWith: \"$grades\" }] )  The operation returns the following results: { \"test\" : 3, \"grade\" : 95, \"mean\" : 85, \"std\" : 6 }{ \"test\" : 1, \"grade\" : 90, \"mean\" : 75, \"std\" : 6 }{ \"test\" : 3, \"grade\" : 91, \"mean\" : 85, \"std\" : 4 }  \n$REPLACEWITH A NEWLY CREATED DOCUMENT \nEXAMPLE 1 \nAn example collection sales is populated with the following documents: db.sales.insertMany([   { \"_id\" : 1, \"item\" : \"butter\", \"price\" : 10, \"quantity\": 2, date: ISODate(\"2019-03-01T08:00:00Z\"), status: \"C\" },   { \"_id\" : 2, \"item\" : \"cream\", \"price\" : 20, \"quantity\": 1, date: ISODate(\"2019-03-01T09:00:00Z\"), status: \"A\" },   { \"_id\" : 3, \"item\" : \"jam\", \"price\" : 5, \"quantity\": 10, date: ISODate(\"2019-03-15T09:00:00Z\"), status: \"C\" },   { \"_id\" : 4, \"item\" : \"muffins\", \"price\" : 5, \"quantity\": 10, date: ISODate(\"2019-03-15T09:00:00Z\"), status: \"C\" }])  Assume that for reporting purposes, you want to calculate for each completed sale, the total amount as of the current report run time. The following operation finds all the sales with status C and creates new documents using the $replaceWith stage. The $replaceWith calculates the total amount as well as uses the variable NOW to get the current time. db.sales.aggregate([   { $match: { status: \"C\" } },   { $replaceWith: { _id: \"$_id\", item: \"$item\", amount: { $multiply: [ \"$price\", \"$quantity\"]}, status: \"Complete\", asofDate: \"$$NOW\" } }])  The operation returns the following documents: { \"_id\" : 1, \"item\" : \"butter\", \"amount\" : 20, \"status\" : \"Complete\", \"asofDate\" : ISODate(\"2019-06-03T22:47:54.812Z\") }{ \"_id\" : 3, \"item\" : \"jam\", \"amount\" : 50, \"status\" : \"Complete\", \"asofDate\" : ISODate(\"2019-06-03T22:47:54.812Z\") }{ \"_id\" : 4, \"item\" : \"muffins\", \"amount\" : 50, \"status\" : \"Complete\", \"asofDate\" : ISODate(\"2019-06-03T22:47:54.812Z\") }  EXAMPLE 2 \nAn example collection reportedsales is populated with the reported sales information by quarter and regions: db.reportedsales.insertMany( [   { _id: 1, quarter: \"2019Q1\", region: \"A\", qty: 400 },   { _id: 2, quarter: \"2019Q1\", region: \"B\", qty: 550 },   { _id: 3, quarter: \"2019Q1\", region: \"C\", qty: 1000 },   { _id: 4, quarter: \"2019Q2\", region: \"A\", qty: 660 },   { _id: 5, quarter: \"2019Q2\", region: \"B\", qty: 500 },   { _id: 6, quarter: \"2019Q2\", region: \"C\", qty: 1200 }] )  Assume that for reporting purposes, you want to view the reported sales data by quarter; e.g. { \"_id\" : \"2019Q1\", \"A\" : 400, \"B\" : 550, \"C\" : 1000 } To view the data grouped by quarter, you can use the following aggregation pipeline:\n\n \nFirst stage: The $addFields stage adds a new obj document field that defines the key k as the region value and the value v as the quantity for that region. For example: { \"_id\" : 1, \"quarter\" : \"2019Q1\", \"region\" : \"A\", \"qty\" : 400, \"obj\" : { \"k\" : \"A\", \"v\" : 400 } } Second stage: The $group stage groups by the quarter and uses $push to accumulate the obj fields into a new items array field. For example: { \"_id\" : \"2019Q1\", \"items\" : [ { \"k\" : \"A\", \"v\" : 400 }, { \"k\" : \"B\", \"v\" : 550 }, { \"k\" : \"C\", \"v\" : 1000 } ] } Third stage: The $project stage uses $concatArrays to create a new array items2 that includes the _id info and the elements from the items array: { \"_id\" : \"2019Q1\", \"items2\" : [ { \"k\" : \"_id\", \"v\" : \"2019Q1\" }, { \"k\" : \"A\", \"v\" : 400 }, { \"k\" : \"B\", \"v\" : 550 }, { \"k\" : \"C\", \"v\" : 1000 } ] } Fourth stage: The $replaceWith uses the $arrayToObject to convert the items2 into a document, using the specified key k and value v pairs and outputs that document to the next stage. For example: { \"_id\" : \"2019Q1\", \"A\" : 400, \"B\" : 550, \"C\" : 1000 } The aggregation returns the following document: { \"_id\" : \"2019Q1\", \"A\" : 400, \"B\" : 550, \"C\" : 1000 }{ \"_id\" : \"2019Q2\", \"A\" : 660, \"B\" : 500, \"C\" : 1200 } \n$REPLACEWITH A NEW DOCUMENT CREATED FROM $$ROOT AND A DEFAULT DOCUMENT \nCreate a collection named contacts with the following documents: db.contacts.insertMany( [   { \"_id\" : 1, name: \"Fred\", email: \"fred@example.net\" },   { \"_id\" : 2, name: \"Frank N. Stine\", cell: \"012-345-9999\" },   { \"_id\" : 3, name: \"Gren Dell\", cell: \"987-654-3210\", email: \"beo@example.net\" }] )  The following operation uses $replaceWith with $mergeObjects to output current documents with default values for missing fields: db.contacts.aggregate( [   { $replaceWith:      { $mergeObjects:         [            { _id: \"\", name: \"\", email: \"\", cell: \"\", home: \"\" },            \"$$ROOT\"         ]      }   }] )  The aggregation returns the following documents: {  _id: 1,  name: 'Fred',  email: 'fred@example.net',  cell: '',  home: ''},{  _id: 2,  name: 'Frank N. Stine',  email: '',  cell: '012-345-9999',  home: ''},{  _id: 3,  name: 'Gren Dell',  email: 'beo@example.net',  cell: '',  home: '987-654-3210'} ←  $replaceRoot (aggregation)$sample (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toString/": " Docs Home → MongoDB Manual \n$TOSTRING (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toString \nConverts a value to a string. If the value cannot be converted to a string, $toString errors. If the value is null or missing, $toString returns null. $toString has the following syntax: {   $toString: <expression>}  The $toString takes any valid expression. The $toString is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"string\" } }  \nTIP \nSEE ALSO:      * $convert  * $dateToString \nBEHAVIOR \nThe following table lists the input types that can be converted to a string: Input Type\nBehavior\nBoolean\nReturns the boolean value as a string.\nDouble\nReturns the double value as a string.\nDecimal\nReturns the decimal value as a string.\nInteger\nReturns the integer value as a string.\nLong\nReturns the long value as a string.\nObjectId\nReturns the ObjectId value as a hexadecimal string..\nString\nNo-op. Returns the string value.\nDate\nReturns the date as a string. The following table lists some conversion to string examples: Example\nResults\n{$toString: true}\n\"true\"\n{$toString: false}\n\"false\"\n{$toString: 2.5}\n\"2.5\"\n{$toString: NumberInt(2)}\n\"2\"\n{$toString: NumberLong(1000)}\n\"1000\"\n{$toString: ObjectId(\"5ab9c3da31c2ab715d421285\")}\n\"5ab9c3da31c2ab715d421285\"\n{$toString: ISODate(\"2018-03-27T16:58:51.538Z\")}\n\"2018-03-27T16:58:51.538Z\" \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: 1, item: \"apple\",  qty: 5, zipcode: 93445 },   { _id: 2, item: \"almonds\", qty: 2, zipcode: \"12345-0030\" },   { _id: 3, item: \"peaches\",  qty: 5, zipcode: 12345 },] )  The following aggregation operation on the orders collection converts the zipcode to string before sorting by the string value: // Define stage to add convertedZipCode field with the converted zipcode value\nzipConversionStage = {   $addFields: {      convertedZipCode: { $toString: \"$zipcode\" }   }};\n// Define stage to sort documents by the converted zipcode\nsortStage = {   $sort: { \"convertedZipCode\": 1 }};\ndb.orders.aggregate( [  zipConversionStage,  sortStage] )  The operation returns the following documents: {  _id: 3,  item: 'peaches',  qty: 5,  zipcode: 12345,  convertedZipCode: '12345'},{  _id: 2,  item: 'almonds',  qty: 2,  zipcode: '12345-0030',  convertedZipCode: '12345-0030'},{  _id: 1,  item: 'apple',  qty: 5,  zipcode: 93445,  convertedZipCode: '93445'}  \nNOTE If the conversion operation encounters an error, the aggregation operation stops and throws an error. To override this behavior, use $convert instead. ←  $topN (aggregation accumulator)$toLower (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/lastN/": " Docs Home → MongoDB Manual \n$LASTN (AGGREGATION ACCUMULATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Restrictions\n * Examples \nDEFINITION \n$lastN \nNew in version 5.2. Returns an aggregation of the last n elements within a group. The elements returned are meaningful only if in a specified sort order. If the group contains fewer than n elements, $lastN returns all elements in the group. \nSYNTAX \n{   $lastN:      {         input: <expression>,         n: <expression>      }}      * input specifies the field(s) from the document to take the last n of. Input can be any expression.  * n has to be a positive integral expression that is either a constant or depends on the _id value for $group. For details see group key example. \nBEHAVIOR  NULL AND MISSING VALUES \n * $lastN does not filter out null values.  * $lastN converts missing values to null. Consider the following aggregation that returns the last five documents from a group: db.aggregate( [   {      $documents: [         { playerId: \"PlayerA\", gameId: \"G1\", score: 1 },         { playerId: \"PlayerB\", gameId: \"G1\", score: 2 },         { playerId: \"PlayerC\", gameId: \"G1\", score: 3 },         { playerId: \"PlayerD\", gameId: \"G1\"},         { playerId: \"PlayerE\", gameId: \"G1\", score: null }      ]   },   {      $group:      {         _id: \"$gameId\",         lastFiveScores:            {               $lastN:                  {                     input: \"$score\",                     n: 5                  }            }      }   }] )  In this example:  * $documents creates the literal documents that contain player scores.  * $group groups the documents by gameId. This example has only one gameId, G1.  * PlayerD has a missing score and PlayerE has a null score. These values are both considered as null.  * The lastFiveScores field is specified using input : \"$score\" and returned as an array.  * Since there is no sort criteria the last 5 score fields are returned. [   {      _id: \"G1\",      lastFiveScores: [ 1, 2, 3, null, null ]   }] \nCOMPARISON OF $LASTN AND $BOTTOMN \nBoth $lastN and $bottomN accumulators can accomplish similar results. In general:  * If the documents coming into $group are already ordered, you should use $lastN.  * If you're sorting and selecting the bottom n elements then you can use $bottomN to accomplish both tasks with one accumulator.  * $lastN can be used as an aggregation expression, $bottomN cannot. \nRESTRICTIONS  WINDOW FUNCTION AND AGGREGATION EXPRESSION SUPPORT \n$lastN is supported as an aggregation expression. For details on aggregation expression usage see Using $lastN as an Aggregation Expression. $lastN is supported as a window operator. \nMEMORY LIMIT CONSIDERATIONS \nAggregation pipelines which call $lastN are subject to the 100 MB limit. If this limit is exceeded for an individual group, the aggregation fails with an error. \nEXAMPLES \nConsider a gamescores collection with the following documents:\n\n  \nFIND THE LAST THREE PLAYER SCORES FOR A SINGLE GAME \nYou can use the $lastN accumulator to find the last three scores in a single game. db.gamescores.aggregate( [   {      $match : { gameId : \"G1\" }   },   {      $group:         {            _id: \"$gameId\",            lastThreeScores:               {                  $lastN:                  {                     input: [\"$playerId\", \"$score\"],                     n:3                  }               }         }   }] )  The example pipeline:  * Uses $match to filter the results on a single gameId. In this case, G1.  * Uses $group to group the results by gameId. In this case, G1.  * Specifies the fields that are output from $lastN with output : [\"$playerId\",\" $score\"].  * Uses $lastN to return the last three documents for the G1 game with n : 3. The operation returns the following results: [   {      _id: \"G1\",      lastThreeScores: [ [ \"PlayerB\", 33 ], [ \"PlayerC\", 99 ], [ \"PlayerD\", 1 ] ]   }] \nFINDING THE LAST THREE PLAYER SCORES ACROSS MULTIPLE GAMES \nYou can use the $lastN accumulator to find the last n input fields in each game. db.gamescores.aggregate( [      {         $group:         {         _id: \"$gameId\", playerId:            {               $lastN:                  {                     input: [ \"$playerId\",\"$score\" ],                     n: 3                  }            }         }      }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Uses $lastN to return the last three documents for each game with n: 3.  * Specifies the fields that are input for $lastN with input : [\"$playerId\", \"$score\"]. The operation returns the following results: [   {      _id: 'G2',      playerId: [ [ 'PlayerB', 14 ], [ 'PlayerC', 66 ], [ 'PlayerD', 80 ] ]   },   {      _id: 'G1',      playerId: [ [ 'PlayerB', 33 ], [ 'PlayerC', 99 ], [ 'PlayerD', 1 ] ]   }] \nUSING $SORT WITH $LASTN \nUsing a $sort stage earlier in the pipeline can influence the results of the $lastN accumulator. In this example:  * {$sort : { score : -1 } } sorts the highest scores to the back of each group.  * lastN returns the three lowest scores from the back of each group.\n\n  The operation returns the following results: [   {      _id: 'G2',      playerId: [ [ 'PlayerC', 66 ], [ 'PlayerB', 14 ], [ 'PlayerA', 10 ] ]   },   {      _id: 'G1',      playerId: [ [ 'PlayerB', 33 ], [ 'PlayerA', 31 ], [ 'PlayerD', 1 ] ]   }] \nCOMPUTING N BASED ON THE GROUP KEY FOR $GROUP \nYou can also assign the value of n dynamically. In this example, the $cond expression is used on the gameId field. db.gamescores.aggregate([   {      $group:      {         _id: {\"gameId\": \"$gameId\"},         gamescores:            {               $lastN:                  {                     input: \"$score\",                     n: { $cond: { if: {$eq: [\"$gameId\",\"G2\"] }, then: 1, else: 3 } }                  }            }      }   }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Specifies the fields that input for $lastN with input : \"$score\".  * If the gameId is G2 then n is 1, otherwise n is 3. The operation returns the following results: [   { _id: { gameId: \"G1\" }, gamescores: [ 33, 99, 1 ] },   { _id: { gameId: \"G2\" }, gamescores: [ 80 ] }] \nUSING $LASTN AS AN AGGREGATION EXPRESSION \nYou can also use $lastN as an aggregation expression. In this example:  * $documents creates the literal document that contains an array of values.  * $project is used to return the output of $lastN.  * _id is omited from the output with _id : 0.  * $lastN uses the input array of [10, 20, 30, 40].  * The last three elements of the array are returned for the input document. db.aggregate( [   {      $documents: [         { array: [10, 20, 30, 40] } ]   },   { $project: {      lastThreeElements:{                           $lastN:                           {                              input: \"$array\",                              n: 3                           }                        }               }   }] )  The operation returns the following results: [ { lastThreeElements: [ 20, 30, 40 ] } ] ←  $last (aggregation accumulator)$last (array operator) → On this page  * Definition\n * Syntax\n * Behavior\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/unionWith/": " Docs Home → MongoDB Manual \n$UNIONWITH (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Considerations\n * Duplicates Results\n * $unionWith a Sharded Collection\n * Collation\n * Atlas Search Support\n * Restrictions\n * Examples\n * Create a Yearly Report from the Union of Quarterly Data Collections \nDEFINITION \n$unionWith \nNew in version 4.4. Performs a union of two collections. $unionWith combines pipeline results from two collections into a single result set. The stage outputs the combined result set (including duplicates) to the next stage. The order in which the combined result set documents are output is unspecified. \nSYNTAX \nThe $unionWith stage has the following syntax: { $unionWith: { coll: \"<collection>\", pipeline: [ <stage1>, ... ] } }  To include all documents from the specified collection without any processing, you can use the simplified form: { $unionWith: \"<collection>\" }  // Include all documents from the specified collection  The $unionWith stage takes a document with the following fields: Field\nDescription\ncoll The collection or view whose pipeline results you wish to include in the result set. pipeline Optional. An aggregation pipeline to apply to the specified coll. [ <stage1>, <stage2>, ...] The pipeline cannot include the $out and $merge stages. Starting in v6.0, the pipeline can contain the Atlas Search $search stage as the first stage inside the pipeline. To learn more, see Atlas Search Support. The $unionWith operation would correspond to the following SQL statement: SELECT *FROM Collection1WHERE ...UNION ALLSELECT *FROM Collection2WHERE ...  \nCONSIDERATIONS  DUPLICATES RESULTS \nThe combined results from the previous stage and the $unionWith stage can include duplicates. For example, create a suppliers collection: db.suppliers.insertMany([  { _id: 1, supplier: \"Aardvark and Sons\", state: \"Texas\" },  { _id: 2, supplier: \"Bears Run Amok.\", state: \"Colorado\"},  { _id: 3, supplier: \"Squid Mark Inc. \", state: \"Rhode Island\" },])  db.warehouses.insertMany([  { _id: 1, warehouse: \"A\", region: \"West\", state: \"California\" },  { _id: 2, warehouse: \"B\", region: \"Central\", state: \"Colorado\"},  { _id: 3, warehouse: \"C\", region: \"East\", state: \"Florida\" },])  The following aggregation which combines the results from the state field projection from the suppliers collection with the results from the state field projection from the warehouse collection: db.suppliers.aggregate([   { $project: { state: 1, _id: 0 } },   { $unionWith: { coll: \"warehouses\", pipeline: [ { $project: { state: 1, _id: 0 } } ]} }])  As can be seen from the returned documents, the result set contains duplicates: { \"state\" : \"Texas\" }{ \"state\" : \"Colorado\" }{ \"state\" : \"Rhode Island\" }{ \"state\" : \"California\" }{ \"state\" : \"Colorado\" }{ \"state\" : \"Florida\" }  To remove the duplicates, you can include a $group stage to group by the state field: db.suppliers.aggregate([   { $project: { state: 1, _id: 0 } },   { $unionWith: { coll: \"warehouses\", pipeline: [ { $project: { state: 1, _id: 0 } } ]} },   { $group: { _id: \"$state\" } }])  The result set no longer contains duplicates: { \"_id\" : \"California\" }{ \"_id\" : \"Texas\" }{ \"_id\" : \"Florida\" }{ \"_id\" : \"Colorado\" }{ \"_id\" : \"Rhode Island\" }  \n$UNIONWITH A SHARDED COLLECTION \nIf the $unionWith stage is part of the $lookup pipeline, the $unionWith coll cannot be sharded. For example, in the following aggregation operation, the inventory_q1 collection cannot be sharded:\n\n \nCOLLATION \nIf the db.collection.aggregate() includes a collation, that collation is used for the operation, ignoring any other collations. If the db.collection.aggregate() does not include a collation, the db.collection.aggregate() method uses the collation for the top-level collection/view on which the db.collection.aggregate() is run:      * If the $unionWith coll is a collection, its collation is ignored.  * If the $unionWith coll is a view, then its collation must match that of the top-level collection/view. Otherwise, the operation errors. \nATLAS SEARCH SUPPORT \nStarting in MongoDB 6.0, you can specify the Atlas Search $search or $searchMeta stage in the $unionWith pipeline to search collections on the Atlas cluster. The $search or the $searchMeta stage must be the first stage inside the $unionWith pipeline.  \nTo see an example of $unionWith with $search, see the Atlas Search tutorial Run an Atlas Search $search Query Using $unionWith. \nRESTRICTIONS \nRestrictions\nDescription\nTransactions\nAn aggregation pipeline cannot use $unionWith inside transactions.\nSharded Collection\nIf the $unionWith stage is part of the $lookup pipeline, the $unionWith coll cannot be sharded.\n$out\nThe $unionWith pipeline cannot include the $out stage.\n$merge\nThe $unionWith pipeline cannot include the $merge stage. \nEXAMPLES  CREATE A YEARLY REPORT FROM THE UNION OF QUARTERLY DATA COLLECTIONS \nCreate a sample sales2019q1 collection with the following documents: db.sales2019q1.insertMany([  { store: \"A\", item: \"Chocolates\", quantity: 150 },  { store: \"B\", item: \"Chocolates\", quantity: 50 },  { store: \"A\", item: \"Cookies\", quantity: 100 },  { store: \"B\", item: \"Cookies\", quantity: 120 },  { store: \"A\", item: \"Pie\", quantity: 10 },  { store: \"B\", item: \"Pie\", quantity: 5 }])  Create a sample sales2019q2 collection with the following documents: db.sales2019q2.insertMany([  { store: \"A\", item: \"Cheese\", quantity: 30 },  { store: \"B\", item: \"Cheese\", quantity: 50 },  { store: \"A\", item: \"Chocolates\", quantity: 125 },  { store: \"B\", item: \"Chocolates\", quantity: 150 },  { store: \"A\", item: \"Cookies\", quantity: 200 },  { store: \"B\", item: \"Cookies\", quantity: 100 },  { store: \"B\", item: \"Nuts\", quantity: 100 },  { store: \"A\", item: \"Pie\", quantity: 30 },  { store: \"B\", item: \"Pie\", quantity: 25 }])  Create a sample sales2019q3 collection with the following documents: db.sales2019q3.insertMany([  { store: \"A\", item: \"Cheese\", quantity: 50 },  { store: \"B\", item: \"Cheese\", quantity: 20 },  { store: \"A\", item: \"Chocolates\", quantity: 125 },  { store: \"B\", item: \"Chocolates\", quantity: 150 },  { store: \"A\", item: \"Cookies\", quantity: 200 },  { store: \"B\", item: \"Cookies\", quantity: 100 },  { store: \"A\", item: \"Nuts\", quantity: 80 },  { store: \"B\", item: \"Nuts\", quantity: 30 },  { store: \"A\", item: \"Pie\", quantity: 50 },  { store: \"B\", item: \"Pie\", quantity: 75 }])  Create a sample sales2019q4 collection with the following documents:\n\n  REPORT 1: ALL SALES BY QUARTER AND STORES AND ITEMS \nThe following aggregation uses $unionWith to combine documents from all four collections to create a yearly sales report that lists all sales by quarter and stores: db.sales2019q1.aggregate( [   { $set: { _id: \"2019Q1\" } },   { $unionWith: { coll: \"sales2019q2\", pipeline: [ { $set: { _id: \"2019Q2\" } } ] } },   { $unionWith: { coll: \"sales2019q3\", pipeline: [ { $set: { _id: \"2019Q3\" } } ] } },   { $unionWith: { coll: \"sales2019q4\", pipeline: [ { $set: { _id: \"2019Q4\" } } ] } },   { $sort: { _id: 1, store: 1, item: 1 } }] )  Specifically, the aggregation pipeline uses:  * a $set stage to update the _id field to contain the quarter. That is, the documents from this stage has the form:\n   \n   { \"_id\" : \"2019Q1\", \"store\" : \"A\", \"item\" : \"Chocolates\", \"quantity\" : 150 }{ \"_id\" : \"2019Q1\", \"store\" : \"B\", \"item\" : \"Chocolates\", \"quantity\" : 50 }...  * a sequence of $unionWith stages to combine all documents from the four collections; each also using the $set stage on its documents. That is, the documents are from all four collections and have the form:\n   \n   { \"_id\" : \"2019Q1\", \"store\" : \"A\", \"item\" : \"Chocolates\", \"quantity\" : 150 }{ \"_id\" : \"2019Q1\", \"store\" : \"B\", \"item\" : \"Chocolates\", \"quantity\" : 50 }...{ \"_id\" : \"2019Q2\", \"store\" : \"A\", \"item\" : \"Cheese\", \"quantity\" : 30 }{ \"_id\" : \"2019Q2\", \"store\" : \"B\", \"item\" : \"Cheese\", \"quantity\" : 50 }...{ \"_id\" : \"2019Q3\", \"store\" : \"A\", \"item\" : \"Cheese\", \"quantity\" : 50 }{ \"_id\" : \"2019Q3\", \"store\" : \"B\", \"item\" : \"Cheese\", \"quantity\" : 20 }...{ \"_id\" : \"2019Q4\", \"store\" : \"A\", \"item\" : \"Cheese\", \"quantity\" : 100 }{ \"_id\" : \"2019Q4\", \"store\" : \"B\", \"item\" : \"Cheese\", \"quantity\" : 100 }\n\n REPORT 2: AGGREGATED YEARLY SALES BY ITEMS \nThe following aggregation uses $unionWith to combine documents from all four collections to create a yearly sales report that lists the yearly sales quantity per item: db.sales2019q1.aggregate( [   { $unionWith: \"sales2019q2\" },   { $unionWith: \"sales2019q3\" },   { $unionWith: \"sales2019q4\" },   { $group: { _id: \"$item\", total: { $sum: \"$quantity\" } } },   { $sort: { total: -1 }}] )   * The sequence of $unionWith stages retrieve documents from the specified collections into the pipeline:\n   \n   { \"_id\" : ObjectId(\"5e505848c15f157c0793fec7\"), \"store\" : \"A\", \"item\" : \"Chocolates\", \"quantity\" : 150 }{ \"_id\" : ObjectId(\"5e505848c15f157c0793fec8\"), \"store\" : \"B\", \"item\" : \"Chocolates\", \"quantity\" : 50 }{ \"_id\" : ObjectId(\"5e505848c15f157c0793fec9\"), \"store\" : \"A\", \"item\" : \"Cookies\", \"quantity\" : 100 }{ \"_id\" : ObjectId(\"5e505848c15f157c0793feca\"), \"store\" : \"B\", \"item\" : \"Cookies\", \"quantity\" : 120 }{ \"_id\" : ObjectId(\"5e505848c15f157c0793fecb\"), \"store\" : \"A\", \"item\" : \"Pie\", \"quantity\" : 10 }{ \"_id\" : ObjectId(\"5e505848c15f157c0793fecc\"), \"store\" : \"B\", \"item\" : \"Pie\", \"quantity\" : 5 }{ \"_id\" : ObjectId(\"5e50584bc15f157c0793fecd\"), \"store\" : \"A\", \"item\" : \"Cheese\", \"quantity\" : 30 }{ \"_id\" : ObjectId(\"5e50584bc15f157c0793fece\"), \"store\" : \"B\", \"item\" : \"Cheese\", \"quantity\" : 50 }{ \"_id\" : ObjectId(\"5e50584bc15f157c0793fecf\"), \"store\" : \"A\", \"item\" : \"Chocolates\", \"quantity\" : 125 }{ \"_id\" : ObjectId(\"5e50584bc15f157c0793fed0\"), \"store\" : \"B\", \"item\" : \"Chocolates\", \"quantity\" : 150 }{ \"_id\" : ObjectId(\"5e50584bc15f157c0793fed1\"), \"store\" : \"A\", \"item\" : \"Cookies\", \"quantity\" : 200 }{ \"_id\" : ObjectId(\"5e50584bc15f157c0793fed2\"), \"store\" : \"B\", \"item\" : \"Cookies\", \"quantity\" : 100 }...  * The $group stage groups by the item field and uses $sum to calculate the yearly total sales quantity per item:\n   \n   { \"_id\" : \"Cookies\", \"total\" : 1720 }{ \"_id\" : \"Pie\", \"total\" : 395 }{ \"_id\" : \"Cheese\", \"total\" : 350 }{ \"_id\" : \"Chocolates\", \"total\" : 1250 }{ \"_id\" : \"Nuts\", \"total\" : 510 }  * The $sort stage orders the documents by descending total.\n   \n   { \"_id\" : \"Cookies\", \"total\" : 1940 }{ \"_id\" : \"Chocolates\", \"total\" : 1450 }{ \"_id\" : \"Nuts\", \"total\" : 510 }{ \"_id\" : \"Pie\", \"total\" : 410 }{ \"_id\" : \"Cheese\", \"total\" : 350 } Alternatively, you could specify the $group stage within each $unionWith stage:\n\n   * The first $group groups the 2019q1 sales totals by items:\n   \n   { \"_id\" : \"Pie\", \"total\" : 30 }{ \"_id\" : \"Cookies\", \"total\" : 440 }{ \"_id\" : \"Chocolates\", \"total\" : 400 }  * The sequence of $unionWith stages groups the sales total by the items from the specified collections into the pipeline:\n   \n   { \"_id\" : \"Cookies\", \"total\" : 440 }{ \"_id\" : \"Chocolates\", \"total\" : 400 }{ \"_id\" : \"Pie\", \"total\" : 30 }{ \"_id\" : \"Chocolates\", \"total\" : 275 }  // From sales2019q2{ \"_id\" : \"Nuts\", \"total\" : 100 }        // From sales2019q2{ \"_id\" : \"Cheese\", \"total\" : 80 }       // From sales2019q2{ \"_id\" : \"Pie\", \"total\" : 55 }          // From sales2019q2{ \"_id\" : \"Cookies\", \"total\" : 300 }     // From sales2019q2...                                      // Results from sales2019 q3 and q4 omitted for brevity  * The last $group stage groups these quarterly groupings:\n   \n   { \"_id\" : \"Cookies\", \"total\" : 1720 }{ \"_id\" : \"Pie\", \"total\" : 395 }{ \"_id\" : \"Cheese\", \"total\" : 350 }{ \"_id\" : \"Chocolates\", \"total\" : 1250 }{ \"_id\" : \"Nuts\", \"total\" : 510 }  * The $sort stage orders the documents by descending total.\n   \n   { \"_id\" : \"Cookies\", \"total\" : 1940 }{ \"_id\" : \"Chocolates\", \"total\" : 1450 }{ \"_id\" : \"Nuts\", \"total\" : 510 }{ \"_id\" : \"Pie\", \"total\" : 410 }{ \"_id\" : \"Cheese\", \"total\" : 350 } ←  $sortByCount (aggregation)$unset (aggregation) → On this page  * Definition\n * Syntax\n * Considerations\n * Duplicates Results\n * $unionWith a Sharded Collection\n * Collation\n * Atlas Search Support\n * Restrictions\n * Examples\n * Create a Yearly Report from the Union of Quarterly Data Collections Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/unsetField/": " Docs Home → MongoDB Manual \n$UNSETFIELD (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$unsetField \nNew in version 5.0. Removes a specified field in a document. You can use $unsetField to remove fields with names that contain periods (.) or that start with dollar signs ($). $unsetField is an alias for $setField using $$REMOVE to remove fields. \nSYNTAX \n$unsetField has the following syntax: {   $unsetField: {      field: <String>,      input: <Object>,   }}  You must provide the following fields: Field\nType\nDescription\nfield\nString\nField in the input object that you want to add, update, or remove. field can be any valid expression that resolves to a string constant.\ninput\nObject\nDocument that contains the field that you want to add or update. input must resolve to an object, missing, null, or undefined. \nBEHAVIOR \n     * If input evaluates to missing, undefined, or null, $unsetField returns null and does not update input.  * If input evaluates to anything other than an object, missing, undefined, or null, $unsetField returns an error.  * If field resolves to anything other than a string constant, $unsetField returns an error.  * If field doesn't exist in input, $unsetField adds it.  * $unsetField doesn't implicitly traverse objects or arrays. For example, $unsetField evaluates a field value of \"a.b.c\" as a top-level field \"a.b.c\" instead of as a nested field, { \"a\": { \"b\": { \"c\": } } }. \nEXAMPLES  REMOVE FIELDS THAT CONTAIN PERIODS (.) \nConsider the inventory collection: db.inventory.insertMany( [   { _id: 1, item: \"sweatshirt\", qty: 300, \"price.usd\": 45.99 },   { _id: 2, item: \"winter coat\", qty: 200, \"price.usd\": 499.99 },   { _id: 3, item: \"sun dress\", qty: 250, \"price.usd\": 199.99 },   { _id: 4, item: \"leather boots\", qty: 300, \"price.usd\": 249.99 },   { _id: 5, item: \"bow tie\", qty: 180, \"price.usd\": 9.99 } ] )  Use the $replaceWith pipeline stage and the $unsetField operator to remove the \"price.usd\" field from each document: db.inventory.aggregate( [   { $replaceWith: {        $unsetField: {           field: \"price.usd\",           input: \"$$ROOT\"   } } } ] )  The operation returns the following results: [  { _id: 1, item: 'sweatshirt', qty: 300 },  { _id: 2, item: 'winter coat', qty: 200 },  { _id: 3, item: 'sun dress', qty: 250 },  { _id: 4, item: 'leather boots', qty: 300 },  { _id: 5, item: 'bow tie', qty: 180 }] \nREMOVE FIELDS THAT START WITH A DOLLAR SIGN ($) \nConsider the inventory collection: db.inventory.insertMany( [   { _id: 1, item: \"sweatshirt\", qty: 300, \"$price\": 45.99 },   { _id: 2, item: \"winter coat\", qty: 200, \"$price\": 499.99 },   { _id: 3, item: \"sun dress\", qty: 250, \"$price\": 199.99 },   { _id: 4, item: \"leather boots\", qty: 300, \"$price\": 249.99 },   { _id: 5, item: \"bow tie\", qty: 180, \"$price\": 9.99 } ] )  Use the $replaceWith pipeline stage with the $unsetField and $literal operators to remove the \"$price\" field from each document:\n\n  The operation returns the following results: [  { _id: 1, item: 'sweatshirt', qty: 300 },  { _id: 2, item: 'winter coat', qty: 200 },  { _id: 3, item: 'sun dress', qty: 250 },  { _id: 4, item: 'leather boots', qty: 300 },  { _id: 5, item: 'bow tie', qty: 180 }] \nREMOVE A SUBFIELD \nConsider the inventory collection: db.inventory.insertMany( [   { _id: 1, item: \"sweatshirt\", qty: 300, \"price\": {\"usd\":45.99, \"euro\": 38.77 } },   { _id: 2, item: \"winter coat\", qty: 200, \"price\": { \"usd\": 499.99, \"euro\": 420.51 } },   { _id: 3, item: \"sun dress\", qty: 250, \"price\": { \"usd\": 199.99, \"euro\": 167.70 } },   { _id: 4, item: \"leather boots\", qty: 300, \"price\": { \"usd\": 249.99, \"euro\": 210.68 } },   { _id: 5, item: \"bow tie\", qty: 180, \"price\": { \"usd\": 9.99, \"euro\": 8.42 } } ] )  The \"price\" field contains a document with two subfields, \"usd\" and \"euro\". You cannot use \"price.euro\" to identify and remove \"euro\" because MongoDB parses \"price.euro\" as a top level field name that happens to contain a period (.). Use the $replaceWith pipeline stage with $setField and a nested $unsetField operation to remove the \"euro\" field: db.inventory.aggregate( [   { $replaceWith: {        $setField: {           field: \"price\",           input: \"$$ROOT\",           value: {              $unsetField: {                 field: \"euro\",                 input: { $getField: \"price\" }   } } } } }] )  The operation returns the following results: [   { _id: 1, item: \"sweatshirt\", qty: 300, price: { usd: 45.99 } },   { _id: 2, item: \"winter coat\", qty: 200, price: { usd: 499.99 } },   { _id: 3, item: \"sun dress\", qty: 250, price: { usd: 199.99 } },   { _id: 4, item: \"leather boots\", qty: 300, price: { usd: 249.99 } },   { _id: 5, item: \"bow tie\", qty: 180, price: { usd: 9.99 } }] \nTIP \nSEE ALSO: $setField ←  $type (aggregation)$week (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/asinh/": " Docs Home → MongoDB Manual \n$ASINH (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $asinh \nNew in version 4.2. Returns the inverse hyperbolic sine (hyperbolic arc sine) of a value. $asinh has the following syntax: { $asinh: <expression> }  $asinh takes any valid expression that resolves to a number. $asinh returns values in radians. Use $radiansToDegrees operator to convert the output value from radians to degrees. By default $asinh returns values as a double. $asinh can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $asinh returns null. If the argument resolves to NaN, $asinh returns NaN. If the argument resolves to negative or positive infinity, $asinh returns negative or positive infinity respectively. Example\nResults\n{ $asinh: NaN }\nNaN\n{ $asinh: null }\nnull\n{ $asinh : Infinity}\nInfinity\n{ $asinh : -Infinity }\n-Infinity \nEXAMPLE  \n←  $asin (aggregation)$atan (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/bottomN/": " Docs Home → MongoDB Manual \n$BOTTOMN (AGGREGATION ACCUMULATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Restrictions\n * Examples \nDEFINITION \n$bottomN \nNew in version 5.2. Returns an aggregation of the bottom n elements within a group, according to the specified sort order. If the group contains fewer than n elements, $bottomN returns all elements in the group. \nSYNTAX \n{   $bottomN:      {         n: <expression>,         sortBy: { <field1>: <sort order>, <field2>: <sort order> ... },         output: <expression>      }}      * n limits the number of results per group and has to be a positive integral expression that is either a constant or depends on the _id value for $group.  * sortBy specifies the order of results, with syntax similar to $sort.  * output represents the output for each element in the group and can be any expression. \nBEHAVIOR  NULL AND MISSING VALUES \n * $bottomN does not filter out null values.  * $bottomN converts missing values to null which are preserved in the output. db.aggregate( [   {      $documents: [         { playerId: \"PlayerA\", gameId: \"G1\", score: 1 },         { playerId: \"PlayerB\", gameId: \"G1\", score: 2 },         { playerId: \"PlayerC\", gameId: \"G1\", score: 3 },         { playerId: \"PlayerD\", gameId: \"G1\"},         { playerId: \"PlayerE\", gameId: \"G1\", score: null }      ]   },   {      $group:      {            _id: \"$gameId\",            playerId:               {                  $bottomN:                     {                        output: [ \"$playerId\", \"$score\" ],                        sortBy: { \"score\": -1 },                        n: 3                     }               }      }   }] )  In this example:  * $documents creates the literal documents that contain player scores.  * $group groups the documents by gameId. This example has only one gameId, G1.  * PlayerD has a missing score and PlayerE has a null score. These values are both considered as null.  * The playerId and score fields are specified as output : [\"$playerId\",\" $score\"] and returned as array values.  * Because of the sortBy: { \"score\" : -1 }, the null values are sorted to the end of the returned playerId array. [   {      _id: \"G1\",      playerId: [ [ \"PlayerA\", 1 ], [ \"PlayerD\", null ], [ \"PlayerE\", null ] ]   }] \nBSON DATA TYPE SORT ORDERING \nWhen sorting different types, the order of BSON data types is used to determine ordering. As an example, consider a collection whose values consist of strings and numbers.  * In an ascending sort, string values are sorted after numeric values.  * In a descending sort, string values are sorted before numeric values.\n\n  In this example:  * PlayerA has an integer score.  * PlayerB has a string \"2\" score.  * PlayerC has an empty string score. Because the sort is in descending { \"score\" : -1 }, the string literal values are sorted before PlayerA's numeric score: [   {      _id: \"G1\",      playerId: [ [ \"PlayerB\", \"2\" ], [ \"PlayerC\", \"\" ], [ \"PlayerA\", 1 ] ]   }] \nRESTRICTIONS  WINDOW FUNCTION AND AGGREGATION EXPRESSION SUPPORT \n$bottomN is not supported as a aggregation expression. $bottomN is supported as a window operator. \nMEMORY LIMIT CONSIDERATIONS \nGroups within the $bottomN aggregation pipeline are subject to the 100 MB limit pipeline limit. If this limit is exceeded for an individual group, the aggregation fails with an error. \nEXAMPLES \nConsider a gamescores collection with the following documents: db.gamescores.insertMany([   { playerId: \"PlayerA\", gameId: \"G1\", score: 31 },   { playerId: \"PlayerB\", gameId: \"G1\", score: 33 },   { playerId: \"PlayerC\", gameId: \"G1\", score: 99 },   { playerId: \"PlayerD\", gameId: \"G1\", score: 1 },   { playerId: \"PlayerA\", gameId: \"G2\", score: 10 },   { playerId: \"PlayerB\", gameId: \"G2\", score: 14 },   { playerId: \"PlayerC\", gameId: \"G2\", score: 66 },   { playerId: \"PlayerD\", gameId: \"G2\", score: 80 }])  \nFIND THE THREE LOWEST SCORES \nYou can use the $bottomN accumulator to find the lowest scoring players in a single game. db.gamescores.aggregate( [   {      $match : { gameId : \"G1\" }   },   {      $group:         {            _id: \"$gameId\",            playerId:               {                  $bottomN:                  {                     output: [\"$playerId\", \"$score\"],                     sortBy: { \"score\": -1 },                     n:3                  }               }         }   }] )  The example pipeline:  * Uses $match to filter the results on a single gameId. In this case, G1.  * Uses $group to group the results by gameId. In this case, G1.  * Uses sort by { \"score\": -1 } to sort the results in descending order.  * Specifies the fields that are output from $bottomN with output : [\"$playerId\",\" $score\"].  * Uses $bottomN to return the bottom three documents with the lowest score for the G1 game with n : 3. The operation returns the following results: [   {      _id: \"G1\",      playerId: [ [ \"PlayerB\", 33 ], [ \"PlayerA\", 31 ], [ \"PlayerD\", 1 ] ]   }] The SQL equivalent to this query is:\n\n \nFINDING THE THREE LOWEST SCORE DOCUMENTS ACROSS MULTIPLE GAMES \nYou can use the $bottomN accumulator to find the lowest scoring players in each game. db.gamescores.aggregate( [      {         $group:         { _id: \"$gameId\", playerId:            {               $bottomN:                  {                     output: [ \"$playerId\",\"$score\" ],                     sortBy: { \"score\": -1 },                     n: 3                  }            }         }      }] )  The example pipeline:  * Uses $group to group the results by gameId.  * Specifies the fields that are output from $bottomN with output : [\"$playerId\", \"$score\"].  * Uses sort by { \"score\": -1 } to sort the results in descending order.  * Uses $bottomN to return the bottom three documents with the lowest score for each game with n: 3. The operation returns the following results: [   {      _id: \"G1\",      playerId: [ [ \"PlayerB\", 33 ], [ \"PlayerA\", 31 ], [ \"PlayerD\", 1 ] ]   },   {      _id: \"G2\",      playerId: [ [ \"PlayerC\", 66 ], [ \"PlayerB\", 14 ], [ \"PlayerA\", 10 ] ]   }] The SQL equivalent to this query is: SELECT PLAYERID,GAMEID,SCOREFROM(   SELECT ROW_NUMBER() OVER (PARTITION BY GAMEID ORDER BY SCORE DESC) AS GAMERANK,   GAMEID,PLAYERID,SCORE   FROM GAMESCORES) AS TWHERE GAMERANK >= 2ORDER BY GAMEID \nCOMPUTING N BASED ON THE GROUP KEY FOR $GROUP \nYou can also assign the value of n dynamically. In this example, the $cond expression is used on the gameId field. db.gamescores.aggregate([   {      $group:      {         _id: {\"gameId\": \"$gameId\"},         gamescores:            {               $bottomN:                  {                     output: \"$score\",                     n: { $cond: { if: {$eq: [\"$gameId\",\"G2\"] }, then: 1, else: 3 } },                     sortBy: { \"score\": -1 }                  }            }      }   }] )  The example pipeline:  * Use $group to group the results by gameId.  * Specifies the fields that are output from $bottomN with output : \"$score\".  * If the gameId is G2 then n is 1, otherwise n is 3.  * Uses sort by { \"score\": -1 } to sort the results in descending order. The operation returns the following results: [   { _id: { gameId: \"G2\" }, gamescores: [ 10 ] },   { _id: { gameId: \"G1\" }, gamescores: [ 33, 31, 1 ] }] ←  $bottom (aggregation accumulator)$bsonSize (aggregation) → On this page\n\n Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/redact/": " Docs Home → MongoDB Manual \n$REDACT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$redact \nRestricts the contents of the documents based on information stored in the documents themselves.  The $redact stage has the following prototype form: { $redact: <expression> }  The argument can be any valid expression as long as it resolves to the $$DESCEND, $$PRUNE, or $$KEEP system variables. For more information on expressions, see Expressions. System Variable\nDescription\n$$DESCEND\n$redact returns the fields at the current document level, excluding embedded documents. To include embedded documents and embedded documents within arrays, apply the $cond expression to the embedded documents to determine access for these embedded documents.\n$$PRUNE\n$redact excludes all fields at this current document/embedded document level, without further inspection of any of the excluded fields. This applies even if the excluded field contains embedded documents that may have different access levels.\n$$KEEP\n$redact returns or keeps all fields at this current document/embedded document level, without further inspection of the fields at this level. This applies even if the included field contains embedded documents that may have different access levels. \nEXAMPLES \nThe examples in this section use the db.collection.aggregate() helper. \nEVALUATE ACCESS AT EVERY DOCUMENT LEVEL \nA forecasts collection contains documents of the following form where the tags field lists the different access values for that document/embedded document level; i.e. a value of [ \"G\", \"STLW\" ] specifies either \"G\" or \"STLW\" can access the data: {  _id: 1,  title: \"123 Department Report\",  tags: [ \"G\", \"STLW\" ],  year: 2014,  subsections: [    {      subtitle: \"Section 1: Overview\",      tags: [ \"SI\", \"G\" ],      content:  \"Section 1: This is the content of section 1.\"    },    {      subtitle: \"Section 2: Analysis\",      tags: [ \"STLW\" ],      content: \"Section 2: This is the content of section 2.\"    },    {      subtitle: \"Section 3: Budgeting\",      tags: [ \"TK\" ],      content: {        text: \"Section 3: This is the content of section3.\",        tags: [ \"HCS\" ]      }    }  ]}  A user has access to view information with either the tag \"STLW\" or \"G\". To run a query on all documents with year 2014 for this user, include a $redact stage as in the following: var userAccess = [ \"STLW\", \"G\" ];db.forecasts.aggregate(   [     { $match: { year: 2014 } },     { $redact: {        $cond: {           if: { $gt: [ { $size: { $setIntersection: [ \"$tags\", userAccess ] } }, 0 ] },           then: \"$$DESCEND\",           else: \"$$PRUNE\"         }       }     }   ]);  The aggregation operation returns the following \"redacted\" document: {  \"_id\" : 1,  \"title\" : \"123 Department Report\",  \"tags\" : [ \"G\", \"STLW\" ],  \"year\" : 2014,  \"subsections\" : [    {      \"subtitle\" : \"Section 1: Overview\",      \"tags\" : [ \"SI\", \"G\" ],      \"content\" : \"Section 1: This is the content of section 1.\"    },    {      \"subtitle\" : \"Section 2: Analysis\",      \"tags\" : [ \"STLW\" ],      \"content\" : \"Section 2: This is the content of section 2.\"    }  ]}  \nTIP \nSEE ALSO:\n\n  * $size  * $setIntersection \nEXCLUDE ALL FIELDS AT A GIVEN LEVEL \nA collection accounts contains the following document: {  _id: 1,  level: 1,  acct_id: \"xyz123\",  cc: {    level: 5,    type: \"yy\",    num: 000000000000,    exp_date: ISODate(\"2015-11-01T00:00:00.000Z\"),    billing_addr: {      level: 5,      addr1: \"123 ABC Street\",      city: \"Some City\"    },    shipping_addr: [      {        level: 3,        addr1: \"987 XYZ Ave\",        city: \"Some City\"      },      {        level: 3,        addr1: \"PO Box 0123\",        city: \"Some City\"      }    ]  },  status: \"A\"}  In this example document, the level field determines the access level required to view the data. To run a query on all documents with status A and exclude all fields contained in a document/embedded document at level 5, include a $redact stage that specifies the system variable \"$$PRUNE\" in the then field: db.accounts.aggregate(  [    { $match: { status: \"A\" } },    {      $redact: {        $cond: {          if: { $eq: [ \"$level\", 5 ] },          then: \"$$PRUNE\",          else: \"$$DESCEND\"        }      }    }  ]);  The $redact stage evaluates the level field to determine access. If the level field equals 5, then exclude all fields at that level, even if the excluded field contains embedded documents that may have different level values, such as the shipping_addr field. The aggregation operation returns the following \"redacted\" document: {  \"_id\" : 1,  \"level\" : 1,  \"acct_id\" : \"xyz123\",  \"status\" : \"A\"}  The result set shows that the $redact stage excluded the field cc as a whole, including the shipping_addr field which contained embedded documents that had level field values equal to 3 and not 5. \nTIP \nSEE ALSO: Implement Field Level Redaction for steps to set up multiple combinations of access for the same data. ←  $project (aggregation)$replaceRoot (aggregation) → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/range/": " Docs Home → MongoDB Manual \n$RANGE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$range \nReturns an array whose elements are a generated sequence of numbers. $range generates the sequence from the specified starting number by successively incrementing the starting number by the specified step value up to but not including the end point. $range has the following operator expression syntax: { $range: [ <start>, <end>, <non-zero step> ] } \nOperand\nDescription\n<start>\nAn integer that specifies the start of the sequence. Can be any valid expression that resolves to an integer.\n<end>\nAn integer that specifies the exclusive upper limit of the sequence. Can be any valid expression that resolves to an integer.\n<non-zero step>\nOptional. An integer that specifies the increment value. Can be any valid expression that resolves to a non-zero integer. Defaults to 1. \nBEHAVIOR \nThe <start> and <end> arguments are required and must be integers. The <non-zero step> argument is optional, and defaults to 1 if omitted. Example\nResults\n{ $range: [ 0, 10, 2 ] }\n[ 0, 2, 4, 6, 8 ]\n{ $range: [ 10, 0, -2 ] }\n[ 10, 8, 6, 4, 2 ]\n{ $range: [ 0, 10, -2 ] }\n[ ]\n{ $range: [ 0, 5 ] }\n[ 0, 1, 2, 3, 4 ] \nEXAMPLE \nThe following example uses a collection called distances that lists cities along with their distance in miles from San Francisco. Documents in the distances collection: { _id: 0, city: \"San Jose\", distance: 42 }{ _id: 1, city: \"Sacramento\", distance: 88 }{ _id: 2, city: \"Reno\", distance: 218 }{ _id: 3, city: \"Los Angeles\", distance: 383 }  A bicyclist is planning to ride from San Francisco to each city listed in the collection and wants to stop and rest every 25 miles. The following aggregation pipeline operation uses the $range operator to determine the stopping points for each trip. db.distances.aggregate([{    $project: {        _id: 0,        city: 1,        \"Rest stops\": { $range: [ 0, \"$distance\", 25 ] }    }}])  The operation returns the following: { \"city\" : \"San Jose\", \"Rest stops\" : [ 0, 25 ] }{ \"city\" : \"Sacramento\", \"Rest stops\" : [ 0, 25, 50, 75 ] }{ \"city\" : \"Reno\", \"Rest stops\" : [ 0, 25, 50, 75, 100, 125, 150, 175, 200 ] }{ \"city\" : \"Los Angeles\", \"Rest stops\" : [ 0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375 ] } \n←  $rand (aggregation)$rank (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/radiansToDegrees/": " Docs Home → MongoDB Manual \n$RADIANSTODEGREES (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$radiansToDegrees \nNew in version 4.2. Converts an input value measured in radians to degrees. $radiansToDegrees has the following syntax: { $radiansToDegrees: <expression> }  $radiansToDegrees takes any valid expression that resolves to a number. By default $radiansToDegrees returns values as a double. $radiansToDegrees can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $radiansToDegrees returns null. If the argument resolves to NaN, $radiansToDegrees returns NaN. If the argument resolves to negative or positive infinity, $radiansToDegrees negative or positive infinity respectively. Example\nResults\n{ $radiansToDegrees: NaN }\nNaN\n{ $radiansToDegrees: null }\nnull\n{ $radiansToDegrees : Infinity}\nInfinity\n{ $radiansToDegrees : -Infinity }\n-Infinity \nEXAMPLE \nThe trigonometry collection contains a document that contains three angles measured in radians: {  \"angle_a\" : NumberDecimal(\"0.9272952180016122324285124629224290\"),  \"angle_b\" : NumberDecimal(\"0.6435011087932843868028092287173227\"),  \"angle_c\" : NumberDecimal(\"1.570796326794896619231321691639752\")}  The following aggregation operation uses the $radiansToDegrees expression to convert each value to its degree equivalent and add them to the input document using the $addFields pipeline stage. db.trigangles.aggregate([  {    $addFields: {      \"angle_a_deg\" : { $radiansToDegrees : \"$angle_a\"},      \"angle_b_deg\" : { $radiansToDegrees : \"$angle_b\"},      \"angle_c_deg\" : { $radiansToDegrees : \"$angle_c\"}    }  }])  The operation returns the following document: {  \"_id\" : ObjectId(\"5c50aec71c75c59232b3ede4\"),  \"angle_a\" : NumberDecimal(\"0.9272952180016122324285124629224290\"),  \"angle_b\" : NumberDecimal(\"0.6435011087932843868028092287173227\"),  \"angle_c\" : NumberDecimal(\"1.570796326794896619231321691639752\"),  \"angle_a_deg\" : NumberDecimal(\"53.13010235415597870314438744090659\"),  \"angle_b_deg\" : NumberDecimal(\"36.86989764584402129685561255909341\"),  \"angle_c_deg\" : NumberDecimal(\"90.00000000000000000000000000000000\")} Since angle_a, angle_b, and angle_c are stored as 128-bit decimals, the output of $radiansToDegrees is a 128-bit decimal. ←  $push (aggregation)$rand (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/exp/": " Docs Home → MongoDB Manual \n$EXP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$exp \nRaises Euler's number (i.e. e ) to the specified exponent and returns the result. $exp has the following syntax: { $exp: <exponent> }  The <exponent> expression can be any valid expression as long as it resolves to a number. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the argument resolves to a value of null or refers to a field that is missing, $exp returns null. If the argument resolves to NaN, $exp returns NaN. Example\nResults\n{ $exp: 0 }\n1\n{ $exp: 2 }\n7.38905609893065\n{ $exp: -2 }\n0.1353352832366127 \nEXAMPLE \nA collection named accounts contains the following documents: { _id: 1, rate: .08, pv: 10000 }{ _id: 2, rate: .0825, pv: 250000 }{ _id: 3, rate: .0425, pv: 1000 }  The following example calculates the effective interest rate for continuous compounding: db.accounts.aggregate( [ { $project: { effectiveRate: { $subtract: [ { $exp: \"$rate\"}, 1 ] } } } ] )  The operation returns the following results: { \"_id\" : 1, \"effectiveRate\" : 0.08328706767495864 }{ \"_id\" : 2, \"effectiveRate\" : 0.08599867343905654 }{ \"_id\" : 3, \"effectiveRate\" : 0.04341605637367807 } \n←  $eq (aggregation)$expMovingAvg (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/lastN-array-element/": " Docs Home → MongoDB Manual \n$LASTN (ARRAY OPERATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example \nDEFINITION \n$lastN \nNew in version 5.2. Returns a specified number of elements from the end of an array. \nTIP \nSEE ALSO:      * $firstN  * $sortArray \nSYNTAX \n$lastN has the following syntax: { $lastN: { n: <expression>, input: <expression> } } \nField\nDescription\nn\nAn expression that resolves to a positive integer. The integer specifies the number of array elements that $lastN returns.\ninput\nAn expression that resolves to the array from which to return n elements. \nBEHAVIOR \n * $lastN returns elements in the same order they appear in the input array.  * $lastN does not filter out null values in the input array.  * You cannot specify a value of n less than 1.  * If the specified n is greater than or equal to the number of elements in the input array, $lastN returns the input array.  * If input resolves to a non-array value, the aggregation operation errors. \nEXAMPLE \nThe collection games has the following documents: db.games.insertMany([    { \"playerId\" : 1, \"score\" : [ 1, 2, 3 ] },    { \"playerId\" : 2, \"score\" : [ 12, 90, 7, 89, 8 ] },    { \"playerId\" : 3, \"score\" : [ null ] },    { \"playerId\" : 4, \"score\" : [ ] },    { \"playerId\" : 5, \"score\" : [ 1293, null, 3489, 9 ]},    { \"playerId\" : 6, \"score\" : [ \"12.1\", 2, NumberLong(\"2090845886852\"), 23 ]}])  The following example uses the $lastN operator to retrieve the last three scores for each player. The scores are returned in the new field lastScores created by $addFields. db.games.aggregate([   { $addFields: { lastScores: { $lastN: { n: 3, input: \"$score\" } } } }])  The operation returns the following results: [{  \"playerId\": 1,  \"score\": [ 1, 2, 3 ],  \"lastScores\": [ 1, 2, 3 ]},{  \"playerId\": 2,  \"score\": [ 12, 90, 7, 89, 8 ],  \"lastScores\": [ 7, 89, 8 ]},{  \"playerId\": 3,  \"score\": [ null ],  \"lastScores\": [ null ]},{  \"playerId\": 4,  \"score\": [ ],  \"lastScores\": [ ]},{  \"playerId\": 5,  \"score\": [ 1293, null, 3489, 9 ],  \"lastScores\": [ null, 3489, 9 ]},{  \"playerId\": 6,  \"score\": [ \"12.1\", 2, NumberLong(\"2090845886852\"), 23 ],  \"lastScores\": [ 2, NumberLong(\"2090845886852\"), 23 ] }] \n←  $last (array operator)$let (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/ifNull/": " Docs Home → MongoDB Manual \n$IFNULL (AGGREGATION) \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$ifNull \nChanged in version 5.0. The $ifNull expression evaluates input expressions for null values and returns:      * The first non-null input expression value found.  * A replacement expression value if all input expressions evaluate to null. $ifNull treats undefined values and missing fields as null. Syntax: {   $ifNull: [      <input-expression-1>,      ...      <input-expression-n>,      <replacement-expression-if-null>   ]} In MongoDB 4.4 and earlier versions, $ifNull only accepts a single input expression: {   $ifNull: [      <input-expression>,      <replacement-expression-if-null>   ]} \nEXAMPLES \nThis inventory collection is used in the examples: db.inventory.insertMany( [   { \"_id\" : 1, \"item\" : \"buggy\", description: \"toy car\", \"quantity\" : 300 },   { \"_id\" : 2, \"item\" : \"bicycle\", description: null, \"quantity\" : 200 },   { \"_id\" : 3, \"item\" : \"flag\" }] )  \nSINGLE INPUT EXPRESSION \nThe following example uses $ifNull to return:  * description if it is non-null.  * \"Unspecified\" string if description is null or missing. db.inventory.aggregate(   [      {         $project: {            item: 1,            description: { $ifNull: [ \"$description\", \"Unspecified\" ] }         }      }   ])  Output: { \"_id\" : 1, \"item\" : \"buggy\", \"description\" : \"toy car\" }{ \"_id\" : 2, \"item\" : \"bicycle\", \"description\" : \"Unspecified\" }{ \"_id\" : 3, \"item\" : \"flag\", \"description\" : \"Unspecified\" } \nMULTIPLE INPUT EXPRESSIONS \nNew in version 5.0. The following example uses $ifNull to return:  * description if it is non-null.  * quantity if description is null or missing and quantity is non-null.  * \"Unspecified\" string if description and quantity are both null or missing. db.inventory.aggregate(   [      {         $project: {            item: 1,            value: { $ifNull: [ \"$description\", \"$quantity\", \"Unspecified\" ] }         }      }   ])  Output: { \"_id\" : 1, \"item\" : \"buggy\", \"value\" : \"toy car\" }{ \"_id\" : 2, \"item\" : \"bicycle\", \"value\" : 200 }{ \"_id\" : 3, \"item\" : \"flag\", \"value\" : \"Unspecified\" } ←  $hour (aggregation)$in (aggregation) → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/slice/": " Docs Home → MongoDB Manual \n$SLICE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$slice \nReturns a subset of an array. $slice has one of two syntax forms: The following syntax returns elements from either the start or end of the array: { $slice: [ <array>, <n> ] }  The following syntax returns elements from the specified position in the array: { $slice: [ <array>, <position>, <n> ] } \nOperand\nDescription\n<array>\nAny valid expression as long as it resolves to an array.\n<position> Optional. Any valid expression as long as it resolves to an integer.      * If positive, $slice determines the starting position from the start of the array. If <position> is greater than the number of elements, the $slice returns an empty array.  * If negative, $slice determines the starting position from the end of the array. If the absolute value of the <position> is greater than the number of elements, the starting position is the start of the array. <n> Any valid expression as long as it resolves to an integer. If <position> is specified, <n> must resolve to a positive integer.  * If positive, $slice returns up to the first n elements in the array. If the <position> is specified, $slice returns the first n elements starting from the position.  * If negative, $slice returns up to the last n elements in the array. n cannot resolve to a negative number if <position> is specified. For more information on expressions, see Expressions. \nBEHAVIOR \nExample\nResults { $slice: [ [ 1, 2, 3 ], 1, 1 ] } [ 2 ] { $slice: [ [ 1, 2, 3 ], -2 ] } [ 2, 3 ] { $slice: [ [ 1, 2, 3 ], 15, 2 ] } [  ] { $slice: [ [ 1, 2, 3 ], -15, 2 ] } [ 1, 2 ] \nEXAMPLE \nA collection named users contains the following documents: { \"_id\" : 1, \"name\" : \"dave123\", favorites: [ \"chocolate\", \"cake\", \"butter\", \"apples\" ] }{ \"_id\" : 2, \"name\" : \"li\", favorites: [ \"apples\", \"pudding\", \"pie\" ] }{ \"_id\" : 3, \"name\" : \"ahn\", favorites: [ \"pears\", \"pecans\", \"chocolate\", \"cherries\" ] }{ \"_id\" : 4, \"name\" : \"ty\", favorites: [ \"ice cream\" ] }  The following example returns at most the first three elements in the favorites array for each user: db.users.aggregate([   { $project: { name: 1, threeFavorites: { $slice: [ \"$favorites\", 3 ] } } }])  The operation returns the following results: { \"_id\" : 1, \"name\" : \"dave123\", \"threeFavorites\" : [ \"chocolate\", \"cake\", \"butter\" ] }{ \"_id\" : 2, \"name\" : \"li\", \"threeFavorites\" : [ \"apples\", \"pudding\", \"pie\" ] }{ \"_id\" : 3, \"name\" : \"ahn\", \"threeFavorites\" : [ \"pears\", \"pecans\", \"chocolate\" ] }{ \"_id\" : 4, \"name\" : \"ty\", \"threeFavorites\" : [ \"ice cream\" ] } \n←  $sinh (aggregation)$sortArray (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/week/": " Docs Home → MongoDB Manual \n$WEEK (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$week \nReturns the week of the year for a date as a number between 0 and 53. Weeks begin on Sundays, and week 1 begins with the first Sunday of the year. Days preceding the first Sunday of the year are in week 0. This behavior is the same as the \"%U\" operator to the strftime standard library function. The $week expression has the following operator expression syntax: { $week: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $week: new Date(\"Jan 1, 2016\") } 0 { $week: { date: new Date(\"2016-01-04\") } } 1 { $week: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 33 { $week: ISODate(\"1998-11-01T00:00:00Z\") } 44 { $week: {    date: ISODate(\"1998-11-01T00:00:00Z\"),    timezone: \"-0500\"} } 43 { $week: \"March 28, 1976\" } error { $week: Date(\"2016-01-01\") } error { $week: \"2009-04-09\" } error \nNOTE \n$WEEK CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $week and other date operators to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $unsetField (aggregation)$year (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/month/": " Docs Home → MongoDB Manual \n$MONTH (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$month \nReturns the month of a date as a number between 1 and 12. The $month expression has the following operator expression syntax: { $month: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $month: new Date(\"2016-01-01\") } 1 { $month: { date: new Date(\"Nov 7, 2003\") } } 11 { $month: ISODate(\"2000-01-01T00:00:00Z\") } 1 { $month: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 8 { $month: {    date: ISODate(\"2000-01-01T00:00:00Z\"),    timezone: \"-0500\"} } 12 { $month: \"March 28, 1976\" } error { $month: {    date: Date(\"2016-01-01\"),    timezone: \"-0500\"} } error { $month: \"2009-04-09\" } error \nNOTE \n$MONTH CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $month and other date operators to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $mod (aggregation)$multiply (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/or/": " Docs Home → MongoDB Manual \n$OR (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Error Handling\n * Example \nDEFINITION \n$or \nEvaluates one or more expressions and returns true if any of the expressions are true. Otherwise, $or returns false. $or has the following syntax: { $or: [ <expression1>, <expression2>, ... ] }  For more information on expressions, see Expressions. \nBEHAVIOR \nIn addition to the false boolean value, $or evaluates as false the following: null, 0, and undefined values. The $or evaluates all other values as true, including non-zero numeric values and arrays. Example\nResult\n{ $or: [ true, false ] }\ntrue\n{ $or: [ [ false ], false ] }\ntrue\n{ $or: [ null, 0, undefined ] }\nfalse\n{ $or: [ ] }\nfalse \nERROR HANDLING \nTo allow the query engine to optimize queries, $or handles errors as follows:      * If any expression supplied to $or would cause an error when evaluated alone, the $or containing the expression may cause an error but an error is not guaranteed.  * An expression supplied after the first expression supplied to $or may cause an error even if the first expression evaluates to true. For example, the following query always produces an error if $x is 0: db.example.find( {   $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] }} )  The following query, which contains multiple expressions supplied to $or, may produce an error if there is any document where $x is 0: db.example.find( {   $or: [      { x: { $eq: 0 } },      { $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] } }   ]} )  \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 }{ \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 }{ \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }  The following operation uses the $or operator to determine if qty is greater than 250 or less than 200: db.inventory.aggregate(   [     {       $project:          {            item: 1,            result: { $or: [ { $gt: [ \"$qty\", 250 ] }, { $lt: [ \"$qty\", 200 ] } ] }          }     }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc1\", \"result\" : true }{ \"_id\" : 2, \"item\" : \"abc2\", \"result\" : false }{ \"_id\" : 3, \"item\" : \"xyz1\", \"result\" : false }{ \"_id\" : 4, \"item\" : \"VWZ1\", \"result\" : true }{ \"_id\" : 5, \"item\" : \"VWZ2\", \"result\" : true } \n←  $objectToArray (aggregation)$pow (aggregation) → On this page  * Definition\n * Behavior\n * Error Handling\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/split/": " Docs Home → MongoDB Manual \n$SPLIT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$split \nDivides a string into an array of substrings based on a delimiter. $split removes the delimiter and returns the resulting substrings as elements of an array. If the delimiter is not found in the string, $split returns the original string as the only element of an array. $split has the following operator expression syntax: { $split: [ <string expression>, <delimiter> ] } \nField\nType\nDescription\nstring expression\nstring\nThe string to be split. string expression can be any valid expression as long as it resolves to a string. For more information on expressions, see Expressions.\ndelimiter\nstring\nThe delimiter to use when splitting the string expression. delimiter can be any valid expression as long as it resolves to a string. \nBEHAVIOR \nThe $split operator returns an array. The <string expression> and <delimiter> inputs must both be strings. Otherwise, the operation fails with an error. Example\nResults { $split: [ \"June-15-2013\", \"-\" ] } [ \"June\", \"15\", \"2013\" ] { $split: [ \"banana split\", \"a\" ] } [ \"b\", \"n\", \"n\", \" split\" ] { $split: [ \"Hello World\", \" \" ] } [ \"Hello\", \"World\" ] { $split: [ \"astronomical\", \"astro\" ] } [ \"\", \"nomical\" ] { $split: [ \"pea green boat\", \"owl\" ] } [ \"pea green boat\" ] { $split: [ \"headphone jack\", 7 ] } Errors with message: \"$split requires an expression that evaluates to a string as a second argument, found: double\" { $split: [ \"headphone jack\", /jack/ ] } Errors with message: \"$split requires an expression that evaluates to a string as a second argument, found: regex\" \nEXAMPLE \nA collection named deliveries contains the following documents: { \"_id\" : 1, \"city\" : \"Berkeley, CA\", \"qty\" : 648 }{ \"_id\" : 2, \"city\" : \"Bend, OR\", \"qty\" : 491 }{ \"_id\" : 3, \"city\" : \"Kensington, CA\", \"qty\" : 233 }{ \"_id\" : 4, \"city\" : \"Eugene, OR\", \"qty\" : 842 }{ \"_id\" : 5, \"city\" : \"Reno, NV\", \"qty\" : 655 }{ \"_id\" : 6, \"city\" : \"Portland, OR\", \"qty\" : 408 }{ \"_id\" : 7, \"city\" : \"Sacramento, CA\", \"qty\" : 574 }  The goal of following aggregation operation is to find the total quantity of deliveries for each state and sort the list in descending order. It has five pipeline stages:      * The $project stage produces documents with two fields, qty (integer) and city_state (array). The $split operator creates an array of strings by splitting the city field, using a space (\" \") as a delimiter.  * The $unwind stage creates a separate record for each element in the city_state field.  * The $match stage uses a regular expression to filter out the city documents, leaving only those containing a state.  * The $group stage groups all the states together and sums the qty field.  * The $sort stage sorts the results by total_qty in descending order. db.deliveries.aggregate([  { $project : { city_state : { $split: [\"$city\", \", \"] }, qty : 1 } },  { $unwind : \"$city_state\" },  { $match : { city_state : /[A-Z]{2}/ } },  { $group : { _id: { \"state\" : \"$city_state\" }, total_qty : { \"$sum\" : \"$qty\" } } },  { $sort : { total_qty : -1 } }]);  The operation returns the following results:\n\n \n←  $sortArray (aggregation)$sqrt (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/cond/": " Docs Home → MongoDB Manual \n$COND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$cond \nEvaluates a boolean expression to return one of the two specified return expressions. The $cond expression has one of two syntaxes: { $cond: { if: <boolean-expression>, then: <true-case>, else: <false-case> } }  Or: { $cond: [ <boolean-expression>, <true-case>, <false-case> ] }  $cond requires all three arguments (if-then-else) for either syntax. If the <boolean-expression> evaluates to true, then $cond evaluates and returns the value of the <true-case> expression. Otherwise, $cond evaluates and returns the value of the <false-case> expression. The arguments can be any valid expression. For more information on expressions, see Expressions. \nTIP \nSEE ALSO: $switch \nEXAMPLE \nThe following example use a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", qty: 250 }  The following aggregation operation uses the $cond expression to set the discount value to 30 if qty value is greater than or equal to 250 and to 20 if qty value is less than 250: db.inventory.aggregate(   [      {         $project:           {             item: 1,             discount:               {                 $cond: { if: { $gte: [ \"$qty\", 250 ] }, then: 30, else: 20 }               }           }      }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc1\", \"discount\" : 30 }{ \"_id\" : 2, \"item\" : \"abc2\", \"discount\" : 20 }{ \"_id\" : 3, \"item\" : \"xyz1\", \"discount\" : 30 }  The following operation uses the array syntax of the $cond expression and returns the same results: db.inventory.aggregate(   [      {         $project:           {             item: 1,             discount:               {                 $cond: [ { $gte: [ \"$qty\", 250 ] }, 30, 20 ]               }           }      }   ]) \n←  $concatArrays (aggregation)$convert (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/bucket/": " Docs Home → MongoDB Manual \n$BUCKET (AGGREGATION) \nOn this page    \n * Definition\n   \n * Considerations\n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$bucket \nCategorizes incoming documents into groups, called buckets, based on a specified expression and bucket boundaries and outputs a document per each bucket. Each output document contains an _id field whose value specifies the inclusive lower bound of the bucket. The output option specifies the fields included in each output document. $bucket only produces output documents for buckets that contain at least one input document. \nCONSIDERATIONS  $BUCKET AND MEMORY RESTRICTIONS \nThe $bucket stage has a limit of 100 megabytes of RAM. By default, if the stage exceeds this limit, $bucket returns an error. To allow more space for stage processing, use the allowDiskUse option to enable aggregation pipeline stages to write data to temporary files. \nTIP \nSEE ALSO: Aggregation Pipeline Limits \nSYNTAX \n{  $bucket: {      groupBy: <expression>,      boundaries: [ <lowerbound1>, <lowerbound2>, ... ],      default: <literal>,      output: {         <output1>: { <$accumulator expression> },         ...         <outputN>: { <$accumulator expression> }      }   }}  The $bucket document contains the following fields: Field\nType\nDescription\ngroupBy\nexpression An expression to group documents by. To specify a field path, prefix the field name with a dollar sign $ and enclose it in quotes. Unless $bucket includes a default specification, each input document must resolve the groupBy field path or expression to a value that falls within one of the ranges specified by the boundaries. boundaries\narray An array of values based on the groupBy expression that specify the boundaries for each bucket. Each adjacent pair of values acts as the inclusive lower boundary and the exclusive upper boundary for the bucket. You must specify at least two boundaries. The specified values must be in ascending order and all of the same type. The exception is if the values are of mixed numeric types, such as: [ 10, NumberLong(20), NumberInt(30) ] \nEXAMPLE An array of [ 0, 5, 10 ] creates two buckets:      * [0, 5) with inclusive lower bound 0 and exclusive upper bound 5.  * [5, 10) with inclusive lower bound 5 and exclusive upper bound 10. default\nliteral Optional. A literal that specifies the _id of an additional bucket that contains all documents whose groupBy expression result does not fall into a bucket specified by boundaries. If unspecified, each input document must resolve the groupBy expression to a value within one of the bucket ranges specified by boundaries or the operation throws an error. The default value must be less than the lowest boundaries value, or greater than or equal to the highest boundaries value. The default value can be of a different type than the entries in boundaries. output\ndocument Optional. A document that specifies the fields to include in the output documents in addition to the _id field. To specify the field to include, you must use accumulator expressions. <outputfield1>: { <accumulator>: <expression1> },...<outputfieldN>: { <accumulator>: <expressionN> }  If you do not specify an output document, the operation returns a count field containing the number of documents in each bucket. If you specify an output document, only the fields specified in the document are returned; i.e. the count field is not returned unless it is explicitly included in the output document. \nBEHAVIOR \n$bucket requires at least one of the following conditions to be met or the operation throws an error:  * Each input document resolves the groupBy expression to a value within one of the bucket ranges specified by boundaries, or  * A default value is specified to bucket documents whose groupBy values are outside of the boundaries or of a different BSON type than the values in boundaries. If the groupBy expression resolves to an array or a document, $bucket arranges the input documents into buckets using the comparison logic from $sort. \nEXAMPLES  BUCKET BY YEAR AND FILTER BY BUCKET RESULTS \nIn mongosh, create a sample collection named artists with the following documents:\n\n  The following operation groups the documents into buckets according to the year_born field and filters based on the count of documents in the buckets: db.artists.aggregate( [  // First Stage  {    $bucket: {      groupBy: \"$year_born\",                        // Field to group by      boundaries: [ 1840, 1850, 1860, 1870, 1880 ], // Boundaries for the buckets      default: \"Other\",                             // Bucket ID for documents which do not fall into a bucket      output: {                                     // Output for each bucket        \"count\": { $sum: 1 },        \"artists\" :          {            $push: {              \"name\": { $concat: [ \"$first_name\", \" \", \"$last_name\"] },              \"year_born\": \"$year_born\"            }          }      }    }  },  // Second Stage  {    $match: { count: {$gt: 3} }  }] ) \nFirst Stage The $bucket stage groups the documents into buckets by the year_born field. The buckets have the following boundaries:  * [1840, 1850) with inclusive lowerbound 1840 and exclusive upper bound 1850.  * [1850, 1860) with inclusive lowerbound 1850 and exclusive upper bound 1860.  * [1860, 1870) with inclusive lowerbound 1860 and exclusive upper bound 1870.  * [1870, 1880) with inclusive lowerbound 1870 and exclusive upper bound 1880.  * If a document did not contain the year_born field or its year_born field was outside the ranges above, it would be placed in the default bucket with the _id value \"Other\". The stage includes the output document to determine the fields to return: Field\nDescription\n_id\nInclusive lower bound of the bucket.\ncount\nCount of documents in the bucket.\nartists Array of documents containing information on each artist in the bucket. Each document contains the artist's  * name, which is a concatenation (i.e. $concat) of the artist's first_name and last_name.  * year_born This stage passes the following documents to the next stage:\n\n Second Stage The $match stage filters the output from the previous stage to only return buckets which contain more than 3 documents. The operation returns the following document: { \"_id\" : 1860, \"count\" : 4, \"artists\" :  [    { \"name\" : \"Emil Bernard\", \"year_born\" : 1868 },    { \"name\" : \"Joszef Rippl-Ronai\", \"year_born\" : 1861 },    { \"name\" : \"Alfred Maurer\", \"year_born\" : 1868 },    { \"name\" : \"Edvard Munch\", \"year_born\" : 1863 }  ]} \nUSE $BUCKET WITH $FACET TO BUCKET BY MULTIPLE FIELDS \nYou can use the $facet stage to perform multiple $bucket aggregations in a single stage. In mongosh, create a sample collection named artwork with the following documents: db.artwork.insertMany([  { \"_id\" : 1, \"title\" : \"The Pillars of Society\", \"artist\" : \"Grosz\", \"year\" : 1926,      \"price\" : NumberDecimal(\"199.99\") },  { \"_id\" : 2, \"title\" : \"Melancholy III\", \"artist\" : \"Munch\", \"year\" : 1902,      \"price\" : NumberDecimal(\"280.00\") },  { \"_id\" : 3, \"title\" : \"Dancer\", \"artist\" : \"Miro\", \"year\" : 1925,      \"price\" : NumberDecimal(\"76.04\") },  { \"_id\" : 4, \"title\" : \"The Great Wave off Kanagawa\", \"artist\" : \"Hokusai\",      \"price\" : NumberDecimal(\"167.30\") },  { \"_id\" : 5, \"title\" : \"The Persistence of Memory\", \"artist\" : \"Dali\", \"year\" : 1931,      \"price\" : NumberDecimal(\"483.00\") },  { \"_id\" : 6, \"title\" : \"Composition VII\", \"artist\" : \"Kandinsky\", \"year\" : 1913,      \"price\" : NumberDecimal(\"385.00\") },  { \"_id\" : 7, \"title\" : \"The Scream\", \"artist\" : \"Munch\", \"year\" : 1893      /* No price*/ },  { \"_id\" : 8, \"title\" : \"Blue Flower\", \"artist\" : \"O'Keefe\", \"year\" : 1918,      \"price\" : NumberDecimal(\"118.42\") }])  The following operation uses two $bucket stages within a $facet stage to create two groupings, one by price and the other by year:\n\n \nFirst Facet The first facet groups the input documents by price. The buckets have the following boundaries:  * [0, 200) with inclusive lowerbound 0 and exclusive upper bound 200.  * [200, 400) with inclusive lowerbound 200 and exclusive upper bound 400.  * \"Other\", the default bucket containing documents without prices or prices outside the ranges above. The $bucket stage includes the output document to determine the fields to return: Field\nDescription\n_id\nInclusive lower bound of the bucket.\ncount\nCount of documents in the bucket.\nartwork\nArray of documents containing information on each artwork in the bucket.\naveragePrice\nEmploys the $avg operator to display the average price of all artwork in the bucket. Second Facet The second facet groups the input documents by year. The buckets have the following boundaries:  * [1890, 1910) with inclusive lowerbound 1890 and exclusive upper bound 1910.  * [1910, 1920) with inclusive lowerbound 1910 and exclusive upper bound 1920.  * [1920, 1940) with inclusive lowerbound 1910 and exclusive upper bound 1940.  * \"Unknown\", the default bucket containing documents without years or years outside the ranges above. The $bucket stage includes the output document to determine the fields to return: Field\nDescription\ncount\nCount of documents in the bucket.\nartwork\nArray of documents containing information on each artwork in the bucket. Output The operation returns the following document:\n\n  \nTIP \nSEE ALSO: $bucketAuto ←  $addFields (aggregation)$bucketAuto (aggregation) → On this page  * Definition\n * Considerations\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/year/": " Docs Home → MongoDB Manual \n$YEAR (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$year \nReturns the year portion of a date. The $year expression has the following operator expression syntax: { $year: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $year: new Date(\"2016-01-01\") } 2016 { $year: { date: new Date(\"Jan 7, 2003\") } } 2003 { $year: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 2011 { $year: ISODate(\"1998-11-07T00:00:00Z\") } 1998 { $year: {    date: ISODate(\"1998-11-07T00:00:00Z\"),    timezone: \"-0400\"} } 1998 { $year: \"March 28, 1976\" } error { $year: Date(\"2016-01-01\") } error { $year: \"2009-04-09\" } error \nNOTE \n$YEAR CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following documents: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $year and other date operators to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $week (aggregation)$zip (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sum/": " Docs Home → MongoDB Manual \n$SUM (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$sum \nChanged in version 5.0. Calculates and returns the collective sum of numeric values. $sum ignores non-numeric values. $sum is available in these stages:      * $addFields (Available starting in MongoDB 3.4)  * $bucket  * $bucketAuto  * $group  * $match stage that includes an $expr expression  * $project  * $replaceRoot (Available starting in MongoDB 3.4)  * $replaceWith (Available starting in MongoDB 4.2)  * $set (Available starting in MongoDB 4.2)  * $setWindowFields (Available starting in MongoDB 5.0) In MongoDB 3.2 and earlier, $sum is available in the $group stage only. \nSYNTAX \nWhen used in the $bucket, $bucketAuto, $group, and $setWindowFields stages, $sum has this syntax: { $sum: <expression> } When used in other supported stages, $sum has one of two syntaxes:  * $sum has one specified expression as its operand:\n   \n   { $sum: <expression> }  * $sum has a list of specified expressions as its operand:\n   \n   { $sum: [ <expression1>, <expression2> ... ]  } For more information on expressions, see Expressions. \nBEHAVIOR  RESULT DATA TYPE \nThe result will have the same type as the input except when it cannot be represented accurately in that type. In these cases:  * A 32-bit integer will be converted to a 64-bit integer if the result is representable as a 64-bit integer.  * A 32-bit integer will be converted to a double if the result is not representable as a 64-bit integer.  * A 64-bit integer will be converted to double if the result is not representable as a 64-bit integer. \nNON-NUMERIC OR NON-EXISTENT FIELDS \nIf used on a field that contains both numeric and non-numeric values, $sum ignores the non-numeric values and returns the sum of the numeric values. If used on a field that does not exist in any document in the collection, $sum returns 0 for that field. If all operands are non-numeric, $sum returns 0. \nARRAY OPERAND \nIn the $group stage, if the expression resolves to an array, $sum treats the operand as a non-numerical value. In the other supported stages:  * With a single expression as its operand, if the expression resolves to an array, $sum traverses into the array to operate on the numerical elements of the array to return a single value.  * With a list of expressions as its operand, if any of the expressions resolves to an array, $sum does not traverse into the array but instead treats the array as a non-numerical value. \nEXAMPLES  USE IN $GROUP STAGE \nConsider a sales collection with the following documents: { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 2, \"date\" : ISODate(\"2014-01-01T08:00:00Z\") }{ \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"quantity\" : 1, \"date\" : ISODate(\"2014-02-03T09:00:00Z\") }{ \"_id\" : 3, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 5, \"date\" : ISODate(\"2014-02-03T09:05:00Z\") }{ \"_id\" : 4, \"item\" : \"abc\", \"price\" : 10, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T08:00:00Z\") }{ \"_id\" : 5, \"item\" : \"xyz\", \"price\" : 5, \"quantity\" : 10, \"date\" : ISODate(\"2014-02-15T09:05:00Z\") }  Grouping the documents by the day and the year of the date field, the following operation uses the $sum accumulator to compute the total amount and the count for each group of documents.\n\n  The operation returns the following results: { \"_id\" : { \"day\" : 46, \"year\" : 2014 }, \"totalAmount\" : 150, \"count\" : 2 }{ \"_id\" : { \"day\" : 34, \"year\" : 2014 }, \"totalAmount\" : 45, \"count\" : 2 }{ \"_id\" : { \"day\" : 1, \"year\" : 2014 }, \"totalAmount\" : 20, \"count\" : 1 } Using $sum on a non-existent field returns a value of 0. The following operation attempts to $sum on qty: db.sales.aggregate(   [     {       $group:         {           _id: { day: { $dayOfYear: \"$date\"}, year: { $year: \"$date\" } },           totalAmount: { $sum: \"$qty\" },           count: { $sum: 1 }         }     }   ])  The operation returns: { \"_id\" : { \"day\" : 46, \"year\" : 2014 }, \"totalAmount\" : 0, \"count\" : 2 }{ \"_id\" : { \"day\" : 34, \"year\" : 2014 }, \"totalAmount\" : 0, \"count\" : 2 }{ \"_id\" : { \"day\" : 1, \"year\" : 2014 }, \"totalAmount\" : 0, \"count\" : 1 } The $count aggregation accumulator can be used in place of { $sum : 1 } in the $group stage. \nTIP \nSEE ALSO: $count (aggregation accumulator) \nUSE IN $PROJECT STAGE \nA collection students contains the following documents: { \"_id\": 1, \"quizzes\": [ 10, 6, 7 ], \"labs\": [ 5, 8 ], \"final\": 80, \"midterm\": 75 }{ \"_id\": 2, \"quizzes\": [ 9, 10 ], \"labs\": [ 8, 8 ], \"final\": 95, \"midterm\": 80 }{ \"_id\": 3, \"quizzes\": [ 4, 5, 5 ], \"labs\": [ 6, 5 ], \"final\": 78, \"midterm\": 70 }  The following example uses the $sum in the $project stage to calculate the total quiz scores, the total lab scores, and the total of the final and the midterm: db.students.aggregate([   {     $project: {       quizTotal: { $sum: \"$quizzes\"},       labTotal: { $sum: \"$labs\" },       examTotal: { $sum: [ \"$final\", \"$midterm\" ] }     }   }])  The operation results in the following documents: { \"_id\" : 1, \"quizTotal\" : 23, \"labTotal\" : 13, \"examTotal\" : 155 }{ \"_id\" : 2, \"quizTotal\" : 19, \"labTotal\" : 16, \"examTotal\" : 175 }{ \"_id\" : 3, \"quizTotal\" : 14, \"labTotal\" : 11, \"examTotal\" : 148 } \nUSE IN $SETWINDOWFIELDS STAGE \nNew in version 5.0. Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA):\n\n  This example uses $sum in the $setWindowFields stage to output the sum of the quantity of cakes sold in each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            sumQuantityForState: {               $sum: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"current\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output sets the sumQuantityForState field to the sum of the quantity values using $sum that is run in a documents window.\n   \n   The window contains documents between an unbounded lower limit and the current document in the output. This means $sum returns the sum of the quantity values for the documents between the beginning of the partition and the current document. In this output, the sum of the quantity values for CA and WA is shown in the sumQuantityForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"sumQuantityForState\" : 162 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"sumQuantityForState\" : 282 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"sumQuantityForState\" : 427 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"sumQuantityForState\" : 134 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"sumQuantityForState\" : 238 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"sumQuantityForState\" : 378 } ←  $subtract (aggregation)$switch (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sample/": " Docs Home → MongoDB Manual \n$SAMPLE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$sample \nRandomly selects the specified number of documents from the input documents. The $sample stage has the following syntax: { $sample: { size: <positive integer N> } }  N is the number of documents to randomly select. \nBEHAVIOR \nIf all of the following conditions are true, $sample uses a pseudo-random cursor to select the N documents:      * $sample is the first stage of the pipeline.  * N is less than 5% of the total documents in the collection.  * The collection contains more than 100 documents. If any of the previous conditions are false, $sample:  * Reads all documents that are output from a preceding aggregation stage or a collection scan.  * Performs a random sort to select N documents. \nNOTE Random sorts are subject to the sort memory restrictions. \nEXAMPLE \nGiven a collection named users with the following documents: { \"_id\" : 1, \"name\" : \"dave123\", \"q1\" : true, \"q2\" : true }{ \"_id\" : 2, \"name\" : \"dave2\", \"q1\" : false, \"q2\" : false  }{ \"_id\" : 3, \"name\" : \"ahn\", \"q1\" : true, \"q2\" : true  }{ \"_id\" : 4, \"name\" : \"li\", \"q1\" : true, \"q2\" : false  }{ \"_id\" : 5, \"name\" : \"annT\", \"q1\" : false, \"q2\" : true  }{ \"_id\" : 6, \"name\" : \"li\", \"q1\" : true, \"q2\" : true  }{ \"_id\" : 7, \"name\" : \"ty\", \"q1\" : false, \"q2\" : true  }  The following aggregation operation randomly selects 3 documents from the collection: db.users.aggregate(   [ { $sample: { size: 3 } } ])  The operation returns three random documents. \nTIP \nSEE ALSO: $rand (aggregation) ←  $replaceWith (aggregation)$search (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/strLenCP/": " Docs Home → MongoDB Manual \n$STRLENCP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$strLenCP \nReturns the number of UTF-8 code points in the specified string. $strLenCP has the following operator expression syntax: { $strLenCP: <string expression> } The argument can be any valid expression that resolves to a string. If the argument resolves to a value of null or refers to a missing field, $strLenCP returns an error. Example\nResults\n{ $strLenCP: \"abcde\" }\n5\n{ $strLenCP: \"Hello World!\" }\n12\n{ $strLenCP: \"cafeteria\" }\n9\n{ $strLenCP: \"cafétéria\" }\n9\n{ $strLenCP: \"\" }\n0\n{ $strLenCP: \"$€λA\" }\n4\n{ $strLenCP: \"寿司\" }\n2 \nBEHAVIOR \nThe $strLenCP operator counts the number of code points in the specified string. This behavior differs from the $strLenBytes operator that counts the number of bytes in the string, where each character uses between one and four bytes. \nEXAMPLE  SINGLE-BYTE AND MULTIBYTE CHARACTER SET \nCreate a food collection: db.food.insertMany( [   { _id: 1, name: \"apple\" },   { _id: 2, name: \"banana\" },   { _id: 3, name: \"éclair\" },   { _id: 4, name: \"hamburger\" },   { _id: 5, name: \"jalapeño\" },   { _id: 6, name: \"pizza\" },   { _id: 7, name: \"tacos\" },   { _id: 8, name: \"寿司\" }] )  The following example uses the $strLenCP operator to calculate the length of each name value: db.food.aggregate( [   {      $project: {         name: 1,         length: { $strLenCP: \"$name\" }      }   }] )  Example output: [   { _id: 1, name: 'apple', length: 5 },   { _id: 2, name: 'banana', length: 6 },   { _id: 3, name: 'éclair', length: 6 },   { _id: 4, name: 'hamburger', length: 9 },   { _id: 5, name: 'jalapeño', length: 8 },   { _id: 6, name: 'pizza', length: 5 },   { _id: 7, name: 'tacos', length: 5 },   { _id: 8, name: '寿司', length: 2 }] \nTIP \nSEE ALSO: $strLenBytes ←  $strLenBytes (aggregation)$substr (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/reverseArray/": " Docs Home → MongoDB Manual \n$REVERSEARRAY (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$reverseArray \nAccepts an array expression as an argument and returns an array with the elements in reverse order. $reverseArray has the following operator expression syntax: { $reverseArray: <array expression> }  The argument can be any valid expression as long as it resolves to an array. \nBEHAVIOR \nIf the argument resolves to a value of null or refers to a missing field, $reverseArray returns null. If the argument does not resolve to an array or null nor refers to a missing field, $reverseArray returns an error. $reverseArray returns an empty array when the argument is an empty array. If the argument contains subarrays, $reverseArray only operates on the top level array elements and will not reverse the contents of subarrays. \nExample [1]\nResults { $reverseArray: { $literal: [ 1, 2, 3 ] } } [ 3, 2, 1 ] { $reverseArray:   { $slice:  [ [ \"foo\", \"bar\", \"baz\", \"qux\" ], 1, 2 ] } }} [ \"baz\", \"bar\" ] { $reverseArray: null } null { $reverseArray: { $literal: [ ] } } [ ] { $reverseArray: { $literal: [ [ 1, 2, 3 ], [ 4, 5, 6 ] ] } } [ [ 4, 5, 6 ], [ 1, 2, 3 ] ] [1] The examples in the table take a literal argument. To avoid parsing ambiguity if the literal argument is an array, you must wrap the literal array in a $literal expression or keep the outer array that designates the argument list (e.g. [ [ 1, 2, 3 ] ] ) to pass in the literal array [1, 2, 3]. \nEXAMPLE \nA collection named users contains the following documents: { \"_id\" : 1, \"name\" : \"dave123\", \"favorites\" : [ \"chocolate\", \"cake\", \"butter\", \"apples\" ] }{ \"_id\" : 2, \"name\" : \"li\", \"favorites\" : [ \"apples\", \"pudding\", \"pie\" ] }{ \"_id\" : 3, \"name\" : \"ahn\", \"favorites\" : [ ] }{ \"_id\" : 4, \"name\" : \"ty\" }  The following example returns an array containing the elements of the favorites array in reverse order: db.users.aggregate([   {     $project:      {         name: 1,         reverseFavorites: { $reverseArray: \"$favorites\" }      }   }])  The operation returns the following results: { \"_id\" : 1, \"name\" : \"dave123\", \"reverseFavorites\" : [ \"apples\", \"butter\", \"cake\", \"chocolate\" ] }{ \"_id\" : 2, \"name\" : \"li\", \"reverseFavorites\" : [ \"pie\", \"pudding\", \"apples\" ] }{ \"_id\" : 3, \"name\" : \"ahn\", \"reverseFavorites\" : [ ] }{ \"_id\" : 4, \"name\" : \"ty\", \"reverseFavorites\" : null } \n←  $replaceAll (aggregation)$round (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/atanh/": " Docs Home → MongoDB Manual \n$ATANH (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $atanh \nNew in version 4.2. Returns the inverse hyperbolic tangent (hyperbolic arc tangent) of a value. $atanh has the following syntax: { $atanh: <expression> }  $atanh takes any valid expression that resolves to a number between -1 and 1, e.g. -1 <= value <= 1. $atanh returns values in radians. Use $radiansToDegrees operator to convert the output value from radians to degrees. By default $atanh returns values as a double. $atanh can also return values as a 128-bit decimal as long as the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the argument resolves to a value of null or refers to a field that is missing, $atanh returns null. If the argument resolves to NaN, $atanh returns NaN. If the argument resolves to negative or positive infinity, $atanh throws an error. If the argument resolves to +1 or -1, $atanh returns Infinity and -Infinity respectively. Example\nResults\n{ $atanh: NaN }\nNaN\n{ $atanh: null }\nnull\n{ $atanh: 1 }\nInfinity\n{ $atanh: -1}\n-Infinity { $atanh : Infinity} or { $atanh : -Infinity } Throws an error message resembling the following formatted output: \"errmsg\" :  \"Failed to optimize pipeline :: caused by :: cannot  apply $atanh to -inf, value must in (-inf,inf)\" \nEXAMPLE  \n←  $atan2 (aggregation)$avg (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/let/": " Docs Home → MongoDB Manual \n$LET (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$let \nBinds variables for use in the specified expression, and returns the result of the expression. The $let expression has the following syntax: {  $let:     {       vars: { <var1>: <expression>, ... },       in: <expression>     }} \nField\nSpecification\nvars Assignment block for the variables accessible in the in expression. To assign a variable, specify a string for the variable name and assign a valid expression for the value. The variable assignments have no meaning outside the in expression, not even within the vars block itself. in\nThe expression to evaluate. To access variables in aggregation expressions, prefix the variable name with double dollar signs ($$) and enclosed in quotes. For more information on expressions, see Expressions. For information on use of variables in the aggregation pipeline, see Variables in Aggregation Expressions. \nBEHAVIOR \n$let can access variables defined outside its expression block, including system variables. If you modify the values of externally defined variables in the vars block, the new values take effect only in the in expression. Outside of the in expression, the variables retain their previous values. In the vars assignment block, the order of the assignment does not matter, and the variable assignments only have meaning inside the in expression. As such, accessing a variable's value in the vars assignment block refers to the value of the variable defined outside the vars block and not inside the same vars block. For example, consider the following $let expression: {  $let:    {      vars: { low: 1, high: \"$$low\" },      in: { $gt: [ \"$$low\", \"$$high\" ] }    }}  In the vars assignment block, \"$$low\" refers to the value of an externally defined variable low and not the variable defined in the same vars block. If low is not defined outside this $let expression block, the expression is invalid. \nEXAMPLE \nA sales collection has the following documents: { _id: 1, price: 10, tax: 0.50, applyDiscount: true }{ _id: 2, price: 10, tax: 0.25, applyDiscount: false }  The following aggregation uses $let in the $project pipeline stage to calculate and return the finalTotal for each document: db.sales.aggregate( [   {      $project: {         finalTotal: {            $let: {               vars: {                  total: { $add: [ '$price', '$tax' ] },                  discounted: { $cond: { if: '$applyDiscount', then: 0.9, else: 1 } }               },               in: { $multiply: [ \"$$total\", \"$$discounted\" ] }            }         }      }   }] )  The aggregation returns the following results: { \"_id\" : 1, \"finalTotal\" : 9.450000000000001 }{ \"_id\" : 2, \"finalTotal\" : 10.25 }  \nTIP \nSEE ALSO: $map ←  $lastN (array operator)$linearFill (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/log10/": " Docs Home → MongoDB Manual \n$LOG10 (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$log10 \nCalculates the log base 10 of a number and returns the result as a double. $log10 has the following syntax: { $log10: <number> }  The <number> expression can be any valid expression as long as it resolves to a non-negative number. For more information on expressions, see Expressions. $log10 is equivalent to $log: [ <number>, 10 ] expression. \nBEHAVIOR \nIf the argument resolves to a value of null or refers to a field that is missing, $log10 returns null. If the argument resolves to NaN, $log10 returns NaN. Example\nResults\n{ $log10: 1 }\n0\n{ $log10: 10 }\n1\n{ $log10: 100 }\n2\n{ $log10: 1000 }\n3 \nEXAMPLE \nCreate a collection named samples with the following documents: db.samples.insertMany(   [      { _id: 1, H3O: 0.0025 },      { _id: 2, H3O: 0.001 },      { _id: 3, H3O: 0.02 }   ])  The following example calculates the pH value of the samples: db.samples.aggregate( [   { $project: { pH: { $multiply: [ -1, { $log10: \"$H3O\" } ] } } }] )  The operation returns the following results: { \"_id\" : 1, \"pH\" : 2.6020599913279625 }{ \"_id\" : 2, \"pH\" : 3 }{ \"_id\" : 3, \"pH\" : 1.6989700043360187 }  \nTIP \nSEE ALSO: $log ←  $log (aggregation)$lt (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/minute/": " Docs Home → MongoDB Manual \n$MINUTE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$minute \nReturns the minute portion of a date as a number between 0 and 59. The $minute expression has the following operator expression syntax: { $minute: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $minute: new Date(\"2016-01-01T12:01:00Z\") } 1 { $minute: { date: new Date(\"Jan 7, 2003\") } } 0 { $minute: {    date: new Date(\"2016-01-01T12:01:00Z\"),    timezone: \"Canada/Newfoundland\"} } 31 { $minute: {    date: ISODate(\"1998-11-07T00:40:00Z\"),    timezone: \"+0530\"} } 10 { $minute: \"March 28, 1976\" } error { $minute: Date(\"2016-01-01\") } error { $minute: \"2009-04-09\" } error \nNOTE \n$MINUTE CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $minute and other date expressions to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $millisecond (aggregation)$mod (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/setWindowFields/": " Docs Home → MongoDB Manual \n$SETWINDOWFIELDS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Window Operators\n * Restrictions\n * Examples \nDEFINITION \n$setWindowFields \nNew in version 5.0. Performs operations on a specified span of documents in a collection, known as a window, and returns the results based on the chosen window operator. For example, you can use the $setWindowFields stage to output the:      * Difference in sales between two documents in a collection.  * Sales rankings.  * Cumulative sales totals.  * Analysis of complex time series information without exporting the data to an external database. \nSYNTAX \nThe $setWindowFields stage syntax: {   $setWindowFields: {      partitionBy: <expression>,      sortBy: {         <sort field 1>: <sort order>,         <sort field 2>: <sort order>,         ...,         <sort field n>: <sort order>      },      output: {         <output field 1>: {            <window operator>: <window operator parameters>,            window: {               documents: [ <lower boundary>, <upper boundary> ],               range: [ <lower boundary>, <upper boundary> ],               unit: <time unit>            }         },         <output field 2>: { ... },         ...         <output field n>: { ... }      }   }} The $setWindowFields stage takes a document with these fields: Field\nNecessity\nDescription\npartitionBy\nOptional Specifies an expression to group the documents. In the $setWindowFields stage, the group of documents is known as a partition. Default is one partition for the entire collection. sortBy\nRequired for some operators (see Restrictions) Specifies the field(s) to sort the documents by in the partition. Uses the same syntax as the $sort stage. Default is no sorting. output\nRequired Specifies the field(s) to append to the documents in the output returned by the $setWindowFields stage. Each field is set to the result returned by the window operator. A field can contain dots to specify embedded document fields and array fields. The semantics for the embedded document dotted notation in the $setWindowFields stage are the same as the $addFields and $set stages. See embedded document $addFields example and embedded document $set example.  * The window operator is the window operator name to use in the $setWindowFields stage.  * The window operator parameters are the parameters to pass to the window operator. window\nOptional Specifies the window boundaries and parameters. Window boundaries are inclusive. Default is an unbounded window, which includes all documents in the partition. Specify either a documents or range window. documents\nOptional A window where the lower and upper boundaries are specified relative to the position of the current document read from the collection. The window boundaries are specified using a two element array containing a lower and upper limit string or integer. Use:  * The \"current\" string for the current document position in the output.  * The \"unbounded\" string for the first or last document position in the partition.  * An integer for a position relative to the current document. Use a negative integer for a position before the current document. Use a positive integer for a position after the current document. 0 is the current document position. See Documents Window Examples. range\nOptional A window where the lower and upper boundaries are defined using a range of values based on the sortBy field in the current document. The window boundaries are specified using a two element array containing a lower and upper limit string or number. Use:  * The \"current\" string for the current document position in the output.  * The \"unbounded\" string for the first or last document position in the partition.\n\n See Range Window Example. unit\nOptional Specifies the units for time range window boundaries. Can be set to one of these strings:  * \"year\"  * \"quarter\"  * \"month\"  * \"week\"  * \"day\"  * \"hour\"  * \"minute\"  * \"second\"  * \"millisecond\" If omitted, default numeric range window boundaries are used. See Time Range Window Examples. \nTIP \nSEE ALSO: Examples \nBEHAVIOR \nThe $setWindowFields stage appends new fields to existing documents. You can include one or more $setWindowFields stages in an aggregation operation. Starting in MongoDB 5.3, you can use the $setWindowFields stage with transactions and the \"snapshot\" read concern. \nWINDOW OPERATORS \nThese operators can be used with the $setWindowFields stage:  * Accumulator operators: $addToSet, $avg, $bottom, $bottomN, $count, $covariancePop, $covarianceSamp, $derivative, $expMovingAvg, $firstN, $integral, $lastN, $max, $maxN, $min, $minN, $push, $stdDevSamp, $stdDevPop, $sum, $top, $topN.  * Gap filling operators: $linearFill and $locf.  * Order operators: $first, $last, and $shift.  * Rank operators: $denseRank, $documentNumber, and $rank. \nRESTRICTIONS \nRestrictions for the $setWindowFields stage:  * Starting in MongoDB 5.1 (and 5.0.4), the $setWindowFields stage cannot be used:\n   \n   * Within transactions.\n   \n   * With \"snapshot\" read concern.  * sortBy is required for:\n   \n   * Rank and order window operators.\n   \n   * Bounded windows (either a documents window or a range window).\n   \n   * $linearFill operator.  * Range windows require all sortBy values to be numbers.  * Time range windows require all sortBy values to be dates.  * Range and time range windows can only contain one sortBy field and the sort must be ascending.  * You cannot specify both a documents window and a range window.  * These operators use an implicit window and return an error if you specify a window option:\n   \n   * Rank operators.\n   \n   * $shift operator.  * For range windows, only numbers in the specified range are included in the window. Missing, undefined, and null values are excluded.  * For time range windows:\n   \n   * Only date and time types are included in the window.\n   \n   * Numeric boundary values must be integers. For example, you can use 2 hours as a boundary but you cannot use 1.5 hours.  * For empty windows or windows with incompatible values (for example, using $sum on strings), the returned value depends on the operator:\n   \n   * For $count and $sum, the returned value is 0.\n   \n   * For $addToSet and $push, the returned value is an empty array.\n   \n   * For all other operators, the returned value is null. \nEXAMPLES \nCreate a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA):\n\n  The following examples use the cakeSales collection. \nDOCUMENTS WINDOW EXAMPLES \nUSE DOCUMENTS WINDOW TO OBTAIN CUMULATIVE QUANTITY FOR EACH STATE \nThis example uses a documents window in $setWindowFields to output the cumulative cake sales quantity for each state: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            cumulativeQuantityForState: {               $sum: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"current\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output:\n   \n   * Sets the cumulativeQuantityForState field to the cumulative quantity for each state, which increases by successive additions to the previous value in the partition.\n   \n   * Calculates the cumulative quantity using the $sum operator run in a documents window.\n     \n     The window contains documents between an unbounded lower limit and the current document. This means $sum returns the cumulative quantity for the documents between the beginning of the partition and the current document. In this example output, the cumulative quantity for CA and WA is shown in the cumulativeQuantityForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"cumulativeQuantityForState\" : 162 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"cumulativeQuantityForState\" : 282 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"cumulativeQuantityForState\" : 427 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"cumulativeQuantityForState\" : 134 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"cumulativeQuantityForState\" : 238 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"cumulativeQuantityForState\" : 378 } USE DOCUMENTS WINDOW TO OBTAIN CUMULATIVE QUANTITY FOR EACH YEAR \nThis example uses a documents window in $setWindowFields to output the cumulative cake sales quantity for each $year in orderDate:\n\n  In the example:  * partitionBy: { $year: \"$orderDate\" } partitions the documents in the collection by $year in orderDate. There are are partitions for 2019, 2020, and 2021.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output:\n   \n   * Sets the cumulativeQuantityForYear field to the cumulative quantity for each year, which increases by successive additions to the previous value in the partition.\n   \n   * Calculates the cumulative quantity using the $sum operator run in a documents window.\n     \n     The window contains documents between an unbounded lower limit and the current document. This means $sum returns the cumulative quantity for the documents between the beginning of the partition and the current document. In this example output, the cumulative quantity for each year is shown in the cumulativeQuantityForYear field: { \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"cumulativeQuantityForYear\" : 134 }{ \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"cumulativeQuantityForYear\" : 296 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"cumulativeQuantityForYear\" : 104 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"cumulativeQuantityForYear\" : 224 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"cumulativeQuantityForYear\" : 145 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"cumulativeQuantityForYear\" : 285 } USE DOCUMENTS WINDOW TO OBTAIN MOVING AVERAGE QUANTITY FOR EACH YEAR \nThis example uses a documents window in $setWindowFields to output the moving average for the cake sales quantity: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: { $year: \"$orderDate\" },         sortBy: { orderDate: 1 },         output: {            averageQuantity: {               $avg: \"$quantity\",               window: {                  documents: [ -1, 0 ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$orderDate\" partitions the documents in the collection by $year in orderDate. There are are partitions for 2019, 2020, and 2021.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.\n\n In this example output, the moving average quantity is shown in the averageQuantity field: { \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"averageQuantity\" : 134 }{ \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"averageQuantity\" : 148 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"averageQuantity\" : 104 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"averageQuantity\" : 112 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"averageQuantity\" : 145 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"averageQuantity\" : 142.5 } USE DOCUMENTS WINDOW TO OBTAIN CUMULATIVE AND MAXIMUM QUANTITY FOR EACH YEAR \nThis example uses a documents window in $setWindowFields to output the cumulative and maximum cake sales quantity values for each $year in orderDate: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: { $year: \"$orderDate\" },         sortBy: { orderDate: 1 },         output: {            cumulativeQuantityForYear: {               $sum: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"current\" ]               }            },            maximumQuantityForYear: {               $max: \"$quantity\",               window: {                  documents: [ \"unbounded\", \"unbounded\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$orderDate\" partitions the documents in the collection by $year in orderDate. There are are partitions for 2019, 2020, and 2021.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.\n\n In this example output, the cumulative quantity is shown in the cumulativeQuantityForYear field and the maximum quantity is shown in the maximumQuantityForYear field: { \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134,  \"cumulativeQuantityForYear\" : 134, \"maximumQuantityForYear\" : 162 }{ \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162,  \"cumulativeQuantityForYear\" : 296, \"maximumQuantityForYear\" : 162 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104,  \"cumulativeQuantityForYear\" : 104, \"maximumQuantityForYear\" : 120 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120,   \"cumulativeQuantityForYear\" : 224, \"maximumQuantityForYear\" : 120 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145,  \"cumulativeQuantityForYear\" : 145, \"maximumQuantityForYear\" : 145 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140,  \"cumulativeQuantityForYear\" : 285, \"maximumQuantityForYear\" : 145 } \nRANGE WINDOW EXAMPLE \nThis example uses a range window in $setWindowFields to return the sum of the quantity values of cakes sold for orders within plus or minus 10 dollars of the current document's price value: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { price: 1 },         output: {            quantityFromSimilarOrders: {               $sum: \"$quantity\",               window: {                  range: [ -10, 10 ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { price: 1 } sorts the documents in each partition by price in ascending order (1), so the lowest price is first.  * output sets the quantityFromSimilarOrders field to the sum of the quantity values from the documents in a range window.\n   \n   * The window contains documents between a lower limit of -10 and an upper limit of 10. The range is inclusive.\n   \n   * $sum returns the sum of quantity values contained in a range of plus or minus 10 dollars of the current document's price value. In this example output, the sum of the quantity values for documents in the window is shown in the quantityFromSimilarOrders field:\n\n \nTIME RANGE WINDOW EXAMPLES \nUSE A TIME RANGE WINDOW WITH A POSITIVE UPPER BOUND \nThe following example uses a window with a positive upper bound time range unit in $setWindowFields. The pipeline outputs an array of orderDate values for each state that match the specified time range. db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            recentOrders: {               $push: \"$orderDate\",               window: {                  range: [ \"unbounded\", 10 ],                  unit: \"month\"               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output:\n   \n   * Sets the orderDateArrayForState array field to orderDate values for the documents in each state. The array elements are expanded with additions to the previous elements in the array.\n   \n   * Uses $push to return an array of orderDate values from the documents in a range window.  * The window contains documents between an unbounded lower limit and an upper limit set to 10 (10 months after the current document's orderDate value) using a time range unit.  * $push returns the array of orderDate values for the documents between the beginning of the partition and the documents with orderDate values inclusively in a range of the current document's orderDate value plus 10 months. In this example output, the array of orderDate values for CA and WA is shown in the recentOrders field:\n\n USE A TIME RANGE WINDOW WITH A NEGATIVE UPPER BOUND \nThe following example uses a window with a negative upper bound time range unit in $setWindowFields. The pipeline outputs an array of orderDate values for each state that match the specified time range. db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            recentOrders: {               $push: \"$orderDate\",               window: {                  range: [ \"unbounded\", -10 ],                  unit: \"month\"               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.  * sortBy: { orderDate: 1 } sorts the documents in each partition by orderDate in ascending order (1), so the earliest orderDate is first.  * output:\n   \n   * Sets the orderDateArrayForState array field to orderDate values for the documents in each state. The array elements are expanded with additions to the previous elements in the array.\n   \n   * Uses $push to return an array of orderDate values from the documents in a range window.  * The window contains documents between an unbounded lower limit and an upper limit set to -10 (10 months before the current document's orderDate value) using a time range unit.  * $push returns the array of orderDate values for the documents between the beginning of the partition and the documents with orderDate values inclusively in a range of the current document's orderDate value minus 10 months. In this example output, the array of orderDate values for CA and WA is shown in the recentOrders field:\n\n \nTIP \nSEE ALSO: For an additional example about IOT Power Consumption, see the Practical MongoDB Aggregations e-book. ←  $set (aggregation)$shardedDataDistribution (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Window Operators\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/gte/": " Docs Home → MongoDB Manual \n$GTE (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$gte \nCompares two values and returns:      * true when the first value is greater than or equivalent to the second value.  * false when the first value is less than the second value. The $gte compares both value and type, using the specified BSON comparison order for values of different types. $gte has the following syntax: { $gte: [ <expression1>, <expression2> ] }  For more information on expressions, see Expressions. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 }{ \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 }{ \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }  The following operation uses the $gte operator to determine if qty is greater than or equal to 250: db.inventory.aggregate(   [     {       $project:          {            item: 1,            qty: 1,            qtyGte250: { $gte: [ \"$qty\", 250 ] },            _id: 0          }     }   ])  The operation returns the following results: { \"item\" : \"abc1\", \"qty\" : 300, \"qtyGte250\" : true }{ \"item\" : \"abc2\", \"qty\" : 200, \"qtyGte250\" : false }{ \"item\" : \"xyz1\", \"qty\" : 250, \"qtyGte250\" : true }{ \"item\" : \"VWZ1\", \"qty\" : 300, \"qtyGte250\" : true }{ \"item\" : \"VWZ2\", \"qty\" : 180, \"qtyGte250\" : false } \n←  $gt (aggregation)$hour (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/millisecond/": " Docs Home → MongoDB Manual \n$MILLISECOND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$millisecond \nReturns the millisecond portion of a date as an integer between 0 and 999. The $millisecond expression has the following operator expression syntax: { $millisecond: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $millisecond: new Date(\"2016-01-01\") } 0 { $millisecond: { date: new Date(\"Jan 7, 2003\") } } 0 { $millisecond: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 0 { $millisecond: ISODate(\"1998-11-07T00:00:00Z\") } 0 { $millisecond: {    date: ISODate(\"1998-11-07T00:00:00Z\"),    timezone: \"-0400\"} } 0 { $millisecond: \"March 28, 1976\" } error { $millisecond: Date(\"2016-01-01\") } error { $millisecond: \"2009-04-09\" } error \nNOTE \n$MILLISECOND CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $millisecond and other date operators to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $minN (array operator)$minute (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dayOfWeek/": " Docs Home → MongoDB Manual \n$DAYOFWEEK (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$dayOfWeek \nReturns the day of the week for a date as a number between 1 (Sunday) and 7 (Saturday). The $dayOfWeek expression has the following operator expression syntax: { $dayOfWeek: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $dayOfWeek: new Date(\"2016-01-01\") } 6 { $dayOfWeek: { date: new Date(\"Jan 7, 2003\") } } 3 { $dayOfWeek: {    date: new Date(\"August 14, 2011\"),    timezone: \"America/Chicago\"} } 1 { $dayOfWeek: ISODate(\"1998-11-07T00:00:00Z\") } 7 { $dayOfWeek: {    date: ISODate(\"1998-11-07T00:00:00Z\"),    timezone: \"-0400\"} } 6 { $dayOfWeek: \"March 28, 1976\" } error { $dayOfWeek: Date(\"2016-01-01\") } error { $dayOfWeek: \"2009-04-09\" } error \nNOTE \n$DAYOFWEEK CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses the $dayOfWeek and other date operators to break down the date field: db.sales.aggregate(   [     {       $project:         {           year: { $year: \"$date\" },           month: { $month: \"$date\" },           day: { $dayOfMonth: \"$date\" },           hour: { $hour: \"$date\" },           minutes: { $minute: \"$date\" },           seconds: { $second: \"$date\" },           milliseconds: { $millisecond: \"$date\" },           dayOfYear: { $dayOfYear: \"$date\" },           dayOfWeek: { $dayOfWeek: \"$date\" },           week: { $week: \"$date\" }         }     }   ])  The operation returns the following result:\n\n \n←  $dayOfMonth (aggregation)$dayOfYear (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/regexFind/": " Docs Home → MongoDB Manual \n$REGEXFIND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$regexFind \nNew in version 4.2. Provides regular expression (regex) pattern matching capability in aggregation expressions. If a match is found, returns a document that contains information on the first match. If a match is not found, returns null. MongoDB uses Perl compatible regular expressions (i.e. \"PCRE\" ) version 8.41 with UTF-8 support. Prior to MongoDB 4.2, aggregation pipeline can only use the query operator $regex in the $match stage. For more information on using regex in a query, see $regex. \nSYNTAX \nThe $regexFind operator has the following syntax: { $regexFind: { input: <expression> , regex: <expression>, options: <expression> } }  \nOPERATOR FIELDS \nField\nDescription\ninput The string on which you wish to apply the regex pattern. Can be a string or any valid expression that resolves to a string. regex The regex pattern to apply. Can be any valid expression that resolves to either a string or regex pattern /<pattern>/. When using the regex /<pattern>/, you can also specify the regex options i and m (but not the s or x options):      * \"pattern\"  * /<pattern>/  * /<pattern>/<options> Alternatively, you can also specify the regex options with the options field. To specify the s or x options, you must use the options field. You cannot specify options in both the regex and the options field. options Optional. The following <options> are available for use with regular expression. \nNOTE You cannot specify options in both the regex and the options field. Option\nDescription\ni\nCase insensitivity to match both upper and lower cases. You can specify the option in the options field or as part of the regex field.\nm For patterns that include anchors (i.e. ^ for the start, $ for the end), match at the beginning or end of each line for strings with multiline values. Without this option, these anchors match at beginning or end of the string. If the pattern contains no anchors or if the string value has no newline characters (e.g. \\n), the m option has no effect. x \"Extended\" capability to ignore all white space characters in the pattern unless escaped or included in a character class. Additionally, it ignores characters in-between and including an un-escaped hash/pound (#) character and the next new line, so that you may include comments in complicated patterns. This only applies to data characters; white space characters may never appear within special character sequences in a pattern. The x option does not affect the handling of the VT character (i.e. code 11). You can specify the option only in the options field. s Allows the dot character (i.e. .) to match all characters including newline characters. You can specify the option only in the options field. \nRETURNS \nIf the operator does not find a match, the result of the operator is a null. If the operator finds a match, the result of the operator is a document that contains:  * the first matching string in the input,  * the code point index (not byte index) of the matching string in the input, and  * An array of the strings that corresponds to the groups captured by the matching string. Capturing groups are specified with unescaped parenthesis () in the regex pattern. { \"match\" : <string>, \"idx\" : <num>, \"captures\" : <array of strings> } \nTIP \nSEE ALSO:  * $regexFindAll  * $regexMatch \nBEHAVIOR  $REGEXFIND AND COLLATION \n$regexFind ignores the collation specified for the collection, db.collection.aggregate(), and the index, if used. For example, the create a sample collection with collation strength 1 (i.e. compare base character only and ignore other differences such as case and diacritics): db.createCollection( \"myColl\", { collation: { locale: \"fr\", strength: 1 } } )  Insert the following documents: db.myColl.insertMany([   { _id: 1, category: \"café\" },   { _id: 2, category: \"cafe\" },   { _id: 3, category: \"cafE\" }])  Using the collection's collation, the following operation performs a case-insensitive and diacritic-insensitive match:\n\n  The operation returns the following 3 documents: { \"_id\" : 1, \"category\" : \"café\" }{ \"_id\" : 2, \"category\" : \"cafe\" }{ \"_id\" : 3, \"category\" : \"cafE\" }  However, the aggregation expression $regexFind ignores collation; that is, the following regular expression pattern matching examples are case-sensitive and diacritic sensitive: db.myColl.aggregate( [ { $addFields: { resultObject: { $regexFind: { input: \"$category\", regex: /cafe/ }  } } } ] )db.myColl.aggregate(   [ { $addFields: { resultObject: { $regexFind: { input: \"$category\", regex: /cafe/ }  } } } ],   { collation: { locale: \"fr\", strength: 1 } }           // Ignored in the $regexFind)  Both operations return the following: { \"_id\" : 1, \"category\" : \"café\", \"resultObject\" : null }{ \"_id\" : 2, \"category\" : \"cafe\", \"resultObject\" : { \"match\" : \"cafe\", \"idx\" : 0, \"captures\" : [ ] } }{ \"_id\" : 3, \"category\" : \"cafE\", \"resultObject\" : null }  To perform a case-insensitive regex pattern matching, use the i Option instead. See i Option for an example. \nCAPTURES OUTPUT BEHAVIOR \nIf your regex pattern contains capture groups and the pattern finds a match in the input, the captures array in the results corresponds to the groups captured by the matching string. Capture groups are specified with unescaped parentheses () in the regex pattern. The length of the captures array equals the number of capture groups in the pattern and the order of the array matches the order in which the capture groups appear. Create a sample collection named contacts with the following documents: db.contacts.insertMany([  { \"_id\": 1, \"fname\": \"Carol\", \"lname\": \"Smith\", \"phone\": \"718-555-0113\" },  { \"_id\": 2, \"fname\": \"Daryl\", \"lname\": \"Doe\", \"phone\": \"212-555-8832\" },  { \"_id\": 3, \"fname\": \"Polly\", \"lname\": \"Andrews\", \"phone\": \"208-555-1932\" },  { \"_id\": 4, \"fname\": \"Colleen\", \"lname\": \"Duncan\", \"phone\": \"775-555-0187\" },  { \"_id\": 5, \"fname\": \"Luna\", \"lname\": \"Clarke\", \"phone\": \"917-555-4414\" }])  The following pipeline applies the regex pattern /(C(ar)*)ol/ to the fname field: db.contacts.aggregate([  {    $project: {      returnObject: {        $regexFind: { input: \"$fname\", regex: /(C(ar)*)ol/ }      }    }  }])  The regex pattern finds a match with fname values Carol and Colleen: { \"_id\" : 1, \"returnObject\" : { \"match\" : \"Carol\", \"idx\" : 0, \"captures\" : [ \"Car\", \"ar\" ] } }{ \"_id\" : 2, \"returnObject\" : null }{ \"_id\" : 3, \"returnObject\" : null }{ \"_id\" : 4, \"returnObject\" : { \"match\" : \"Col\", \"idx\" : 0, \"captures\" : [ \"C\", null ] } }{ \"_id\" : 5, \"returnObject\" : null } The pattern contains the capture group (C(ar)*) which contains the nested group (ar). The elements in the captures array correspond to the two capture groups. If a matching document is not captured by a group (e.g. Colleen and the group (ar)), $regexFind replaces the group with a null placeholder. As shown in the previous example, the captures array contains an element for each capture group (using null for non-captures). Consider the following example which searches for phone numbers with New York City area codes by applying a logical or of capture groups to the phone field. Each group represents a New York City area code:\n\n  For documents which are matched by the regex pattern, the captures array includes the matching capture group and replaces any non-capturing groups with null: { \"_id\" : 1, \"nycContacts\" : { \"match\" : \"718-555-0113\", \"idx\" : 0, \"captures\" : [ \"718\", null, null ] } }{ \"_id\" : 2, \"nycContacts\" : { \"match\" : \"212-555-8832\", \"idx\" : 0, \"captures\" : [ null, \"212\", null ] } }{ \"_id\" : 3, \"nycContacts\" : null }{ \"_id\" : 4, \"nycContacts\" : null }{ \"_id\" : 5, \"nycContacts\" : { \"match\" : \"917-555-4414\", \"idx\" : 0, \"captures\" : [ null, null, \"917\" ] } } \nEXAMPLES  $REGEXFIND AND ITS OPTIONS \nTo illustrate the behavior of the $regexFind operator as discussed in this example, create a sample collection products with the following documents: db.products.insertMany([   { _id: 1, description: \"Single LINE description.\" },   { _id: 2, description: \"First lines\\nsecond line\" },   { _id: 3, description: \"Many spaces before     line\" },   { _id: 4, description: \"Multiple\\nline descriptions\" },   { _id: 5, description: \"anchors, links and hyperlinks\" },   { _id: 6, description: \"métier work vocation\" }])  By default, $regexFind performs a case-sensitive match. For example, the following aggregation performs a case-sensitive $regexFind on the description field. The regex pattern /line/ does not specify any grouping: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex: /line/ } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"returnObject\" : null }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 6, \"captures\" : [ ] } }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 23, \"captures\" : [ ] } }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 9, \"captures\" : [ ] } }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"returnObject\" : null }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"returnObject\" : null } The following regex pattern /lin(e|k)/ specifies a grouping (e|k) in the pattern: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex: /lin(e|k)/ } } } }])  The operation returns the following:\n\n In the return option, the idx field is the code point index and not the byte index. To illustrate, consider the following example that uses the regex pattern /tier/: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex: /tier/ } } } }])  The operation returns the following where only the last record matches the pattern and the returned idx is 2 (instead of 3 if using a byte index) { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"returnObject\" : null }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"returnObject\" : null }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"returnObject\" : null }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"returnObject\" : null }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"returnObject\" : null }{ \"_id\" : 6, \"description\" : \"métier work vocation\",             \"returnObject\" : { \"match\" : \"tier\", \"idx\" : 2, \"captures\" : [ ] } } I OPTION  NOTE You cannot specify options in both the regex and the options field. To perform case-insensitive pattern matching, include the i option as part of the regex field or in the options field: // Specify i as part of the regex field{ $regexFind: { input: \"$description\", regex: /line/i } }\n// Specify i in the options field{ $regexFind: { input: \"$description\", regex: /line/, options: \"i\" } }{ $regexFind: { input: \"$description\", regex: \"line\", options: \"i\" } } For example, the following aggregation performs a case-insensitive $regexFind on the description field. The regex pattern /line/ does not specify any grouping: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex: /line/i } } } }])  The operation returns the following documents: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"returnObject\" : { \"match\" : \"LINE\", \"idx\" : 7, \"captures\" : [ ] } }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 6, \"captures\" : [ ] } }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 23, \"captures\" : [ ] } }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 9, \"captures\" : [ ] } }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"returnObject\" : null }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"returnObject\" : null } M OPTION  NOTE You cannot specify options in both the regex and the options field. To match the specified anchors (e.g. ^, $) for each line of a multiline string, include the m option as part of the regex field or in the options field: // Specify m as part of the regex field{ $regexFind: { input: \"$description\", regex: /line/m } }\n// Specify m in the options field{ $regexFind: { input: \"$description\", regex: /line/, options: \"m\" } }{ $regexFind: { input: \"$description\", regex: \"line\", options: \"m\" } } The following example includes both the i and the m options to match lines starting with either the letter s or S for multiline strings: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex: /^s/im } } } }])  The operation returns the following:\n\n X OPTION  NOTE You cannot specify options in both the regex and the options field. To ignore all unescaped white space characters and comments (denoted by the un-escaped hash # character and the next new-line character) in the pattern, include the s option in the options field: // Specify x in the options field{ $regexFind: { input: \"$description\", regex: /line/, options: \"x\" } }{ $regexFind: { input: \"$description\", regex: \"line\", options: \"x\" } } The following example includes the x option to skip unescaped white spaces and comments: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex: /lin(e|k) # matches line or link/, options:\"x\" } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"returnObject\" : null }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 6, \"captures\" : [ \"e\" ] } }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 23, \"captures\" : [ \"e\" ] } }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"returnObject\" : { \"match\" : \"line\", \"idx\" : 9, \"captures\" : [ \"e\" ] } }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"returnObject\" : { \"match\" : \"link\", \"idx\" : 9, \"captures\" : [ \"k\" ] } }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"returnObject\" : null } S OPTION  NOTE You cannot specify options in both the regex and the options field. To allow the dot character (i.e. .) in the pattern to match all characters including the new line character, include the s option in the options field: // Specify s in the options field{ $regexFind: { input: \"$description\", regex: /m.*line/, options: \"s\" } }{ $regexFind: { input: \"$description\", regex: \"m.*line\", options: \"s\" } } The following example includes the s option to allow the dot character (i.e. .) to match all characters including new line as well as the i option to perform a case-insensitive match: db.products.aggregate([   { $addFields: { returnObject: { $regexFind: { input: \"$description\", regex:/m.*line/, options: \"si\"  } } } }])  The operation returns the following: { \"_id\" : 1, \"description\" : \"Single LINE description.\", \"returnObject\" : null }{ \"_id\" : 2, \"description\" : \"First lines\\nsecond line\", \"returnObject\" : null }{ \"_id\" : 3, \"description\" : \"Many spaces before     line\", \"returnObject\" : { \"match\" : \"Many spaces before     line\", \"idx\" : 0, \"captures\" : [ ] } }{ \"_id\" : 4, \"description\" : \"Multiple\\nline descriptions\", \"returnObject\" : { \"match\" : \"Multiple\\nline\", \"idx\" : 0, \"captures\" : [ ] } }{ \"_id\" : 5, \"description\" : \"anchors, links and hyperlinks\", \"returnObject\" : null }{ \"_id\" : 6, \"description\" : \"métier work vocation\", \"returnObject\" : null } \nUSE $REGEXFIND TO PARSE EMAIL FROM STRING \nCreate a sample collection feedback with the following documents:\n\n  The following aggregation uses the $regexFind to extract the email from the comment field (case insensitive). db.feedback.aggregate( [    { $addFields: {       \"email\": { $regexFind: { input: \"$comment\", regex: /[a-z0-9_.+-]+@[a-z0-9_.+-]+\\.[a-z0-9_.+-]+/i } }    } },    { $set: { email: \"$email.match\"} }] ) \nFirst Stage The stage uses the $addFields stage to add a new field email to the document. The new field contains the result of performing the $regexFind on the comment field: { \"_id\" : 1, \"comment\" : \"Hi, I'm just reading about MongoDB -- aunt.arc.tica@example.com\", \"email\" : { \"match\" : \"aunt.arc.tica@example.com\", \"idx\" : 38, \"captures\" : [ ] } }{ \"_id\" : 2, \"comment\" : \"I wanted to concatenate a string\", \"email\" : null }{ \"_id\" : 3, \"comment\" : \"I can't find how to convert a date to string. cam@mongodb.com\", \"email\" : { \"match\" : \"cam@mongodb.com\", \"idx\" : 46, \"captures\" : [ ] } }{ \"_id\" : 4, \"comment\" : \"It's just me. I'm testing.  fred@MongoDB.com\", \"email\" : { \"match\" : \"fred@MongoDB.com\", \"idx\" : 28, \"captures\" : [ ] } } Second Stage The stage use the $set stage to reset the email to the current \"$email.match\" value. If the current value of email is null, the new value of email is set to null. { \"_id\" : 1, \"comment\" : \"Hi, I'm just reading about MongoDB -- aunt.arc.tica@example.com\", \"email\" : \"aunt.arc.tica@example.com\" }{ \"_id\" : 2, \"comment\" : \"I wanted to concatenate a string\" }{ \"_id\" : 3, \"comment\" : \"I can't find how to convert a date to string. cam@mongodb.com\", \"email\" : \"cam@mongodb.com\" }{ \"_id\" : 4, \"comment\" : \"It's just me. I'm testing.  fred@MongoDB.com\", \"email\" : \"fred@MongoDB.com\" } \nAPPLY $REGEXFIND TO STRING ELEMENTS OF AN ARRAY \nCreate a sample collection contacts with the following documents: db.contacts.insertMany([   { \"_id\" : 1, name: \"Aunt Arc Tikka\", details: [ \"+672-19-9999\", \"aunt.arc.tica@example.com\" ] },   { \"_id\" : 2, name: \"Belle Gium\",  details: [ \"+32-2-111-11-11\", \"belle.gium@example.com\" ] },   { \"_id\" : 3, name: \"Cam Bo Dia\",  details: [ \"+855-012-000-0000\", \"cam.bo.dia@example.com\" ] },   { \"_id\" : 4, name: \"Fred\", details: [ \"+1-111-222-3333\" ] }])  The following aggregation uses the $regexFind to convert the details array into an embedded document with an email and phone fields:\n\n \nFirst Stage The stage $unwinds the array into separate documents: { \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"details\" : \"+672-19-9999\" }{ \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"details\" : \"aunt.arc.tica@example.com\" }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"details\" : \"+32-2-111-11-11\" }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"details\" : \"belle.gium@example.com\" }{ \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"details\" : \"+855-012-000-0000\" }{ \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"details\" : \"cam.bo.dia@example.com\" }{ \"_id\" : 4, \"name\" : \"Fred\", \"details\" : \"+1-111-222-3333\" } Second Stage The stage uses the $addFields stage to add new fields to the document that contains the result of the $regexFind for phone number and email: { \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"details\" : \"+672-19-9999\", \"regexemail\" : null, \"regexphone\" : { \"match\" : \"+672-19-9999\", \"idx\" : 0, \"captures\" : [ ] } }{ \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"details\" : \"aunt.arc.tica@example.com\", \"regexemail\" : { \"match\" : \"aunt.arc.tica@example.com\", \"idx\" : 0, \"captures\" : [ ] }, \"regexphone\" : null }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"details\" : \"+32-2-111-11-11\", \"regexemail\" : null, \"regexphone\" : { \"match\" : \"+32-2-111-11-11\", \"idx\" : 0, \"captures\" : [ ] } }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"details\" : \"belle.gium@example.com\", \"regexemail\" : { \"match\" : \"belle.gium@example.com\", \"idx\" : 0, \"captures\" : [ ] }, \"regexphone\" : null }{ \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"details\" : \"+855-012-000-0000\", \"regexemail\" : null, \"regexphone\" : { \"match\" : \"+855-012-000-0000\", \"idx\" : 0, \"captures\" : [ ] } }{ \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"details\" : \"cam.bo.dia@example.com\", \"regexemail\" : { \"match\" : \"cam.bo.dia@example.com\", \"idx\" : 0, \"captures\" : [ ] }, \"regexphone\" : null }{ \"_id\" : 4, \"name\" : \"Fred\", \"details\" : \"+1-111-222-3333\", \"regexemail\" : null, \"regexphone\" : { \"match\" : \"+1-111-222-3333\", \"idx\" : 0, \"captures\" : [ ] } } Third Stage The stage use the $project stage to output documents with the _id field, the name field and the details field. The details field is set to a document with email and phone fields, whose values are determined from the regexemail and regexphone fields, respectively.\n\n \nFourth Stage The stage uses the $group stage to groups the input documents by their _id value. The stage uses the $mergeObjects expression to merge the details documents. { \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"details\" : { \"phone\" : \"+855-012-000-0000\", \"email\" : \"cam.bo.dia@example.com\" } }{ \"_id\" : 4, \"name\" : \"Fred\", \"details\" : { \"phone\" : \"+1-111-222-3333\" } }{ \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"details\" : { \"phone\" : \"+672-19-9999\", \"email\" : \"aunt.arc.tica@example.com\" } }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"details\" : { \"phone\" : \"+32-2-111-11-11\", \"email\" : \"belle.gium@example.com\" } } Fifth Stage The stage uses the $sort stage to sort the documents by the _id field. { \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"details\" : { \"phone\" : \"+672-19-9999\", \"email\" : \"aunt.arc.tica@example.com\" } }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"details\" : { \"phone\" : \"+32-2-111-11-11\", \"email\" : \"belle.gium@example.com\" } }{ \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"details\" : { \"phone\" : \"+855-012-000-0000\", \"email\" : \"cam.bo.dia@example.com\" } }{ \"_id\" : 4, \"name\" : \"Fred\", \"details\" : { \"phone\" : \"+1-111-222-3333\" } } \nUSE CAPTURED GROUPINGS TO PARSE USER NAME \nCreate a sample collection employees with the following documents: db.employees.insertMany([   { \"_id\" : 1, name: \"Aunt Arc Tikka\", \"email\" : \"aunt.tica@example.com\" },   { \"_id\" : 2, name: \"Belle Gium\", \"email\" : \"belle.gium@example.com\" },   { \"_id\" : 3, name: \"Cam Bo Dia\", \"email\" : \"cam.dia@example.com\" },   { \"_id\" : 4, name: \"Fred\"  }])  The employee email has the format <firstname>.<lastname>@example.com. Using the captured field returned in the $regexFind results, you can parse out user names for employees. db.employees.aggregate( [    { $addFields: {       \"username\": { $regexFind: { input: \"$email\", regex: /^([a-z0-9_.+-]+)@[a-z0-9_.+-]+\\.[a-z0-9_.+-]+$/, options: \"i\" } },    } },    { $set: { username: { $arrayElemAt:  [ \"$username.captures\", 0 ] } } }] ) \nFirst Stage The stage uses the $addFields stage to add a new field username to the document. The new field contains the result of performing the $regexFind on the email field:\n\n Second Stage The stage use the $set stage to reset the username to the zero-th element of the \"$username.captures\" array. If the current value of username is null, the new value of username is set to null. { \"_id\" : 1, \"name\" : \"Aunt Arc Tikka\", \"email\" : \"aunt.tica@example.com\", \"username\" : \"aunt.tica\" }{ \"_id\" : 2, \"name\" : \"Belle Gium\", \"email\" : \"belle.gium@example.com\", \"username\" : \"belle.gium\" }{ \"_id\" : 3, \"name\" : \"Cam Bo Dia\", \"email\" : \"cam.dia@example.com\", \"username\" : \"cam.dia\" }{ \"_id\" : 4, \"name\" : \"Fred\", \"username\" : null } \nTIP \nSEE ALSO: For more information on the behavior of the captures array and additional examples, see captures Output Behavior. ←  $reduce (aggregation)$regexFindAll (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/planCacheStats/": " Docs Home → MongoDB Manual \n$PLANCACHESTATS \nOn this page    \n * Definition\n   \n * Considerations\n * Output\n * Examples \nDEFINITION \n$planCacheStats \nNew in version 4.2. Returns plan cache information for a collection. The stage returns a document for each plan cache entry. The $planCacheStats stage must be the first stage in the pipeline. The stage takes an empty document as a parameter and has the following syntax: { $planCacheStats: { } }  \nNOTE \n4.4 CHANGES Starting in version 4.4,      * $planCacheStats stage can be run on mongos instances as well as on mongod instances. In 4.2, $planCacheStats stage can only run on mongod instances.  * $planCacheStats includes new fields: the host field and, when run against a mongos, the shard field.  * mongo shell provides the method PlanCache.list() as a wrapper for $planCacheStats aggregation stage.  * MongoDB removes the following:\n   \n   * planCacheListPlans and planCacheListQueryShapes commands, and\n   \n   * PlanCache.getPlansByQuery() and PlanCache.listQueryShapes() methods.\n   \n   Use $planCacheStats or PlanCache.list() instead. \nTIP \nSEE ALSO: Query Plans \nCONSIDERATIONS  PIPELINE \n$planCacheStats must be the first stage in an aggregation pipeline. \nRESTRICTIONS \n * $planCacheStats is not allowed in:\n   \n   * transactions\n   \n   * $facet aggregation stage  * $planCacheStats requires read concern level \"local\". \nACCESS CONTROL \nOn systems running with authorization, the user must have the planCacheRead privilege for the collection. \nREAD PREFERENCE \n$planCacheStats observes the read preference in selecting the host(s) from which to return the plan cache information. Applications may target different members of a replica set. As such, each replica set member might receive different read commands and have plan cache information that differs from other members. Nevertheless, running $planCacheStats on a replica set or a sharded cluster obeys the normal read preference rules. That is, on a replica set, the operation gathers plan cache information from just one member of replica set, and on a sharded cluster, the operation gathers plan cache information from just one member of each shard replica set. \nOUTPUT \nFor each plan cache entry, the $planCacheStats stage returns a document similar to the following:\n\n  Each document includes various query plan and execution stats, including: Field\nDescription\ncreatedFromQuery A document that contains the specific query that resulted in this cache entry; i.e. {  \"query\" : <document>,  \"sort\" : <document>,  \"projection\" : <document>} isActive A boolean that indicates whether the entry is active or inactive.  * If active, the query planner is currently using the entry to generate query plans.  * If inactive, the query planner is not currently using the entry to generate query plans. \nTIP \nSEE ALSO: Plan Cache Entry State queryHash A hexadecimal string that represents the hash of the query shape. For more information, see explain.queryPlanner.queryHash planCacheKey A hexadecimal string that represents the hash of the key used to find the plan cache entry associated with this query. The plan cache key is a function of both the query shape and the currently available indexes for that shape. For more information, see explain.queryPlanner.planCacheKey cachedPlan The details of the cached plan. See explain.queryPlanner. works\nThe number of \"work units\" performed by the query execution plan during the trial period when the query planner evaluates candidate plans. For more information, see explain.executionStats.executionStages.works\ntimeOfCreation\nTime of creation for the entry.\ncreationExecStats An array of execution stats documents. The array contains a document for each candidate plan. For details on the execution stats, see explain.executionStats. candidatePlanScores An array of scores for the candidate plans listed in the creationExecStats array. indexFilterSet\nA boolean that indicates whether the an index filter exists for the query shape.\nestimatedSizeBytes A number that describes the estimated size in bytes of a plan cache entry. New in version 5.0. Starting in MongoDB 5.0, 4.4.3, and 4.2.12, this field is available. host The hostname and port of the mongod instance from which the plan cache information was returned. When run on a sharded cluster, the operation returns plan cache entry information from a single member in each shard replica set. This member is identified with the shard and host fields. See also Read Preference. New in version 4.4. shard The name of the shard from which $planCacheStats retrieved the cache entry. Only available if run on a sharded cluster. New in version 4.4. \nEXAMPLES \nThe examples in this section use the following orders collection: db.orders.insertMany( [   { \"_id\" : 1, \"item\" : \"abc\", \"price\" : NumberDecimal(\"12\"), \"quantity\" : 2, \"type\": \"apparel\" },   { \"_id\" : 2, \"item\" : \"jkl\", \"price\" : NumberDecimal(\"20\"), \"quantity\" : 1, \"type\": \"electronics\" },   { \"_id\" : 3, \"item\" : \"abc\", \"price\" : NumberDecimal(\"10\"), \"quantity\" : 5, \"type\": \"apparel\" },   { \"_id\" : 4, \"item\" : \"abc\", \"price\" : NumberDecimal(\"8\"), \"quantity\" : 10, \"type\": \"apparel\" },   { \"_id\" : 5, \"item\" : \"jkl\", \"price\" : NumberDecimal(\"15\"), \"quantity\" : 15, \"type\": \"electronics\" }] )  Create the following indexes on the collection: db.orders.createIndex( { item: 1 } );db.orders.createIndex( { item: 1, quantity: 1 } );db.orders.createIndex( { quantity: 1 } );db.orders.createIndex( { quantity: 1, type: 1 } );db.orders.createIndex(   { item: 1, price: 1 },   { partialFilterExpression: { price: { $gte: NumberDecimal(\"10\")} } });  \nNOTE Index { item: 1, price: 1 } is a partial index and only indexes documents with price field greater than or equal to NumberDecimal(\"10\"). Run some queries against the collection:\n\n  \nRETURN INFORMATION FOR ALL ENTRIES IN THE QUERY CACHE \nThe following aggregation pipeline uses $planCacheStats to return information on the plan cache entries for the collection: db.orders.aggregate( [   { $planCacheStats: { } }] )  The operation returns all entries in the cache:\n\n  See also planCacheKey. \nLIST QUERY SHAPES \nMongoDB 4.4 removes the deprecated planCacheListQueryShapes command and its helper method PlanCache.listQueryShapes(). As an alternative, you can use the $planCacheStats stage to obtain a list of all of the query shapes for which there is a cached plan. For example, the following uses the $project stage to only output the createdFromQuery field and the queryHash field. db.orders.aggregate( [ { $planCacheStats: { } } , { $project: {createdFromQuery: 1, queryHash: 1 } } ] )  The operation returns the following query shapes: { \"createdFromQuery\" : { \"query\" : { \"item\" : \"abc\", \"price\" : { \"$gte\" : NumberDecimal(\"5\") } }, \"sort\" : {  }, \"projection\" : {  } }, \"queryHash\" : \"117A6B10\" }{ \"createdFromQuery\" : { \"query\" : { \"quantity\" : { \"$gte\" : 5 }, \"type\" : \"apparel\" }, \"sort\" : {  }, \"projection\" : {  } }, \"queryHash\" : \"4D151C4C\" }{ \"createdFromQuery\" : { \"query\" : { \"quantity\" : { \"$gte\" : 20 } }, \"sort\" : {  }, \"projection\" : {  } }, \"queryHash\" : \"23B19B75\" }{ \"createdFromQuery\" : { \"query\" : { \"item\" : \"abc\", \"price\" : { \"$gte\" : NumberDecimal(\"10\") } }, \"sort\" : {  }, \"projection\" : {  } }, \"queryHash\" : \"117A6B10\" } \nFIND CACHE ENTRY DETAILS FOR A QUERY SHAPE \nTo return plan cache information for a particular query shape, the $planCacheStats stage can be followed by a $match on the planCacheKey field. The following aggregation pipeline uses $planCacheStats followed by a $match and $project to return specific information for a particular query shape: db.orders.aggregate( [   { $planCacheStats: { } },   { $match: { planCacheKey: \"DD67E353\"} }] )  The operation returns the following:\n\n See also planCacheKey and queryHash. ←  $out (aggregation)$project (aggregation) → On this page  * Definition\n * Considerations\n * Output\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/searchMeta/": " Docs Home → MongoDB Manual \n$SEARCHMETA (AGGREGATION) \nsearchMeta returns different types of metadata result documents for Atlas Search queries on the field or fields in an Atlas collection. The fields must be covered by an Atlas Search index. See:      * Index Definitions to learn more about creating and managing Atlas Search indexes.  * Atlas Search Aggregation Pipeline Stages to learn more about the $searchMeta pipeline stage syntax, usage, and results. \nIMPORTANT The $searchMeta aggregation pipeline stage is only available for collections hosted on MongoDB Atlas cluster tiers running MongoDB version 4.4.9 or later. To learn more, see Atlas Search. ←  $search (aggregation)$set (aggregation) → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/gt/": " Docs Home → MongoDB Manual \n$GT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$gt \nCompares two values and returns:      * true when the first value is greater than the second value.  * false when the first value is less than or equivalent to the second value. The $gt compares both value and type, using the specified BSON comparison order for values of different types. $gt has the following syntax: { $gt: [ <expression1>, <expression2> ] }  For more information on expressions, see Expressions. \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"abc1\", description: \"product 1\", qty: 300 }{ \"_id\" : 2, \"item\" : \"abc2\", description: \"product 2\", qty: 200 }{ \"_id\" : 3, \"item\" : \"xyz1\", description: \"product 3\", qty: 250 }{ \"_id\" : 4, \"item\" : \"VWZ1\", description: \"product 4\", qty: 300 }{ \"_id\" : 5, \"item\" : \"VWZ2\", description: \"product 5\", qty: 180 }  The following operation uses the $gt operator to determine if qty is greater than 250: db.inventory.aggregate(   [     {       $project:          {            item: 1,            qty: 1,            qtyGt250: { $gt: [ \"$qty\", 250 ] },            _id: 0          }     }   ])  The operation returns the following results: { \"item\" : \"abc1\", \"qty\" : 300, \"qtyGt250\" : true }{ \"item\" : \"abc2\", \"qty\" : 200, \"qtyGt250\" : false }{ \"item\" : \"xyz1\", \"qty\" : 250, \"qtyGt250\" : false }{ \"item\" : \"VWZ1\", \"qty\" : 300, \"qtyGt250\" : true }{ \"item\" : \"VWZ2\", \"qty\" : 180, \"qtyGt250\" : false } \n←  $getField (aggregation)$gte (aggregation) → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/changeStream/": " Docs Home → MongoDB Manual \n$CHANGESTREAM (AGGREGATION) \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$changeStream \nReturns a Change Stream cursor on a collection, a database, or an entire cluster. Must be used as the first stage in an aggregation pipeline. The $changeStream stage has the following syntax: {  $changeStream: {    allChangesForCluster: <boolean>,    fullDocument: <string>,    fullDocumentBeforeChange: <string>,    resumeAfter: <int>    showExpandedEvents: <boolean>,    startAfter: <document>    startAtOperationTime: <timestamp>  }} \nParameter\nDescription\nallChangesForCluster\nOptional: Sets whether the change stream should include all changes in the cluster. May only be opened on the admin database.\nfullDocument Optional: Specifies whether change notifications include a copy of the full document when modified by update operations.      * default: Change notifications do not include the full document for update operations.  * required: Change notifications includes a copy of the modified document as it appeared immediately after the change. If the document cannot be found, the change stream throws an error.\n   \n   To use this option, you must first use the collMod command to enable the changeStreamPreAndPostImages option.\n   \n   New in version 6.0.  * updateLookup: Change notifications includes a copy of the document modified by the change. This document is the current majority-committed document or null if it no longer exists.  * whenAvailable: Change notification includes a copy of the modified document as it appeared immediately after the change or null if the document is unavailable.\n   \n   To use this option, you must first use the collMod command to enable the changeStreamPreAndPostImages option.\n   \n   New in version 6.0. In the case of partial updates, the change notification also provides a description of the change. fullDocumentBeforeChange Include the full document from before the change. This field accepts the following values:  * off: Disables inclusion of the document from before the change.  * whenAvailable: Includes document from before the change. The query does not fail if the unmodified document is not available.  * required: Includes document from before the change. The query fails if the unmodified document is not available. resumeAfter\nSpecifies a resume token as the logical starting point for the change stream. Cannot be used with startAfter or startAtOperationTime fields.\nshowExpandedEvents Specifies whether to include additional change events, such as such as DDL and index operations. New in version 6.0. startAfter\nSpecifies a resume token as the logical starting point for the change stream. Cannot be used with resumeAfter or startAtOperationTime fields.\nstartAtOperationTime\nSpecifies a time as the logical starting point for the change stream. Cannot be used with resumeAfter or startAfter fields. \nEXAMPLES \nTo create a change stream cursor using the aggregation stage, run the aggregate command. var cur = db.names.aggregate( [   { $changeStream: {} }] )cur.next()  When the change stream detects a change, the next() method returns a change event notification. For example: {   \"_id\": {      _data: \"8262E2EE54000000022B022C0100296E5A100448E5E3DD01364019AE8FE8C6859527E046645F6964006462E2EE54C8756C0D5CF6F0720004\"   },   \"operationType\": \"insert\",   \"clusterTime\": Timestamp({ t: 1659039316, i: 2 }),   \"wallTime\": ISODate(\"2022-07-28T20:15:16.148Z\"),   \"fullDocument\": {      \"_id\": ObjectId(\"62e2ee54c8756c0d5cf6f072\"),      \"name\": \"Walker Percy\"   },   \"ns\": {      \"db\": \"test\",      \"coll\": \"names\"   },   \"documentKey\": { _id: ObjectId(\"62e2ee54c8756c0d5cf6f072\") }} For more information on change stream notifications, see Change Events\n\n On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toInt/": " Docs Home → MongoDB Manual \n$TOINT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toInt \nConverts a value to an integer. If the value cannot be converted to an integer, $toInt errors. If the value is null or missing, $toInt returns null. $toInt has the following syntax: {   $toInt: <expression>}  The $toInt takes any valid expression. The $toInt is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"int\" } }  \nTIP \nSEE ALSO: $convert \nBEHAVIOR \nThe following table lists the input types that can be converted to an integer: Input Type\nBehavior\nBoolean\nReturns 0 for false.\nReturns 1 for true.\nDouble Returns truncated value. The truncated double value must fall within the minimum and maximum value for an integer. You cannot convert a double value whose truncated value is less than the minimum integer value or is greater than the maximum integer value. Decimal Returns truncated value. The truncated decimal value must fall within the minimum and maximum value for an integer. You cannot convert a decimal value whose truncated value is less than the minimum integer value or is greater than the maximum integer value. Integer\nNo-op. Returns the integer value.\nLong Returns the long value as an integer. The long value must fall within the minimum and maximum value for an integer. You cannot convert a long value that is less than the minimum integer value or is greater than the maximum integer value. String Returns the numerical value of the string as an integer. The string value must be a base 10 integer; e.g. \"-5\", \"123456\"). You cannot convert a string value of a float or decimal or non-base 10 number (e.g. \"-5.0\", \"0x6400\") The following table lists some conversion to integer examples: Example\nResults\n$toInt: true\n1\n$toInt: false\n0\n$toInt: 1.99999\n1\n$toInt: NumberDecimal(\"5.5000\")\n5\n$toInt: NumberDecimal(\"9223372036000.000\")\nError\n$toInt: NumberLong(\"5000\")\n5000\n$toInt: NumberLong(\"922337203600\")\nError\n$toInt: \"-2\"\n-2\n$toInt: \"2.5\"\nError\n$toInt: null\nnull \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: 1, item: \"apple\", qty: \"5\", price: 10 },   { _id: 2, item: \"pie\", qty: \"10\", price: NumberDecimal(\"20.0\") },   { _id: 3, item: \"ice cream\", qty: \"2\", price: \"4.99\" },   { _id: 4, item: \"almonds\" ,  qty: \"5\", price: 5 }] )  The following aggregation operation:      * converts qty to an integer,  * converts price to a decimal,  * calculates the total price: // Define stage to add convertedPrice and convertedQty fields with the converted price and qty values\npriceQtyConversionStage = {   $addFields: {      convertedPrice: { $toDecimal: \"$price\" },      convertedQty: { $toInt: \"$qty\" },   }};\n// Define stage to calculate total price by multiplying convertedPrice and convertedQty fields totalPriceCalculationStage = {   $project: { item: 1, totalPrice: { $multiply: [ \"$convertedPrice\", \"$convertedQty\" ] } }};\ndb.orders.aggregate( [   priceQtyConversionStage,   totalPriceCalculationStage] )  The operation returns the following documents: { _id: 1, item: 'apple', totalPrice: Decimal128(\"50\") },{ _id: 2, item: 'pie', totalPrice: Decimal128(\"200.0\") },{ _id: 3, item: 'ice cream', totalPrice: Decimal128(\"9.98\") },{ _id: 4, item: 'almonds', totalPrice: Decimal128(\"25\") }  \nNOTE\n\n ←  $toDouble(aggregation)$toLong (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toObjectId/": " Docs Home → MongoDB Manual \n$TOOBJECTID (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toObjectId \nConverts a value to an ObjectId(). If the value cannot be converted to an ObjectId, $toObjectId errors. If the value is null or missing, $toObjectId returns null. $toObjectId has the following syntax: {   $toObjectId: <expression>}  The $toObjectId takes any valid expression. The $toObjectId is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"objectId\" } }  \nTIP \nSEE ALSO: $convert \nBEHAVIOR \nThe following table lists the input types that can be converted to an ObjectId: Input Type\nBehavior\nString Returns an ObjectId for the hexadecimal string of length 24. You cannot convert a string value that is not a hexadecimal string of length 24. The following table lists some conversion to date examples: Example\nResults\n{$toObjectId: \"5ab9cbfa31c2ab715d42129e\"}\nObjectId(\"5ab9cbfa31c2ab715d42129e\")\n{$toObjectId: \"5ab9cbfa31c2ab715d42129\"}\nError \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: \"5ab9cbe531c2ab715d42129a\", item: \"apple\", qty: 10 },   { _id: ObjectId(\"5ab9d0b831c2ab715d4212a8\"), item: \"pie\", qty: 5 },   { _id: ObjectId(\"5ab9d2d331c2ab715d4212b3\"), item: \"ice cream\", qty: 20 },   { _id: \"5ab9e16431c2ab715d4212b4\", item: \"almonds\", qty: 50 },] )  The following aggregation operation on the orders collection converts the _id to ObjectId before sorting by the value: // Define stage to add convertedId field with converted _id value\nidConversionStage = {   $addFields: {      convertedId: { $toObjectId: \"$_id\" }   }};\n// Define stage to sort documents by the converted qty values\nsortStage = {   $sort: { \"convertedId\": -1 }}; db.orders.aggregate( [   idConversionStage,   sortStage] )  The operation returns the following documents: {  _id: '5ab9e16431c2ab715d4212b4',  item: 'almonds',  qty: 50,  convertedId: ObjectId(\"5ab9e16431c2ab715d4212b4\")},{  _id: ObjectId(\"5ab9d2d331c2ab715d4212b3\"),  item: 'ice cream',  qty: 20,  convertedId: ObjectId(\"5ab9d2d331c2ab715d4212b3\")},{  _id: ObjectId(\"5ab9d0b831c2ab715d4212a8\"),  item: 'pie',  qty: 5,  convertedId: ObjectId(\"5ab9d0b831c2ab715d4212a8\")},{  _id: '5ab9cbe531c2ab715d42129a',  item: 'apple',  qty: 10,  convertedId: ObjectId(\"5ab9cbe531c2ab715d42129a\")}  \nNOTE If the conversion operation encounters an error, the aggregation operation stops and throws an error. To override this behavior, use $convert instead. ←  $toLong (aggregation)$top (aggregation accumulator) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toLower/": " Docs Home → MongoDB Manual \n$TOLOWER (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toLower \nConverts a string to lowercase, returning the result. $toLower has the following syntax: { $toLower: <expression> }  The argument can be any expression as long as it resolves to a string. For more information on expressions, see Expressions. If the argument resolves to null, $toLower returns an empty string \"\". \nBEHAVIOR \n$toLower only has a well-defined behavior for strings of ASCII characters. \nEXAMPLE \nConsider a inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \"PRODUCT 1\" }{ \"_id\" : 2, \"item\" : \"abc2\", quarter: \"13Q4\", \"description\" : \"Product 2\" }{ \"_id\" : 3, \"item\" : \"xyz1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $toLower operator return lowercase item and lowercase description value: db.inventory.aggregate(   [     {       $project:         {           item: { $toLower: \"$item\" },           description: { $toLower: \"$description\" }         }     }   ])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"abc1\", \"description\" : \"product 1\" }{ \"_id\" : 2, \"item\" : \"abc2\", \"description\" : \"product 2\" }{ \"_id\" : 3, \"item\" : \"xyz1\", \"description\" : \"\" } \n←  $toString (aggregation)$toUpper (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dateToString/": " Docs Home → MongoDB Manual \n$DATETOSTRING (AGGREGATION) \nOn this page    \n * Definition\n   \n * Format Specifiers\n * Example \nDEFINITION \n$dateToString \nConverts a date object to a string according to a user-specified format. The $dateToString expression has the following operator expression syntax: { $dateToString: {    date: <dateExpression>,    format: <formatString>,    timezone: <tzExpression>,    onNull: <expression>} }  The $dateToString takes a document with the following fields: Field\nDescription\ndate Changed in version 3.6. The date to convert to string. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID. format Optional. The date format specification. <formatString> can be any string literal, containing 0 or more format specifiers. For a list of specifiers available, see Format Specifiers. If unspecified, $dateToString uses \"%Y-%m-%dT%H:%M:%S.%LZ\" as the default format. timezone Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC. Format\nExamples\nOlson Timezone Identifier \"America/New_York\"\"Europe/London\"\"GMT\" \nUTC Offset +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"  onNull Optional. The value to return if the date is null or missing. The arguments can be any valid expression. If unspecified, $dateToString returns null if the date is null or missing. \nTIP \nSEE ALSO:      * $toString  * $convert \nFORMAT SPECIFIERS \nThe following format specifiers are available for use in the <formatString>: Specifiers\nDescription\nPossible Values\n%d\nDay of Month (2 digits, zero padded)\n01-31\n%G\nYear in ISO 8601 format\n0000-9999\n%H\nHour (2 digits, zero padded, 24-hour clock)\n00-23\n%j\nDay of year (3 digits, zero padded)\n001-366\n%L\nMillisecond (3 digits, zero padded)\n000-999\n%m\nMonth (2 digits, zero padded)\n01-12\n%M\nMinute (2 digits, zero padded)\n00-59\n%S\nSecond (2 digits, zero padded)\n00-60\n%w\nDay of week (1-Sunday, 7-Saturday)\n1-7\n%u\nDay of week number in ISO 8601 format (1-Monday, 7-Sunday)\n1-7\n%U\nWeek of year (2 digits, zero padded)\n00-53\n%V\nWeek of Year in ISO 8601 format\n01-53\n%Y\nYear (4 digits, zero padded)\n0000-9999\n%z\nThe timezone offset from UTC.\n+/-[hh][mm]\n%Z\nThe minutes offset from UTC as a number. For example, if the timezone offset (+/-[hhmm]) was +0445, the minutes offset is +285.\n+/-mmm\n%%\nPercent Character as a Literal\n% \nEXAMPLE \nConsider a sales collection with the following document: {  \"_id\" : 1,  \"item\" : \"abc\",  \"price\" : 10,  \"quantity\" : 2,  \"date\" : ISODate(\"2014-01-01T08:15:39.736Z\")}  The following aggregation uses $dateToString to return the date field as formatted strings:\n\n  The operation returns the following result: {   \"_id\" : 1,   \"yearMonthDayUTC\" : \"2014-01-01\",   \"timewithOffsetNY\" : \"03:15:39:736-0500\",   \"timewithOffset430\" : \"12:45:39:736+0430\",   \"minutesOffsetNY\" : \"-300\",   \"minutesOffset430\" : \"270\"} \n←  $dateToParts (aggregation)$dateTrunc (aggregation) → On this page  * Definition\n * Format Specifiers\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/dateTrunc/": " Docs Home → MongoDB Manual \n$DATETRUNC (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$dateTrunc \nNew in version 5.0. Truncates a date. $dateTrunc syntax: {   $dateTrunc: {      date: <Expression>,      unit: <Expression>,      binSize: <Expression>,      timezone: <tzExpression>,      startOfWeek: <Expression>   }} Field\nRequired / Optional\nDescription\ndate\nRequired The date to truncate, specified in UTC. The date can be any expression that resolves to a Date, a Timestamp, or an ObjectID. unit\nRequired The unit of time, specified as an expression that must resolve to one of these strings:      * year  * quarter  * week  * month  * day  * hour  * minute  * second Together, binSize and unit specify the time period used in the $dateTrunc calculation. binSize\nOptional The numeric time value, specified as an expression that must resolve to a positive non-zero number. Defaults to 1. Together, binSize and unit specify the time period used in the $dateTrunc calculation. timezone\nOptional The timezone for the $dateTrunc calculation, specified as an expression that must resolve to a string that contains one of these values:  * Olson Timezone Identifier  * UTC Offset If no timezone is provided, the $dateTrunc calculation is performed in UTC. Format\nExamples\nOlson Timezone Identifier  * America/New_York  * Europe/London  * GMT UTC Offset  * +/-[hh]:[mm] (example, +04:45)  * +/-[hh][mm] (example, -0530)  * +/-[hh] (example, +03) startOfWeek\nOptional The start of the week. Used when unit is week. Defaults to Sunday. startOfWeek is an expression that must resolve to one of these case insensitive strings:  * monday (or mon)  * tuesday (or tue)  * wednesday (or wed)  * thursday (or thu)  * friday (or fri)  * saturday (or sat)  * sunday (or sun) \nTIP \nSEE ALSO:  * Expressions  * BSON Types \nBEHAVIOR \n$dateTrunc:  * Returns null if:\n   \n   * any of the input fields except startOfWeek is missing or set to null, or\n   \n   * if unit is week and startOfWeek is missing or set to null.  * Uses the proleptic Gregorian calendar for dates preceding the year 1583.  * Accounts for Daylight Savings Time, but does not account for leap seconds. \nBINSIZE AND UNIT FIELDS \nTogether, binSize and unit specify the time period used in the $dateTrunc calculation. For example:  * If binSize is 1 and unit is hours, the time period is one hour. For the date 2021-03-20T11:30:05Z, $dateTrunc returns 2021-03-20T11:00:00Z.  * If binSize is 2 and unit is hours, the time period is two hours. For the date 2021-03-20T11:30:05Z, $dateTrunc returns 2021-03-20T10:00:00Z. $dateTrunc:\n\n  * Returns the lower boundary of the time period that the date is in. The boundary is returned as an ISODate. If the binSize field is 1, $dateTrunc sets the least significant parts (as determined by unit) of the returned ISODate to 0 and keeps the rest of the ISODate the same. If unit is:  * year: $dateTrunc returns the ISODate for the start of January 1 for the year in date.  * quarter: $dateTrunc returns the ISODate for the start of the first day of the calendar quarter in date.\n   \n   The quarters are:\n   \n   * January to March\n   \n   * April to June\n   \n   * July to September\n   \n   * October to December  * month: $dateTrunc returns the ISODate for the start of the first day of the month in date.  * week: $dateTrunc returns the ISODate for the start of the startOfWeek day in date. The default for startOfWeek is Sunday.  * day: $dateTrunc returns the ISODate for the start of the day in date.  * hour: $dateTrunc returns the ISODate for the start of the hour in date.  * minute: $dateTrunc returns the ISODate for the start of the minute in date.  * second: $dateTrunc returns the the ISODate for start of the second in date. \nUNIT AND STARTOFWEEK FIELDS \nIf unit is:  * A string other than week, startOfWeek is ignored.  * Equal to week and startOfWeek is:\n   \n   * Specified: $dateTrunc uses startOfWeek as the first day of the week for the calculation.\n   \n   * Omitted: $dateTrunc uses Sunday as the start of the week for the calculation. \nEXAMPLES \nCreate a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  The cakeSales collection is used in the following examples. \nTRUNCATE ORDER DATES IN A $PROJECT PIPELINE STAGE \nThis example uses $dateTrunc in a $project stage to truncate the cake sales orderDate values to two weeks: db.cakeSales.aggregate( [   {      $project: {         _id: 1,         orderDate: 1,         truncatedOrderDate: {            $dateTrunc: {               date: \"$orderDate\", unit: \"week\", binSize: 2,               timezone: \"America/Los_Angeles\", startOfWeek: \"Monday\"            }         }      }   }] )  In the example:\n\n  * $dateTrunc truncates the orderDate field to a 2 binSize week unit time period in the America/Los_Angeles timezone with startOfWeek set to Monday. In this example output, the truncated orderDate is shown in the truncatedOrderDate field: [   {      _id: 0,      orderDate: ISODate(\"2020-05-18T14:10:30.000Z\"),      truncatedOrderDate: ISODate(\"2020-05-11T07:00:00.000Z\")   },   {      _id: 1,      orderDate: ISODate(\"2021-03-20T11:30:05.000Z\"),      truncatedOrderDate: ISODate(\"2021-03-15T07:00:00.000Z\")   },   {      _id: 2,      orderDate: ISODate(\"2021-01-11T06:31:15.000Z\"),      truncatedOrderDate: ISODate(\"2021-01-04T08:00:00.000Z\")   },   {      _id: 3,      orderDate: ISODate(\"2020-02-08T13:13:23.000Z\"),      truncatedOrderDate: ISODate(\"2020-02-03T08:00:00.000Z\")   },   {      _id: 4,      orderDate: ISODate(\"2019-05-18T16:09:01.000Z\"),      truncatedOrderDate: ISODate(\"2019-05-13T07:00:00.000Z\")   },   {      _id: 5,      orderDate: ISODate(\"2019-01-08T06:12:03.000Z\"),      truncatedOrderDate: ISODate(\"2019-01-07T08:00:00.000Z\")   }] \nTRUNCATE ORDER DATES AND OBTAIN QUANTITY SUM IN A $GROUP PIPELINE STAGE \nThis example uses $dateTrunc in a $group stage to truncate the cake sales orderDate values to six months and return the sum of the quantity values: db.cakeSales.aggregate( [   {      $group: {         _id: {            truncatedOrderDate: {               $dateTrunc: {                  date: \"$orderDate\", unit: \"month\", binSize: 6               }            }         },         sumQuantity: { $sum: \"$quantity\" }      }   }] )  In the example:  * $group has the _id field set to the truncatedOrderDate field to group the cakeSales documents, and returns the sum of the quantity values for each group using $sum.  * $dateTrunc truncates the orderDate field to a 6 binSize month unit time period. In this example output, the truncated orderDate is shown in the truncatedOrderDate field and the quantity sum is shown in the sumQuantity field: [   {      _id: { truncatedOrderDate: ISODate(\"2020-01-01T00:00:00.000Z\") },      sumQuantity: 224   },   {      _id: { truncatedOrderDate: ISODate(\"2021-01-01T00:00:00.000Z\") },      sumQuantity: 285   },   {      _id: { truncatedOrderDate: ISODate(\"2019-01-01T00:00:00.000Z\") },      sumQuantity: 296   }] ←  $dateToString (aggregation)$dayOfMonth (aggregation) → On this page\n\n Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toDouble/": " Docs Home → MongoDB Manual \n$TODOUBLE(AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toDouble \nConverts a value to a double. If the value cannot be converted to an double, $toDouble errors. If the value is null or missing, $toDouble returns null. $toDouble has the following syntax: {   $toDouble: <expression>}  The $toDouble takes any valid expression. The $toDouble is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"double\" } }  \nBEHAVIOR \nThe following table lists the input types that can be converted to a double: Input Type\nBehavior\nBoolean\nReturns 0 for false.\nReturns 1 for true.\nDouble\nNo-op. Returns the double.\nDecimal Returns the decimal value as a double. The decimal value must fall within the minimum and maximum value for a double. You cannot convert a decimal value whose value is less than the minimum double value or is greater than the maximum double value. Integer\nReturns the int value as a double.\nLong\nReturns the long value as a double.\nString Returns the numerical value of the string as a double. The string value must be of a base 10 numeric value (e.g. \"-5.5\", \"123456\") and fall within the minimum and maximum value for a double. You cannot convert a string value of a non-base 10 number (e.g. \"0x6400\") or a value that falls outside the minimum and maximum value for a double. Date\nReturns the number of milliseconds since the epoch that corresponds to the date value. The following table lists some conversion to double examples: Example\nResults\n$toDouble: true\n1\n$toDouble: false\n0\n$toDouble: 2.5\n2.5\n$toDouble: NumberInt(5)\n5\n$toDouble: NumberLong(10000)\n10000\n$toDouble: \"-5.5\"\n-5.5\n$toDouble: ISODate(\"2018-03-27T05:04:47.890Z\")\n1522127087890 \nEXAMPLE \nCreate a collection weather with the following documents: db.weather.insertMany( [   { _id: 1, date: new Date(\"2018-06-01\"), temp: \"26.1C\" },   { _id: 2,  date: new Date(\"2018-06-02\"), temp: \"25.1C\" },   { _id: 3,  date: new Date(\"2018-06-03\"), temp: \"25.4C\" },] )  The following aggregation operation on the weather collection parses the temp value and converts to a double: // Define stage to add degrees field with converted value\ntempConversionStage = {   $addFields: {      degrees: { $toDouble: { $substrBytes: [ \"$temp\", 0, 4 ] } }   }}; db.weather.aggregate( [   tempConversionStage,] )  The operation returns the following documents: { \"_id\" : 1, \"date\" : ISODate(\"2018-06-01T00:00:00Z\"), \"temp\" : \"26.1C\", \"degrees\" : 26.1 }{ \"_id\" : 2, \"date\" : ISODate(\"2018-06-02T00:00:00Z\"), \"temp\" : \"25.1C\", \"degrees\" : 25.1 }{ \"_id\" : 3, \"date\" : ISODate(\"2018-06-03T00:00:00Z\"), \"temp\" : \"25.4C\", \"degrees\" : 25.4 }  \nNOTE If the conversion operation encounters an error, the aggregation operation stops and throws an error. To override this behavior, use $convert instead. ←  $toDecimal (aggregation)$toInt (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/listLocalSessions/": " Docs Home → MongoDB Manual \n$LISTLOCALSESSIONS \nOn this page    \n * Definition\n   \n * Restrictions\n * Examples \nDEFINITION \n$listLocalSessions \nLists the sessions cached in memory by the mongod or mongos instance. \nIMPORTANT When a user creates a session on a mongod or mongos instance, the record of the session initially exists only in-memory on the instance; i.e. the record is local to the instance. Periodically, the instance will sync its cached sessions to the system.sessions collection in the config database, at which time, they are visible to $listSessions and all members of the deployment. Until the session record exists in the system.sessions collection, you can only list the session via the $listLocalSessions operation. The $listLocalSessions operation uses the db.aggregate() method and not the db.collection.aggregate(). To run $listLocalSessions, it must be the first stage in the pipeline. The stage has the following syntax: { $listLocalSessions: <document> }  The $listLocalSessions stage takes a document with one of the following contents: Field\nDescription\n{ } If running with access control, returns all sessions for the current authenticated user. If running without access control, returns all sessions. { users: [ { user: <user>, db: <db> }, ... ] }\nReturns all sessions for the specified users. If running with access control, the authenticated user must have privileges with listSessions action on the cluster to list sessions for other users.\n{ allUsers: true }\nReturns all sessions for all users. If running with access control, the authenticated user must have privileges with listSessions action on the cluster. \nRESTRICTIONS \n$listLocalSessions is not allowed in transactions. \nEXAMPLES  LIST ALL LOCAL SESSIONS \nFrom the connected mongod / mongos instance's in-memory cache of sessions, the following aggregation operation lists all sessions: \nNOTE If running with access control, the current user must have privileges with listSessions action on the cluster. db.aggregate( [  { $listLocalSessions: { allUsers: true } } ] )  \nLIST ALL LOCAL SESSIONS FOR THE SPECIFIED USERS \nFrom the connected mongod / mongos instance's in-memory cache, the following aggregation operation lists all the sessions for the specified user myAppReader@test: \nNOTE If running with access control and the current user is not the specified user, the current user must have privileges with listSessions action on the cluster. db.aggregate( [ { $listLocalSessions: { users: [ { user: \"myAppReader\", db: \"test\" } ] } } ] )  \nLIST ALL LOCAL SESSIONS FOR THE CURRENT USER \nFrom the connected mongod / mongos instance's in-memory cache, the following aggregation operation lists all sessions for the current user if run with access control: db.aggregate( [ { $listLocalSessions: { } } ] )  If run without access control, the operation lists all local sessions. ←  $limit (aggregation)$listSessions → On this page  * Definition\n * Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sinh/": " Docs Home → MongoDB Manual \n$SINH (AGGREGATION) \nOn this page    \n * Behavior\n   \n * Example $sinh \nNew in version 4.2. Returns the hyperbolic sine of a value that is measured in radians. $sinh has the following syntax: { $sinh: <expression> }  $sinh takes any valid expression that resolves to a number, measured in radians. If the expression returns a value in degrees, use the $degreesToRadians operator to convert the value to radians. By default $sinh returns values as a double. $sinh can also return values as a 128-bit decimal if the <expression> resolves to a 128-bit decimal value. For more information on expressions, see Expressions. \nBEHAVIOR  NULL, NAN, AND +/- INFINITY \nIf the input argument resolves to a value of null or refers to a field that is missing, $sinh returns null. If the argument resolves to NaN, $sinh returns NaN. If the argument resolves to negative or positive Infinity, $sinh returns negative or positive Infinity respectively. Example\nResults\n{ $sinh: NaN }\nNaN\n{ $sinh: null }\nnull\n{ $sinh: -Infinity }\n-Infinity\n{ $sinh: Infinity }\nInfinity \nEXAMPLE  \n←  $sin (aggregation)$slice (aggregation) → On this page  * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/covariancePop/": " Docs Home → MongoDB Manual \n$COVARIANCEPOP (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \nNew in version 5.0. $covariancePop \nReturns the population covariance of two numeric expressions that are evaluated using documents in the $setWindowFields stage window. $covariancePop is only available in the $setWindowFields stage. $covariancePop syntax: {   $covariancePop: {      [         <numeric expression 1>,         <numeric expression 2>      ]   }} \nBEHAVIOR \n$covariancePop behavior:      * Ignores non-numeric values, null values, and missing fields in a window.  * If the window contains one document, returns null. (Compare to $covarianceSamp, which returns null if the window contains one document.)  * If the window is empty, returns null.  * If the window contains a NaN value, returns NaN.  * If the window contains one or more Infinity value(s) that are all positive or all negative, returns Infinity. The returned Infinity value has the same sign as the Infinity values in the window.  * If the window contains Infinity values with different signs, returns NaN.  * If the window contains a decimal value, returns a decimal value.  * If none of the previous points apply, returns a double value. The returned values in order of precedence are as follows:  * NaN  * Infinity  * decimal  * double \nEXAMPLE \nCreate a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA): db.cakeSales.insertMany( [   { _id: 0, type: \"chocolate\", orderDate: new Date(\"2020-05-18T14:10:30Z\"),     state: \"CA\", price: 13, quantity: 120 },   { _id: 1, type: \"chocolate\", orderDate: new Date(\"2021-03-20T11:30:05Z\"),     state: \"WA\", price: 14, quantity: 140 },   { _id: 2, type: \"vanilla\", orderDate: new Date(\"2021-01-11T06:31:15Z\"),     state: \"CA\", price: 12, quantity: 145 },   { _id: 3, type: \"vanilla\", orderDate: new Date(\"2020-02-08T13:13:23Z\"),     state: \"WA\", price: 13, quantity: 104 },   { _id: 4, type: \"strawberry\", orderDate: new Date(\"2019-05-18T16:09:01Z\"),     state: \"CA\", price: 41, quantity: 162 },   { _id: 5, type: \"strawberry\", orderDate: new Date(\"2019-01-08T06:12:03Z\"),     state: \"WA\", price: 43, quantity: 134 }] )  This example uses $covariancePop in the $setWindowFields stage to output the population covariance values for the cake sales orderDate year and quantity values: db.cakeSales.aggregate( [   {      $setWindowFields: {         partitionBy: \"$state\",         sortBy: { orderDate: 1 },         output: {            covariancePopForState: {               $covariancePop: [ { $year: \"$orderDate\" }, \"$quantity\" ],               window: {                  documents: [ \"unbounded\", \"current\" ]               }            }         }      }   }] )  In the example:  * partitionBy: \"$state\" partitions the documents in the collection by state. There are partitions for CA and WA.\n\n  * output sets the population covariance values for the orderDate year and quantity values using $covariancePop run in a documents window.\n   \n   The window contains documents between an unbounded lower limit and the current document in the output. This means $covariancePop sets the covariancePopForState field to the population covariance values for the documents between the beginning of the partition and the current document. In this output, the population covariance is shown in the covariancePopForState field: { \"_id\" : 4, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-05-18T16:09:01Z\"),  \"state\" : \"CA\", \"price\" : 41, \"quantity\" : 162, \"covariancePopForState\" : 0 }{ \"_id\" : 0, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2020-05-18T14:10:30Z\"),  \"state\" : \"CA\", \"price\" : 13, \"quantity\" : 120, \"covariancePopForState\" : -10.5 }{ \"_id\" : 2, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2021-01-11T06:31:15Z\"),  \"state\" : \"CA\", \"price\" : 12, \"quantity\" : 145, \"covariancePopForState\" : -5.666666666666671 }{ \"_id\" : 5, \"type\" : \"strawberry\", \"orderDate\" : ISODate(\"2019-01-08T06:12:03Z\"),  \"state\" : \"WA\", \"price\" : 43, \"quantity\" : 134, \"covariancePopForState\" : 0 }{ \"_id\" : 3, \"type\" : \"vanilla\", \"orderDate\" : ISODate(\"2020-02-08T13:13:23Z\"),  \"state\" : \"WA\", \"price\" : 13, \"quantity\" : 104, \"covariancePopForState\" : -7.5 }{ \"_id\" : 1, \"type\" : \"chocolate\", \"orderDate\" : ISODate(\"2021-03-20T11:30:05Z\"),  \"state\" : \"WA\", \"price\" : 14, \"quantity\" : 140, \"covariancePopForState\" : 2 } ←  $count (aggregation accumulator)$covarianceSamp (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/unwind/": " Docs Home → MongoDB Manual \n$UNWIND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behaviors\n * Examples\n * Additional Resources \nDEFINITION \n$unwind \nDeconstructs an array field from the input documents to output a document for each element. Each output document is the input document with the value of the array field replaced by the element. \nSYNTAX \nYou can pass a field path operand or a document operand to unwind an array field. \nFIELD PATH OPERAND \nYou can pass the array field path to $unwind. When using this syntax, $unwind does not output a document if the field value is null, missing, or an empty array. { $unwind: <field path> }  When you specify the field path, prefix the field name with a dollar sign $ and enclose in quotes. \nDOCUMENT OPERAND WITH OPTIONS \nYou can pass a document to $unwind to specify various behavior options. {  $unwind:    {      path: <field path>,      includeArrayIndex: <string>,      preserveNullAndEmptyArrays: <boolean>    }} \nField\nType\nDescription\npath\nstring Field path to an array field. To specify a field path, prefix the field name with a dollar sign $ and enclose in quotes. includeArrayIndex\nstring Optional. The name of a new field to hold the array index of the element. The name cannot start with a dollar sign $. preserveNullAndEmptyArrays\nboolean Optional.      * If true, if the path is null, missing, or an empty array, $unwind outputs the document.  * If false, if path is null, missing, or an empty array, $unwind does not output a document. The default value is false. \nBEHAVIORS  NON-ARRAY FIELD PATH \n * When the operand does not resolve to an array, but is not missing, null, or an empty array, $unwind treats the operand as a single element array.  * When the operand is null, missing, or an empty array $unwind follows the behavior set for the preserveNullAndEmptyArrays option. \nMISSING FIELD \nIf you specify a path for a field that does not exist in an input document or the field is an empty array, $unwind, by default, ignores the input document and will not output documents for that input document. To output documents where the array field is missing, null or an empty array, use the preserveNullAndEmptyArrays option. \nEXAMPLES  UNWIND ARRAY \nIn mongosh, create a sample collection named inventory with the following document: db.inventory.insertOne({ \"_id\" : 1, \"item\" : \"ABC1\", sizes: [ \"S\", \"M\", \"L\"] })  The following aggregation uses the $unwind stage to output a document for each element in the sizes array: db.inventory.aggregate( [ { $unwind : \"$sizes\" } ] )  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"sizes\" : \"S\" }{ \"_id\" : 1, \"item\" : \"ABC1\", \"sizes\" : \"M\" }{ \"_id\" : 1, \"item\" : \"ABC1\", \"sizes\" : \"L\" } Each document is identical to the input document except for the value of the sizes field which now holds a value from the original sizes array. \nMISSING OR NON-ARRAY VALUES \nConsider the clothing collection: db.clothing.insertMany([  { \"_id\" : 1, \"item\" : \"Shirt\", \"sizes\": [ \"S\", \"M\", \"L\"] },  { \"_id\" : 2, \"item\" : \"Shorts\", \"sizes\" : [ ] },  { \"_id\" : 3, \"item\" : \"Hat\", \"sizes\": \"M\" },  { \"_id\" : 4, \"item\" : \"Gloves\" },  { \"_id\" : 5, \"item\" : \"Scarf\", \"sizes\" : null }])  $unwind treats the sizes field as a single element array if:  * the field is present,  * the value is not null, and  * the value is not an empty array. Expand the sizes arrays with $unwind: db.clothing.aggregate( [ { $unwind: { path: \"$sizes\" } } ] ) \n\n { _id: 1, item: 'Shirt', sizes: 'S' },{ _id: 1, item: 'Shirt', sizes: 'M' },{ _id: 1, item: 'Shirt', sizes: 'L' },{ _id: 3, item: 'Hat', sizes: 'M' }  * In document \"_id\": 1, sizes is a populated array. $unwind returns a document for each element in the sizes field.  * In document \"_id\": 3, sizes resolves to a single element array.  * Documents \"_id\": 2, \"_id\": 4, and \"_id\": 5 do not return anything because the sizes field cannot be reduced to a single element array. \nNOTE The { path: <FIELD> } syntax is optional. The following $unwind operations are equivalent. db.clothing.aggregate( [ { $unwind: \"$sizes\" } ] )db.clothing.aggregate( [ { $unwind: { path: \"$sizes\" } } ] )  \nPRESERVENULLANDEMPTYARRAYS AND INCLUDEARRAYINDEX \nThe preserveNullAndEmptyArrays and includeArrayIndex examples use the following collection: db.inventory2.insertMany([   { \"_id\" : 1, \"item\" : \"ABC\", price: NumberDecimal(\"80\"), \"sizes\": [ \"S\", \"M\", \"L\"] },   { \"_id\" : 2, \"item\" : \"EFG\", price: NumberDecimal(\"120\"), \"sizes\" : [ ] },   { \"_id\" : 3, \"item\" : \"IJK\", price: NumberDecimal(\"160\"), \"sizes\": \"M\" },   { \"_id\" : 4, \"item\" : \"LMN\" , price: NumberDecimal(\"10\") },   { \"_id\" : 5, \"item\" : \"XYZ\", price: NumberDecimal(\"5.75\"), \"sizes\" : null }])  PRESERVENULLANDEMPTYARRAYS \nThe following $unwind operation uses the preserveNullAndEmptyArrays option to include documents whose sizes field is null, missing, or an empty array. db.inventory2.aggregate( [   { $unwind: { path: \"$sizes\", preserveNullAndEmptyArrays: true } }] )  The output includes those documents where the sizes field is null, missing, or an empty array: { \"_id\" : 1, \"item\" : \"ABC\", \"price\" : NumberDecimal(\"80\"), \"sizes\" : \"S\" }{ \"_id\" : 1, \"item\" : \"ABC\", \"price\" : NumberDecimal(\"80\"), \"sizes\" : \"M\" }{ \"_id\" : 1, \"item\" : \"ABC\", \"price\" : NumberDecimal(\"80\"), \"sizes\" : \"L\" }{ \"_id\" : 2, \"item\" : \"EFG\", \"price\" : NumberDecimal(\"120\") }{ \"_id\" : 3, \"item\" : \"IJK\", \"price\" : NumberDecimal(\"160\"), \"sizes\" : \"M\" }{ \"_id\" : 4, \"item\" : \"LMN\", \"price\" : NumberDecimal(\"10\") }{ \"_id\" : 5, \"item\" : \"XYZ\", \"price\" : NumberDecimal(\"5.75\"), \"sizes\" : null } INCLUDEARRAYINDEX \nThe following $unwind operation uses the includeArrayIndex option to include the array index in the output. db.inventory2.aggregate( [  {    $unwind:      {        path: \"$sizes\",        includeArrayIndex: \"arrayIndex\"      }   }])  The operation unwinds the sizes array and includes the array index in the new arrayIndex field. If the sizes field does not resolve to a populated array but is not missing, null, or an empty array, the arrayIndex field is null.\n\n \nGROUP BY UNWOUND VALUES \nIn mongosh, create a sample collection named inventory2 with the following documents: db.inventory2.insertMany([  { \"_id\" : 1, \"item\" : \"ABC\", price: NumberDecimal(\"80\"), \"sizes\": [ \"S\", \"M\", \"L\"] },  { \"_id\" : 2, \"item\" : \"EFG\", price: NumberDecimal(\"120\"), \"sizes\" : [ ] },  { \"_id\" : 3, \"item\" : \"IJK\", price: NumberDecimal(\"160\"), \"sizes\": \"M\" },  { \"_id\" : 4, \"item\" : \"LMN\" , price: NumberDecimal(\"10\") },  { \"_id\" : 5, \"item\" : \"XYZ\", price: NumberDecimal(\"5.75\"), \"sizes\" : null }])  The following pipeline unwinds the sizes array and groups the resulting documents by the unwound size values: db.inventory2.aggregate( [   // First Stage   {     $unwind: { path: \"$sizes\", preserveNullAndEmptyArrays: true }   },   // Second Stage   {     $group:       {         _id: \"$sizes\",         averagePrice: { $avg: \"$price\" }       }   },   // Third Stage   {     $sort: { \"averagePrice\": -1 }   }] ) \nFirst Stage: The $unwind stage outputs a new document for each element in the sizes array. The stage uses the preserveNullAndEmptyArrays option to include in the output those documents where sizes field is missing, null or an empty array. This stage passes the following documents to the next stage: { \"_id\" : 1, \"item\" : \"ABC\", \"price\" : NumberDecimal(\"80\"), \"sizes\" : \"S\" }{ \"_id\" : 1, \"item\" : \"ABC\", \"price\" : NumberDecimal(\"80\"), \"sizes\" : \"M\" }{ \"_id\" : 1, \"item\" : \"ABC\", \"price\" : NumberDecimal(\"80\"), \"sizes\" : \"L\" }{ \"_id\" : 2, \"item\" : \"EFG\", \"price\" : NumberDecimal(\"120\") }{ \"_id\" : 3, \"item\" : \"IJK\", \"price\" : NumberDecimal(\"160\"), \"sizes\" : \"M\" }{ \"_id\" : 4, \"item\" : \"LMN\", \"price\" : NumberDecimal(\"10\") }{ \"_id\" : 5, \"item\" : \"XYZ\", \"price\" : NumberDecimal(\"5.75\"), \"sizes\" : null } Second Stage: The $group stage groups the documents by sizes and calculates the average price of each size. This stage passes the following documents to the next stage: { \"_id\" : \"S\", \"averagePrice\" : NumberDecimal(\"80\") }{ \"_id\" : \"L\", \"averagePrice\" : NumberDecimal(\"80\") }{ \"_id\" : \"M\", \"averagePrice\" : NumberDecimal(\"120\") }{ \"_id\" : null, \"averagePrice\" : NumberDecimal(\"45.25\") } Third Stage: The $sort stage sorts the documents by averagePrice in descending order. The operation returns the following result: { \"_id\" : \"M\", \"averagePrice\" : NumberDecimal(\"120\") }{ \"_id\" : \"L\", \"averagePrice\" : NumberDecimal(\"80\") }{ \"_id\" : \"S\", \"averagePrice\" : NumberDecimal(\"80\") }{ \"_id\" : null, \"averagePrice\" : NumberDecimal(\"45.25\") } \nTIP \nSEE ALSO:  * $group  * $sort \nUNWIND EMBEDDED ARRAYS \nIn mongosh, create a sample collection named sales with the following documents:\n\n  The following operation groups the items sold by their tags and calculates the total sales amount per each tag. db.sales.aggregate([  // First Stage  { $unwind: \"$items\" },\n  // Second Stage  { $unwind: \"$items.tags\" },\n  // Third Stage  {    $group:      {        _id: \"$items.tags\",        totalSalesAmount:          {            $sum: { $multiply: [ \"$items.price\", \"$items.quantity\" ] }          }      }  }]) \nFirst Stage The first $unwind stage outputs a new document for each element in the items array: { \"_id\" : \"1\", \"items\" : { \"name\" : \"pens\", \"tags\" : [ \"writing\", \"office\", \"school\", \"stationary\" ], \"price\" : NumberDecimal(\"12.00\"), \"quantity\" : 5 } }{ \"_id\" : \"1\", \"items\" : { \"name\" : \"envelopes\", \"tags\" : [ \"stationary\", \"office\" ], \"price\" : NumberDecimal(\"19.95\"), \"quantity\" : 8 } }{ \"_id\" : \"2\", \"items\" : { \"name\" : \"laptop\", \"tags\" : [ \"office\", \"electronics\" ], \"price\" : NumberDecimal(\"800.00\"), \"quantity\" : 1 } }{ \"_id\" : \"2\", \"items\" : { \"name\" : \"notepad\", \"tags\" : [ \"stationary\", \"school\" ], \"price\" : NumberDecimal(\"14.95\"), \"quantity\" : 3 } } Second Stage The second $unwind stage outputs a new document for each element in the items.tags arrays: { \"_id\" : \"1\", \"items\" : { \"name\" : \"pens\", \"tags\" : \"writing\", \"price\" : NumberDecimal(\"12.00\"), \"quantity\" : 5 } }{ \"_id\" : \"1\", \"items\" : { \"name\" : \"pens\", \"tags\" : \"office\", \"price\" : NumberDecimal(\"12.00\"), \"quantity\" : 5 } }{ \"_id\" : \"1\", \"items\" : { \"name\" : \"pens\", \"tags\" : \"school\", \"price\" : NumberDecimal(\"12.00\"), \"quantity\" : 5 } }{ \"_id\" : \"1\", \"items\" : { \"name\" : \"pens\", \"tags\" : \"stationary\", \"price\" : NumberDecimal(\"12.00\"), \"quantity\" : 5 } }{ \"_id\" : \"1\", \"items\" : { \"name\" : \"envelopes\", \"tags\" : \"stationary\", \"price\" : NumberDecimal(\"19.95\"), \"quantity\" : 8 } }{ \"_id\" : \"1\", \"items\" : { \"name\" : \"envelopes\", \"tags\" : \"office\", \"price\" : NumberDecimal(\"19.95\"), \"quantity\" : 8 } }{ \"_id\" : \"2\", \"items\" : { \"name\" : \"laptop\", \"tags\" : \"office\", \"price\" : NumberDecimal(\"800.00\"), \"quantity\" : 1 } }{ \"_id\" : \"2\", \"items\" : { \"name\" : \"laptop\", \"tags\" : \"electronics\", \"price\" : NumberDecimal(\"800.00\"), \"quantity\" : 1 } }{ \"_id\" : \"2\", \"items\" : { \"name\" : \"notepad\", \"tags\" : \"stationary\", \"price\" : NumberDecimal(\"14.95\"), \"quantity\" : 3 } }{ \"_id\" : \"2\", \"items\" : { \"name\" : \"notepad\", \"tags\" : \"school\", \"price\" : NumberDecimal(\"14.95\"), \"quantity\" : 3 } } Third Stage The $group stage groups the documents by the tag and calculates the total sales amount of items with each tag:\n\n \nTIP \nSEE ALSO:  * $group  * $sum  * $multiply \nADDITIONAL RESOURCES \n * Aggregation with the Zip Code Data Set,  * Aggregation with User Preference Data ←  $unset (aggregation)Aggregation Pipeline Operators → On this page  * Definition\n * Syntax\n * Behaviors\n * Examples\n * Additional Resources Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/mergeObjects/": " Docs Home → MongoDB Manual \n$MERGEOBJECTS (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Examples \nDEFINITION \n$mergeObjects \nCombines multiple documents into a single document. $mergeObjects is available in these stages:      * $bucket  * $bucketAuto  * $group \nSYNTAX \nWhen used as a $bucket, $bucketAuto, or $group stage accumulator, $mergeObjects has this syntax: { $mergeObjects: <document> } When used in other expressions (including in $bucket, $bucketAuto, and $group stages) but not as an accumulator, $mergeObjects has this syntax: { $mergeObjects: [ <document1>, <document2>, ... ] } The <document> can be any valid expression that resolves to a document. \nBEHAVIOR \n$mergeObjects ignores null operands. If all the operands to $mergeObjects resolves to null, $mergeObjects returns an empty document { }. $mergeObjects overwrites the field values as it merges the documents. If documents to merge include the same field name, the field, in the resulting document, has the value from the last document merged for the field. Example\nResults { $mergeObjects: [ { a: 1 }, null ] } { a: 1 } { $mergeObjects: [ null, null ] } { } {   $mergeObjects: [      { a: 1 },      { a: 2, b: 2 },      { a: 3, c: 3 }   ]} { a: 3, b: 2, c: 3 } {  $mergeObjects: [    { a: 1 },    { a: 2, b: 2 },    { a: 3, b: null, c: 3 }  ]} { a: 3, b: null, c: 3 } \nEXAMPLES  $MERGEOBJECTS \nCreate a collection orders with the following documents: db.orders.insertMany( [  { \"_id\" : 1, \"item\" : \"abc\", \"price\" : 12, \"ordered\" : 2 },  { \"_id\" : 2, \"item\" : \"jkl\", \"price\" : 20, \"ordered\" : 1 }] )  Create another collection items with the following documents: db.items.insertMany( [  { \"_id\" : 1, \"item\" : \"abc\", description: \"product 1\", \"instock\" : 120 },  { \"_id\" : 2, \"item\" : \"def\", description: \"product 2\", \"instock\" : 80 },  { \"_id\" : 3, \"item\" : \"jkl\", description: \"product 3\", \"instock\" : 60 }] )  The following operation first uses the $lookup stage to join the two collections by the item fields and then uses $mergeObjects in the $replaceRoot to merge the joined documents from items and orders: db.orders.aggregate( [   {      $lookup: {         from: \"items\",         localField: \"item\",    // field in the orders collection         foreignField: \"item\",  // field in the items collection         as: \"fromItems\"      }   },   {      $replaceRoot: { newRoot: { $mergeObjects: [ { $arrayElemAt: [ \"$fromItems\", 0 ] }, \"$$ROOT\" ] } }   },   { $project: { fromItems: 0 } }] )  The operation returns the following documents: {  _id: 1,  item: 'abc',  description: 'product 1',  instock: 120,  price: 12,  ordered: 2},{  _id: 2,  item: 'jkl',  description: 'product 3',  instock: 60,  price: 20,  ordered: 1}  \n$MERGEOBJECTS AS AN ACCUMULATOR \nCreate a collection sales with the following documents:\n\n  The following operation uses $mergeObjects as a accumulator in a $group stage that groups documents by the item field: \nNOTE When used as an accumulator, $mergeObjects operator accepts a single operand. db.sales.aggregate( [   { $group: { _id: \"$item\", mergedSales: { $mergeObjects: \"$quantity\" } } }] )  The operation returns the following documents: {  _id: 'A',  mergedSales: { '2017Q1': 500, '2017Q2': 500, '2016Q1': 400, '2016Q2': 300, '2016Q3': 0, '2016Q4': 0 }},{  _id: 'B',  mergedSales: { '2017Q1': 300, '2016Q3': 100, '2016Q4': 250 }}  \nNOTE If the documents to merge include the same field name, the field in the resulting document has the value from the last document merged for the field. ←  $maxN (array operator)$meta → On this page  * Definition\n * Syntax\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/interface/": " Docs Home → MongoDB Manual \nAGGREGATION COMMANDS \nOn this page    \n * Aggregation Commands\n   \n * Aggregation Methods \nNOTE For details on a specific operator, including syntax and examples, click on the link to the operator's reference page. \nAGGREGATION COMMANDS \nName\nDescription\naggregate\nPerforms aggregation tasks such as $group using an aggregation pipeline.\ncount\nCounts the number of documents in a collection or a view.\ndistinct\nDisplays the distinct values found for a specified key in a collection or a view.\nmapReduce\nPerforms map-reduce aggregation for large data sets. \nAGGREGATION METHODS \nName\nDescription\ndb.collection.aggregate()\nProvides access to the aggregation pipeline.\ndb.collection.mapReduce()\nPerforms map-reduce aggregation for large data sets. ←  Aggregation Pipeline Quick ReferenceAggregation Commands Comparison → On this page  * Aggregation Commands\n * Aggregation Methods Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/minN-array-element/": " Docs Home → MongoDB Manual \n$MINN (ARRAY OPERATOR) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior\n * Example \nDEFINITION \n$minN \nNew in version 5.2. Returns the n smallest values in an array. \nTIP \nSEE ALSO: $maxN \nSYNTAX \n$minN has the following syntax: { $minN: { n: <expression>, input: <expression> } } \nField\nDescription\nn\nAn expression that resolves to a positive integer. The integer specifies the number of array elements that $minN returns.\ninput\nAn expression that resolves to the array from which to return the minimal n elements. \nBEHAVIOR \n     * You cannot specify a value of n less than 1.  * $minN filters out null values found in the input array.  * If the specified n is greater than or equal to the number of elements in the input array, $minN returns all elements in the input array.  * If input resolves to a non-array value, the aggregation operation errors.  * If input contains both numeric and string elements, the numeric elements are sorted before string elements according to the BSON comparison order. \nEXAMPLE \nCreate a scores collection with the following documents: db.scores.insertMany([    { \"playerId\" : 1, \"score\" : [ 1, 2, 3 ] },    { \"playerId\" : 2, \"score\" : [ 12, 90, 7, 89, 8 ] },    { \"playerId\" : 3, \"score\" : [ null ] },    { \"playerId\" : 4, \"score\" : [ ] },    { \"playerId\" : 5, \"score\" : [ 1293, \"2\", 3489, 9 ]}])  The following example uses the $minN operator to retrieve the two lowest scores for each player. The lowest scores are returned in the new field minScores created by $addFields. db.scores.aggregate([   { $addFields: { minScores: { $minN: { n: 2, input: \"$score\" } } } }])  The operation returns the following results: [{  \"playerId\": 1,  \"score\": [ 1, 2, 3 ],  \"minScores\": [ 1, 2 ]},{  \"playerId\": 2,  \"score\": [ 12, 90, 7, 89, 8 ],  \"minScores\": [ 7, 8 ]},{  \"playerId\": 3,  \"score\": [ null ],  \"minScores\": [ ]},{  \"playerId\": 4,  \"score\": [ ],  \"minScores\": [ ]},{  \"playerId\": 5,  \"score\": [ 1293, \"2\", 3489, 9 ],  \"minScores\": [ 9, 1293 ]}] \n←  $minN (aggregation accumulator)$millisecond (aggregation) → On this page  * Definition\n * Syntax\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/trim/": " Docs Home → MongoDB Manual \n$TRIM (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$trim \nRemoves whitespace characters, including null, or the specified characters from the beginning and end of a string. $trim has the following syntax: { $trim: { input: <string>,  chars: <string> } }  The $trim takes a document with the following fields: Field\nDescription\ninput\nThe string to trim. The argument can be any valid expression that resolves to a string. For more information on expressions, see Expressions.\nchars Optional. The character(s) to trim from input. The argument can be any valid expression that resolves to a string. The $trim operator breaks down the string into individual UTF code point to trim from input. If unspecified, $trim removes whitespace characters, including the null character. For the list of whitespace characters, see Whitespace Characters. \nTIP \nSEE ALSO:      * $ltrim  * $rtrim \nBEHAVIOR \n * By default, $trim removes whitespace characters, including the null character:\n   \n   Example\n   Results\n   { $trim: { input: \" \\n good bye \\t \" } }\n   \"good bye\"  * You can override the default characters to trim using the chars field.\n   \n   For example, the following trims any g and e from the start and end of the input. Since the input starts with a whitespace, neither character can be trimmed from the start of the string.\n   \n   Example\n   Results\n   { $trim: { input: \" ggggoodbyeeeee\", chars: \"ge\" } }\n   \" ggggoodby\"  * If overriding the default characters to trim, you can explicitly include the whitespace character(s) to trim in the chars field.\n   \n   For example, the following trims any space, g, e from the start and end of the input.\n   \n   Example\n   Results\n   { $trim: { input: \" ggggoodbyeeeee\", chars: \" ge\" } }\n   \"oodby\" \nWHITESPACE CHARACTERS \nBy default, $trim removes the following whitespaces, including the null character: Unicode\nEscape sequence\nDescription\nU+0000\n'0'\nNull character\nU+0020\n' '\nSpace\nU+0009\n't'\nHorizontal tab\nU+000A\n'n'\nLine feed/new line\nU+000B\n'v'\nVertical tab\nU+000C\n'f'\nForm feed\nU+000D\n'r'\nCarriage return\nU+00A0 Non-breaking space\nU+1680 Ogham space mark\nU+2000 En quad\nU+2001 Em quad\nU+2002 En space\nU+2003 Em space\nU+2004 Three-per-em space\nU+2005 Four-per-em space\nU+2006 Six-per-em space\nU+2007 Figure space\nU+2008 Punctuation space\nU+2009 Thin space\nU+200A Hair space \nEXAMPLE \nConsider an inventory collection with the following documents: { \"_id\" : 1, \"item\" : \"ABC1\", quarter: \"13Q1\", \"description\" : \" product 1\" }{ \"_id\" : 2, \"item\" : \"ABC2\", quarter: \"13Q4\", \"description\" : \"product 2 \\n The product is in stock.  \\n\\n  \" }{ \"_id\" : 3, \"item\" : \"XYZ1\", quarter: \"14Q2\", \"description\" : null }  The following operation uses the $trim operator to remove leading and trailing whitespaces from the description field: db.inventory.aggregate([   { $project: { item: 1, description: { $trim: { input: \"$description\" } } } }])  The operation returns the following results: { \"_id\" : 1, \"item\" : \"ABC1\", \"description\" : \"product 1\" }{ \"_id\" : 3, \"item\" : \"XYZ1\", \"description\" : null }{ \"_id\" : 2, \"item\" : \"ABC2\", \"description\" : \"product 2 \\n The product is in stock.\" }\n\n On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/sqrt/": " Docs Home → MongoDB Manual \n$SQRT (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$sqrt \nCalculates the square root of a positive number and returns the result as a double. $sqrt has the following syntax: { $sqrt: <number> }  The argument can be any valid expression as long as it resolves to a non-negative number. For more information on expressions, see Expressions. \nBEHAVIOR \nIf the argument resolves to a value of null or refers to a field that is missing, $sqrt returns null. If the argument resolves to NaN, $sqrt returns NaN. $sqrt errors on negative numbers. Example\nResults\n{ $sqrt: 25 }\n5\n{ $sqrt: 30 }\n5.477225575051661\n{ $sqrt: null }\nnull \nEXAMPLE \nA collection points contains the following documents: { _id: 1, p1: { x: 5, y: 8 }, p2: { x: 0, y: 5} }{ _id: 2, p1: { x: -2, y: 1 }, p2: { x: 1, y: 5} }{ _id: 3, p1: { x: 4, y: 4 }, p2: { x: 4, y: 0} }  The following example uses $sqrt to calculate the distance between p1 and p2: db.points.aggregate([   {     $project: {        distance: {           $sqrt: {               $add: [                  { $pow: [ { $subtract: [ \"$p2.y\", \"$p1.y\" ] }, 2 ] },                  { $pow: [ { $subtract: [ \"$p2.x\", \"$p1.x\" ] }, 2 ] }               ]           }        }     }   }])  The operation returns the following results: { \"_id\" : 1, \"distance\" : 5.830951894845301 }{ \"_id\" : 2, \"distance\" : 5 }{ \"_id\" : 3, \"distance\" : 4 } \n←  $split (aggregation)$stdDevPop (aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/tsSecond/": " Docs Home → MongoDB Manual \n$TSSECOND (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$tsSecond \nNew in version 5.1. Returns the seconds from a timestamp as a long. $tsSecond syntax: { $tsSecond: <expression> } The expression must resolve to a timestamp. \nTIP \nSEE ALSO:      * Expressions  * BSON Types  * $tsIncrement \nBEHAVIOR \n$tsSecond returns:  * Null if the input expression evaluates to null or refers to a field that is missing.  * An error if the input expression does not evaluate to a timestamp. \nEXAMPLES  OBTAIN THE NUMBER OF SECONDS FROM A TIMESTAMP FIELD \nCreate a stockSales collection that contains company stock financial market sales: db.stockSales.insertMany( [   { _id: 0, symbol: \"ABC\", saleTimestamp: Timestamp(1622731060, 1) },   { _id: 1, symbol: \"ABC\", saleTimestamp: Timestamp(1622731060, 2) },   { _id: 2, symbol: \"DEF\", saleTimestamp: Timestamp(1714124193, 1) },   { _id: 3, symbol: \"DEF\", saleTimestamp: Timestamp(1714124193, 2) },   { _id: 4, symbol: \"DEF\", saleTimestamp: Timestamp(1714124193, 3) }] )  In the timestamp constructor, the:  * First value is the number of seconds after the Unix epoch.  * Second value is the incrementing ordinal. When multiple events happen within the same second, the incrementing ordinal uniquely identifies each event. The following example uses $tsSecond in a $project pipeline stage to return the seconds from the stock sales saleTimestamp field: db.stockSales.aggregate( [   {      $project:      {         _id: 0, saleTimestamp: 1, saleSeconds: { $tsSecond: \"$saleTimestamp\" }      }   }] )  Example output: {  saleTimestamp: Timestamp({ t: 1622731060, i: 1 }),  saleSeconds: Long(\"1622731060\")},{  saleTimestamp: Timestamp({ t: 1622731060, i: 2 }),  saleSeconds: Long(\"1622731060\")},{  saleTimestamp: Timestamp({ t: 1714124193, i: 1 }),  saleSeconds: Long(\"1714124193\")},{  saleTimestamp: Timestamp({ t: 1714124193, i: 2 }),  saleSeconds: Long(\"1714124193\")},{  saleTimestamp: Timestamp({ t: 1714124193, i: 3 }),  saleSeconds: Long(\"1714124193\")} \nUSE $TSSECOND IN A CHANGE STREAM CURSOR TO MONITOR COLLECTION CHANGES \nThe example in this section uses $tsSecond in a change stream cursor to monitor changes to a collection. Create a change stream cursor on a collection named cakeSales that you will see later in this section: cakeSalesCursor = db.cakeSales.watch( [   {      $addFields: {         clusterTimeSeconds: { $tsSecond: \"$clusterTime\" }      }   }] )  In the example, the:  * db.collection.watch() method creates a change stream cursor for the cakeSales collection and stores the cursor in cakeSalesCursor.  * $addFields stage adds a field named clusterTimeSeconds to cakeSalesCursor.\n   \n   * $clusterTime is the timestamp from the oplog entry for the cakeSales collection change. See Command Response.\n   \n   * $tsSecond returns the seconds from $clusterTime, which is stored in clusterTimeSeconds. Create a cakeSales collection that contains cake sales in the states of California (CA) and Washington (WA):\n\n  To monitor the cakeSales collection changes, use cakeSalesCursor. For example, to obtain the next document from cakeSalesCursor, use the next() method: cakeSalesCursor.next()  The following example output shows the insert details for the first document added to the cakeSales collection. The clusterTimeSeconds field contains the seconds from the clusterTime field. _id: {  _data: '82613A4A51000000032B022C0100296E5A100495189B4131584C56AC8BA9D540799F23461E5F696400290004'},operationType: 'insert',clusterTime: Timestamp({ t: 1631210065, i: 3 }),fullDocument: {  _id: 0,  type: 'chocolate',  orderDate: ISODate(\"2020-05-18T14:10:30.000Z\"),  state: 'CA',  price: 13,  quantity: 120},ns: { db: 'test', coll: 'cakeSales' },documentKey: { _id: 0 },clusterTimeSeconds: 1631210065 ←  $tsIncrement (aggregation)$trim (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/toDecimal/": " Docs Home → MongoDB Manual \n$TODECIMAL (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$toDecimal \nConverts a value to a decimal. If the value cannot be converted to a decimal, $toDecimal errors. If the value is null or missing, $toDecimal returns null. $toDecimal has the following syntax: {   $toDecimal: <expression>}  The $toDecimal takes any valid expression. The $toDecimal is a shorthand for the following $convert expression: { $convert: { input: <expression>, to: \"decimal\" } }  \nTIP \nSEE ALSO: $convert \nBEHAVIOR \nThe following table lists the input types that can be converted to a decimal: Input Type\nBehavior\nBoolean\nReturns Decimal128(\"0\") for false.\nReturns Decimal128(\"1\") for true.\nDouble\nReturns double value as a decimal.\nDecimal\nNo-op. Returns the decimal.\nInteger\nReturns the int value as a decimal.\nLong\nReturns the long value as a decimal.\nString Returns the numerical value of the string as a decimal. The string value must be of a base 10 numeric value (e.g. \"-5.5\", \"123456\"). You cannot convert a string value of a non-base 10 number (e.g. \"0x6400\") Date\nReturns the number of milliseconds since the epoch that corresponds to the date value. The following table lists some conversion to decimal examples: Example\nResults\n{$toDecimal: true}\nDecimal128(\"1\")\n{$toDecimal: false}\nDecimal128(\"0\")\n{$toDecimal: 2.5}\nDecimal128(\"2.50000000000000\")\n{$toDecimal: NumberInt(5)}\nDecimal128(\"5\")\n{$toDecimal: NumberLong(10000)}\nDecimal128(\"10000\")\n{$toDecimal: \"-5.5\"}\nDecimal128(\"-5.5\")\n{$toDecimal: ISODate(\"2018-03-27T05:04:47.890Z\")}\nDecimal128(\"1522127087890\") \nEXAMPLE \nCreate a collection orders with the following documents: db.orders.insertMany( [   { _id: 1, item: \"apple\", qty: 5, price: 10 },   { _id: 2, item: \"pie\", qty: 10, price: NumberDecimal(\"20.0\") },   { _id: 3, item: \"ice cream\", qty: 2, price: \"4.99\" },   { _id: 4, item: \"almonds\",  qty: 5, price: 5 }] )  The following aggregation operation on the orders collection converts the price to a decimal and the qty to an integer before calculating the total price: // Define stage to add convertedPrice and convertedQty fields with the converted price and qty values\npriceQtyConversionStage = {   $addFields: {      convertedPrice: { $toDecimal: \"$price\" },      convertedQty: { $toInt: \"$qty\" },   }};\n// Define stage to calculate total price by multiplying convertedPrice and convertedQty fields totalPriceCalculationStage = {   $project: { item: 1, totalPrice: { $multiply: [ \"$convertedPrice\", \"$convertedQty\" ] } }};\ndb.orders.aggregate( [   priceQtyConversionStage,   totalPriceCalculationStage] )  The operation returns the following documents: { _id: 1, item: 'apple', totalPrice: Decimal128(\"50\") },{ _id: 2, item: 'pie', totalPrice: Decimal128(\"200.0\") },{ _id: 3, item: 'ice cream', totalPrice: Decimal128(\"9.98\") },{ _id: 4, item: 'almonds', totalPrice: Decimal128(\"25\") }  \nNOTE If the conversion operation encounters an error, the aggregation operation stops and throws an error. To override this behavior, use $convert instead. ←  $toDate (aggregation)$toDouble(aggregation) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/isoWeekYear/": " Docs Home → MongoDB Manual \n$ISOWEEKYEAR (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$isoWeekYear \nReturns the year number in ISO 8601 format. The year starts with the Monday of week 1 and ends with the Sunday of the last week. The $isoWeekYear expression has the following operator expression syntax: { $isoWeekYear: <dateExpression> }  The argument can be:      * An expression that resolves to a Date, a Timestamp, or an ObjectID.  * A document with this format:\n   \n   { date: <dateExpression>, timezone: <tzExpression> }\n   \n   \n   Field\n   Description\n   date\n   The date to which the operator is applied. <dateExpression> must be a valid expression that resolves to a Date, a Timestamp, or an ObjectID.\n   timezone\n   \n   Optional. The timezone of the operation result. <tzExpression> must be a valid expression that resolves to a string formatted as either an Olson Timezone Identifier or a UTC Offset. If no timezone is provided, the result is displayed in UTC.\n   \n   Format\n   Examples\n   Olson Timezone Identifier\n   \n   \"America/New_York\"\"Europe/London\"\"GMT\"\n   \n   \n   UTC Offset\n   \n   +/-[hh]:[mm], e.g. \"+04:45\"+/-[hh][mm], e.g. \"-0530\"+/-[hh], e.g. \"+03\"\n   \n    \nBEHAVIOR \nExample\nResult { $isoWeekYear: new Date(\"2015-05-26\") } 2015 { $isoWeekYear: { date: new Date(\"Jan 7, 2003\") } } 2003 { $isoWeekYear: ISODate(\"2017-01-02T00:00:00Z\") } 2017 { $isoWeekYear: {    date: ISODate(\"2017-01-02T00:00:00Z\"),    timezone: \"-0500\"} } 2016 { $isoWeekYear: {    date: new Date(\"April 08, 2024\"),    timezone: \"America/Chicago\"} } 2024 { $isoWeekYear: \"March 28, 1976\" } error { $isoWeekYear: Date(\"2016-01-01\") } error { $isoWeekYear: \"2009-04-09\" } error \nNOTE \n$ISOWEEKYEAR CANNOT TAKE A STRING AS AN ARGUMENT.  \nEXAMPLE \nA collection called anniversaries contains the following documents: { \"_id\" : 1, \"date\" : ISODate(\"2016-01-01T00:00:00Z\") }{ \"_id\" : 2, \"date\" : ISODate(\"2016-01-04T00:00:00Z\") }{ \"_id\" : 3, \"date\" : ISODate(\"2015-01-01T00:00:00Z\") }{ \"_id\" : 4, \"date\" : ISODate(\"2014-04-21T00:00:00Z\") }  The following operation returns the year number in ISO 8601 format for each date field. db.anniversaries.aggregate( [  {    $project: {      yearNumber: { $isoWeekYear: \"$date\" }    }  }] )  The operation returns the following results: { \"_id\" : 1, \"yearNumber\" : 2015 }{ \"_id\" : 2, \"yearNumber\" : 2016 }{ \"_id\" : 3, \"yearNumber\" : 2015 }{ \"_id\" : 4, \"yearNumber\" : 2014 }  \nTIP \nSEE ALSO:  * $isoDayOfWeek (aggregation)  * $isoWeek (aggregation) ←  $isoWeek (aggregation)$last (aggregation accumulator) → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/meta/natural/": " Docs Home → MongoDB Manual \n$NATURAL  DEFINITION \n$natural \nChanged in version 4.4: Use in conjunction with cursor.hint() to perform a collection scan to return documents in natural order. For usage, see Force Collection Scans example in the cursor.hint() reference page. Starting in MongoDB 4.4, you can specify a $natural sort when running a find operation against a view. ←  $randUpdate Operators → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/expMovingAvg/": " Docs Home → MongoDB Manual \n$EXPMOVINGAVG (AGGREGATION) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \nNew in version 5.0. $expMovingAvg \nReturns the exponential moving average of numeric expressions applied to documents in a partition defined in the $setWindowFields stage. $expMovingAvg is only available in the $setWindowFields stage. $expMovingAvg syntax: {   $expMovingAvg: {      input: <input expression>,      N: <integer>,      alpha: <float>   }} $expMovingAvg takes a document with these fields: Field\nDescription\ninput Specifies the expression to evaluate. Non-numeric expressions are ignored. N An integer that specifies the number of historical documents that have a significant mathematical weight in the exponential moving average calculation, with the most recent documents contributing the most weight. You must specify either N or alpha. You cannot specify both. The N value is used in this formula to calculate the current result based on the expression value from the current document being read and the previous result of the calculation: current result = current value * ( 2 / ( N + 1 ) ) +                 previous result * ( 1 - ( 2 / ( N + 1 ) ) ) alpha A double that specifies the exponential decay value to use in the exponential moving average calculation. A higher alpha value assigns a lower mathematical significance to previous results from the calculation. You must specify either N or alpha. You cannot specify both. The alpha value is used in this formula to calculate the current result based on the expression value from the current document being read and the previous result of the calculation: current result = current value * alpha +                 previous result * ( 1 - alpha ) \nBEHAVIOR \nYou must specify either N or alpha. You cannot specify both. $expMovingAvg ignores non-numeric values, null values, and missing fields. \nEXAMPLES \nCreate a stockPrices collection that contains prices for stocks named \"ABC\" and \"DEF\": db.stockPrices.insertMany( [   { stock: \"ABC\", date: new Date( \"2020-05-18T20:00:00Z\" ), price: 13 },   { stock: \"ABC\", date: new Date( \"2020-05-19T20:00:00Z\" ), price: 15.4 },   { stock: \"ABC\", date: new Date( \"2020-05-20T20:00:00Z\" ), price: 12 },   { stock: \"ABC\", date: new Date( \"2020-05-21T20:00:00Z\" ), price: 11.7 },   { stock: \"DEF\", date: new Date( \"2020-05-18T20:00:00Z\" ), price: 82 },   { stock: \"DEF\", date: new Date( \"2020-05-19T20:00:00Z\" ), price: 94 },   { stock: \"DEF\", date: new Date( \"2020-05-20T20:00:00Z\" ), price: 112 },   { stock: \"DEF\", date: new Date( \"2020-05-21T20:00:00Z\" ), price: 97.3 }] )  \nEXPONENTIAL MOVING AVERAGE USING N \nThis example uses $expMovingAvg in the $setWindowFields stage to output the exponential moving average for the stock prices weighted for two historical documents (two days for the example documents) using N set to 2: db.stockPrices.aggregate( [   {      $setWindowFields: {         partitionBy: \"$stock\",         sortBy: { date: 1 },         output: {            expMovingAvgForStock: {               $expMovingAvg: { input: \"$price\", N: 2 }            }         }      }   }] )  In the example:      * partitionBy: \"$stock\" partitions the documents in the collection by stock. There are partitions for \"ABC\" and \"DEF\".\n\n  * output returns the exponential moving average for the stock price field with N set to 2:\n   \n   * In the input documents, there is one document for each day and the documents are ordered by date. Therefore, with N is set to 2, the price in the current document and the price in the previous document, if available, are allocated the highest weight in the exponential moving average formula.\n   \n   * The exponential moving average for the price field is stored in a new field called expMovingAvgForStocks, as shown in the following results. { \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286b\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-18T20:00:00Z\"), \"price\" : 13,  \"expMovingAvgForStock\" : 13 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286c\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-19T20:00:00Z\"), \"price\" : 15.4,  \"expMovingAvgForStock\" : 14.6 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286d\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-20T20:00:00Z\"), \"price\" : 12,  \"expMovingAvgForStock\" : 12.866666666666667 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286e\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-21T20:00:00Z\"), \"price\" : 11.7,  \"expMovingAvgForStock\" : 12.088888888888889 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286f\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-18T20:00:00Z\"), \"price\" : 82,  \"expMovingAvgForStock\" : 82 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e62870\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-19T20:00:00Z\"), \"price\" : 94,  \"expMovingAvgForStock\" : 90 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e62871\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-20T20:00:00Z\"), \"price\" : 112,  \"expMovingAvgForStock\" : 104.66666666666667 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e62872\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-21T20:00:00Z\"), \"price\" : 97.3,  \"expMovingAvgForStock\" : 99.75555555555556 } \nEXPONENTIAL MOVING AVERAGE USING ALPHA \nThis example uses $expMovingAvg in the $setWindowFields stage to output the exponential moving average for the stock prices using alpha set to 0.75: db.stockPrices.aggregate( [   {      $setWindowFields: {         partitionBy: \"$stock\",         sortBy: { date: 1 },         output: {            expMovingAvgForStock: {               $expMovingAvg: { input: \"$price\", alpha: 0.75 }            }         }      }   }] )  In the example:  * partitionBy: \"$stock\" partitions the documents in the collection by stock. There are partitions for \"ABC\" and \"DEF\".  * sortBy: { date: 1 } sorts the documents in each partition by date in ascending order (1), so the earliest date is first.\n\n { \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286b\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-18T20:00:00Z\"), \"price\" : 13,  \"expMovingAvgForStock\" : 13 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286c\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-19T20:00:00Z\"), \"price\" : 15.4,  \"expMovingAvgForStock\" : 14.8 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286d\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-20T20:00:00Z\"), \"price\" : 12,  \"expMovingAvgForStock\" : 12.7 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286e\"), \"stock\" : \"ABC\",  \"date\" : ISODate(\"2020-05-21T20:00:00Z\"), \"price\" : 11.7,  \"expMovingAvgForStock\" : 11.95 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e6286f\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-18T20:00:00Z\"), \"price\" : 82,  \"expMovingAvgForStock\" : 82 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e62870\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-19T20:00:00Z\"), \"price\" : 94,  \"expMovingAvgForStock\" : 91 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e62871\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-20T20:00:00Z\"), \"price\" : 112,  \"expMovingAvgForStock\" : 106.75 }{ \"_id\" : ObjectId(\"60d11fef833dfeadc8e62872\"), \"stock\" : \"DEF\",  \"date\" : ISODate(\"2020-05-21T20:00:00Z\"), \"price\" : 97.3,  \"expMovingAvgForStock\" : 99.6625 } ←  $exp (aggregation)$filter (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/densify/": " Docs Home → MongoDB Manual \n$DENSIFY (AGGREGATION) \nOn this page    \n * Definition\n   \n * Syntax\n * Behavior and Restrictions\n * Examples \nDEFINITION \n$densify \nNew in version 5.1. Creates new documents in a sequence of documents where certain values in a field are missing. You can use $densify to:      * Fill gaps in time series data.  * Add missing values between groups of data.  * Populate your data with a specified range of values. \nSYNTAX \nThe $densify stage has this syntax: {   $densify: {      field: <fieldName>,      partitionByFields: [ <field 1>, <field 2> ... <field n> ],      range: {         step: <number>,         unit: <time unit>,         bounds: < \"full\" || \"partition\" > || [ < lower bound >, < upper bound > ]      }   }}  The $densify stage takes a document with these fields: Field\nNecessity\nDescription\nfield\nRequired The field to densify. The values of the specified field must either be all numeric values or all dates. Documents that do not contain the specified field continue through the pipeline unmodified. To specify a <field> in an embedded document or in an array, use dot notation. For restrictions, see field Restrictions. partitionByFields\nOptional The set of fields to act as the compound key to group the documents. In the $densify stage, each group of documents is known as a partition. If you omit this field, $densify uses one partition for the entire collection. For an example, see Densifiction with Partitions. For restrictions, see partitionByFields Restrictions. range\nRequired An object that specifies how the data is densified. range.bounds\nRequired You can specify range.bounds as either:  * An array: [ < lower bound >, < upper bound > ],  * A string: either \"full\" or \"partition\". If bounds is an array:  * $densify adds documents spanning the range of values within the specified bounds.  * The data type for the bounds must correspond to the data type in the field being densified.  * For behavior details, see range.bounds Behavior. If bounds is \"full\":  * $densify adds documents spanning the full range of values of the field being densified. If bounds is \"partition\":  * $densify adds documents to each partition, similar to if you had run a full range densification on each partition individually. range.step\nRequired The amount to increment the field value in each document. $densify creates a new document for each step between the existing documents. If range.unit is specified, step must be an integer. Otherwise, step can be any numeric value. range.unit\nRequired if field is a date. The unit to apply to the step field when incrementing date values in field. You can specify one of the following values for unit as a string:  * millisecond  * second  * minute  * hour  * day  * week  * month  * quarter  * year For an example, see Densify Time Series Data. \nBEHAVIOR AND RESTRICTIONS  FIELD RESTRICTIONS \nFor documents that contain the specified field, $densify errors if:  * Any document in the collection has a field value of type date and the unit field is not specified.  * Any document in the collection has a field value of type numeric and the unit field is specified.  * The field name begins with $. You must rename the field if you want to densify it. To rename fields, use $project. \nPARTITIONBYFIELDS RESTRICTIONS \n$densify errors if any field name in the partitionByFields array:  * Evaluates to a non-string value.  * Begins with $. \nRANGE.BOUNDS BEHAVIOR \nIf range.bounds is an array:  * The lower bound indicates the start value for the added documents, irrespective of documents already in the collection.  * The lower bound is inclusive.  * The upper bound is exclusive.  * $densify does not filter out documents with field values outside of the specified bounds. \nORDER OF OUTPUT \n$densify does not guarantee sort order of the documents it outputs. To guarantee sort order, use $sort on the field you want to sort by. \nEXAMPLES \n\n \nCreate a weather collection that contains temperature readings over four hour intervals. db.weather.insertMany( [   {       \"metadata\": { \"sensorId\": 5578, \"type\": \"temperature\" },       \"timestamp\": ISODate(\"2021-05-18T00:00:00.000Z\"),       \"temp\": 12   },   {       \"metadata\": { \"sensorId\": 5578, \"type\": \"temperature\" },       \"timestamp\": ISODate(\"2021-05-18T04:00:00.000Z\"),       \"temp\": 11   },   {       \"metadata\": { \"sensorId\": 5578, \"type\": \"temperature\" },       \"timestamp\": ISODate(\"2021-05-18T08:00:00.000Z\"),       \"temp\": 11   },   {       \"metadata\": { \"sensorId\": 5578, \"type\": \"temperature\" },       \"timestamp\": ISODate(\"2021-05-18T12:00:00.000Z\"),       \"temp\": 12   }] )  This example uses the $densify stage to fill in the gaps between the four-hour intervals to achieve hourly granularity for the data points: db.weather.aggregate( [   {      $densify: {         field: \"timestamp\",         range: {            step: 1,            unit: \"hour\",            bounds:[ ISODate(\"2021-05-18T00:00:00.000Z\"), ISODate(\"2021-05-18T08:00:00.000Z\") ]         }      }   }] )  In the example:  * The $densify stage fills in the gaps of time in between the recorded temperatures.\n   \n   * field: \"timestamp\" densifies the timestamp field.\n   \n   * range:\n     \n     * step: 1 increments the timestamp field by 1 unit.\n     \n     * unit: hour densifies the timestamp field by the hour.\n     \n     * bounds: [ ISODate(\"2021-05-18T00:00:00.000Z\"), ISODate(\"2021-05-18T08:00:00.000Z\") ] sets the range of time that is densified. In the following output, the $densify stage fills in the gaps of time between the hours of 00:00:00 and 08:00:00.\n\n \nDENSIFICTION WITH PARTITIONS \nCreate a coffee collection that contains data for two varieties of coffee beans: db.coffee.insertMany( [   {      \"altitude\": 600,      \"variety\": \"Arabica Typica\",      \"score\": 68.3   },   {      \"altitude\": 750,      \"variety\": \"Arabica Typica\",      \"score\": 69.5   },   {      \"altitude\": 950,      \"variety\": \"Arabica Typica\",      \"score\": 70.5   },   {      \"altitude\": 1250,      \"variety\": \"Gesha\",      \"score\": 88.15   },   {     \"altitude\": 1700,     \"variety\": \"Gesha\",     \"score\": 95.5,     \"price\": 1029   }] )  DENSIFY THE FULL RANGE OF VALUES \nThis example uses $densify to densify the altitude field for each coffee variety: db.coffee.aggregate( [   {      $densify: {         field: \"altitude\",         partitionByFields: [ \"variety\" ],         range: {            bounds: \"full\",            step: 200         }      }   }] )  The example aggregation:  * Partitions the documents by variety to create one grouping for Arabica Typica and one for Gesha coffee.  * Specifies a full range, meaning that the data is densified across the full range of existing documents for each partition.  * Specifies a step of 200, meaning new documents are created at altitude intervals of 200. The aggregation outputs the following documents: [   {     _id: ObjectId(\"618c031814fbe03334480475\"),     altitude: 600,     variety: 'Arabica Typica',     score: 68.3   },   {     _id: ObjectId(\"618c031814fbe03334480476\"),     altitude: 750,     variety: 'Arabica Typica',     score: 69.5   },   { variety: 'Arabica Typica', altitude: 800 },   {     _id: ObjectId(\"618c031814fbe03334480477\"),     altitude: 950,     variety: 'Arabica Typica',     score: 70.5   },   { variety: 'Gesha', altitude: 600 },   { variety: 'Gesha', altitude: 800 },   { variety: 'Gesha', altitude: 1000 },   { variety: 'Gesha', altitude: 1200 },   {     _id: ObjectId(\"618c031814fbe03334480478\"),     altitude: 1250,     variety: 'Gesha',     score: 88.15   },   { variety: 'Gesha', altitude: 1400 },   { variety: 'Gesha', altitude: 1600 },   {     _id: ObjectId(\"618c031814fbe03334480479\"),     altitude: 1700,     variety: 'Gesha',     score: 95.5,     price: 1029   },   { variety: 'Arabica Typica', altitude: 1000 },   { variety: 'Arabica Typica', altitude: 1200 },   { variety: 'Arabica Typica', altitude: 1400 },   { variety: 'Arabica Typica', altitude: 1600 } ] This image visualizes the documents created with $densify:   * The darker squares represent the original documents in the collection.  * The lighter squares represent the documents created with $densify. DENSIFY VALUES WITHIN EACH PARTITION\n\n db.coffee.aggregate( [   {      $densify: {         field: \"altitude\",         partitionByFields: [ \"variety\" ],         range: {            bounds: \"partition\",            step: 200         }      }   }] )  The example aggregation:  * Partitions the documents by variety to create one grouping for Arabica Typica and one for Gesha coffee.  * Specifies a partition range, meaning that the data is densified within each partition.\n   \n   * For the Arabica Typica partition, the range is 600-900.\n   \n   * For the Gesha partition, the range is 1250-1700.  * Specifies a step of 200, meaning new documents are created at altitude intervals of 200. The aggregation outputs the following documents: [   {     _id: ObjectId(\"618c031814fbe03334480475\"),     altitude: 600,     variety: 'Arabica Typica',     score: 68.3   },   {     _id: ObjectId(\"618c031814fbe03334480476\"),     altitude: 750,     variety: 'Arabica Typica',     score: 69.5   },   { variety: 'Arabica Typica', altitude: 800 },   {     _id: ObjectId(\"618c031814fbe03334480477\"),     altitude: 950,     variety: 'Arabica Typica',     score: 70.5   },   {     _id: ObjectId(\"618c031814fbe03334480478\"),     altitude: 1250,     variety: 'Gesha',     score: 88.15   },   { variety: 'Gesha', altitude: 1450 },   { variety: 'Gesha', altitude: 1650 },   {     _id: ObjectId(\"618c031814fbe03334480479\"),     altitude: 1700,     variety: 'Gesha',     score: 95.5,     price: 1029   } ] This image visualizes the documents created with $densify:   * The darker squares represent the original documents in the collection.  * The lighter squares represent the documents created with $densify. ←  $currentOp (aggregation)$documents (aggregation) → On this page  * Definition\n * Syntax\n * Behavior and Restrictions\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/projection/positional/": " Docs Home → MongoDB Manual \n$ (PROJECTION) \nOn this page    \n * Definition\n   \n * Usage Considerations\n * Behavior\n * Examples\n * Further Reading \nDEFINITION \n$ \nThe positional $ operator limits the contents of an <array> to return the first element that matches the query condition on the array. Use $ in the projection document of the find() method or the findOne() method when you only need one particular array element in selected documents. See the aggregation operator $filter to return an array with only those elements that match the specified condition. \nNOTE \nDISAMBIGUATION To specify an array element to update, see the positional $ operator for updates. \nUSAGE CONSIDERATIONS \nBoth the $ operator and the $elemMatch operator project the first matching element from an array based on a condition. The $ operator projects the first matching array element from each document in a collection based on some condition from the query statement. The $elemMatch projection operator takes an explicit condition argument. This allows you to project based on a condition not in the query, or if you need to project based on multiple fields in the array's embedded documents. See Array Field Limitations for an example. db.collection.find() operations on views do not support $ projection operator. \nBEHAVIOR  SYNTAX \nTo return the first array element that matches the specified query condition on the array: db.collection.find( { <array>: <condition> ... },                    { \"<array>.$\": 1 } )db.collection.find( { <array.field>: <condition> ...},                    { \"<array>.$\": 1 } )  Changed in version 4.4: You can use the $ operator to limit an <array> field which does not appear in the query document. In previous versions of MongoDB, the <array> field being limited must appear in the query document. db.collection.find( { <someOtherArray>: <condition> ... },                    { \"<array>.$\" : 1 } )  \nIMPORTANT To ensure expected behavior, the arrays used in the query document and the projection document must be the same length. If the arrays are different lenghts, the operation may error in certain scenarios. \nARRAY FIELD LIMITATIONS \nMongoDB requires the following when dealing with projection over arrays:      * Only one positional $ operator may appear in the projection document.  * Only one array field should appear in the query document. Additional array fields in the query document may lead to undefined behavior.\n   \n   For example, the following projection may lead to undefined behavior:\n   \n   db.collection.find( { <array>: <value>, <someOtherArray>: <value2> },                    { \"<array>.$\": 1 } )\n   \n     * The query document should only contain a single condition on the array field to which it is applied. Multiple conditions may override each other internally and lead to undefined behavior.\n   \n   To specify criteria on multiple fields of documents inside that array, use the $elemMatch query operator. The following query returns the first document inside a grades array that has a mean of greater than 70 and a grade of greater than 90.\n   \n   db.students.find( { grades: { $elemMatch: {                                            mean: { $gt: 70 },                                            grade: { $gt:90 }                                          } } },                  { \"grades.$\": 1 } )\n   \n   \n   \n   You must use the $elemMatch operator if you need separate conditions for selecting documents and for choosing fields within those documents.\n\n \nWhen the find() method includes a sort(), the find() method applies the sort() to order the matching documents before it applies the positional $ projection operator. If an array field contains multiple documents with the same field name and the find() method includes a sort() on that repeating field, the returned documents may not reflect the sort order because the sort was applied to the elements of the array before the $ projection operator. \nPOSITIONAL OPERATOR PLACEMENT RESTRICTION \nStarting in MongoDB 4.4, the $ projection operator can only appear at the end of the field path; e.g. \"field.$\" or \"fieldA.fieldB.$\". For example, starting in MongoDB 4.4, the following operation is invalid: db.inventory.find( { }, { \"instock.$.qty\": 1 } ) // Invalid starting in 4.4 To resolve, remove the component of the field path that follows the $ projection operator. In previous versions, MongoDB ignores the part of the path that follows the $; i.e. the projection is treated as \"instock.$\". \nPOSITIONAL OPERATOR AND $SLICE RESTRICTION \nStarting in MongoDB 4.4, find and findAndModify projection cannot include $slice projection expression as part of a $ projection expression. For example, starting in MongoDB 4.4, the following operation is invalid: db.inventory.find( { \"instock.qty\": { $gt: 25 } }, { \"instock.$\": { $slice: 1 } } ) // Invalid starting in 4.4 In previous versions, MongoDB returns the first element (instock.$) in the instock array that matches the query condition; i.e. the positional projection \"instock.$\" takes precedence and the $slice:1 is a no-op. The \"instock.$\": { $slice: 1 } does not exclude any other document field. \nEXAMPLES  PROJECT ARRAY VALUES \nA collection students contains the following documents: { \"_id\" : 1, \"semester\" : 1, \"grades\" : [ 70, 87, 90 ] }{ \"_id\" : 2, \"semester\" : 1, \"grades\" : [ 90, 88, 92 ] }{ \"_id\" : 3, \"semester\" : 1, \"grades\" : [ 85, 100, 90 ] }{ \"_id\" : 4, \"semester\" : 2, \"grades\" : [ 79, 85, 80 ] }{ \"_id\" : 5, \"semester\" : 2, \"grades\" : [ 88, 88, 92 ] }{ \"_id\" : 6, \"semester\" : 2, \"grades\" : [ 95, 90, 96 ] }  In the following query, the projection { \"grades.$\": 1 } returns only the first element greater than or equal to 85 for the grades field. db.students.find( { semester: 1, grades: { $gte: 85 } },                  { \"grades.$\": 1 } )  The operation returns the following documents: { \"_id\" : 1, \"grades\" : [ 87 ] }{ \"_id\" : 2, \"grades\" : [ 90 ] }{ \"_id\" : 3, \"grades\" : [ 85 ] }  Although the array field grades may contain multiple elements that are greater than or equal to 85, the $ projection operator returns only the first matching element from the array. \nPROJECT ARRAY DOCUMENTS \nA students collection contains the following documents where the grades field is an array of documents; each document contain the three field names grade, mean, and std:\n\n  In the following query, the projection { \"grades.$\": 1 } returns only the first element with the mean greater than 70 for the grades field: db.students.find(   { \"grades.mean\": { $gt: 70 } },   { \"grades.$\": 1 })  The operation returns the following documents: { \"_id\" : 7, \"grades\" : [  {  \"grade\" : 80,  \"mean\" : 75,  \"std\" : 8 } ] }{ \"_id\" : 8, \"grades\" : [  {  \"grade\" : 92,  \"mean\" : 88,  \"std\" : 8 } ] }  \nFURTHER READING \n$elemMatch (projection) ←  Projection Operators$elemMatch (projection) → On this page  * Definition\n * Usage Considerations\n * Behavior\n * Examples\n * Further Reading Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/aggregation/meta/": " Docs Home → MongoDB Manual \n$META \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$meta \nReturns the metadata associated with a document, e.g. \"textScore\" when performing text search. A $meta expression has the following syntax: { $meta: <metaDataKeyword> }  The $meta expression can specify the following values as the <metaDataKeyword>: Keyword\nDescription\n\"textScore\" Returns the score associated with the corresponding $text query for each matching document. The text score signifies how well the document matched the search term or terms. Starting in MongoDB 4.4, must be used in conjunction with a $text query. In earlier versions, if not used in conjunction with a $text query, returns a score of null. \"indexKey\" Returns an index key for the document if a non-text index is used. The { $meta: \"indexKey\" } expression is for debugging purposes only, and not for application logic, and is preferred over cursor.returnKey(). New in version 4.4. MongoDB Atlas Search provides additional $meta keywords, such as:      * \"searchScore\" and  * \"searchHighlights\". Refer to the Atlas Search documentation for details. \nBEHAVIOR  TEXT SCORE METADATA $META: \"TEXTSCORE\" \nREQUIRES $TEXT SEARCH \n * The { $meta: \"textScore\" } expression must be used in conjunction with $text search. For example:\n   \n   * In aggregation, you must specify a $match stage with a $text query in the pipeline to use the { $meta: \"textScore\" } expression in later stage(s). If you do not specify the $text query in the $match stage, the operation fails.\n   \n   * In find, you must specify the $text operator in the query predicate to use { $meta: \"textScore\" }. Starting in MongoDB 4.4, if you do not specify the $text operator in the query predicate, the operation fails. AVAILABILITY \n * In aggregation, the { $meta: \"textScore\" } expression can be included in various stages that accept aggregation expressions, such as $project, $group $sort, etc.  * In find, the { $meta: \"textScore\" } expression can be included in projection and in sort(). USAGE IN PROJECTION \n * The { $meta: \"textScore\" } expression can be a part of the projection document to include the text score metadata.  * The $meta expression can be present in either an inclusion or an exclusion projection.  * If you set the expression to a field name that already exists in the document, the projected metadata value overwrites the existing value. FILTER ON TEXT SCORE \n * In aggregation, following a stage that outputs a field with the text score value, you can specify a query condition or operate on the field in subsequent stages. For example, see Text Search in the Aggregation Pipeline.  * In find, you cannot specify a query condition on the text score. Use aggregation instead. USAGE IN SORT \n * The { $meta: \"textScore\" } expression can be used as a part of a sort operation to sort by the text score metadata; i.e.,\n   \n   * In aggregation, $sort stage.\n   \n   * In find, sort() method.  * The \"textScore\" metadata sorts in descending order.  * To use in a sort operation, set the { $meta: \"textScore\" } expression to an arbitrary field name. The field name is disregarded by the query system. SORT WITHOUT PROJECTION \n * In aggregation, you can sort the resulting documents by { $meta: \"textScore\" } without also having to project the textScore.  * In find, starting in MongoDB 4.4, you can sort the resulting documents by { $meta: \"textScore\" } without also having to project the textScore.\n   \n   In MongoDB 4.2 and earlier, to use { $meta: \"textScore\" } expression with sort() for a find operation, you must also include the same expression in the projection. SORT WITH PROJECTION \n * In aggregation, if you include the { $meta: \"textScore\" } expression in both the projection and sort, the projection and sort can have different field names for the expression. The field name in the sort is disregarded by the query system.\n\n \nINDEX KEY METADATA $META: \"INDEXKEY\" (AGGREGATION AND FIND) \nUSAGE \n * The { $meta: \"indexKey\" } expression is for debugging purposes only and not for application logic.  * The { $meta: \"indexKey\" } expression is preferred over cursor.returnKey(). AVAILABILITY \n * In aggregation, the { $meta: \"indexKey\" } expression can be included in various stages that accept aggregation expressions, such as $project, $group $sortByCount, etc., but not $sort. However, with an aggregation pipeline, you can first project the { $meta: \"indexKey\" } expression (such as in a $project, $addFields, etc. ) and then, sort by that field in a subsequent $sort stage.  * In find, the { $meta: \"indexKey\" } expression is only available as part of the projection document. RETURN VALUE \n * The value returned depends on how the database decides to represent values in an index and may change across versions. The represented value may not be the actual value for the field.  * The value returned depends on the execution plan chosen by the system. For example, if there are two possible indexes which can be used to answer the query, then the value of the \"indexKey\" metadata depends on which index is selected.  * If an index is not used, the { $meta: \"indexKey\" } expression does not return a value and the field is not included as part of the output. \nEXAMPLES  $META: \"TEXTSCORE\" \nCreate an articles collection with the following documents: db.articles.insertMany([   { \"_id\" : 1, \"title\" : \"cakes and ale\" },   { \"_id\" : 2, \"title\" : \"more cakes\" },   { \"_id\" : 3, \"title\" : \"bread\" },   { \"_id\" : 4, \"title\" : \"some cakes\" },   { \"_id\" : 5, \"title\" : \"two cakes to go\" },   { \"_id\" : 6, \"title\" : \"pie\" }])  Create a text index on the title field: db.articles.createIndex( { title: \"text\"} )   \n$META: \"INDEXKEY\"  NOTE The { $meta: \"indexKey\" } expression is for debugging purposes only and not for application logic. MongoDB returns the value associated with the index chosen by the query system. The system can choose a different index upon subsequent execution. For the selected index, the value returned depends on how the database decides to represent values in an index and may change across versions. The represented value may not be the actual value for the field. Create an orders collection with the following documents: db.orders.insertMany([   { \"item\" : \"abc\", \"price\" : NumberDecimal(\"12\"), \"quantity\" : 2, \"type\": \"apparel\" },   { \"item\" : \"jkl\", \"price\" : NumberDecimal(\"20\"), \"quantity\" : 1, \"type\": \"electronics\" },   { \"item\" : \"abc\", \"price\" : NumberDecimal(\"10\"), \"quantity\" : 5, \"type\": \"apparel\" }])  Create the following compound index on the type and item fields: db.orders.createIndex( { type: 1, item: 1 } )   The operation returns the matching documents with their corresponding index key: {   \"_id\" : ObjectId(\"5e98a33ceaf5e9dcf2b8dcde\"),   \"item\" : \"abc\",   \"price\" : NumberDecimal(\"12\"),   \"quantity\" : 2,   \"type\" : \"apparel\",   \"idxKey\" : { \"type\" : \"apparel\", \"item\" : \"abc\" }}{   \"_id\" : ObjectId(\"5e98a33ceaf5e9dcf2b8dce0\"),   \"item\" : \"abc\",   \"price\" : NumberDecimal(\"10\"),   \"quantity\" : 5,   \"type\" : \"apparel\",   \"idxKey\" : { \"type\" : \"apparel\", \"item\" : \"abc\" }} If no index is used, the { $meta: \"indexKey\" } does not return anything.  \nThe operation returns the matching documents without the idxKey field:\n\n ←  $mergeObjects (aggregation)$min (aggregation) → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/maxDistance/": " Docs Home → MongoDB Manual \n$MAXDISTANCE \nOn this page    \n * Definition\n   \n * Example \nDEFINITION \n$maxDistance \nThe $maxDistance operator constrains the results of a geospatial $near or $nearSphere query to the specified distance. The measuring units for the maximum distance are determined by the coordinate system in use. For GeoJSON point objects, specify the distance in meters, not radians. You must specify a non-negative number for $maxDistance. The 2dsphere and 2d geospatial indexes both support $maxDistance: . \nEXAMPLE \nThe following example query returns documents with location values that are 10 or fewer units from the point [ -74 , 40 ]. db.places.find( {   loc: { $near: [ -74 , 40 ],  $maxDistance: 10 }} )  MongoDB orders the results by their distance from [ -74 , 40 ]. The operation returns the first 100 results, unless you modify the query with the cursor.limit() method. ←  $geometry$minDistance → On this page  * Definition\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/nearSphere/": " Docs Home → MongoDB Manual \n$NEARSPHERE \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$nearSphere \nSpecifies a point for which a geospatial query returns the documents from nearest to farthest. MongoDB calculates distances for $nearSphere using spherical geometry. $nearSphere requires a geospatial index:      * 2dsphere index for location data defined as GeoJSON points  * 2d index for location data defined as legacy coordinate pairs. To use a 2d index on GeoJSON points, create the index on the coordinates field of the GeoJSON object. The $nearSphere operator can specify either a GeoJSON point or legacy coordinate point. To specify a GeoJSON Point, use the following syntax: {  $nearSphere: {     $geometry: {        type : \"Point\",        coordinates : [ <longitude>, <latitude> ]     },     $minDistance: <distance in meters>,     $maxDistance: <distance in meters>  }}   * The optional $minDistance limits the results to those documents that are at least the specified distance from the center point.  * The optional $maxDistance is available for either index. To specify a point using legacy coordinates, use the following syntax: {  $nearSphere: [ <x>, <y> ],  $minDistance: <distance in radians>,  $maxDistance: <distance in radians>}   * The optional $minDistance is available only if the query uses the 2dsphere index. $minDistance limits the results to those documents that are at least the specified distance from the center point.  * The optional $maxDistance is available for either index. If you use longitude and latitude for legacy coordinates, specify the longitude first, then latitude. \nTIP \nSEE ALSO: 2d Indexes and Geospatial Near Queries \nBEHAVIOR  SPECIAL INDEXES RESTRICTION \nYou cannot combine the $nearSphere operator, which requires a special geospatial index, with a query operator or command that requires another special index. For example you cannot combine $nearSphere with the $text query. \nSHARDED COLLECTIONS \nStarting in MongoDB 4.0, $nearSphere queries are supported for sharded collections. In earlier MongoDB versions, $nearSphere queries are not supported for sharded collections; instead, for sharded clusters, you must use the $geoNear aggregation stage or the geoNear command (available in MongoDB 4.0 and earlier). \nSORT OPERATION \nThe $nearSphere operator sorts documents by distance.  * If you use the sort() method in your query, MongoDB performs a second sort operation, re-ordering the matching documents. When querying large collections, this can negatively affect query performance.  * If the order of the documents is not important to you, consider using the $geoWithin operator instead, as it returns unsorted results.  * $nearSphere is a Match Execution operator and is not permitted in aggregation pipelines. \nEXAMPLES  SPECIFY CENTER POINT USING GEOJSON \nConsider a collection places that contains documents with a location field and has a 2dsphere index. Then, the following example returns whose location is at least 1000 meters from and at most 5000 meters from the specified point, ordered from nearest to farthest: db.places.find(   {     location: {        $nearSphere: {           $geometry: {              type : \"Point\",              coordinates : [ -73.9667, 40.78 ]           },           $minDistance: 1000,           $maxDistance: 5000        }     }   })  \nSPECIFY CENTER POINT USING LEGACY COORDINATES \n2D INDEX \nConsider a collection legacyPlaces that contains documents with legacy coordinates pairs in the location field and has a 2d index. Then, the following example returns those documents whose location is at most 0.10 radians from the specified point, ordered from nearest to farthest: db.legacyPlaces.find(   { location : { $nearSphere : [ -73.9667, 40.78 ], $maxDistance: 0.10 } }) \n\n \nIf the collection has a 2dsphere index instead, you can also specify the optional $minDistance specification. For example, the following example returns the documents whose location is at least 0.0004 radians from the specified point, ordered from nearest to farthest: db.legacyPlaces.find(   { location : { $nearSphere : [ -73.9667, 40.78 ], $minDistance: 0.0004 } }) \n←  $near$box → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/and/": " Docs Home → MongoDB Manual \n$AND \nOn this page    \n * Behavior\n   \n * Examples $and \nSyntax: { $and: [ { <expression1> }, { <expression2> } , ... , { <expressionN> } ] } $and performs a logical AND operation on an array of one or more expressions (<expression1>, <expression2>, and so on) and selects the documents that satisfy all the expressions. \nNOTE MongoDB provides an implicit AND operation when specifying a comma separated list of expressions. \nBEHAVIOR \nTo allow the query engine to optimize queries, $and handles errors as follows:      * If any expression supplied to $and would cause an error when evaluated alone, the $and containing the expression may cause an error but an error is not guaranteed.  * An expression supplied after the first expression supplied to $and may cause an error even if the first expression evaluates to false. For example, the following query always produces an error if $x is 0: db.example.find( {   $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] }} )  The following query, which contains multiple expressions supplied to $and, may produce an error if there is any document where $x is 0: db.example.find( {   $and: [      { x: { $ne: 0 } },      { $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] } }   ]} )  \nEXAMPLES  AND QUERIES WITH MULTIPLE EXPRESSIONS SPECIFYING THE SAME FIELD \nConsider this query: db.inventory.find( { $and: [ { price: { $ne: 1.99 } }, { price: { $exists: true } } ] } )  The query selects all documents in the inventory collection where:  * the price field value is not equal to 1.99 and  * the price field exists. The query can be rewritten with an implicit AND operation that combines the operator expressions for the price field: db.inventory.find( { price: { $ne: 1.99, $exists: true } } )  \nAND QUERIES WITH MULTIPLE EXPRESSIONS SPECIFYING THE SAME OPERATOR \nConsider this query: db.inventory.find( {    $and: [        { $or: [ { qty: { $lt : 10 } }, { qty : { $gt: 50 } } ] },        { $or: [ { sale: true }, { price : { $lt : 5 } } ] }    ]} )  The query selects all documents where:  * the qty field value is less than 10 or greater than 50, and  * the sale field value is equal to true or the price field value is less than 5. The query cannot use an implicit AND operation because it uses the $or operator more than once. \nTIP \nSEE ALSO:  * find()  * updateMany()  * $ne  * $exists  * $set ←  Logical Query Operators$not → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/bitsAnySet/": " Docs Home → MongoDB Manual \n$BITSANYSET \nOn this page    \n * Behavior\n   \n * Examples $bitsAnySet \n$bitsAnySet matches documents where any of the bit positions given by the query are set (i.e. 1) in field. { <field>: { $bitsAnySet: <numeric bitmask> } }\n{ <field>: { $bitsAnySet: < BinData bitmask> } }\n{ <field>: { $bitsAnySet: [ <position1>, <position2>, ... ] } } The field value must be either numeric or a BinData instance. Otherwise, $bitsAnySet will not match the current document. Numeric BitmaskYou can provide a numeric bitmask to be matched against the operand field. It must be representable as a non-negative 32-bit signed integer. Otherwise, $bitsAnySet will return an error.BinData BitmaskYou can also use an arbitrarily large BinData instance as a bitmask.Position ListIf querying a list of bit positions, each <position> must be a non-negative integer. Bit positions start at 0 from the least significant bit. For example, the decimal number 254 would have the following bit positions:\nBit Value\n1\n1\n1\n1\n1\n1\n1\n0\nPosition\n7\n6\n5\n4\n3\n2\n1\n0 \nBEHAVIOR  INDEXES \nQueries cannot use indexes for the $bitsAnySet portion of a query, although the other portions of a query can use indexes, if applicable. \nFLOATING POINT VALUES \n$bitsAnySet will not match numerical values that cannot be represented as a signed 64-bit integer. This can be the case if a value is either too large or too small to fit in a signed 64-bit integer, or if it has a fractional component. \nSIGN EXTENSION \nNumbers are sign extended. For example, $bitsAnySet considers bit position 200 to be set for the negative number -5, but bit position 200 to be clear for the positive number +5. In contrast, BinData instances are zero-extended. For example, given the following document: db.collection.save({ x: BinData(0, \"ww==\"), binaryValueofA: \"11000011\" })  $bitsAnySet will consider all bits outside of x to be clear. \nEXAMPLES \nThe following examples will use a collection with the following documents: db.collection.save({ _id: 1, a: 54, binaryValueofA: \"00110110\" })db.collection.save({ _id: 2, a: 20, binaryValueofA: \"00010100\" })db.collection.save({ _id: 3, a: 20.0, binaryValueofA: \"00010100\" })db.collection.save({ _id: 4, a: BinData(0, \"Zg==\"), binaryValueofA: \"01100110\" })  \nBIT POSITION ARRAY \nThe following query uses the $bitsAnySet operator to test whether field a has either bit position 1 or bit position 5 set, where the least significant bit is position 0. db.collection.find( { a: { $bitsAnySet: [ 1, 5 ] } } )  The query matches the following documents: { \"_id\" : 1, \"a\" : 54, \"binaryValueofA\" : \"00110110\" }{ \"_id\" : 4, \"a\" : BinData(0,\"Zg==\"), \"binaryValueofA\" : \"01100110\" }  \nINTEGER BITMASK \nThe following query uses the $bitsAnySet operator to test whether field a has any bits set at positions 0, 1, and 5 (the binary representation of the bitmask 35 is 00100011). db.collection.find( { a: { $bitsAnySet: 35 } } )  The query matches the following documents: { \"_id\" : 1, \"a\" : 54, \"binaryValueofA\" : \"00110110\" }{ \"_id\" : 4, \"a\" : BinData(0,\"Zg==\"), \"binaryValueofA\" : \"01100110\" }  \nBINDATA BITMASK \nThe following query uses the $bitsAnySet operator to test whether field a has any bits set at positions 4, and 5 (the binary representation of BinData(0, \"MC==\") is 00110000). db.collection.find( { a: { $bitsAnySet: BinData(0, \"MC==\") } } )  The query matches the following documents:\n\n \n←  $bitsAnyClearProjection Operators → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/nin/": " Docs Home → MongoDB Manual \n$NIN \nOn this page    \n * Examples $nin \nSyntax: { field: { $nin: [ <value1>, <value2> ... <valueN> ] } } $nin selects the documents where:      * the field value is not in the specified array or  * the field does not exist. If the field holds an array, then the $nin operator selects the documents whose field holds an array with no element equal to a value in the specified array (for example, <value1>, <value2>, and so on). For comparison of different BSON type values, see the specified BSON comparison order. \nEXAMPLES \nCreate the inventory collection: db.inventory.insertMany( [   { \"item\": \"Pens\", \"quantity\": 350, \"tags\": [ \"school\", \"office\" ] },   { \"item\": \"Erasers\", \"quantity\": 15, \"tags\": [ \"school\", \"home\" ] },   { \"item\": \"Maps\", \"tags\": [ \"office\", \"storage\" ] },   { \"item\": \"Books\", \"quantity\": 5, \"tags\": [ \"school\", \"storage\", \"home\" ] }] )  \nSELECT ON UNMATCHING DOCUMENTS \nThe following query selects all documents from the inventory collection where the quantity does not equal either 5 or 15. The query also matches documents that do not have a quantity field. db.inventory.find( { quantity: { $nin: [ 5, 15 ] } }, { _id: 0 } )  Example output: { item: 'Pens', quantity: 350, tags: [ 'school', 'office' ] },{ item: 'Maps', tags: [ 'office', 'storage' ] }  \nSELECT ON ELEMENTS NOT IN AN ARRAY \nSet the exclude field to true for documents that don't have the \"school\" tag. db.inventory.updateMany(    { tags: { $nin: [ \"school\" ] } },    { $set: { exclude: true } })  updateMany() also selects a document when the document does not contain the field $nin is matching on. The inequality operator $nin is not very selective since it often matches a large portion of the index. As a result, in many cases, a $nin query with an index may perform no better than a $nin query that must scan all documents in a collection. See also Query Selectivity. \nTIP \nSEE ALSO:  * find()  * updateMany()  * $set ←  $neLogical Query Operators → On this page  * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/lt/": " Docs Home → MongoDB Manual \n$LT \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$lt \nSyntax: { field: { $lt: value } } $lt selects the documents where the value of the field is less than (i.e. <) the specified value. For most data types, comparison operators only perform comparisons on fields where the BSON type matches the query value's type. MongoDB supports limited cross-BSON comparison through Type Bracketing. \nEXAMPLES \nThe following examples use the inventory collection. Create the collection: db.inventory.insertMany( [   {      \"item\": \"nuts\", \"quantity\": 30,      \"carrier\": { \"name\": \"Shipit\", \"fee\": 3 }   },   {      \"item\": \"bolts\", \"quantity\": 50,      \"carrier\": { \"name\": \"Shipit\", \"fee\": 4 }   },   {      \"item\": \"washers\", \"quantity\": 10,      \"carrier\": { \"name\": \"Shipit\", \"fee\": 1 }   }] )  \nMATCH DOCUMENT FIELDS \nSelect all documents in the inventory collection where quantity is less than 20: db.inventory.find( { quantity: { $lt: 20 } } )  Example output: {  _id: ObjectId(\"61ba634dfe687fce2f04241f\"),  item: 'washers',  quantity: 10,  carrier: { name: 'Shipit', fee: 1 }}  \nPERFORM AN UPDATE BASED ON EMBEDDED DOCUMENT FIELDS \nThe following example sets the price field based on a $lt comparison against a field in an embedded document. db.inventory.updateMany( { \"carrier.fee\": { $lt: 20 } }, { $set: { price: 9.99 } } )  Example output: {  _id: ObjectId(\"61ba634dfe687fce2f04241d\"),  item: 'nuts',  quantity: 30,  carrier: { name: 'Shipit', fee: 3 },  price: 9.99},{  _id: ObjectId(\"61ba634dfe687fce2f04241e\"),  item: 'bolts',  quantity: 50,  carrier: { name: 'Shipit', fee: 4 },  price: 9.99},{  _id: ObjectId(\"61ba634dfe687fce2f04241f\"),  item: 'washers',  quantity: 10,  carrier: { name: 'Shipit', fee: 1 },  price: 9.99}  This updateMany() operation searches for an embedded document, carrier, with a subfield named fee. It sets { price: 9.99 } in each document where fee has a value less than 20. To set the value of the price field in only the first document where carrier.fee is less than 20, use updateOne(). \nTIP \nSEE ALSO:      * find()  * $set ←  $in$lte → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/nor/": " Docs Home → MongoDB Manual \n$NOR \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$nor \n$nor performs a logical NOR operation on an array of one or more query expression and selects the documents that fail all the query expressions in the array. The $nor has the following syntax: { $nor: [ { <expression1> }, { <expression2> }, ...  { <expressionN> } ] }  \nEXAMPLES  $NOR QUERY WITH TWO EXPRESSIONS \nConsider the following query which uses only the $nor operator: db.inventory.find( { $nor: [ { price: 1.99 }, { sale: true } ]  } )  This query will return all documents that:      * contain the price field whose value is not equal to 1.99 and contain the sale field whose value is not equal to true or  * contain the price field whose value is not equal to 1.99 but do not contain the sale field or  * do not contain the price field but contain the sale field whose value is not equal to true or  * do not contain the price field and do not contain the sale field \n$NOR AND ADDITIONAL COMPARISONS \nConsider the following query: db.inventory.find( { $nor: [ { price: 1.99 }, { qty: { $lt: 20 } }, { sale: true } ] } )  This query will select all documents in the inventory collection where:  * the price field value does not equal 1.99 and  * the qty field value is not less than 20 and  * the sale field value is not equal to true including those documents that do not contain these field(s). The exception in returning documents that do not contain the field in the $nor expression is when the $nor operator is used with the $exists operator. \n$NOR AND $EXISTS \nCompare that with the following query which uses the $nor operator with the $exists operator: db.inventory.find( { $nor: [ { price: 1.99 }, { price: { $exists: false } },                             { sale: true }, { sale: { $exists: false } } ] } )  This query will return all documents that:  * contain the price field whose value is not equal to 1.99 and contain the sale field whose value is not equal to true \nTIP \nSEE ALSO:  * find()  * $or  * $set  * $exists ←  $not$or → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/regex/": " Docs Home → MongoDB Manual \n$REGEX \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nNOTE This page describes regular expression search capabilities for self-managed (non-Atlas) deployments. For data hosted on MongoDB Atlas, MongoDB offers an improved full-text search solution, Atlas Search, which has its own $regex operator. To learn more, see $regex in the Atlas Search documentation. \nDEFINITION \n$regex \nProvides regular expression capabilities for pattern matching strings in queries. MongoDB uses Perl compatible regular expressions (i.e. \"PCRE\" ) version 8.42 with UTF-8 support. To use $regex, use one of the following syntaxes: { <field>: { $regex: /pattern/, $options: '<options>' } }{ <field>: { $regex: 'pattern', $options: '<options>' } }{ <field>: { $regex: /pattern/<options> } }  In MongoDB, you can also use regular expression objects (i.e. /pattern/) to specify regular expressions: { <field>: /pattern/<options> }  For restrictions on particular syntax use, see $regex vs. /pattern/ Syntax. $options \nThe following <options> are available for use with regular expression. Option\nDescription\nSyntax Restrictions\ni\nCase insensitivity to match upper and lower cases. For an example, see Perform Case-Insensitive Regular Expression Match. m For patterns that include anchors (i.e. ^ for the start, $ for the end), match at the beginning or end of each line for strings with multiline values. Without this option, these anchors match at beginning or end of the string. For an example, see Multiline Match for Lines Starting with Specified Pattern. If the pattern contains no anchors or if the string value has no newline characters (e.g. \\n), the m option has no effect. \nx \"Extended\" capability to ignore all white space characters in the $regex pattern unless escaped or included in a character class. Additionally, it ignores characters in-between and including an un-escaped hash/pound (#) character and the next new line, so that you may include comments in complicated patterns. This only applies to data characters; white space characters may never appear within special character sequences in a pattern. The x option does not affect the handling of the VT character (i.e. code 11). Requires $regex with $options syntax\ns\nAllows the dot character (i.e. .) to match all characters including newline characters. For an example, see Use the . Dot Character to Match New Line.\nRequires $regex with $options syntax \nNOTE The $regex operator does not support the global search modifier g. \nBEHAVIOR  $REGEX VS. /PATTERN/ SYNTAX \n$IN EXPRESSIONS \nTo include a regular expression in an $in query expression, you can only use JavaScript regular expression objects (i.e. /pattern/ ). For example: { name: { $in: [ /^acme/i, /^ack/ ] } }  You cannot use $regex operator expressions inside an $in. IMPLICIT AND CONDITIONS FOR THE FIELD \nTo include a regular expression in a comma-separated list of query conditions for the field, use the $regex operator. For example: { name: { $regex: /acme.*corp/i, $nin: [ 'acmeblahcorp' ] } }{ name: { $regex: /acme.*corp/, $options: 'i', $nin: [ 'acmeblahcorp' ] } }{ name: { $regex: 'acme.*corp', $options: 'i', $nin: [ 'acmeblahcorp' ] } }  X AND S OPTIONS \nTo use either the x option or s options, you must use the $regex operator expression with the $options operator. For example, to specify the i and the s options, you must use $options for both: { name: { $regex: /acme.*corp/, $options: \"si\" } }{ name: { $regex: 'acme.*corp', $options: \"si\" } }  PCRE VS JAVASCRIPT \nTo use PCRE supported features in a regular expression that aren't supported in JavaScript, you must use the $regex operator and specify the regular expression as a string. To match case-insensitive strings:    \n\n  * \"(?-i)\" ends a case-insensitive match. For example, the regular expression \"(?i)a(?-i)cme\" matches strings that:  * Begin with \"a\" or \"A\". This is a case-insensitive match.  * End with \"cme\". This is a case-sensitive match. These strings match the example regular expression:  * \"acme\"  * \"Acme\" The following example uses the $regex operator to find name field strings that match the regular expression \"(?i)a(?-i)cme\": { name: { $regex: \"(?i)a(?-i)cme\" } }  $REGEX AND $NOT \nThe $not operator can perform logical NOT operation on both:  * Regular expression objects (i.e. /pattern/)\n   \n   For example:\n   \n   db.inventory.find( { item: { $not: /^p.*/ } } )\n   \n     * $regex operator expressions\n   \n   For example:\n   \n   db.inventory.find( { item: { $not: { $regex: \"^p.*\" } } } )db.inventory.find( { item: { $not: { $regex: /^p.*/ } } } )\n   \n    In 4.0.6 and earlier, you could use $not operator with regular expression objects (i.e. /pattern/) but not with $regex operator expressions. \nINDEX USE \nFor case sensitive regular expression queries, if an index exists for the field, then MongoDB matches the regular expression against the values in the index, which can be faster than a collection scan. Further optimization can occur if the regular expression is a \"prefix expression\", which means that all potential matches start with the same string. This allows MongoDB to construct a \"range\" from that prefix and only match against those values from the index that fall within that range. A regular expression is a \"prefix expression\" if it starts with a caret (^) or a left anchor (\\A), followed by a string of simple symbols. For example, the regex /^abc.*/ will be optimized by matching only against the values from the index that start with abc. Additionally, while /^a/, /^a.*/, and /^a.*$/ match equivalent strings, they have different performance characteristics. All of these expressions use an index if an appropriate index exists; however, /^a.*/, and /^a.*$/ are slower. /^a/ can stop scanning after matching the prefix. Case insensitive regular expression queries generally cannot use indexes effectively. The $regex implementation is not collation-aware and is unable to utilize case-insensitive indexes. \nEXAMPLES \nThe examples in this section use the following products collection: db.products.insertMany( [   { _id: 100, sku: \"abc123\", description: \"Single line description.\" },   { _id: 101, sku: \"abc789\", description: \"First line\\nSecond line\" },   { _id: 102, sku: \"xyz456\", description: \"Many spaces before     line\" },   { _id: 103, sku: \"xyz789\", description: \"Multiple\\nline description\" },   { _id: 104, sku: \"Abc789\", description: \"SKU starts with A\" }] )  \nPERFORM A LIKE MATCH \nThe following example matches all documents where the sku field is like \"%789\": db.products.find( { sku: { $regex: /789$/ } } )  The example is analogous to the following SQL LIKE statement: SELECT * FROM productsWHERE sku like \"%789\";  Example output: [   { _id: 101, sku: 'abc789', description: 'First line\\nSecond line' },   { _id: 103, sku: 'xyz789', description: 'Multiple\\nline description' },   { _id: 104, sku: 'Abc789', description: 'SKU starts with A' }] \nPERFORM CASE-INSENSITIVE REGULAR EXPRESSION MATCH \nThe following example uses the i option perform a case-insensitive match for documents with sku value that starts with ABC. db.products.find( { sku: { $regex: /^ABC/i } } )  Example output:\n\n \nMULTILINE MATCH FOR LINES STARTING WITH SPECIFIED PATTERN \nThe following example uses the m option to match lines starting with the letter S for multiline strings: db.products.find( { description: { $regex: /^S/, $options: 'm' } } )  Example output: [   { _id: 100, sku: 'abc123', description: 'Single line description.' },   { _id: 101, sku: 'abc789', description: 'First line\\nSecond line' },   { _id: 104, sku: 'Abc789', description: 'SKU starts with A' }] Without the m option, the example output is: [   { _id: 100, sku: 'abc123', description: 'Single line description.' },   { _id: 104, sku: 'Abc789', description: 'SKU starts with A' }] If the $regex pattern does not contain an anchor, the pattern matches against the string as a whole, as in the following example: db.products.find( { description: { $regex: /S/ } } )  Example output: [   { _id: 100, sku: 'abc123', description: 'Single line description.' },   { _id: 101, sku: 'abc789', description: 'First line\\nSecond line' },   { _id: 104, sku: 'Abc789', description: 'SKU starts with A' }] \nUSE THE . DOT CHARACTER TO MATCH NEW LINE \nThe following example uses the s option to allow the dot character (i.e. .) to match all characters including new line as well as the i option to perform a case-insensitive match: db.products.find( { description: { $regex: /m.*line/, $options: 'si' } } )  Example output: [   { _id: 102, sku: 'xyz456', description: 'Many spaces before     line' },   { _id: 103, sku: 'xyz789', description: 'Multiple\\nline description' }] Without the s option, the example output is: [   { _id: 102, sku: 'xyz456', description: 'Many spaces before     line' }] \nIGNORE WHITE SPACES IN PATTERN \nThe following example uses the x option ignore white spaces and the comments, denoted by the # and ending with the \\n in the matching pattern: var pattern = \"abc #category code\\n123 #item number\"db.products.find( { sku: { $regex: pattern, $options: \"x\" } } )  Example output: [   { _id: 100, sku: 'abc123', description: 'Single line description.' }] \nUSE A REGULAR EXPRESSION TO MATCH CASE IN STRINGS \nThe following example uses the regular expression \"(?i)a(?-i)bc\" to match sku field strings that contain:  * \"abc\"  * \"Abc\" db.products.find( { sku: { $regex: \"(?i)a(?-i)bc\" } } )  Example output: [   { _id: 100, sku: 'abc123', description: 'Single line description.' },   { _id: 101, sku: 'abc789', description: 'First line\\nSecond line' },   { _id: 104, sku: 'Abc789', description: 'SKU starts with A' }] ←  $mod$text → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/not/": " Docs Home → MongoDB Manual \n$NOT \nOn this page    \n * Definition\n   \n * Behavior \nDEFINITION \n$not \nSyntax: { field: { $not: { <operator-expression> } } } $not performs a logical NOT operation on the specified <operator-expression> and selects the documents that do not match the <operator-expression>. This includes documents that do not contain the field. Consider the following query: db.inventory.find( { price: { $not: { $gt: 1.99 } } } )  This query will select all documents in the inventory collection where:      * the price field value is less than or equal to 1.99 or  * the price field does not exist { $not: { $gt: 1.99 } } is different from the $lte operator. { $lte: 1.99 } returns only the documents where price field exists and its value is less than or equal to 1.99. Remember that the $not operator only affects other operators and cannot check fields and documents independently. So, use the $not operator for logical disjunctions and the $ne operator to test the contents of fields directly. \nBEHAVIOR  $NOT AND DATA TYPES \nThe operation of the $not operator is consistent with the behavior of other operators but may yield unexpected results with some data types like arrays. \n$NOT AND REGULAR EXPRESSIONS \n$not operator can perform logical NOT operation on:  * Regular expression objects (i.e. /pattern/)\n   \n   For example, the following query selects all documents in the inventory collection where the item field value does not start with the letter p.\n   \n   db.inventory.find( { item: { $not: /^p.*/ } } )\n   \n     * $regex operator expression\n   \n   For example, the following query selects all documents in the inventory collection where the item field value does not start with the letter p.\n   \n   db.inventory.find( { item: { $not: { $regex: \"^p.*\" } } } )db.inventory.find( { item: { $not: { $regex: /^p.*/ } } } )\n   \n     * driver language's regular expression objects\n   \n   For example, the following PyMongo query uses Python's re.compile() method to compile a regular expression:\n   \n   import refor noMatch in db.inventory.find( { \"item\": { \"$not\": re.compile(\"^p.*\") } } ):    print noMatch\n   \n    \nTIP \nSEE ALSO:  * find()  * $set  * $gt  * $regex ←  $and$nor → On this page  * Definition\n * Behavior Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/ne/": " Docs Home → MongoDB Manual \n$NE \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$ne \nSyntax: { field: { $ne: value } } $ne selects the documents where the value of the field is not equal to the specified value. This includes documents that do not contain the field. For comparison of different BSON type values, see the specified BSON comparison order. \nEXAMPLES \nThe following examples use the inventory collection. Create the collection: db.inventory.insertMany( [   {      \"item\": \"nuts\", \"quantity\": 30,      \"carrier\": { \"name\": \"Shipit\", \"fee\": 3 }   },   {      \"item\": \"bolts\", \"quantity\": 50,      \"carrier\": { \"name\": \"Shipit\", \"fee\": 4 }   },   {      \"item\": \"washers\", \"quantity\": 10,      \"carrier\": { \"name\": \"Shipit\", \"fee\": 1 }   }] )  \nMATCH DOCUMENT FIELDS \nSelect all documents in the inventory collection where quantity is not equal to 20: db.inventory.find( { quantity: { $ne: 20 } } )  The query will also select documents that do not have the quantity field. Example output: {  _id: ObjectId(\"61ba667dfe687fce2f042420\"),  item: 'nuts',  quantity: 30,  carrier: { name: 'Shipit', fee: 3 }},{  _id: ObjectId(\"61ba667dfe687fce2f042421\"),  item: 'bolts',  quantity: 50,  carrier: { name: 'Shipit', fee: 4 }},{  _id: ObjectId(\"61ba667dfe687fce2f042422\"),  item: 'washers',  quantity: 10,  carrier: { name: 'Shipit', fee: 1 }}  \nPERFORM AN UPDATE BASED ON EMBEDDED DOCUMENT FIELDS \nThe following example sets the price field based on a $ne comparison against a field in an embedded document. db.inventory.updateMany( { \"carrier.fee\": { $ne: 1 } }, { $set: { \"price\": 9.99 } } )  Example output: {  _id: ObjectId(\"61ba66e2fe687fce2f042423\"),  item: 'nuts',  quantity: 30,  carrier: { name: 'Shipit', fee: 3 },  price: 9.99},{  _id: ObjectId(\"61ba66e2fe687fce2f042424\"),  item: 'bolts',  quantity: 50,  carrier: { name: 'Shipit', fee: 4 },  price: 9.99},{  _id: ObjectId(\"61ba66e2fe687fce2f042425\"),  item: 'washers',  quantity: 10,  carrier: { name: 'Shipit', fee: 1 }}  This updateMany() operation searches for an embedded document, carrier, with a subfield named fee. It sets { price: 9.99 } in each document where fee has a value that does not equal 1 or where the fee subfield does not exist. The inequality operator $ne is not very selective since it often matches a large portion of the index. As a result, in many cases, a $ne query with an index may perform no better than a $ne query that must scan all documents in a collection. See also Query Selectivity. \nTIP \nSEE ALSO:      * find()  * $set ←  $lte$nin → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/geometry/": " Docs Home → MongoDB Manual \n$GEOMETRY \n$geometry \nThe $geometry operator specifies a GeoJSON geometry for use with the following geospatial query operators: $geoWithin, $geoIntersects, $near, and $nearSphere. $geometry uses EPSG:4326 as the default coordinate reference system (CRS). To specify GeoJSON objects with the default CRS, use the following prototype for $geometry: $geometry: {   type: \"<GeoJSON object type>\",   coordinates: [ <coordinates> ]}  To specify a single-ringed GeoJSON polygon with a custom MongoDB CRS, use the following prototype (available only for $geoWithin and $geoIntersects): $geometry: {   type: \"Polygon\",   coordinates: [ <coordinates> ],   crs: {      type: \"name\",      properties: { name: \"urn:x-mongodb:crs:strictwinding:EPSG:4326\" }   }}  The custom MongoDB coordinate reference system has a strict counter-clockwise winding order. \nIMPORTANT \nIMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.      * Valid longitude values are between -180 and 180, both inclusive.  * Valid latitude values are between -90 and 90, both inclusive. ←  $centerSphere$maxDistance → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/size/": " Docs Home → MongoDB Manual \n$SIZE \nOn this page    \n * Additional Examples $size \nThe $size operator matches any array with the number of elements specified by the argument. For example: db.collection.find( { field: { $size: 2 } } );  returns all documents in collection where field is an array with 2 elements. For instance, the above expression will return { field: [ red, green ] } and { field: [ apple, lime ] } but not { field: fruit } or { field: [ orange, lemon, grapefruit ] }. To match fields with only one element within an array use $size with a value of 1, as follows: db.collection.find( { field: { $size: 1 } } );  $size does not accept ranges of values. To select documents based on fields with different numbers of elements, create a counter field that you increment when you add elements to a field. Queries cannot use indexes for the $size portion of a query, although the other portions of a query can use indexes if applicable. \nADDITIONAL EXAMPLES \nFor additional examples in querying arrays, see:      * Query an Array  * Query an Array of Embedded Documents For additional examples in querying, see:  * Query Documents \nTIP \nSEE ALSO: db.collection.find() ←  $elemMatch (query)Bitwise Query Operators → On this page  * Additional Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/center/": " Docs Home → MongoDB Manual \n$CENTER \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$center \nThe $center operator specifies a circle for a $geoWithin query. The query returns legacy coordinate pairs that are within the bounds of the circle. The operator does not return GeoJSON objects. To use the $center operator, specify an array that contains:      * The grid coordinates of the circle's center point, and  * The circle's radius, as measured in the units used by the coordinate system. {   <location field>: {      $geoWithin: { $center: [ [ <x>, <y> ] , <radius> ] }   }}  \nIMPORTANT \nIF YOU USE LONGITUDE AND LATITUDE, SPECIFY LONGITUDE FIRST.  \nBEHAVIOR \nThe query calculates distances using flat (planar) geometry. Applications can use $center without having a geospatial index. However, geospatial indexes support much faster queries than the unindexed equivalents. Only the 2d geospatial index supports $center. \nEXAMPLE \nThe following example query returns all documents that have coordinates that exist within the circle centered on [ -74, 40.74 ] and with a radius of 10: db.places.find(   { loc: { $geoWithin: { $center: [ [-74, 40.74], 10 ] } } }) \n←  $box$centerSphere → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/expr/": " Docs Home → MongoDB Manual \n$EXPR \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \nChanged in version 5.0. $expr \nAllows the use of aggregation expressions within the query language. $expr has the following syntax: { $expr: { <expression> } }  The arguments can be any valid aggregation expression. For more information, see Expressions. \nBEHAVIOR \n$expr can build query expressions that compare fields from the same document in a $match stage. If the $match stage is part of a $lookup stage, $expr can compare fields using let variables. See Perform Multiple Joins and a Correlated Subquery with $lookup for an example. Starting in MongoDB 5.0, the $eq, $lt, $lte, $gt, and $gte comparison operators placed in an $expr operator can use an index on the from collection referenced in a $lookup stage. Limitations:      * Multikey indexes are not used.  * Indexes are not used for comparisons where the operand is an array or the operand type is undefined.  * Indexes are not used for comparisons with more than one field path operand. \nEXAMPLES  COMPARE TWO FIELDS FROM A SINGLE DOCUMENT \nConsider an monthlyBudget collection with the following documents: { \"_id\" : 1, \"category\" : \"food\", \"budget\": 400, \"spent\": 450 }{ \"_id\" : 2, \"category\" : \"drinks\", \"budget\": 100, \"spent\": 150 }{ \"_id\" : 3, \"category\" : \"clothes\", \"budget\": 100, \"spent\": 50 }{ \"_id\" : 4, \"category\" : \"misc\", \"budget\": 500, \"spent\": 300 }{ \"_id\" : 5, \"category\" : \"travel\", \"budget\": 200, \"spent\": 650 }  The following operation uses $expr to find documents where the spent amount exceeds the budget: db.monthlyBudget.find( { $expr: { $gt: [ \"$spent\" , \"$budget\" ] } } )  The operation returns the following results: { \"_id\" : 1, \"category\" : \"food\", \"budget\" : 400, \"spent\" : 450 }{ \"_id\" : 2, \"category\" : \"drinks\", \"budget\" : 100, \"spent\" : 150 }{ \"_id\" : 5, \"category\" : \"travel\", \"budget\" : 200, \"spent\" : 650 }  \nUSING $EXPR WITH CONDITIONAL STATEMENTS \nSome queries require the ability to execute conditional logic when defining a query filter. The aggregation pipeline provides the $cond operator to express conditional statements. By using $expr with the $cond operator, you can specify a conditional filter for your query statement. Create a sample supplies collection with the following documents: db.supplies.insertMany([   { \"_id\" : 1, \"item\" : \"binder\", \"qty\" : NumberInt(\"100\"), \"price\" : NumberDecimal(\"12\") },   { \"_id\" : 2, \"item\" : \"notebook\", \"qty\" : NumberInt(\"200\"), \"price\" : NumberDecimal(\"8\") },   { \"_id\" : 3, \"item\" : \"pencil\", \"qty\" : NumberInt(\"50\"), \"price\" : NumberDecimal(\"6\") },   { \"_id\" : 4, \"item\" : \"eraser\", \"qty\" : NumberInt(\"150\"), \"price\" : NumberDecimal(\"3\") },   { \"_id\" : 5, \"item\" : \"legal pad\", \"qty\" : NumberInt(\"42\"), \"price\" : NumberDecimal(\"10\") }])  Assume that for an upcoming sale next month, you want to discount the prices such that:  * If qty is greater than or equal to 100, the discounted price will be 0.5 of the price.  * If qty is less than 100, the discounted price is 0.75 of the price. Before applying the discounts, you would like to know which items in the supplies collection have a discounted price of less than 5. The following example uses $expr with $cond to calculate the discounted price based on the qty and $lt to return documents whose calculated discount price is less than NumberDecimal(\"5\"):\n\n  The following table shows the discounted price for each document and whether discounted price is less than NumberDecimal(\"5\") (i.e. whether the document meets the query condition). Document\nDiscounted Price\n< NumberDecimal(\"5\")\n{\"_id\": 1, \"item\": \"binder\", \"qty\": 100, \"price\": NumberDecimal(\"12\") }\nNumberDecimal(\"6.00\")\nfalse\n{\"_id\": 2, \"item\": \"noteboook\", \"qty\": 200, \"price\": NumberDecimal(\"8\") }\nNumberDecimal(\"4.00\")\ntrue\n{\"_id\": 3, \"item\": \"pencil\", \"qty\": 50, \"price\": NumberDecimal(\"6\") }\nNumberDecimal(\"4.50\")\ntrue\n{\"_id\": 4, \"item\": \"eraser\", \"qty\": 150, \"price\": NumberDecimal(\"3\") }\nNumberDecimal(\"1.50\")\ntrue\n{\"_id\": 5, \"item\": \"legal pad\", \"qty\": 42, \"price\": NumberDecimal(\"10\") }\nNumberDecimal(\"7.50\")\nfalse The db.collection.find() operation returns the documents whose calculated discount price is less than NumberDecimal(\"5\"): { \"_id\" : 2, \"item\" : \"notebook\", \"qty\": 200 , \"price\": NumberDecimal(\"8\") }{ \"_id\" : 3, \"item\" : \"pencil\", \"qty\": 50 , \"price\": NumberDecimal(\"6\") }{ \"_id\" : 4, \"item\" : \"eraser\", \"qty\": 150 , \"price\": NumberDecimal(\"3\") }  Even though $cond calculates an effective discounted price, that price is not reflected in the returned documents. Instead, the returned documents represent the matching documents in their original state. The find operation did not return the binder or legal pad documents, as their discounted price was greater than 5. ←  Evaluation Query Operators$jsonSchema → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/near/": " Docs Home → MongoDB Manual \n$NEAR \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$near \nSpecifies a point for which a geospatial query returns the documents from nearest to farthest. The $near operator can specify either a GeoJSON point or legacy coordinate point. $near requires a geospatial index:      * 2dsphere index if specifying a GeoJSON point,  * 2d index if specifying a point using legacy coordinates. To specify a GeoJSON point, $near operator requires a 2dsphere index and has the following syntax: {   <location field>: {     $near: {       $geometry: {          type: \"Point\" ,          coordinates: [ <longitude> , <latitude> ]       },       $maxDistance: <distance in meters>,       $minDistance: <distance in meters>     }   }}  \nIMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.  * Valid longitude values are between -180 and 180, both inclusive.  * Valid latitude values are between -90 and 90, both inclusive. When specifying a GeoJSON point, you can use the optional $minDistance and $maxDistance specifications to limit the $near results by distance in meters:  * $minDistance limits the results to those documents that are at least the specified distance from the center point.  * $maxDistance limits the results to those documents that are at most the specified distance from the center point. To specify a point using legacy coordinates, $near requires a 2d index and has the following syntax: {  $near: [ <x>, <y> ],  $maxDistance: <distance in radians>}  When specifying a legacy coordinate, you can use the optional $maxDistance specification to limit the $near results by distance in radians. $maxDistance limits the results to those documents that are at most the specified distance from the center point. \nBEHAVIOR  SPECIAL INDEXES RESTRICTION \nYou cannot combine the $near operator, which requires a special geospatial index, with a query operator or command that requires another special index. For example you cannot combine $near with the $text query. \nSHARDED COLLECTIONS \nStarting in MongoDB 4.0, $near queries are supported for sharded collections. In earlier MongoDB versions, $near queries are not supported for sharded collections; instead, for sharded clusters, you must use the $geoNear aggregation stage or the geoNear command (available in MongoDB 4.0 and earlier). \nSORT OPERATION \nThe $near operator sorts documents by distance.  * If you use the sort() method in your query, MongoDB performs a second sort operation, re-ordering the matching documents. When querying large collections, this can negatively affect query performance.  * If the order of the documents is not important to you, consider using the $geoWithin operator instead, as it returns unsorted results.  * $near is a Match Execution operator and is not permitted in aggregation pipelines. \nTIP \nSEE ALSO: 2d Indexes and Geospatial Near Queries \nEXAMPLES  QUERY ON GEOJSON DATA  IMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.  * Valid longitude values are between -180 and 180, both inclusive.  * Valid latitude values are between -90 and 90, both inclusive. Consider a collection places that has a 2dsphere index. The following example returns documents that are at least 1000 meters from and at most 5000 meters from the specified GeoJSON point, sorted from nearest to farthest: db.places.find(   {     location:       { $near :          {            $geometry: { type: \"Point\",  coordinates: [ -73.9667, 40.78 ] },            $minDistance: 1000,            $maxDistance: 5000          }       }   })  \nQUERY ON LEGACY COORDINATES  IMPORTANT \nIMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.  * Valid longitude values are between -180 and 180, both inclusive.\n\n Consider a collection legacy2d that has a 2d index. The following example returns documents that are at most 0.10 radians from the specified legacy coordinate pair, sorted from nearest to farthest: db.legacy2d.find(   { location : { $near : [ -73.9667, 40.78 ], $maxDistance: 0.10 } }) \n←  $geoWithin$nearSphere → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/in/": " Docs Home → MongoDB Manual \n$IN \nOn this page    \n * Examples\n   \n * Use the $in Operator with a Regular Expression $in \nThe $in operator selects the documents where the value of a field equals any value in the specified array. To specify an $in expression, use the following prototype: { field: { $in: [<value1>, <value2>, ... <valueN> ] } }  For comparison of different BSON type values, see the specified BSON comparison order. If the field holds an array, then the $in operator selects the documents whose field holds an array that contains at least one element that matches a value in the specified array (for example, <value1>, <value2>, and so on). The $in operator compares each parameter to each document in the collection, which can lead to performance issues. To improve performance:      * It is recommended that you limit the number of parameters passed to the $in operator to tens of values. Using hundreds of parameters or more can negatively impact query performance.  * Create an index on the field you want to query. \nNOTE This document describes the $in query operator. For the $in aggregation operator, see $in (aggregation). \nEXAMPLES \nCreate the inventory collection: db.inventory.insertMany( [   { \"item\": \"Pens\", \"quantity\": 350, \"tags\": [ \"school\", \"office\" ] },   { \"item\": \"Erasers\", \"quantity\": 15, \"tags\": [ \"school\", \"home\" ] },   { \"item\": \"Maps\", \"tags\": [ \"office\", \"storage\" ] },   { \"item\": \"Books\", \"quantity\": 5, \"tags\": [ \"school\", \"storage\", \"home\" ] }] )  \nUSE THE $IN OPERATOR TO MATCH VALUES \nConsider the following example: db.inventory.find( { quantity: { $in: [ 5, 15 ] } }, { _id: 0 } )  This query selects all documents in the inventory collection where the value of the quantity field is either 5 or 15. { item: 'Erasers', quantity: 15, tags: [ 'school', 'home' ] },{ item: 'Books', quantity: 5, tags: [ 'school', 'storage', 'home' ] }  Although you can write this query using the $or operator, use the $in operator rather than the $or operator when performing equality checks on the same field. \nUSE THE $IN OPERATOR TO MATCH VALUES IN AN ARRAY \nThe following updateMany() operation sets the exclude field to false when the tags array has at least one element that matches either \"home\" or \"school\". db.inventory.updateMany(   { tags: { $in: [ \"home\", \"school\" ] } },   { $set: { exclude: false } })  Example output: {   item: 'Pens',   quantity: 350,   tags: [ 'school', 'office' ],   exclude: false }, {   item: 'Erasers',   quantity: 15,   tags: [ 'school', 'home' ],   exclude: false }, {   item: 'Maps',   tags: [ 'office', 'storage' ] }, {   item: 'Books',   quantity: 5,   tags: [ 'school', 'storage', 'home' ],   exclude: false }  For additional examples in querying arrays, see:  * Query an Array  * Query an Array of Embedded Documents For additional examples in querying, see:  * Query Documents \nUSE THE $IN OPERATOR WITH A REGULAR EXPRESSION \nThe $in operator can specify matching values using regular expressions of the form /pattern/. You cannot use $regex operator expressions inside an $in. Consider the following example: db.inventory.find( { tags: { $in: [ /^be/, /^st/ ] } } )  This query selects all documents in the inventory collection where the tags field holds either a string that starts with be or st or an array with at least one element that starts with be or st. \nTIP \nSEE ALSO:  * find()  * updateMany()  * $or  * $set  * $elemMatch ←  $gte$lt → On this page  * Examples\n * Use the $in Operator with a Regular Expression Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/mod/": " Docs Home → MongoDB Manual \n$MOD \nOn this page    \n * Behavior\n   \n * Examples $mod \nSelect documents where the value of a field divided by a divisor has the specified remainder (i.e. perform a modulo operation to select documents). To specify a $mod expression, use the following syntax: { field: { $mod: [ divisor, remainder ] } }  \nBEHAVIOR \nThe $mod operator returns an error if the [ divisor, remainder ] array contains fewer or more than two elements. For examples, see Not Enough Elements Error and Too Many Elements Error respectively. Also, starting in MongoDB 5.1 (and 5.0.4 and 4.4.10), $mod returns an error if the divisor or remainder values evaluate to:      * NaN (not a number) or Infinity.  * A value that cannot be represented using a 64-bit integer. \nEXAMPLES  USE $MOD TO SELECT DOCUMENTS \nCreate an inventory collection: db.inventory.insertMany( [   { \"_id\" : 1, \"item\" : \"abc123\", \"qty\" : 0 },   { \"_id\" : 2, \"item\" : \"xyz123\", \"qty\" : 5 },   { \"_id\" : 3, \"item\" : \"ijk123\", \"qty\" : 12 }] )  Then, the following query selects those documents in the inventory collection where value of the qty field modulo 4 equals 0: db.inventory.find( { qty: { $mod: [ 4, 0 ] } } )  The query returns the following documents: { \"_id\" : 1, \"item\" : \"abc123\", \"qty\" : 0 }{ \"_id\" : 3, \"item\" : \"ijk123\", \"qty\" : 12 } \nNOT ENOUGH ELEMENTS ERROR \nThe $mod operator errors when passed an array with fewer than two elements. ARRAY WITH SINGLE ELEMENT \nThe following operation incorrectly passes the $mod operator an array that contains a single element: db.inventory.find( { qty: { $mod: [ 4 ] } } )  The statement results in the following error: MongoServerError: malformed mod, not enough elements EMPTY ARRAY \nThe following operation incorrectly passes the $mod operator an empty array: db.inventory.find( { qty: { $mod: [ ] } } )  The statement results in the following error: MongoServerError: malformed mod, not enough elements \nTOO MANY ELEMENTS ERROR \nThe $mod operator errors when passed an array with more than two elements. For example, the following operation attempts to use the $mod operator with an array that contains four elements: db.inventory.find( { qty: { $mod: [ 4, 1, 2, 3 ] } } )  The statement results in the following error: MongoServerError: malformed mod, too many elements \nFLOATING POINT ARGUMENTS \nThe $mod expression rounds decimal input towards zero. The following examples demonstrate this behavior: \nEXAMPLE Input query: db.inventory.find( { qty: { $mod: [ 4.0, 0 ] } } )  Results: { _id: 1, item: 'abc123', qty: 0 }{ _id: 3, item: 'ijk123', qty: 12 } \nEXAMPLE Input query: db.inventory.find( { qty: { $mod: [ 4.5, 0 ] } } )  Results: { _id: 1, item: 'abc123', qty: 0 }{ _id: 3, item: 'ijk123', qty: 12 } \nEXAMPLE Input query: db.inventory.find( { qty: { $mod: [ 4.99, 0 ] } } )  Results: { _id: 1, item: 'abc123', qty: 0 }{ _id: 3, item: 'ijk123', qty: 12 } Each query applies 4 to the $mod expression regardless of decimal points, resulting in the same result set. ←  $jsonSchema$regex → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/bitsAllClear/": " Docs Home → MongoDB Manual \n$BITSALLCLEAR \nOn this page    \n * Behavior\n   \n * Examples $bitsAllClear \n$bitsAllClear matches documents where all of the bit positions given by the query are clear (i.e. 0) in field. { <field>: { $bitsAllClear: <numeric bitmask> } }\n{ <field>: { $bitsAllClear: < BinData bitmask> } }\n{ <field>: { $bitsAllClear: [ <position1>, <position2>, ... ] } } The field value must be either numeric or a BinData instance. Otherwise, $bitsAllClear will not match the current document. Numeric BitmaskYou can provide a numeric bitmask to be matched against the operand field. It must be representable as a non-negative 32-bit signed integer. Otherwise, $bitsAllClear will return an error.BinData BitmaskYou can also use an arbitrarily large BinData instance as a bitmask.Position ListIf querying a list of bit positions, each <position> must be a non-negative integer. Bit positions start at 0 from the least significant bit. For example, the decimal number 254 would have the following bit positions:\nBit Value\n1\n1\n1\n1\n1\n1\n1\n0\nPosition\n7\n6\n5\n4\n3\n2\n1\n0 \nBEHAVIOR  INDEXES \nQueries cannot use indexes for the $bitsAllClear portion of a query, although the other portions of a query can use indexes, if applicable. \nFLOATING POINT VALUES \n$bitsAllClear will not match numerical values that cannot be represented as a signed 64-bit integer. This can be the case if a value is either too large or too small to fit in a signed 64-bit integer, or if it has a fractional component. \nSIGN EXTENSION \nNumbers are sign extended. For example, $bitsAllClear considers bit position 200 to be set for the negative number -5, but bit position 200 to be clear for the positive number +5. In contrast, BinData instances are zero-extended. For example, given the following document: db.collection.save({ x: BinData(0, \"ww==\"), binaryValueofA: \"11000011\" })  $bitsAllClear will consider all bits outside of x to be clear. \nEXAMPLES \nThe following examples will use a collection with the following documents: db.collection.save({ _id: 1, a: 54, binaryValueofA: \"00110110\" })db.collection.save({ _id: 2, a: 20, binaryValueofA: \"00010100\" })db.collection.save({ _id: 3, a: 20.0, binaryValueofA: \"00010100\" })db.collection.save({ _id: 4, a: BinData(0, \"Zg==\"), binaryValueofA: \"01100110\" })  \nBIT POSITION ARRAY \nThe following query uses the $bitsAllClear operator to test whether field a has bits clear at position 1 and position 5, where the least significant bit is position 0. db.collection.find( { a: { $bitsAllClear: [ 1, 5 ] } } )  The query matches the following documents: { \"_id\" : 2, \"a\" : 20, \"binaryValueofA\" : \"00010100\" }{ \"_id\" : 3, \"a\" : 20, \"binaryValueofA\" : \"00010100\" }  \nINTEGER BITMASK \nThe following query uses the $bitsAllClear operator to test whether field a has bits clear at positions 0, 1, and 5 (the binary representation of the bitmask 35 is 00100011). db.collection.find( { a: { $bitsAllClear: 35 } } )  The query matches the following documents: { \"_id\" : 2, \"a\" : 20, \"binaryValueofA\" : \"00010100\" }{ \"_id\" : 3, \"a\" : 20, \"binaryValueofA\" : \"00010100\" }  \nBINDATA BITMASK \nThe following query uses the $bitsAllClear operator: db.collection.find( { a: { $bitsAllClear: BinData(0, \"ID==\") } } )  The query:      * Specifies 0 as the first value for BinData, which indicates ID== is to be interpreted as binary. The base-64 value ID== in binary is 00100000, which has 1 in position 5.\n\n The query returns the following documents: { \"_id\" : 2, \"a\" : 20, \"binaryValueofA\" : \"00010100\" }{ \"_id\" : 3, \"a\" : 20, \"binaryValueofA\" : \"00010100\" } \n←  Bitwise Query Operators$bitsAllSet → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/polygon/": " Docs Home → MongoDB Manual \n$POLYGON \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$polygon \nSpecifies a polygon for a geospatial $geoWithin query on legacy coordinate pairs. The query returns pairs that are within the bounds of the polygon. The operator does not query for GeoJSON objects. To define the polygon, specify an array of coordinate points: {   <location field>: {      $geoWithin: {         $polygon: [ [ <x1> , <y1> ], [ <x2> , <y2> ], [ <x3> , <y3> ], ... ]      }   }}  The last point is always implicitly connected to the first. You can specify as many points, i.e. sides, as you like. \nIMPORTANT \nIF YOU USE LONGITUDE AND LATITUDE, SPECIFY LONGITUDE FIRST.  \nBEHAVIOR \nThe $polygon operator calculates distances using flat (planar) geometry. Applications can use $polygon without having a geospatial index. However, geospatial indexes support much faster queries than the unindexed equivalents. Only the 2d geospatial index supports the $polygon operator. \nEXAMPLE \nThe following query returns all documents that have coordinates that exist within the polygon defined by [ 0 , 0 ], [ 3 , 6 ], and [ 6 , 0 ]: db.places.find(  {     loc: {       $geoWithin: { $polygon: [ [ 0 , 0 ], [ 3 , 6 ], [ 6 , 0 ] ] }     }  }) \n←  $minDistanceArray Query Operators → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/minDistance/": " Docs Home → MongoDB Manual \n$MINDISTANCE \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$minDistance \nFilters the results of a geospatial $near or $nearSphere query to those documents that are at least the specified distance from the center point. If $near or $nearSphere query specifies the center point as a GeoJSON point, specify the distance as a non-negative number in meters. If $nearSphere query specifies the center point as legacy coordinate pair, specify the distance as a non-negative number in radians. $near can only use the 2dsphere index if the query specifies the center point as a GeoJSON point. \nEXAMPLES  USE WITH $NEAR  IMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.      * Valid longitude values are between -180 and 180, both inclusive.  * Valid latitude values are between -90 and 90, both inclusive. Consider a collection places that has a 2dsphere index. The following example returns documents that are at least 1000 meters from and at most 5000 meters from the specified GeoJSON point, sorted from nearest to farthest: db.places.find(   {     location:       { $near :          {            $geometry: { type: \"Point\",  coordinates: [ -73.9667, 40.78 ] },            $minDistance: 1000,            $maxDistance: 5000          }       }   })  \nUSE WITH $NEARSPHERE \nConsider a collection places that contains documents with a location field and has a 2dsphere index. Then, the following example returns whose location is at least 1000 meters from and at most 5000 meters from the specified point, ordered from nearest to farthest: db.places.find(   {     location: {        $nearSphere: {           $geometry: {              type : \"Point\",              coordinates : [ -73.9667, 40.78 ]           },           $minDistance: 1000,           $maxDistance: 5000        }     }   })  For an example that specifies the center point as legacy coordinate pair, see $nearSphere ←  $maxDistance$polygon → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/type/": " Docs Home → MongoDB Manual \n$TYPE \nOn this page    \n * Definition\n   \n * Behavior\n * Examples\n * Querying by Array Type\n * Additional Information \nDEFINITION \n$type \n$type selects documents where the value of the field is an instance of the specified BSON type(s). Querying by data type is useful when dealing with highly unstructured data where data types are not predictable. A $type expression for a single BSON type has the following syntax: { field: { $type: <BSON type> } }  You can specify either the number or alias for the BSON type The $type expression can also accept an array of BSON types and has the following syntax: { field: { $type: [ <BSON type1> , <BSON type2>, ... ] } }  The above query will match documents where the field value is any of the listed types. The types specified in the array can be either numeric or string aliases. See Querying by Multiple Data Type for an example. Available Types describes the BSON types and their corresponding numeric and string aliases. \nTIP \nSEE ALSO:      * $isNumber - checks if the argument is a number. New in MongoDB 4.4  * $type (Aggregation) - returns the BSON type of the argument. \nBEHAVIOR \n$type returns documents where the BSON type of the field matches the BSON type passed to $type. \nARRAYS \nFor documents where field is an array, $type returns documents in which at least one array element matches a type passed to $type. Queries for $type: \"array\" return documents where the field itself is an array. \nAVAILABLE TYPES \nThe $type operator accepts string aliases for the BSON types in addition to the numbers corresponding to the BSON types. [1] Type\nNumber\nAlias\nNotes\nDouble\n1\n\"double\" String\n2\n\"string\" Object\n3\n\"object\" Array\n4\n\"array\" Binary data\n5\n\"binData\" Undefined\n6\n\"undefined\"\nDeprecated.\nObjectId\n7\n\"objectId\" Boolean\n8\n\"bool\" Date\n9\n\"date\" Null\n10\n\"null\" Regular Expression\n11\n\"regex\" DBPointer\n12\n\"dbPointer\"\nDeprecated.\nJavaScript\n13\n\"javascript\" Symbol\n14\n\"symbol\"\nDeprecated.\nJavaScript code with scope\n15\n\"javascriptWithScope\"\nDeprecated in MongoDB 4.4.\n32-bit integer\n16\n\"int\" Timestamp\n17\n\"timestamp\" 64-bit integer\n18\n\"long\" Decimal128\n19\n\"decimal\" Min key\n-1\n\"minKey\" Max key\n127\n\"maxKey\" \n$type supports the number alias, which will match against the following BSON types:  * double  * 32-bit integer  * 64-bit integer  * decimal For examples, see Examples. [1] Starting in MongoDB 4.2, users can no longer use the query filter $type: 0 as a synonym for $exists:false. To query for null or missing fields, see Query for Null or Missing Fields. \nTIP \nSEE ALSO: $isNumber New in MongoDB 4.4 \nMINKEY AND MAXKEY \nMinKey and MaxKey are used in comparison operations and exist primarily for internal use. For all possible BSON element values, MinKey will always be the smallest value while MaxKey will always be the greatest value. Querying for minKey or maxKey with $type will only return fields that match the special MinKey or MaxKey values. Suppose that the data collection has two documents with MinKey and MaxKey: { \"_id\" : 1, x : { \"$minKey\" : 1 } }{ \"_id\" : 2, y : { \"$maxKey\" : 1 } }  The following query will return the document with _id: 1: db.data.find( { x: { $type: \"minKey\" } } )  The following query will return the document with _id: 2: db.data.find( { y: { $type: \"maxKey\" } } )  \nEXAMPLES  QUERYING BY DATA TYPE \nThe addressBook contains addresses and zipcodes, where zipCode has string, int, double, and long values:\n\n  The following queries return all documents where zipCode is the BSON type string or is an array containing an element of the specified type: db.addressBook.find( { \"zipCode\" : { $type : 2 } } );db.addressBook.find( { \"zipCode\" : { $type : \"string\" } } );  These queries return: { \"_id\" : 1, \"address\" : \"2030 Martian Way\", \"zipCode\" : \"90698345\" }{ \"_id\" : 5, \"address\" : \"104 Venus Drive\", \"zipCode\" : [ \"834847278\", \"1893289032\" ] }  The following queries return all documents where zipCode is the BSON type double or is an array containing an element of the specified type: db.addressBook.find( { \"zipCode\" : { $type : 1 } } )db.addressBook.find( { \"zipCode\" : { $type : \"double\" } } )  These queries return: { \"_id\" : 2, \"address\" : \"156 Lunar Place\", \"zipCode\" : 43339374 }  The following query uses the number alias to return documents where zipCode is the BSON type double, int, or long or is an array containing an element of the specified types: db.addressBook.find( { \"zipCode\" : { $type : \"number\" } } )  These queries return: { \"_id\" : 2, \"address\" : \"156 Lunar Place\", \"zipCode\" : 43339374 }{ \"_id\" : 3, \"address\" : \"2324 Pluto Place\", \"zipCode\" : NumberLong(3921412) }{ \"_id\" : 4, \"address\" : \"55 Saturn Ring\", \"zipCode\" : 88602117 }  \nQUERYING BY MULTIPLE DATA TYPE \nThe grades collection contains names and averages, where classAverage has string, int, and double values: db.grades.insertMany(   [      { \"_id\" : 1, name : \"Alice King\" , classAverage : 87.333333333333333 },      { \"_id\" : 2, name : \"Bob Jenkins\", classAverage : \"83.52\" },      { \"_id\" : 3, name : \"Cathy Hart\", classAverage: \"94.06\" },      { \"_id\" : 4, name : \"Drew Williams\" , classAverage : NumberInt(\"93\") }   ])  The following queries return all documents where classAverage is the BSON type string or double or is an array containing an element of the specified types. The first query uses numeric aliases while the second query uses string aliases. db.grades.find( { \"classAverage\" : { $type : [ 2 , 1 ] } } );db.grades.find( { \"classAverage\" : { $type : [ \"string\" , \"double\" ] } } );  These queries return the following documents: { \"_id\" : 1, \"name\" : \"Alice King\", \"classAverage\" : 87.33333333333333 }{ \"_id\" : 2, \"name\" : \"Bob Jenkins\", \"classAverage\" : \"83.52\" }{ \"_id\" : 3, \"name\" : \"Cathy Hart\", \"classAverage\" : \"94.06\" }  \nQUERYING BY MINKEY AND MAXKEY \nThe restaurants collection uses minKey for any grade that is a failing grade: {   \"_id\": 1,   \"address\": {      \"building\": \"230\",      \"coord\": [ -73.996089, 40.675018 ],      \"street\": \"Huntington St\",      \"zipcode\": \"11231\"   },   \"borough\": \"Brooklyn\",   \"cuisine\": \"Bakery\",   \"grades\": [      { \"date\": new Date(1393804800000), \"grade\": \"C\", \"score\": 15 },      { \"date\": new Date(1378857600000), \"grade\": \"C\", \"score\": 16 },      { \"date\": new Date(1358985600000), \"grade\": MinKey(), \"score\": 30 },      { \"date\": new Date(1322006400000), \"grade\": \"C\", \"score\": 15 }   ],   \"name\": \"Dirty Dan's Donuts\",   \"restaurant_id\": \"30075445\"} \n\n {   \"_id\": 2,   \"address\": {      \"building\": \"1166\",      \"coord\": [ -73.955184, 40.738589 ],      \"street\": \"Manhattan Ave\",      \"zipcode\": \"11222\"   },   \"borough\": \"Brooklyn\",   \"cuisine\": \"Bakery\",   \"grades\": [      { \"date\": new Date(1393804800000), \"grade\": MaxKey(), \"score\": 2 },      { \"date\": new Date(1378857600000), \"grade\": \"B\", \"score\": 6 },      { \"date\": new Date(1358985600000), \"grade\": MaxKey(), \"score\": 3 },      { \"date\": new Date(1322006400000), \"grade\": \"B\", \"score\": 5 }   ],   \"name\": \"Dainty Daisey's Donuts\",   \"restaurant_id\": \"30075449\"}  The following query returns any restaurant whose grades.grade field contains minKey or is an array containing an element of the specified type: db.restaurants.find(   { \"grades.grade\" : { $type : \"minKey\" } })  This returns {   \"_id\" : 1,   \"address\" : {      \"building\" : \"230\",      \"coord\" : [ -73.996089, 40.675018 ],      \"street\" : \"Huntington St\",      \"zipcode\" : \"11231\"   },   \"borough\" : \"Brooklyn\",   \"cuisine\" : \"Bakery\",   \"grades\" : [      { \"date\" : ISODate(\"2014-03-03T00:00:00Z\"), \"grade\" : \"C\", \"score\" : 15 },      { \"date\" : ISODate(\"2013-09-11T00:00:00Z\"), \"grade\" : \"C\", \"score\" : 16 },      { \"date\" : ISODate(\"2013-01-24T00:00:00Z\"), \"grade\" : { \"$minKey\" : 1 }, \"score\" : 30 },      { \"date\" : ISODate(\"2011-11-23T00:00:00Z\"), \"grade\" : \"C\", \"score\" : 15 }   ],   \"name\" : \"Dirty Dan's Donuts\",   \"restaurant_id\" : \"30075445\"}  The following query returns any restaurant whose grades.grade field contains maxKey or is an array containing an element of the specified type: db.restaurants.find(   { \"grades.grade\" : { $type : \"maxKey\" } })  This returns {   \"_id\" : 2,   \"address\" : {      \"building\" : \"1166\",      \"coord\" : [ -73.955184, 40.738589 ],      \"street\" : \"Manhattan Ave\",      \"zipcode\" : \"11222\"   },   \"borough\" : \"Brooklyn\",   \"cuisine\" : \"Bakery\",   \"grades\" : [      { \"date\" : ISODate(\"2014-03-03T00:00:00Z\"), \"grade\" : { \"$maxKey\" : 1 }, \"score\" : 2 },      { \"date\" : ISODate(\"2013-09-11T00:00:00Z\"), \"grade\" : \"B\", \"score\" : 6 },      { \"date\" : ISODate(\"2013-01-24T00:00:00Z\"), \"grade\" : { \"$maxKey\" : 1 }, \"score\" : 3 },      { \"date\" : ISODate(\"2011-11-23T00:00:00Z\"), \"grade\" : \"B\", \"score\" : 5 }   ],   \"name\" : \"Dainty Daisey's Donuts\",   \"restaurant_id\" : \"30075449\"}  \nQUERYING BY ARRAY TYPE\n\n {   \"_id\": 1,   \"readings\": [      25,      23,      [ \"Warn: High Temp!\", 55 ],      [ \"ERROR: SYSTEM SHUTDOWN!\", 66 ]   ]},{   \"_id\": 2,   \"readings\": [      25,      25,      24,      23   ]},{   \"_id\": 3,   \"readings\": [      22,      24,      []   ]},{   \"_id\": 4,   \"readings\": []},{   \"_id\": 5,   \"readings\": 24}  The following query returns any document in which the readings field is an array, empty or non-empty. db.SensorReading.find( { \"readings\" : { $type: \"array\" } } )  The above query returns the following documents: {   \"_id\": 1,   \"readings\": [      25,      23,      [ \"Warn: High Temp!\", 55 ],      [ \"ERROR: SYSTEM SHUTDOWN!\", 66 ]   ]},{   \"_id\": 2,   \"readings\": [      25,      25,      24,      23   ]},{   \"_id\": 3,   \"readings\": [      22,      24,      []   ]},{   \"_id\": 4,   \"readings\": []}  In the documents with _id : 1, _id : 2, _id : 3, and _id : 4, the readings field is an array. \nADDITIONAL INFORMATION \n * Query for Null or Missing Fields  * db.collection.find()  * BSON Types. ←  $existsEvaluation Query Operators → On this page  * Definition\n * Behavior\n * Examples\n * Querying by Array Type\n * Additional Information Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/geoIntersects/": " Docs Home → MongoDB Manual \n$GEOINTERSECTS \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$geoIntersects \nSelects documents whose geospatial data intersects with a specified GeoJSON object; i.e. where the intersection of the data and the specified object is non-empty. The $geoIntersects operator uses the $geometry operator to specify the GeoJSON object. To specify a GeoJSON polygons or multipolygons using the default coordinate reference system (CRS), use the following syntax: {  <location field>: {     $geoIntersects: {        $geometry: {           type: \"<GeoJSON object type>\" ,           coordinates: [ <coordinates> ]        }     }  }}  For $geoIntersects queries that specify GeoJSON geometries with areas greater than a single hemisphere, the use of the default CRS results in queries for the complementary geometries. To specify a single-ringed GeoJSON polygon with a custom MongoDB CRS, use the following prototype that specifies the custom MongoDB CRS in the $geometry expression: {  <location field>: {     $geoIntersects: {        $geometry: {           type: \"Polygon\" ,           coordinates: [ <coordinates> ],           crs: {              type: \"name\",              properties: { name: \"urn:x-mongodb:crs:strictwinding:EPSG:4326\" }           }        }     }  }}  The custom MongoDB CRS uses a counter-clockwise winding order and allows $geoIntersects to support queries with a single-ringed GeoJSON polygon whose area is greater than or equal to a single hemisphere. If the specified polygon is smaller than a single hemisphere, the behavior of $geoIntersects with the MongoDB CRS is the same as with the default CRS. See also \"Big\" Polygons. \nIMPORTANT \nIMPORTANT If specifying latitude and longitude coordinates, list the longitude first, and then latitude.      * Valid longitude values are between -180 and 180, both inclusive.  * Valid latitude values are between -90 and 90, both inclusive. \nBEHAVIOR  GEOSPATIAL INDEXES \n$geoIntersects uses spherical geometry. $geoIntersects does not require a geospatial index. However, a geospatial index will improve query performance. Only the 2dsphere geospatial index supports $geoIntersects. \nDEGENERATE GEOMETRY \n$geoIntersects does not guarantee that it will consider a polygon to intersect with its own edges; its own vertices; or another polygon sharing vertices or edges but no interior space. \n\"BIG\" POLYGONS \nFor $geoIntersects, if you specify a single-ringed polygon that has an area greater than a single hemisphere, include the custom MongoDB coordinate reference system in the $geometry expression; otherwise, $geoIntersects queries for the complementary geometry. For all other GeoJSON polygons with areas greater than a hemisphere, $geoIntersects queries for the complementary geometry. \nEXAMPLES  INTERSECTS A POLYGON \nThe following example uses $geoIntersects to select all loc data that intersect with the Polygon defined by the coordinates array. The area of the polygon is less than the area of a single hemisphere:\n\n  For single-ringed polygons with areas greater than a single hemisphere, see Intersects a \"Big\" Polygon. \nINTERSECTS A \"BIG\" POLYGON \nTo query with a single-ringed GeoJSON polygon whose area is greater than a single hemisphere, the $geometry expression must specify the custom MongoDB coordinate reference system. For example: db.places.find(   {     loc: {       $geoIntersects: {          $geometry: {             type : \"Polygon\",             coordinates: [               [                 [ -100, 60 ], [ -100, 0 ], [ -100, -60 ], [ 100, -60 ], [ 100, 60 ], [ -100, 60 ]               ]             ],             crs: {                type: \"name\",                properties: { name: \"urn:x-mongodb:crs:strictwinding:EPSG:4326\" }             }          }       }     }   }) \n←  Geospatial Query Operators$geoWithin → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/rand/": " Docs Home → MongoDB Manual \n$RAND \nOn this page    \n * Definition\n   \n * Examples \nDEFINITION \n$rand \n$rand returns a random float between 0 and 1. $rand has the following syntax: { $rand: {} }  \nEXAMPLES  GENERATE RANDOM DATA POINTS \nThis example models charitable donations. The collection starts with a list of donors. db.donors.insertMany(   [     { donorId: 1000, amount: 0, frequency: 1 },     { donorId: 1001, amount: 0, frequency: 2 },     { donorId: 1002, amount: 0, frequency: 1 },     { donorId: 1003, amount: 0, frequency: 2 },     { donorId: 1004, amount: 0, frequency: 1 }   ])  Then we construct an operation to update each document with a random donation amount: db.donors.updateMany(   {},   [      { $set:         { amount:            { $floor:               { $multiply: [ { $rand: {} }, 100 ] }            }         }      }    ])  The empty update filter matches every document in the collection. For each document we generate a value between 0 and 1 using $rand then scale the value with $multiply. The $floor operator removes the decimal portion so the updated amount is an integer value. After updating the collection, the documents look like this: { \"donorId\" : 1000, \"amount\" : 2, \"frequency\" : 1 }{ \"donorId\" : 1001, \"amount\" : 58, \"frequency\" : 2 }{ \"donorId\" : 1002, \"amount\" : 27, \"frequency\" : 1 }{ \"donorId\" : 1003, \"amount\" : 26, \"frequency\" : 2 }{ \"donorId\" : 1004, \"amount\" : 42, \"frequency\" : 1 } \nSELECT RANDOM ITEMS FROM A COLLECTION \nThe $rand operator can be used to select random documents from a collection. Given a collection of voter records: db.voters.insertMany(   [     { name: \"Archibald\", voterId: 4321, district: 3, registered: true },     { name: \"Beckham\", voterId: 4331, district: 3, registered: true },     { name: \"Carolin\", voterId: 5321, district: 4, registered: true },     { name: \"Debarge\", voterId: 4343, district: 3, registered: false },     { name: \"Eckhard\", voterId: 4161, district: 3, registered: false },     { name: \"Faberge\", voterId: 4300, district: 1, registered: true },     { name: \"Grimwald\", voterId: 4111, district: 3, registered: true },     { name: \"Humphrey\", voterId: 2021, district: 3, registered: true },     { name: \"Idelfon\", voterId: 1021, district: 4, registered: true },     { name: \"Justo\", voterId: 9891, district: 3, registered: false }   ])  Imagine you want to select about half of the voters in District 3 to do some polling. db.voters.find(   {  district: 3,      $expr: { $lt: [0.5, {$rand: {} } ] }   },   { _id: 0, name: 1, registered: 1 })  The intial match on the district field selects documents where the voter is from district 3. The $expr operator uses $rand to further refine the find operation. For each document, $rand generates a value between 0 and 1. The threshold of 0.5 means the less than ($lt) comparison will be true for about half the documents in the set. There are 7 voters in District 3, running the code selects about half of them.\n\n \nTIP \nSEE ALSO: $rand (aggregation) ←  $comment$natural → On this page  * Definition\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/all/": " Docs Home → MongoDB Manual \n$ALL \nOn this page    \n * Behavior\n   \n * Examples\n * Additional Examples $all \nThe $all operator selects the documents where the value of a field is an array that contains all the specified elements. To specify an $all expression, use the following prototype: { <field>: { $all: [ <value1> , <value2> ... ] } }  \nBEHAVIOR  EQUIVALENT TO $AND OPERATION \nThe $all is equivalent to an $and operation of the specified values; i.e. the following statement: { tags: { $all: [ \"ssl\" , \"security\" ] } }  is equivalent to: { $and: [ { tags: \"ssl\" }, { tags: \"security\" } ] }  \nNESTED ARRAY \nWhen passed an array of a nested array (e.g. [ [ \"A\" ] ] ), $all matches documents where the field contains the nested array as an element (e.g. field: [ [ \"A\" ], ... ]), or the field equals the nested array (e.g. field: [ \"A\" ]). For example, consider the following query [1]: db.articles.find( { tags: { $all: [ [ \"ssl\", \"security\" ] ] } } )  The query is equivalent to: db.articles.find( { $and: [ { tags: [ \"ssl\", \"security\" ] } ] } )  which is equivalent to: db.articles.find( { tags: [ \"ssl\", \"security\" ] } )  As such, the $all expression matches documents where the tags field is an array that contains the nested array [ \"ssl\", \"security\" ] or is an array that equals the nested array: tags: [ [ \"ssl\", \"security\" ], ... ]tags: [ \"ssl\", \"security\" ]  [1] The $all expression with a single element is for illustrative purposes since the $all expression is unnecessary if matching only a single element. Instead, when matching a single element, a \"contains\" expression (i.e. arrayField: element ) is more suitable. \nEXAMPLES \nThe following examples use the inventory collection that contains the documents: {   _id: ObjectId(\"5234cc89687ea597eabee675\"),   code: \"xyz\",   tags: [ \"school\", \"book\", \"bag\", \"headphone\", \"appliance\" ],   qty: [          { size: \"S\", num: 10, color: \"blue\" },          { size: \"M\", num: 45, color: \"blue\" },          { size: \"L\", num: 100, color: \"green\" }        ]}\n{   _id: ObjectId(\"5234cc8a687ea597eabee676\"),   code: \"abc\",   tags: [ \"appliance\", \"school\", \"book\" ],   qty: [          { size: \"6\", num: 100, color: \"green\" },          { size: \"6\", num: 50, color: \"blue\" },          { size: \"8\", num: 100, color: \"brown\" }        ]}\n{   _id: ObjectId(\"5234ccb7687ea597eabee677\"),   code: \"efg\",   tags: [ \"school\", \"book\" ],   qty: [          { size: \"S\", num: 10, color: \"blue\" },          { size: \"M\", num: 100, color: \"blue\" },          { size: \"L\", num: 100, color: \"green\" }        ]}\n{   _id: ObjectId(\"52350353b2eff1353b349de9\"),   code: \"ijk\",   tags: [ \"electronics\", \"school\" ],   qty: [          { size: \"M\", num: 100, color: \"green\" }        ]}  \nUSE $ALL TO MATCH VALUES\n\n db.inventory.find( { tags: { $all: [ \"appliance\", \"school\", \"book\" ] } } )  The above query returns the following documents: {   _id: ObjectId(\"5234cc89687ea597eabee675\"),   code: \"xyz\",   tags: [ \"school\", \"book\", \"bag\", \"headphone\", \"appliance\" ],   qty: [          { size: \"S\", num: 10, color: \"blue\" },          { size: \"M\", num: 45, color: \"blue\" },          { size: \"L\", num: 100, color: \"green\" }        ]}\n{   _id: ObjectId(\"5234cc8a687ea597eabee676\"),   code: \"abc\",   tags: [ \"appliance\", \"school\", \"book\" ],   qty: [          { size: \"6\", num: 100, color: \"green\" },          { size: \"6\", num: 50, color: \"blue\" },          { size: \"8\", num: 100, color: \"brown\" }        ]}  \nUSE $ALL WITH $ELEMMATCH \nIf the field contains an array of documents, you can use the $all with the $elemMatch operator. The following operation queries the inventory collection for documents where the value of the qty field is an array whose elements match the $elemMatch criteria: db.inventory.find( {                     qty: { $all: [                                    { \"$elemMatch\" : { size: \"M\", num: { $gt: 50} } },                                    { \"$elemMatch\" : { num : 100, color: \"green\" } }                                  ] }                   } )  The query returns the following documents: {   \"_id\" : ObjectId(\"5234ccb7687ea597eabee677\"),   \"code\" : \"efg\",   \"tags\" : [ \"school\", \"book\"],   \"qty\" : [             { \"size\" : \"S\", \"num\" : 10, \"color\" : \"blue\" },             { \"size\" : \"M\", \"num\" : 100, \"color\" : \"blue\" },             { \"size\" : \"L\", \"num\" : 100, \"color\" : \"green\" }           ]}\n{   \"_id\" : ObjectId(\"52350353b2eff1353b349de9\"),   \"code\" : \"ijk\",   \"tags\" : [ \"electronics\", \"school\" ],   \"qty\" : [             { \"size\" : \"M\", \"num\" : 100, \"color\" : \"green\" }           ]}  The $all operator exists to support queries on arrays. But you may use the $all operator to select against a non-array field, as in the following example: db.inventory.find( { \"qty.num\": { $all: [ 50 ] } } )  However, use the following form to express the same query: db.inventory.find( { \"qty.num\" : 50 } )  Both queries will select all documents in the inventory collection where the value of the num field equals 50. \nNOTE In most cases, MongoDB does not treat arrays as sets. This operator provides a notable exception to this approach. \nADDITIONAL EXAMPLES\n\n      * Query an Array  * Query an Array of Embedded Documents For additional examples in querying, see:  * Query Documents \nTIP \nSEE ALSO: db.collection.find() ←  Array Query Operators$elemMatch (query) → On this page  * Behavior\n * Examples\n * Additional Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/eq/": " Docs Home → MongoDB Manual \n$EQ \nOn this page    \n * Behavior\n   \n * Examples $eq \nSpecifies equality condition. The $eq operator matches documents where the value of a field equals the specified value. { <field>: { $eq: <value> } }  Specifying the $eq operator is equivalent to using the form { field: <value> } except when the <value> is a regular expression. See below for examples. \nBEHAVIOR  COMPARISON ORDER \nFor comparison of different BSON type values, see the specified BSON comparison order. \nMATCH A DOCUMENT VALUE \nIf the specified <value> is a document, the order of the fields in the document matters. \nMATCH AN ARRAY VALUE \nIf the specified <value> is an array, MongoDB matches documents where the <field> matches the array exactly or the <field> contains an element that matches the array exactly. The order of the elements matters. For an example, see Equals an Array Value. \nMATCH A REGULAR EXPRESSION \nThe expression { field: <value> } implicitly specifies a match on <value>. MongoDB translates the implicit match to a more explicit form. When the <value> is fixed, like a particular string, the expression is equivalent to using the $eq operator { field: { $eq: <value> } }. If <value> is a regular expression, the statement is expanded using the $regex operator { field: { $regex: <value> } }. For examples illustrating this behaviour, see Regex Match Behaviour. \nSECURITY IMPLICATIONS \nAlways use the explicit form { field: { $eq: <value> } } with user-supplied input to avoid problems with maliciously formed queries. \nEXAMPLES \nThe following examples query against the inventory collection with the following documents: { _id: 1, item: { name: \"ab\", code: \"123\" }, qty: 15, tags: [ \"A\", \"B\", \"C\" ] }{ _id: 2, item: { name: \"cd\", code: \"123\" }, qty: 20, tags: [ \"B\" ] }{ _id: 3, item: { name: \"ij\", code: \"456\" }, qty: 25, tags: [ \"A\", \"B\" ] }{ _id: 4, item: { name: \"xy\", code: \"456\" }, qty: 30, tags: [ \"B\", \"A\" ] }{ _id: 5, item: { name: \"mn\", code: \"000\" }, qty: 20, tags: [ [ \"A\", \"B\" ], \"C\" ] }  \nEQUALS A SPECIFIED VALUE \nThe following example queries the inventory collection to select all documents where the value of the qty field equals 20: db.inventory.find( { qty: { $eq: 20 } } )  The query is equivalent to: db.inventory.find( { qty: 20 } )  Both queries match the following documents: { _id: 2, item: { name: \"cd\", code: \"123\" }, qty: 20, tags: [ \"B\" ] }{ _id: 5, item: { name: \"mn\", code: \"000\" }, qty: 20, tags: [ [ \"A\", \"B\" ], \"C\" ] }  \nFIELD IN EMBEDDED DOCUMENT EQUALS A VALUE \nThe following example queries the inventory collection to select all documents where the value of the name field in the item document equals \"ab\". To specify a condition on a field in an embedded document, use the dot notation. db.inventory.find( { \"item.name\": { $eq: \"ab\" } } )  The query is equivalent to: db.inventory.find( { \"item.name\": \"ab\" } )  Both queries match the following document: { _id: 1, item: { name: \"ab\", code: \"123\" }, qty: 15, tags: [ \"A\", \"B\", \"C\" ] }  \nTIP \nSEE ALSO: Query Embedded Documents \nARRAY ELEMENT EQUALS A VALUE \nThe following example queries the inventory collection to select all documents where the tags array contains an element with the value \"B\" [1]: db.inventory.find( { tags: { $eq: \"B\" } } )  The query is equivalent to: db.inventory.find( { tags: \"B\" } )  Both queries match the following documents:\n\n  \nTIP \nSEE ALSO:      * $elemMatch  * Query Arrays [1] The query will also match documents where the value of the tags field is the string \"B\". \nEQUALS AN ARRAY VALUE \nThe following example queries the inventory collection to select all documents where the tags array equals exactly the specified array or the tags array contains an element that equals the array [ \"A\", \"B\" ]. db.inventory.find( { tags: { $eq: [ \"A\", \"B\" ] } } )  The query is equivalent to: db.inventory.find( { tags: [ \"A\", \"B\" ] } )  Both queries match the following documents: { _id: 3, item: { name: \"ij\", code: \"456\" }, qty: 25, tags: [ \"A\", \"B\" ] }{ _id: 5, item: { name: \"mn\", code: \"000\" }, qty: 20, tags: [ [ \"A\", \"B\" ], \"C\" ] }  \nREGEX MATCH BEHAVIOUR \nThe following examples demonstrate the differences in behavior between implicit and explict regular expression matching. Consider a collection with these documents: { _id: 001, company: \"MongoDB\" }{ _id: 002, company: \"MongoDB2\" } \n$eq match on a string A string expands to return the same values whether an implicit match or an explicit use of $eq. Both of these queries: db.collection.find( { company: \"MongoDB\" }, {_id: 0 })db.collection.find( { company: { $eq: \"MongoDB\" } }, {_id: 0 } )  return the following result: { \"company\" : \"MongoDB\" } $eq match on a regular expression An explicit query using $eq and a regular expression will only match an object which is also a regular expresssion. The example query won't return anything since values in the company field are strings. db.collection.find( { company: { $eq: /MongoDB/ } }, {_id: 0 } ) \nRegular expression matches A query with an implicit match against a regular expression is equivalent to a making a query with the $regex operator. Both of these queries: db.collection.find( { company: /MongoDB/ }, {_id: 0 })db.collection.find( { company: { $regex: /MongoDB/ } }, {_id: 0 } )  return the same results: { \"company\" : \"MongoDB\" }{ \"company\" : \"MongoDB2\" } ←  Comparison Query Operators$gt → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/or/": " Docs Home → MongoDB Manual \n$OR \nOn this page    \n * Behaviors $or \nThe $or operator performs a logical OR operation on an array of one or more <expressions> and selects the documents that satisfy at least one of the <expressions>. The $or has the following syntax: { $or: [ { <expression1> }, { <expression2> }, ... , { <expressionN> } ] }  Consider the following example: db.inventory.find( { $or: [ { quantity: { $lt: 20 } }, { price: 10 } ] } )  This query will select all documents in the inventory collection where either the quantity field value is less than 20 or the price field value equals 10. \nBEHAVIORS  $OR CLAUSES AND INDEXES \nWhen evaluating the clauses in the $or expression, MongoDB either performs a collection scan or, if all the clauses are supported by indexes, MongoDB performs index scans. That is, for MongoDB to use indexes to evaluate an $or expression, all the clauses in the $or expression must be supported by indexes. Otherwise, MongoDB will perform a collection scan. When using indexes with $or queries, each clause of an $or can use its own index. Consider the following query: db.inventory.find( { $or: [ { quantity: { $lt: 20 } }, { price: 10 } ] } )  To support this query, rather than a compound index, you would create one index on quantity and another index on price: db.inventory.createIndex( { quantity: 1 } )db.inventory.createIndex( { price: 1 } )  MongoDB can use all but the geoHaystack index to support $or clauses. \n$OR AND TEXT QUERIES \nIf $or includes a $text query, all clauses in the $or array must be supported by an index. This is because a $text query must use an index, and $or can only use indexes if all its clauses are supported by indexes. If the $text query cannot use an index, the query will return an error. \n$OR AND GEOSPATIAL QUERIES \n$or supports geospatial clauses with the following exception for the near clause (near clause includes $nearSphere and $near). $or cannot contain a near clause with any other clause. \n$OR AND SORT OPERATIONS \nWhen executing $or queries with a sort(), MongoDB can now use indexes that support the $or clauses. Previous versions did not use the indexes. \n$OR AND PARTIAL INDEXES \nYou can create partial indexes with $or. Use the partialFilterExpression of the db.collection.createIndex() method to create a partial index. \n$OR VERSUS $IN \nWhen using $or with <expressions> that are equality checks for the value of the same field, use the $in operator instead of the $or operator. For example, to select all documents in the inventory collection where the quantity field value equals either 20 or 50, use the $in operator: db.inventory.find ( { quantity: { $in: [20, 50] } } )  \nNESTED $OR CLAUSES \nYou may nest $or operations. \nTIP \nSEE ALSO:      * $and  * find()  * sort()  * $in \nERROR HANDLING \nTo allow the query engine to optimize queries, $or handles errors as follows:  * If any expression supplied to $or would cause an error when evaluated alone, the $or containing the expression may cause an error but an error is not guaranteed.  * An expression supplied after the first expression supplied to $or may cause an error even if the first expression evaluates to true. For example, the following query always produces an error if $x is 0: db.example.find( {   $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] }} )  The following query, which contains multiple expressions supplied to $or, may produce an error if there is any document where $x is 0: db.example.find( {   $or: [      { x: { $eq: 0 } },      { $expr: { $eq: [ { $divide: [ 1, \"$x\" ] }, 3 ] } }   ]} ) \n←  $norElement Query Operators → On this page  * Behaviors Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/box/": " Docs Home → MongoDB Manual \n$BOX \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$box \nSpecifies a rectangle for a geospatial $geoWithin query to return documents that are within the bounds of the rectangle, according to their point-based location data. When used with the $box operator, $geoWithin returns documents based on grid coordinates and does not query for GeoJSON shapes. To use the $box operator, you must specify the bottom left and top right corners of the rectangle in an array object: {  <location field>: {     $geoWithin: {        $box: [          [ <bottom left coordinates> ],          [ <upper right coordinates> ]        ]     }  }}  \nIMPORTANT \nIF YOU USE LONGITUDE AND LATITUDE, SPECIFY LONGITUDE FIRST.  \nBEHAVIOR \nThe query calculates distances using flat (planar) geometry. Applications can use $box without having a geospatial index. However, geospatial indexes support much faster queries than the unindexed equivalents. Only the 2d geospatial index supports $box. \nEXAMPLE \nThe following example query returns all documents that are within the box having points at: [ 0 , 0 ], [ 0 , 100 ], [ 100 , 0 ], and [ 100 , 100 ]. db.places.find( {   loc: { $geoWithin: { $box:  [ [ 0, 0 ], [ 100, 100 ] ] } }} ) \n←  $nearSphere$center → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/centerSphere/": " Docs Home → MongoDB Manual \n$CENTERSPHERE \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$centerSphere \nDefines a circle for a geospatial query that uses spherical geometry. The query returns documents that are within the bounds of the circle. You can use the $centerSphere operator on both GeoJSON objects and legacy coordinate pairs. To use $centerSphere, specify an array that contains:      * The grid coordinates of the circle's center point, and  * The circle's radius measured in radians. To calculate radians, see Calculate Distance Using Spherical Geometry. {   <location field>: {      $geoWithin: { $centerSphere: [ [ <x>, <y> ], <radius> ] }   }}  \nIMPORTANT \nIF YOU USE LONGITUDE AND LATITUDE, SPECIFY LONGITUDE FIRST.  \nBEHAVIOR \nApplications can use $centerSphere without having a geospatial index. However, geospatial indexes support much faster queries than the unindexed equivalents. Both 2dsphere and 2d geospatial indexes support $centerSphere. \nEXAMPLE \nThe following example queries grid coordinates and returns all documents within a 10 mile radius of longitude 88 W and latitude 30 N. The query converts the distance to radians by dividing by the approximate equatorial radius of the earth, 3963.2 miles: db.places.find( {  loc: { $geoWithin: { $centerSphere: [ [ -88, 30 ], 10/3963.2 ] } }} ) \n←  $center$geometry → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/each/": " Docs Home → MongoDB Manual \n$EACH \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$each \nThe $each modifier is available for use with the $addToSet operator and the $push operator. Use with the $addToSet operator to add multiple values to an array <field> if the values do not exist in the <field>. { $addToSet: { <field>: { $each: [ <value1>, <value2> ... ] } } }  Use with the $push operator to append multiple values to an array <field>. { $push: { <field>: { $each: [ <value1>, <value2> ... ] } } }  The $push operator can use $each modifier with other modifiers. For a list of modifiers available for $push, see Modifiers. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nEXAMPLES  USE $EACH WITH $PUSH OPERATOR \nThe following example appends each element of [ 90, 92, 85 ] to the scores array for the document where the name field equals joe: db.students.updateOne(   { name: \"joe\" },   { $push: { scores: { $each: [ 90, 92, 85 ] } } })  \nUSE $EACH WITH $ADDTOSET OPERATOR \nA collection inventory has the following document: { _id: 2, item: \"cable\", tags: [ \"electronics\", \"supplies\" ] }  Then the following operation uses the $addToSet operator with the $each modifier to add multiple elements to the tags array: db.inventory.updateOne(   { _id: 2 },   { $addToSet: { tags: { $each: [ \"camera\", \"electronics\", \"accessories\" ] } } } )  The operation only adds \"camera\" and \"accessories\" to the tags array. \"electronics\" was already in the array: {  _id: 2,  item: \"cable\",  tags: [ \"electronics\", \"supplies\", \"camera\", \"accessories\" ]} \n←  $pullAll$position → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/min/": " Docs Home → MongoDB Manual \n$MIN \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$min \nThe $min updates the value of the field to a specified value if the specified value is less than the current value of the field. The $min operator can compare values of different types, using the BSON comparison order. { $min: { <field1>: <value1>, ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. If the field does not exist, the $min operator sets the field to the specified value. For comparisons between values of different types, such as a number and a null, $min uses the BSON comparison order. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $min with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLES  USE $MIN TO COMPARE NUMBERS \nCreate the scores collection: db.scores.insertOne( { _id: 1, highScore: 800, lowScore: 200 } )  The lowScore for the document currently has the value 200. The following operation uses $min to compare 200 to the specified value 150 and updates the value of lowScore to 150 since 150 is less than 200: db.scores.updateOne( { _id: 1 }, { $min: { lowScore: 150 } } )  The scores collection now contains the following modified document: { _id: 1, highScore: 800, lowScore: 150 }  The next operation has no effect since the current value of the field lowScore, i.e 150, is less than 250: db.scores.updateOne( { _id: 1 }, { $min: { lowScore: 250 } } )  The document remains unchanged in the scores collection: { _id: 1, highScore: 800, lowScore: 150 }  \nUSE $MIN TO COMPARE DATES \nCreate the tags collection: db.tags.insertOne(   {     _id: 1,     desc: \"crafts\",     dateEntered: ISODate(\"2013-10-01T05:00:00Z\"),     dateExpired: ISODate(\"2013-10-01T16:38:16Z\")   })  The following operation compares the current value of the dateEntered field, i.e. ISODate(\"2013-10-01T05:00:00Z\"), with the specified date new Date(\"2013-09-25\") to determine whether to update the field: db.tags.updateOne(   { _id: 1 },   { $min: { dateEntered: new Date(\"2013-09-25\") } })  The operation updates the dateEntered field: {  _id: 1,  desc: \"crafts\",  dateEntered: ISODate(\"2013-09-25T00:00:00Z\"),  dateExpired: ISODate(\"2013-10-01T16:38:16Z\")}  \nTIP \nSEE ALSO:      * db.collection.updateMany()  * db.collection.findAndModify() ←  $inc$max → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/position/": " Docs Home → MongoDB Manual \n$POSITION \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$position \nThe $position modifier specifies the location in the array at which the $push operator inserts elements. Without the $position modifier, the $push operator inserts elements to the end of the array. See $push modifiers for more information. To use the $position modifier, it must appear with the $each modifier. {  $push: {    <field>: {       $each: [ <value1>, <value2>, ... ],       $position: <num>    }  }}  <num> indicates the position in the array, based on a zero-based index:      * A non-negative number corresponds to the position in the array, starting from the beginning of the array. If the value of <num> is greater or equal to the length of the array, the $position modifier has no effect and $push adds elements to the end of the array.  * A negative number corresponds to the position in the array, counting from (but not including) the last element of the array. For example, -1 indicates the position just before the last element in the array. If you specify multiple elements in the $each array, the last added element is in the specified position from the end. If the absolute value of <num> is greater than or equal to the length of the array, the $push adds elements to the beginning of the array. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nEXAMPLES  ADD ELEMENTS AT THE START OF THE ARRAY \nCreate the students collection: db.students.insertOne( { \"_id\" : 1, \"scores\" : [ 100 ] } )  The following operation updates the scores field to add the elements 50, 60 and 70 to the beginning of the array: db.students.updateOne(   { _id: 1 },   {     $push: {        scores: {           $each: [ 50, 60, 70 ],           $position: 0        }     }   })  The operation results in the following updated document: { \"_id\" : 1, \"scores\" : [  50,  60,  70,  100 ] }  \nADD ELEMENTS TO THE MIDDLE OF THE ARRAY \nAdd a document to the students collection: db.students.insertOne( { \"_id\" : 2, \"scores\" : [  50,  60,  70,  100 ] } )  The following operation updates the scores field to add the elements 20 and 30 at the array index of 2: db.students.updateOne(   { _id: 2 },   {     $push: {        scores: {           $each: [ 20, 30 ],           $position: 2        }     }   })  The operation results in the following updated document: { \"_id\" : 2, \"scores\" : [  50,  60,  20,  30,  70,  100 ] }  \nUSE A NEGATIVE INDEX TO ADD ELEMENTS TO THE ARRAY \n$position can accept a negative array index value to indicate the position starting from the end, counting from (but not including) the last element of the array. For example, -1 indicates the position just before the last element in the array. Add the following document to the students collection: db.students.insertOne(   { \"_id\" : 3, \"scores\" : [  50,  60,  20,  30,  70,  100 ] })  The following operation specifies -2 for the $position to add 90 at the position two places before the last element, and then 80 at the position two places before the last element. \nIMPORTANT With a negative index position, if you specify multiple elements in the $each array, the last added element is in the specified position from the end.\n\n  The operation results in the following updated document: { \"_id\" : 3, \"scores\" : [ 50, 60, 20, 30, 90, 80, 70, 100 ] } \n←  $each$slice → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/unset/": " Docs Home → MongoDB Manual \n$UNSET \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION  NOTE \nDISAMBIGUATION The following page refers to the update operator $unset. For the aggregation stage $unset, available starting in MongoDB 4.2, see $unset. $unset \nThe $unset operator deletes a particular field. Consider the following syntax: { $unset: { <field1>: \"\", ... } }  The specified value in the $unset expression (i.e. \"\") does not impact the operation. To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. If the field does not exist, then $unset does nothing (i.e. no operation). When used with $ to match an array element, $unset replaces the matching element with null rather than removing the matching element from the array. This behavior keeps consistent the array size and element positions. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $unset with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLE \nCreate the products collection: db.products.insertMany( [   { \"item\": \"chisel\", \"sku\": \"C001\", \"quantity\": 4, \"instock\": true },   { \"item\": \"hammer\", \"sku\": \"unknown\", \"quantity\": 3, \"instock\": true },   { \"item\": \"nails\", \"sku\": \"unknown\", \"quantity\": 100, \"instock\": true }] )  Update the first document in the products collection where the value of sku is unknown: db.products.updateOne(   { sku: \"unknown\" },   { $unset: { quantity: \"\", instock: \"\" } })  updateOne() uses the $unset operator to:      * remove the quantity field  * remove the instock field {  item: 'chisel',  sku: 'C001',  quantity: 4,  instock: true},{  item: 'hammer',  sku: 'unknown'},{  item: 'nails',  sku: 'unknown',  quantity: 100,  instock: true} \nTIP \nSEE ALSO: db.collection.updateMany() db.collection.findAndModify() ←  $setOnInsertArray Update Operators → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/query/geoWithin/": " Docs Home → MongoDB Manual \n$GEOWITHIN \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$geoWithin \nSelects documents with geospatial data that exists entirely within a specified shape. The specified shape can be either a GeoJSON Polygon (either single-ringed or multi-ringed), a GeoJSON MultiPolygon, or a shape defined by legacy coordinate pairs. The $geoWithin operator uses the $geometry operator to specify the GeoJSON object. To specify a GeoJSON polygons or multipolygons using the default coordinate reference system (CRS), use the following syntax: {   <location field>: {      $geoWithin: {         $geometry: {            type: <\"Polygon\" or \"MultiPolygon\"> ,            coordinates: [ <coordinates> ]         }      }   }}  For $geoWithin queries that specify GeoJSON geometries with areas greater than a single hemisphere, the use of the default CRS results in queries for the complementary geometries. To specify a single-ringed GeoJSON polygon with a custom MongoDB CRS, use the following prototype that specifies the custom MongoDB CRS in the $geometry expression: {   <location field>: {      $geoWithin: {         $geometry: {           type: \"Polygon\" ,           coordinates: [ <coordinates> ],           crs: {              type: \"name\",              properties: { name: \"urn:x-mongodb:crs:strictwinding:EPSG:4326\" }           }         }      }   }}  The custom MongoDB CRS uses a counter-clockwise winding order and allows $geoWithin to support queries with a single-ringed GeoJSON polygon whose area is greater than or equal to a single hemisphere. If the specified polygon is smaller than a single hemisphere, the behavior of $geoWithin with the MongoDB CRS is the same as with the default CRS. See also \"Big\" Polygons. If querying for inclusion in a shape defined by legacy coordinate pairs on a plane, use the following syntax: {   <location field>: {      $geoWithin: { <shape operator>: <coordinates> }   }}  The available shape operators are:      * $box,  * $polygon,  * $center (defines a circle), and  * $centerSphere (defines a circle on a sphere). \nIMPORTANT If you use longitude and latitude, specify coordinates in order of longitude, latitude. \nBEHAVIOR  GEOSPATIAL INDEXES \n$geoWithin does not require a geospatial index. However, a geospatial index will improve query performance. Both 2dsphere and 2d geospatial indexes support $geoWithin. \nUNSORTED RESULTS \nThe $geoWithin operator does not return sorted results. As such, MongoDB can return $geoWithin queries more quickly than geospatial $near or $nearSphere queries, which sort results. \nDEGENERATE GEOMETRY \n$geoWithin does not guarantee that it will consider a piece of geometry to contain its component geometry, or another polygon sharing its component geometry. \n\"BIG\" POLYGONS \nFor $geoWithin, if you specify a single-ringed polygon that has an area greater than a single hemisphere, include the custom MongoDB coordinate reference system in the $geometry expression; otherwise, $geoWithin queries for the complementary geometry. For all other GeoJSON polygons with areas greater than a hemisphere, $geoWithin queries for the complementary geometry. \nEXAMPLES  WITHIN A POLYGON \nThe following example selects all loc data that exist entirely within a GeoJSON Polygon. The area of the polygon is less than the area of a single hemisphere:\n\n  For single-ringed polygons with areas greater than a single hemisphere, see Within a \"Big\" Polygon. \nWITHIN A \"BIG\" POLYGON \nTo query with a single-ringed GeoJSON polygon whose area is greater than a single hemisphere, the $geometry expression must specify the custom MongoDB coordinate reference system. For example: db.places.find(   {     loc: {       $geoWithin: {          $geometry: {             type : \"Polygon\" ,             coordinates: [               [                 [ -100, 60 ], [ -100, 0 ], [ -100, -60 ], [ 100, -60 ], [ 100, 60 ], [ -100, 60 ]               ]             ],             crs: {                type: \"name\",                properties: { name: \"urn:x-mongodb:crs:strictwinding:EPSG:4326\" }             }          }       }     }   }) \n$within \nDeprecated since version 2.4: $geoWithin replaces $within in MongoDB 2.4. ←  $geoIntersects$near → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/addToSet/": " Docs Home → MongoDB Manual \n$ADDTOSET \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$addToSet \nThe $addToSet operator adds a value to an array unless the value is already present, in which case $addToSet does nothing to that array. The $addToSet operator has the form: { $addToSet: { <field1>: <value1>, ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. $addToSet only ensures that there are no duplicate items added to the set and does not affect existing duplicate elements. $addToSet does not guarantee a particular ordering of elements in the modified set. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $addToSet with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nMISSING FIELD \nIf you use $addToSet on a field that is absent from the document to update, $addToSet creates the array field with the specified value as its element. \nFIELD IS NOT AN ARRAY \nIf you use $addToSet on a field that is not an array, the operation will fail. For example, create the pigments collection: db.pigments.insertOne( { _id: 1, colors: \"blue, green, red\" } )  The colors field is not an array. The following $addToSet operation fails: db.pigments.updateOne(   { _id: 1 },   { $addToSet: { colors: \"mauve\" } })  \nVALUE TO ADD IS AN ARRAY \nIf the value is an array, $addToSet appends the whole array as a single element. Create the alphabet collection: db.alphabet.insertOne( { _id: 1, letters: [\"a\", \"b\"] } )  The following operation appends the array [ \"c\", \"d\" ] to the letters field: db.alphabet.updateOne(   { _id: 1 },   { $addToSet: { letters: [ \"c\", \"d\" ] } })  The array [ \"c\", \"d\" ] is added as a single element: { _id: 1, letters: [ 'a', 'b', [ 'c', 'd' ] ] }  \nTIP To add each element of the value separately, use the $each modifier with $addToSet. See $each Modifier for details. \nVALUE TO ADD IS A DOCUMENT \nIf the value is a document, MongoDB determines that the document is a duplicate if an existing document in the array matches the to-be-added document exactly; i.e. the existing document has the exact same fields and values and the fields are in the same order. As such, field order matters and you cannot specify that MongoDB compare only a subset of the fields in the document to determine whether the document is a duplicate of an existing array element. \nEXAMPLES \nCreate the inventory collection: db.inventory.insertOne(   { _id: 1, item: \"polarizing_filter\", tags: [ \"electronics\", \"camera\" ] })  \nADD TO ARRAY \nThe following operation adds the element \"accessories\" to the tags array since \"accessories\" does not exist in the array: db.inventory.updateOne(   { _id: 1 },   { $addToSet: { tags: \"accessories\" } })  \nVALUE ALREADY EXISTS \nThe following $addToSet operation has no effect because \"camera\" is already an element of the tags array: db.inventory.updateOne(   { _id: 1 },   { $addToSet: { tags: \"camera\"  } })  \n$EACH MODIFIER \nYou can use the $addToSet operator with the $each modifier. The $each modifier allows the $addToSet operator to add multiple values to the array field. A collection inventory has the following document: { _id: 2, item: \"cable\", tags: [ \"electronics\", \"supplies\" ] } \n\n db.inventory.updateOne(   { _id: 2 },   { $addToSet: { tags: { $each: [ \"camera\", \"electronics\", \"accessories\" ] } } } )  The operation only adds \"camera\" and \"accessories\" to the tags array. \"electronics\" was already in the array: {  _id: 2,  item: \"cable\",  tags: [ \"electronics\", \"supplies\", \"camera\", \"accessories\" ]}  \nTIP \nSEE ALSO:      * db.collection.updateMany()  * db.collection.findAndModify()  * $push  * $pull ←  $[<identifier>]$pop → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/mul/": " Docs Home → MongoDB Manual \n$MUL \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$mul \nMultiply the value of a field by a number. To specify a $mul expression, use the following prototype: { $mul: { <field1>: <number1>, ... } }  The field to update must contain a numeric value. To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $mul with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). Starting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nMISSING FIELD \nIf the field does not exist in a document, $mul creates the field and sets the value to zero of the same numeric type as the multiplier. \nATOMIC \n$mul is an atomic operation within a single document. \nMIXED TYPE \nMultiplication with values of mixed numeric types (32-bit integer, 64-bit integer, float) may result in conversion of numeric type. For multiplication with values of mixed numeric types, the following type conversion rules apply: 32-bit Integer\n64-bit Integer\nFloat\n32-bit Integer\n32-bit or 64-bit Integer\n64-bit Integer\nFloat\n64-bit Integer\n64-bit Integer\n64-bit Integer\nFloat\nFloat\nFloat\nFloat\nFloat \nNOTE      * If the product of two 32-bit integers exceeds the maximum value for a 32-bit integer, the result is a 64-bit integer.  * Integer operations of any type that exceed the maximum value for a 64-bit integer produce an error. \nEXAMPLES  MULTIPLY THE VALUE OF A FIELD \nCreate the products collection: db.products.insertOne(   { \"_id\" : 1, \"item\" : \"Hats\", \"price\" : Decimal128(\"10.99\"), \"quantity\" : 25 })  In the following operation, db.collection.updateOne() updates the document. The $mul operator multiplies the price field by 1.25 and the quantity field by 2: db.products.updateOne(   { _id: 1 },   { $mul:      {         price: Decimal128( \"1.25\" ),         quantity: 2       }   })  In the updated document:  * price is the original value, 10.99, multiplied by 1.25  * quantity is the original value, 25, multiplied by 2 { _id: 1, item: 'Hats', price: Decimal128(\"13.7375\"), quantity: 50 }  \nAPPLY $MUL OPERATOR TO A NON-EXISTING FIELD \nAdd the following document to the products collection: db.products.insertOne( { _id: 2,  item: \"Unknown\" } )  In the following operation, db.collection.updateOne() attempts to apply the $mul operator to a field that is not in the document: db.products.updateOne(   { _id: 2 },   { $mul: { price: Decimal128(\"100\") } })  The db.collection.updateOne() operation  * inserts the price field  * sets Decimal128(\"0\") { \"_id\" : 2, \"item\" : \"Unknown\", \"price\" : NumberLong(0) }  The price field has the same type, Decimal128, as the multiplier. \nMULTIPLY MIXED NUMERIC TYPES \nAdd the following document to the products collection: db.products.insertOne( { _id: 3,  item: \"Scarf\", price: Decimal128(\"10\") } )  In the following operation, db.collection.updateOne() uses the $mul operator to multiply the value in the price field Decimal128(10) by Int32(5): db.products.updateOne(   { _id: 3 },   { $mul: { price: Int32(5) } })  The operation results in the following document: { _id: 3, item: 'Scarf', price: Decimal128(\"50\") } \n\n \nTIP \nSEE ALSO:  * db.collection.updateMany()  * db.collection.findAndModify() ←  $max$rename → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/currentDate/": " Docs Home → MongoDB Manual \n$CURRENTDATE \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$currentDate \nThe $currentDate operator sets the value of a field to the current date, either as a Date or a timestamp. The default type is Date. The $currentDate operator has the form: { $currentDate: { <field1>: <typeSpecification1>, ... } }  <typeSpecification> can be either:      * a boolean true to set the field value to the current date as a Date, or  * a document { $type: \"timestamp\" } or { $type: \"date\" } which explicitly specifies the type. The operator is case-sensitive and accepts only the lowercase \"timestamp\" or the lowercase \"date\". To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. $currentDate sets the specified field to the date when $currentDate was run. If the field does not exist, $currentDate adds the field to a document. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $currentDate with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLE \nCreate a sample collection customers with the following document: db.customers.insertOne(   { _id: 1, status: \"a\", lastModified: ISODate(\"2013-10-02T01:11:18.965Z\") })  The following operation updates the lastModified field to the current date, the \"cancellation.date\" field to the current timestamp as well as updating the status field to \"D\" and the \"cancellation.reason\" to \"user request\". db.customers.updateOne(   { _id: 1 },   {     $currentDate: {        lastModified: true,        \"cancellation.date\": { $type: \"timestamp\" }     },     $set: {        \"cancellation.reason\": \"user request\",        status: \"D\"     }   })  $currentDate sets the specified field to the date when $currentDate was run. To verify the update: db.customers.find()  The updated document resembles: {   \"_id\" : 1,   \"status\" : \"D\",   \"lastModified\" : ISODate(\"2020-01-22T21:21:41.052Z\"),   \"cancellation\" : {      \"date\" : Timestamp(1579728101, 1),      \"reason\" : \"user request\"   }} The lastModified field is set to the date when $currentDate was run in the update example shown earlier. \nAGGREGATION ALTERNATIVE TO $CURRENTDATE \nStarting in version 4.2, update methods can accept an aggregation pipeline. As such, the previous example can be rewritten as the following using the aggregation stage $set and the aggregation variables NOW (for the current datetime) and CLUSTER_TIME (for the current timestamp): \nTIP  * To access aggregation variables, prefix the variable with double dollar signs $$ and enclose in quotes.  * CLUSTER_TIME is available only on replica sets and sharded clusters.  * The NOW and CLUSTER_TIME values remain the same throughout the pipeline. db.customers.updateOne(  { _id: 1 },  [   { $set: { lastModified: \"$$NOW\", cancellation: {date: \"$$CLUSTER_TIME\", reason: \"user request\"}, status: \"D\" } }  ])  After the operation, you can query the collection to verify the update: db.customers.find().pretty()  The query should return the following document:\n\n \nTIP \nSEE ALSO:  * db.collection.updateOne()  * db.collection.updateMany()  * db.collection.findAndModify()  * db.collection.findOneAndUpdate() ←  Field Update Operators$inc → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/positional-all/": " Docs Home → MongoDB Manual \n$[] \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$[] \nThe all positional operator $[] indicates that the update operator should modify all elements in the specified array field. The $[] operator has the following form: { <update operator>: { \"<array>.$[]\" : value } }  Use in update operations, e.g. db.collection.updateOne() and db.collection.findAndModify(), to modify all array elements for the document or documents that match the query condition. For example: db.collection.updateOne(   { <query conditions> },   { <update operator>: { \"<array>.$[]\" : value } })  For an example, see Update All Elements in an Array. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nUPSERT \nIf an upsert operation results in an insert, the query must include an exact equality match on the array field in order to use the $[] positional operator in the update statement. For example, the following upsert operation, which uses $[] in the update document, specifies an exact equality match condition on the array field: db.collection.updateOne(   { myArray: [ 5, 8 ] },   { $set: { \"myArray.$[]\": 10 } },   { upsert: true })  If no such document exists, the operation would result in an insertion of the following document: { \"_id\" : ObjectId(...), \"myArray\" : [ 10, 10 ] }  If the upsert operation did not include an exact equality match and no matching documents were found to update, the upsert operation would error. For example, the following operations would error if no matching documents were found to update: db.emptyCollection.updateOne(   { },   { $set: { \"myArray.$[]\": 10 } },   { upsert: true })\ndb.emptyCollection.updateOne(   { myArray: 5 },   { $set: { \"myArray.$[]\": 10 } },   { upsert: true })  \nNESTED ARRAYS \nThe $[] operator can be used for queries that traverse more than one array and nested arrays. For an example, see Update Nested Arrays in Conjunction with $[<identifier>]. \nEXAMPLES  UPDATE ALL ELEMENTS IN AN ARRAY \nCreate the students collection: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 85, 82, 80 ] },   { \"_id\" : 2, \"grades\" : [ 88, 90, 92 ] },   { \"_id\" : 3, \"grades\" : [ 85, 100, 90 ] }] )  To increment all elements in the grades array by 10 for all documents in the collection, use the all positional $[] operator: db.students.updateMany(   { },   { $inc: { \"grades.$[]\": 10 } },)  The all positional $[] operator acts as a placeholder for all elements in the array field. After the operation, the students collection contains the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] }{ \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }  \nUPDATE ALL DOCUMENTS IN AN ARRAY \nThe $[] positional operator facilitates updates to arrays that contain embedded documents. To access the fields in the embedded documents, use the dot notation with the $[] operator. db.collection.updateOne(   { <query selector> },   { <update operator>: { \"array.$[].field\" : value } })  Create the students2 collection:\n\n  To modify the value of the std field for all elements in the grades array, use the positional $[] operator: db.students2.updateMany(   { },   { $inc: { \"grades.$[].std\" : -2 } },)  After the operation, the collection has the following documents: {   \"_id\" : 1,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 90, \"std\" : 4 },      { \"grade\" : 85, \"mean\" : 85, \"std\" : 6 }   ]}{   \"_id\" : 2,   \"grades\" : [      { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },      { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }   ]}  \nUPDATE ARRAYS SPECIFIED USING A NEGATION QUERY OPERATOR \nCreate the results collection: db.results.insertMany( [   { \"_id\" : 1, \"grades\" : [ 85, 82, 80 ] },   { \"_id\" : 2, \"grades\" : [ 88, 90, 92 ] },   { \"_id\" : 3, \"grades\" : [ 85, 100, 90 ] }] )  To increment all elements in the grades array by 10 for all documents except those with the value 100 in the grades array, use the all positional $[] operator: db.results.updateMany(   { \"grades\" : { $ne: 100 } },   { $inc: { \"grades.$[]\": 10 } },)  The all positional $[] operator acts as a placeholder for all elements in the array field. After the operation, the students collection contains the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] }{ \"_id\" : 3, \"grades\" : [ 85, 100, 90 ] }  \nUPDATE NESTED ARRAYS IN CONJUNCTION WITH $[<IDENTIFIER>] \nThe $[] positional operator, in conjunction with filter $[<identifier>] positional operator can be used to update nested arrays. Create a collection students3 with the following documents: db.students3.insertMany( [   { \"_id\" : 1,      \"grades\" : [        { type: \"quiz\", questions: [ 10, 8, 5 ] },        { type: \"quiz\", questions: [ 8, 9, 6 ] },        { type: \"hw\", questions: [ 5, 4, 3 ] },        { type: \"exam\", questions: [ 25, 10, 23, 0 ] },      ]   }] )  To update all values that are greater than or equal to 8 in the nested grades.questions array, regardless of type: db.students3.updateMany(   {},   { $inc: { \"grades.$[].questions.$[score]\": 2 } },   { arrayFilters: [  { \"score\": { $gte: 8 } } ] })  The updated documents look like this: {  _id: 1,  grades: [    { type: 'quiz', questions: [ 12, 10, 5 ] },    { type: 'quiz', questions: [ 10, 11, 6 ] },    { type: 'hw', questions: [ 5, 4, 3 ] },    { type: 'exam', questions: [ 27, 12, 25, 0 ] }  ]}  \nTIP \nSEE ALSO:      * $[<identifier>]  * db.collection.updateMany()  * db.collection.findAndModify()  * $elemMatch ←  $ (update)$[<identifier>] → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/bit/": " Docs Home → MongoDB Manual \n$BIT \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$bit \nThe $bit operator performs a bitwise update of a field. The operator supports bitwise and, bitwise or, and bitwise xor (i.e. exclusive or) operations. To specify a $bit operator expression, use the following prototype: { $bit: { <field>: { <and|or|xor>: <int> } } }  Only use this operator with integer fields (either 32-bit integer or 64-bit integer). To specify a <field> in an embedded document or in an array, use dot notation. \nNOTE All numbers in mongosh are doubles, not integers. Use the NumberInt() or the NumberLong() constructor to specify integers. See Int32 or Long for more information. \nBEHAVIOR \nStarting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $bit with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). Starting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nEXAMPLES \nThe following examples use the switches collection: db.switches.insertMany( [   { _id: 1, expdata: Int32(13) },   { _id: 2, expdata: Int32(3) },   { _id: 3, expdata: Int32(1) }] )  \nBITWISE AND \nUse a bitwise and in the updateOne() operation to update expdata. db.switches.updateOne(   { _id: 1 },   { $bit: { expdata: { and: Int32( 10 ) } } })  The bitwise and operation:      * gets the bitwise value of expdata  * uses and to apply the bitwise value of Int32(10)  * updates expdata with the result, 1000 1101   // expdata1010   // Int32(10)----1000 Binary 1000 is equivalent to Int32(8). The db.switches.find( { _id: 1 } ) command returns the following document: { \"_id\" : 1, \"expdata\" : 8 }  \nBITWISE OR \nUse a bitwise or in the updateOne() operation to update expdata. db.switches.updateOne(   { _id: 2 },   { $bit: { expdata: { or: Int32( 5 ) } } })  The bitwise or operation:  * gets the bitwise value of expdata  * uses or to apply the bitwise value of Int32(5)  * updates expdata with the result, 0111 0111   // expdata0101   // Int32(5)----0111 Binary 0111 is equivalent to Int32(7). The db.switches.find( { _id: 2 } ) command returns the following document: { \"_id\" : 2, \"expdata\" : 7 }  \nBITWISE XOR \nUse a bitwise xor in the updateOne() operation to update expdata. db.switches.updateOne(   { _id: 3 },   { $bit: { expdata: { xor: Int32( 5 ) } } })  The bitwise and operation:  * gets the bitwise value of expdata  * uses and to apply the bitwise value of Int32(5)  * updates expdata with the result, 0100 0001   // expdata0101   // Int32(5)----0100 Binary 0100 is equivalent to Int32(4). The db.switches.find( { _id: 3 } ) command returns the following document: { \"_id\" : 1, \"expdata\" : 4 }  \nTIP \nSEE ALSO:  * db.collection.updateOne()  * db.collection.findAndModify() ←  Bitwise Update OperatorAggregation Pipeline Stages → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/setOnInsert/": " Docs Home → MongoDB Manual \n$SETONINSERT \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$setOnInsert \nIf an update operation with upsert: true results in an insert of a document, then $setOnInsert assigns the specified values to the fields in the document. If the update operation does not result in an insert, $setOnInsert does nothing. You can specify the upsert option for:      * db.collection.updateOne()  * db.collection.updateMany()  * db.collection.findAndModify() db.collection.updateOne(   <query>,   { $setOnInsert: { <field1>: <value1>, ... } },   { upsert: true })  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $setOnInsert with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLE \nThe products collection contains no documents. Insert a new document using db.collection.updateOne() the upsert: true parameter. db.products.updateOne(  { _id: 1 },  {     $set: { item: \"apple\" },     $setOnInsert: { defaultQty: 100 }  },  { upsert: true })  MongoDB uses <query> to create a new document with _id: 1. $setOnInsert updates the document as specified. The products collection contains the newly-inserted document: { \"_id\" : 1, \"item\" : \"apple\", \"defaultQty\" : 100 }  When the upsert parameter is true db.collection.updateOne():  * creates a new document  * applies the $set operation  * applies the $setOnInsert operation If db.collection.updateOne() matches an existing document, MongoDB only applies the $set operation. \nTIP \nSEE ALSO:  * db.collection.updateOne()  * db.collection.findAndModify() ←  $set$unset → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/max/": " Docs Home → MongoDB Manual \n$MAX \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$max \nThe $max operator updates the value of the field to a specified value if the specified value is greater than the current value of the field. The $max operator can compare values of different types, using the BSON comparison order. The $max operator expression has the form: { $max: { <field1>: <value1>, ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. If the field does not exists, the $max operator sets the field to the specified value. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $max with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLES  USE $MAX TO COMPARE NUMBERS \nCreate the scores collection: db.scores.insertOne( { _id: 1, highScore: 800, lowScore: 200 } )  The highScore for the document currently has the value 800. The following operation:      * Compares the highscore, 800, to the specified value, 950  * Updates highScore to 950 since 950 is greater than 800 db.scores.updateOne( { _id: 1 }, { $max: { highScore: 950 } } )  The scores collection now contains the following modified document: { _id: 1, highScore: 950, lowScore: 200 }  The next operation has no effect since the value of highScore, 950, is greater than 870: db.scores.updateOne( { _id: 1 }, { $max: { highScore: 870 } } )  The document remains unchanged in the scores collection: { _id: 1, highScore: 950, lowScore: 200 }  \nUSE $MAX TO COMPARE DATES \nCreate the tags collection: db.tags.insertOne(   {     _id: 1,     desc: \"crafts\",     dateEntered: ISODate(\"2013-10-01T05:00:00Z\"),     dateExpired: ISODate(\"2013-10-01T16:38:16.163Z\")   })  The following operation compares the current value of the dateExpired field, ISODate(\"2013-10-01T16:38:16.163Z\"), with the specified date new Date(\"2013-09-30\") to determine whether to update the field: db.tags.updateOne(   { _id: 1 },   { $max: { dateExpired: new Date(\"2013-09-30\") } })  new Date(\"2013-09-30\") is not the newest date, so the operation does not update the dateExpired field: {   _id: 1,   desc: \"decorative arts\",   dateEntered: ISODate(\"2013-10-01T05:00:00Z\"),   dateExpired: ISODate(\"2013-10-01T16:38:16.163Z\")}  \nTIP \nSEE ALSO:  * db.collection.updateMany()  * db.collection.findAndModify() ←  $min$mul → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/rename/": " Docs Home → MongoDB Manual \n$RENAME \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$rename \nThe $rename operator updates the name of a field and has the following form: {$rename: { <field1>: <newName1>, <field2>: <newName2>, ... } }  The new field name must differ from the existing field name. To specify a <field> in an embedded document, use dot notation. Consider the following example: db.students.updateOne(   { _id: 1 },   { $rename: { 'nickname': 'alias', 'cell': 'mobile' } })  This operation renames the field nickname to alias, and the field cell to mobile. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. The $rename operator logically performs an $unset of both the old name and the new name, and then performs a $set operation with the new name. As such, the operation may not preserve the order of the fields in the document; i.e. the renamed field may move within the document. If the document already has a field with the <newName>, the $rename operator removes that field and renames the specified <field> to <newName>. If the field to rename does not exist in a document, $rename does nothing (i.e. no operation). For fields in embedded documents, the $rename operator can rename these fields as well as move the fields in and out of embedded documents. $rename does not work if these fields are in array elements. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $rename with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLES \nCreate the students collection: db.students.insertMany( [   {     \"_id\": 1,     \"alias\": [ \"The American Cincinnatus\", \"The American Fabius\" ],     \"mobile\": \"555-555-5555\",     \"nmae\": { \"first\" : \"george\", \"last\" : \"washington\" }   },   {     \"_id\": 2,     \"alias\": [ \"My dearest friend\" ],     \"mobile\": \"222-222-2222\",     \"nmae\": { \"first\" : \"abigail\", \"last\" : \"adams\" }   },   {     \"_id\": 3,     \"alias\": [ \"Amazing grace\" ],     \"mobile\": \"111-111-1111\",     \"nmae\": { \"first\" : \"grace\", \"last\" : \"hopper\" }   }] )  The documents contain an error, nmae should be name. The examples in the following sections update the documents in the collection. \nRENAME A FIELD \nTo rename a field, call the $rename operator with the current name of the field and the new name: db.students.updateMany( {}, { $rename: { \"nmae\": \"name\" } } )  This operation renames the field nmae to name for all documents in the collection: {  \"_id\": 1,  \"alias\": [ \"The American Cincinnatus\", \"The American Fabius\" ],  \"mobile\": \"555-555-5555\",  \"name\": { \"first\" : \"george\", \"last\" : \"washington\" }}\n{   \"_id\" : 2,   \"alias\" : [ \"My dearest friend\" ],   \"mobile\" : \"222-222-2222\",   \"name\" : { \"first\" : \"abigail\", \"last\" : \"adams\" }}\n{ \"_id\" : 3,  \"alias\" : [ \"Amazing grace\" ],  \"mobile\" : \"111-111-1111\",  \"name\" : { \"first\" : \"grace\", \"last\" : \"hopper\" } }  \nRENAME A FIELD IN AN EMBEDDED DOCUMENT\n\n db.students.updateOne( { _id: 1 }, { $rename: { \"name.first\": \"name.fname\" } } )  This operation renames the embedded field first to fname: {  _id: 1,  alias: [ 'The American Cincinnatus', 'The American Fabius' ],  mobile: '555-555-5555',  name: { last: 'washington', fname: 'george' }}  \nRENAME A FIELD THAT DOES NOT EXIST \nWhen renaming a field and the existing field name refers to a field that does not exist, the $rename operator does nothing, as in the following: db.students.updateOne( { _id: 1 }, { $rename: { 'wife': 'spouse' } } )  This operation does nothing because there is no field named wife. \nTIP \nSEE ALSO:      * db.collection.updateMany()  * db.collection.findAndModify() ←  $mul$set → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/positional-filtered/": " Docs Home → MongoDB Manual \n$[<IDENTIFIER>] \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$[<identifier>] \nThe filtered positional operator $[<identifier>] identifies the array elements that match the arrayFilters conditions for an update operation, e.g. db.collection.updateMany() and db.collection.findAndModify(). Used in conjunction with the arrayFilters option, the $[<identifier>] operator has the following form: { <update operator>: { \"<array>.$[<identifier>]\" : value } },{ arrayFilters: [ { <identifier>: <condition> } ] }  Use in conjunction with the arrayFilters option to update all elements that match the arrayFilters conditions in the document or documents that match the query conditions. For example: db.collection.updateMany(   { <query conditions> },   { <update operator>: { \"<array>.$[<identifier>]\" : value } },   { arrayFilters: [ { <identifier>: <condition> } ] })  \nNOTE The <identifier> must begin with a lowercase letter and contain only alphanumeric characters. For an example, see Update All Array Elements That Match arrayFilters. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nUPSERT \nIf an upsert operation results in an insert, the query must include an exact equality match on the array field in order to use $[<identifier>] in the update statement. For example, the following upsert operation, which uses $[<identifier>] in the update document, specifies an exact equality match condition on the array field: db.collection.updateOne(   { myArray: [ 0, 1 ] },   { $set: { \"myArray.$[element]\": 2 } },   { arrayFilters: [ { element: 0 } ], upsert: true })  If no such document exists, the operation would result in an insert of a document that resembles the following: { \"_id\" : ObjectId(...), \"myArray\" : [ 2, 1 ] }  If the upsert operation did not include an exact equality match and no matching documents were found to update, the upsert operation would error. For example, the following operations would error if no matching documents were found to update: db.array.updateOne(   { },   { $set: { \"myArray.$[element]\": 10 } },   { arrayFilters: [ { element: 9 } ], upsert: true })  The operation would return an error that resembles the following: MongoServerError: The path 'myArray' must exist in the document in order to apply array updates. \nNESTED ARRAYS \nThe filtered positional operator $[<identifier>] can be used for queries which traverse more than one array and nested arrays. For an example, see Update Nested Arrays in Conjunction with $[]. \nEXAMPLES  UPDATE ALL ARRAY ELEMENTS THAT MATCH ARRAYFILTERS \nCreate the students collection: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] },   { \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] },   { \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }] )  To update all elements that are greater than or equal to 100 in the grades array, use the filtered positional operator $[<identifier>] with the arrayFilters: db.students.updateMany(   { },   { $set: { \"grades.$[element]\" : 100 } },   { arrayFilters: [ { \"element\": { $gte: 100 } } ] })  The positional $[<identifier>] operator acts as a placeholder for all elements in the array field that match the conditions specified in arrayFilters. After the operation, the students collection contains the following documents: { \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }{ \"_id\" : 2, \"grades\" : [ 98, 100, 100 ] }{ \"_id\" : 3, \"grades\" : [ 95, 100, 100 ] }  \nUPDATE ALL DOCUMENTS THAT MATCH ARRAYFILTERS IN AN ARRAY\n\n db.collection.updateMany(   { <query selector> },   { <update operator>: { \"array.$[<identifier>].field\" : value } },   { arrayFilters: [ { <identifier>: <condition> } } ] })  Create the students2 collection: db.students2.insertMany( [   {      \"_id\" : 1,      \"grades\" : [         { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 85, \"mean\" : 90, \"std\" : 4 },         { \"grade\" : 85, \"mean\" : 85, \"std\" : 6 }      ]   },   {      \"_id\" : 2,      \"grades\" : [         { \"grade\" : 90, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 87, \"mean\" : 90, \"std\" : 3 },         { \"grade\" : 85, \"mean\" : 85, \"std\" : 4 }      ]   }] )  To modify the value of the mean field for all elements in the grades array where the grade is greater than or equal to 85, use the positional $[<identifier>] operator and arrayFilters: db.students2.updateMany(   { },   { $set: { \"grades.$[elem].mean\" : 100 } },   { arrayFilters: [ { \"elem.grade\": { $gte: 85 } } ] })  After the operation, the collection has the following documents: {   \"_id\" : 1,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 6 }   ]}{   \"_id\" : 2,   \"grades\" : [      { \"grade\" : 90, \"mean\" : 100, \"std\" : 6 },      { \"grade\" : 87, \"mean\" : 100, \"std\" : 3 },      { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 }   ]}  \nUPDATE ALL ARRAY ELEMENTS THAT MATCH MULTIPLE CONDITIONS \nCreate the students3 collection: db.students3.insertMany( [   {      \"_id\" : 1,      \"grades\" : [         { \"grade\" : 80, \"mean\" : 75, \"std\" : 6 },         { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 },         { \"grade\" : 85, \"mean\" : 100, \"std\" : 6 }      ]   },   {      \"_id\" : 2,      \"grades\" : [         { \"grade\" : 90, \"mean\" : 100, \"std\" : 6 },         { \"grade\" : 87, \"mean\" : 100, \"std\" : 3 },         { \"grade\" : 85, \"mean\" : 100, \"std\" : 4 }      ]   }] )  To modify the value of the std field for all elements in the grades array where both the grade is greater than or equal to 80 and the std is greater than or equal to 5, use the positional $[<identifier>] operator and arrayFilters: db.students3.updateMany(   { },   { $inc: { \"grades.$[elem].std\" : -1 } },   { arrayFilters: [ { \"elem.grade\": { $gte: 80 }, \"elem.std\": { $gt: 5 } } ] })  After the operation, the collection has the following documents:\n\n  \nUPDATE ARRAY ELEMENTS USING A NEGATION OPERATOR \nCreate the alumni collection: db.alumni.insertMany( [   {      \"_id\": 1,      \"name\": \"Christine Franklin\",      \"degrees\": [         { \"level\": \"Master\" },         { \"level\": \"Bachelor\" }      ],  },   {      \"_id\": 2,      \"name\": \"Reyansh Sengupta\",      \"degrees\": [ { \"level\": \"Bachelor\" } ],   }] )  To modify all elements in the degrees array that do not have \"level\": \"Bachelor\", use the positional $[<identifier>] operation with the $ne query operator: db.alumni.updateMany(   { },   { $set : { \"degrees.$[degree].gradcampaign\" : 1 } },   { arrayFilters : [ {\"degree.level\" : { $ne: \"Bachelor\" } } ] })  After the operation, the collection has the following documents:  {  _id: 1,  name: 'Christine Franklin',  degrees: [     { level: 'Master', gradcampaign: 1 },     { level: 'Bachelor' }  ]},{  _id: 2,  name: 'Reyansh Sengupta',  degrees: [ { level: 'Bachelor' } ]}  \nUPDATE NESTED ARRAYS IN CONJUNCTION WITH $[] \nThe $[<identifier>] filtered positional operator, in conjunction with the $[] all positional operator, can be used to update nested arrays. Create a collection students4 with the following document: db.students4.insertOne(   { \"_id\" : 1,      \"grades\" : [        { type: \"quiz\", questions: [ 10, 8, 5 ] },        { type: \"quiz\", questions: [ 8, 9, 6 ] },        { type: \"hw\", questions: [ 5, 4, 3 ] },        { type: \"exam\", questions: [ 25, 10, 23, 0 ] },      ]   })  The following updates the values that are greater than or equal to 8 in the nested grades.questions array if the associated grades.type field is quiz. db.students4.updateMany(   {},   { $inc: { \"grades.$[t].questions.$[score]\": 2 } },   { arrayFilters: [ { \"t.type\": \"quiz\" }, { \"score\": { $gte: 8 } } ] })\n.. note::\n   The spacing is significant in the array identifier. If you write   the identifier as ``grades.$[ t ].questions.$[ score ]``, the   operation will fail.  After the operation, the collection has the following document: {   \"_id\" : 1,   \"grades\" : [      { \"type\" : \"quiz\", \"questions\" : [ 12, 10, 5 ] },      { \"type\" : \"quiz\", \"questions\" : [ 10, 11, 6 ] },      { \"type\" : \"hw\", \"questions\" : [ 5, 4, 3 ] },      { \"type\" : \"exam\", \"questions\" : [ 25, 10, 23, 0 ] }   ]}  To update all values that are greater than or equal to 8 in the nested grades.questions array, regardless of type: db.students4.updateMany(   {},   { $inc: { \"grades.$[].questions.$[score]\": 2 } },   { arrayFilters: [  { \"score\": { $gte: 8 } } ] })  \nTIP \nSEE ALSO:      * $[]  * db.collection.updateMany()  * db.collection.findAndModify()  * $elemMatch ←  $[]$addToSet → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/positional/": " Docs Home → MongoDB Manual \n$ (UPDATE) \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$ \nThe positional $ operator identifies an element in an array to update without explicitly specifying the position of the element in the array. \nNOTE \nDISAMBIGUATION      * To project, or return, an array element from a read operation, see the $ projection operator instead.  * To update all elements in an array, see the all positional operator $[] instead.  * To update all elements that match an array filter condition or conditions, see the filtered positional operator instead $[<identifier>]. The positional $ operator has the form: { \"<array>.$\" : value }  When used with update operations, e.g. db.collection.updateOne() and db.collection.findAndModify(),  * the positional $ operator acts as a placeholder for the first element that matches the query document, and  * the array field must appear as part of the query document. For example: db.collection.updateOne(   { <array>: value ... },   { <update operator>: { \"<array>.$\" : value } })  \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. \nUPSERT \nDo not use the positional operator $ with upsert operations because inserts will use the $ as a field name in the inserted document. \nNESTED ARRAYS \nThe positional $ operator cannot be used for queries which traverse more than one array, such as queries that traverse arrays nested within other arrays, because the replacement for the $ placeholder is a single value \nUNSETS \nWhen used with the $unset operator, the positional $ operator does not remove the matching element from the array but rather sets it to null. \nNEGATIONS \nIf the query matches the array using a negation operator, such as $ne, $not, or $nin, then you cannot use the positional operator to update values from this array. However, if the negated portion of the query is inside of an $elemMatch expression, then you can use the positional operator to update this field. \nMULTIPLE ARRAY MATCHES \nThe positional $ update operator behaves ambiguously when filtering on multiple array fields. When the server executes an update method, it first runs a query to determine which documents you want to update. If the update filters documents on multiple array fields, the subsequent call to the positional $ update operator doesn't always update the required position in the array. For more information, see the example. \nEXAMPLES  UPDATE VALUES IN AN ARRAY \nCreate a collection students with the following documents: db.students.insertMany( [   { \"_id\" : 1, \"grades\" : [ 85, 80, 80 ] },   { \"_id\" : 2, \"grades\" : [ 88, 90, 92 ] },   { \"_id\" : 3, \"grades\" : [ 85, 100, 90 ] }] )  To update the first element whose value is 80 to 82 in the in the grades array, use the positional $ operator if you do not know the position of the element in the array: \nIMPORTANT You must include the array field as part of the query document. db.students.updateOne(   { _id: 1, grades: 80 },   { $set: { \"grades.$\" : 82 } })  The positional $ operator acts as a placeholder for the first match of the update query document. After the operation, the students collection contains the following documents: { \"_id\" : 1, \"grades\" : [ 85, 82, 80 ] }{ \"_id\" : 2, \"grades\" : [ 88, 90, 92 ] }{ \"_id\" : 3, \"grades\" : [ 85, 100, 90 ] }  \nUPDATE DOCUMENTS IN AN ARRAY \nThe positional $ operator facilitates updates to arrays that contain embedded documents. Use the positional $ operator to access the fields in the embedded documents with the dot notation on the $ operator. db.collection.updateOne(   { <query selector> },   { <update operator>: { \"array.$.field\" : value } })  Consider the following document in the students collection whose grades element value is an array of embedded documents:\n\n  Use the positional $ operator to update the std field of the first array element that matches the grade equal to 85 condition: \nIMPORTANT You must include the array field as part of the query document. db.students.updateOne(   { _id: 4, \"grades.grade\": 85 },   { $set: { \"grades.$.std\" : 6 } })  After the operation, the document has the following updated values: {   \"_id\" : 4,   \"grades\" : [      { \"grade\" : 80, \"mean\" : 75, \"std\" : 8 },      { \"grade\" : 85, \"mean\" : 90, \"std\" : 6 },      { \"grade\" : 85, \"mean\" : 85, \"std\" : 8 }   ]}  \nUPDATE EMBEDDED DOCUMENTS USING MULTIPLE FIELD MATCHES \nThe $ operator can update the first array element that matches multiple query criteria specified with the $elemMatch operator. Consider the following document in the students collection whose grades field value is an array of embedded documents: {  _id: 5,  grades: [     { grade: 80, mean: 75, std: 8 },     { grade: 85, mean: 90, std: 5 },     { grade: 90, mean: 85, std: 3 }  ]}  In the example below, the $ operator updates the value of the std field in the first embedded document that has grade field with a value less than or equal to 90 and a mean field with a value greater than 80: db.students.updateOne(   {     _id: 5,     grades: { $elemMatch: { grade: { $lte: 90 }, mean: { $gt: 80 } } }   },   { $set: { \"grades.$.std\" : 6 } })  This operation updates the first embedded document that matches the criteria, namely the second embedded document in the array: {  _id: 5,  grades: [    { grade: 80, mean: 75, std: 8 },    { grade: 85, mean: 90, std: 6 },    { grade: 90, mean: 85, std: 3 }  ]}  \nUPDATE WITH MULTIPLE ARRAY MATCHES \nThe positional $ update operator behaves ambiguously when the query has multiple array fields to filter documents in the collection. Consider a document in the students_deans_list collection, which holds arrays of student information: db.students_deans_list.insertMany( [   {      _id: 8,      activity_ids: [ 1, 2 ],      grades: [ 90, 95 ],      deans_list: [ 2021, 2020 ]   }] )  In the following example, the user attempts to modify the deans_list field, filtering documents using the activity_ids, deans_list, and grades fields, and updating the 2021 value in the deans_list field to 2022: db.students_deans_list.updateOne(   { activity_ids: 1, grades: 95, deans_list: 2021 },   { $set: { \"deans_list.$\": 2022 } })  When the server executes the updateOne method above, it filters the available documents using values in the supplied array fields. Although the deans_list field is used in the filter, it is not the field used by the positional $ update operator to determine which position in the array to update: db.students_deans_list.find( { _id: 8 } )  Example output: {   _id: 8,   activity_ids: [ 1, 2 ],   grades: [ 90, 95 ],   deans_list: [ 2021, 2022 ]}  The updateOne method matched the deans_list field on 2021, but the positional $ update operator instead changed the 2020 value to 2022. To avoid unexpected results when matching on multiple arrays, instead use the filtered positional operator $[<identifier>]. \nTIP \nSEE ALSO:  * db.collection.updateMany()  * db.collection.findAndModify()  * $elemMatch ←  Array Update Operators$[] → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/slice/": " Docs Home → MongoDB Manual \n$SLICE \nOn this page    \n * Behavior\n   \n * Examples $slice \nThe $slice modifier limits the number of array elements during a $push operation. To project, or return, a specified number of array elements from a read operation, see the $slice projection operator instead. To use the $slice modifier, it must appear with the $each modifier. You can pass an empty array [] to the $each modifier such that only the $slice modifier has an effect. {  $push: {     <field>: {       $each: [ <value1>, <value2>, ... ],       $slice: <num>     }  }}  The <num> can be: Value\nDescription\nZero\nTo update the array <field> to an empty array.\nNegative\nTo update the array <field> to contain only the last <num> elements.\nPositive\nTo update the array <field> contain only the first <num> elements. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. The order in which the modifiers appear is immaterial. Previous versions required the $each modifier to appear as the first modifier if used in conjunction with $slice. For a list of modifiers available for $push, see Modifiers. Trying to use the $slice modifier without the $each modifier results in an error. \nEXAMPLES  SLICE FROM THE END OF THE ARRAY \nA collection students contains the following document: { \"_id\" : 1, \"scores\" : [ 40, 50, 60 ] }  The following operation adds new elements to the scores array, and then uses the $slice modifier to trim the array to the last five elements: db.students.updateOne(   { _id: 1 },   {     $push: {       scores: {         $each: [ 80, 78, 86 ],         $slice: -5       }     }   })  The result of the operation is slice the elements of the updated scores array to the last five elements: { \"_id\" : 1, \"scores\" : [  50,  60,  80,  78,  86 ] }  \nSLICE FROM THE FRONT OF THE ARRAY \nA collection students contains the following document: { \"_id\" : 2, \"scores\" : [ 89, 90 ] }  The following operation adds new elements to the scores array, and then uses the $slice modifier to trim the array to the first three elements. db.students.updateOne(   { _id: 2 },   {     $push: {       scores: {         $each: [ 100, 20 ],         $slice: 3       }     }   })  The result of the operation is to slice the elements of the updated scores array to the first three elements: { \"_id\" : 2, \"scores\" : [  89,  90,  100 ] }  \nUPDATE ARRAY USING SLICE ONLY \nA collection students contains the following document: { \"_id\" : 3, \"scores\" : [  89,  70,  100,  20 ] }  To update the scores field with just the effects of the $slice modifier, specify the number of elements to slice (e.g. -3) for the $slice modifier and an empty array [] for the $each modifier, as in the following: db.students.updateOne(  { _id: 3 },  {    $push: {      scores: {         $each: [ ],         $slice: -3      }    }  })  The result of the operation is to slice the elements of the scores array to the last three elements: { \"_id\" : 3, \"scores\" : [  70,  100,  20 ] }  \nUSE $SLICE WITH OTHER $PUSH MODIFIERS \nAdd the following document to the students collection:\n\n  The following $push operation uses:      * the $each modifier to add multiple documents to the quizzes array,  * the $sort modifier to sort all the elements of the modified quizzes array by the score field in descending order, and  * the $slice modifier to keep only the first three sorted elements of the quizzes array. db.students.updateOne(   { _id: 5 },   {     $push: {       quizzes: {          $each: [ { wk: 5, score: 8 }, { wk: 6, score: 7 }, { wk: 7, score: 6 } ],          $sort: { score: -1 },          $slice: 3       }     }   })  After the operation only the three highest scoring quizzes are in the array: {  \"_id\" : 5,  \"quizzes\" : [     { \"wk\" : 1, \"score\" : 10 },     { \"wk\" : 2, \"score\" : 8 },     { \"wk\" : 5, \"score\" : 8 }  ]}  The order of the modifiers is immaterial to the order in which the modifiers are processed. See Modifiers for details. ←  $position$sort → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/inc/": " Docs Home → MongoDB Manual \n$INC \nOn this page    \n * Definition\n   \n * Behavior\n * Example \nDEFINITION \n$inc \nThe $inc operator increments a field by a specified value and has the following form: { $inc: { <field1>: <amount1>, <field2>: <amount2>, ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. The $inc operator accepts positive and negative values. If the field does not exist, $inc creates the field and sets the field to the specified value. Use of the $inc operator on a field with a null value will generate an error. $inc is an atomic operation within a single document. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $inc with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLE \nCreate the products collection: db.products.insertOne(   {     _id: 1,     sku: \"abc123\",     quantity: 10,     metrics: { orders: 2, ratings: 3.5 }   })  The following updateOne() operation uses the $inc operator to:      * increase the \"metrics.orders\" field by 1  * increase the quantity field by -2 (which decreases quantity) db.products.updateOne(   { sku: \"abc123\" },   { $inc: { quantity: -2, \"metrics.orders\": 1 } })  The updated document would resemble: {  _id: 1,  sku: 'abc123',  quantity: 8,  metrics: { orders: 3, ratings: 3.5 }}  \nTIP \nSEE ALSO:  * db.collection.updateMany()  * db.collection.findAndModify() ←  $currentDate$min → On this page  * Definition\n * Behavior\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/pullAll/": " Docs Home → MongoDB Manual \n$PULLALL \nOn this page    \n * Definition\n   \n * Behavior\n * Examples \nDEFINITION \n$pullAll \nThe $pullAll operator removes all instances of the specified values from an existing array. Unlike the $pull operator that removes elements by specifying a query, $pullAll removes elements that match the listed values. The $pullAll operator has the form: { $pullAll: { <field1>: [ <value1>, <value2> ... ], ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. If a <value> to remove is a document or an array, $pullAll removes only the elements in the array that match the specified <value> exactly, including order. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $pullAll with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLES \nCreate the survey collection: db.survey.insertOne( { _id: 1, scores: [ 0, 2, 5, 5, 1, 0 ] } )  The following operation removes all instances of the values \"0\" and \"5\" from the scores array: db.survey.updateOne( { _id: 1 }, { $pullAll: { scores: [ 0, 5 ] } } )  After the update, the scores field no longer has any instances of \"0\" or \"5\". { \"_id\" : 1, \"scores\" : [ 2, 1 ] }  \nTIP \nSEE ALSO:      * db.collection.updateMany()  * db.collection.findAndModify() ←  $push$each → On this page  * Definition\n * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/pull/": " Docs Home → MongoDB Manual \n$PULL \nOn this page    \n * Behavior\n   \n * Examples $pull \nThe $pull operator removes from an existing array all instances of a value or values that match a specified condition. The $pull operator has the form: { $pull: { <field1>: <value|condition>, <field2>: <value|condition>, ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. If you specify a <condition> and the array elements are embedded documents, $pull operator applies the <condition> as if each array element were a document in a collection. See Remove All Items That Match a Specified $pull Condition With bulkWrite() for an example. If the specified <value> to remove is an array, $pull removes only the elements in the array that match the specified <value> exactly, including order. If the specified <value> to remove is a document, $pull removes only the elements in the array that have the exact same fields and values. The ordering of the fields can differ. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $pull with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nEXAMPLES  REMOVE ALL ITEMS THAT EQUAL A SPECIFIED VALUE \nCreate the stores collection: db.stores.insertMany( [   {      _id: 1,      fruits: [ \"apples\", \"pears\", \"oranges\", \"grapes\", \"bananas\" ],      vegetables: [ \"carrots\", \"celery\", \"squash\", \"carrots\" ]   },   {      _id: 2,      fruits: [ \"plums\", \"kiwis\", \"oranges\", \"bananas\", \"apples\" ],      vegetables: [ \"broccoli\", \"zucchini\", \"carrots\", \"onions\" ]   }] )  The following operation removes      * \"apples\" and \"oranges\" from the fruits array  * \"carrots\" from the vegetables array db.stores.updateMany(    { },    { $pull: { fruits: { $in: [ \"apples\", \"oranges\" ] }, vegetables: \"carrots\" } })  Confirm the result with db.collection.find(): {  _id: 1,  fruits: [ 'pears', 'grapes', 'bananas' ],  vegetables: [ 'celery', 'squash' ]},{  _id: 2,  fruits: [ 'plums', 'kiwis', 'bananas' ],  vegetables: [ 'broccoli', 'zucchini', 'onions' ]}  \nREMOVE ALL ITEMS THAT MATCH A SPECIFIED $PULL CONDITION \nCreate the profiles collection: db.profiles.insertOne( { _id: 1, votes: [ 3, 5, 6, 7, 7, 8 ] } )  The following operation will remove all items from the votes array that are greater than or equal to ( $gte ) 6: db.profiles.updateOne( { _id: 1 }, { $pull: { votes: { $gte: 6 } } } )  After the update operation, the document only has values less than 6: { _id: 1, votes: [  3,  5 ] }  \nREMOVE ALL ITEMS THAT MATCH A SPECIFIED $PULL CONDITION WITH BULKWRITE() \nThe following db.collection.bulkWrite() operation:  * Creates the profilesBulkWrite collection.  * Removes all items from the votes array that are greater than or equal to ( $gte ) 6.  * Removes all items from the votes array that are less than or equal to ( $lte ) 3.\n\n  \nNOTE \nBULKWRITE() The db.collection.bulkWrite() method executes multiple write operations listed in an array. In this example, the db.collection.bulkWrite() performs multiple operations on the profiles collection. After the db.collection.bulkWrite() operation, you can confirm the document only has values less than 6 and greater than 3 using the following operation: db.profilesBulkWrite.find()  The operation returns the following: [ { _id: 1, votes: [ 5 ] } ] \nREMOVE ITEMS FROM AN ARRAY OF DOCUMENTS \nCreate the survey collection: db.survey.insertMany([   {      _id: 1,      results: [         { item: \"A\", score: 5 },         { item: \"B\", score: 8 }      ]   },   {      _id: 2,      results: [         { item: \"C\", score: 8 },         { item: \"B\", score: 4 }      ]   }] )  The following operation removes all elements from the results array that contain both a score field equal to 8 and an item field equal to \"B\": db.survey.updateMany(  { },  { $pull: { results: { score: 8 , item: \"B\" } } })  The $pull expression applies the condition to each element of the results array as though it were a top-level document. After the operation, the results array contains no documents that contain both a score field equal to 8 and an item field equal to \"B\". { _id: 1, results: [ { item: 'A', score: 5 } ] },{  _id: 2,  results: [ { item: 'C', score: 8 }, { item: 'B', score: 4 } ]}  The $pull operator treats each element as a top-level object. The query is applied to each element. The expression does not need to use $elemMatch to specify match conditions. On the contrary, the following operation does not $pull any elements from the original collection: db.survey.updateMany(  { },  { $pull: { results: { $elemMatch: { score: 8 , item: \"B\" } } } })  \nNOTE Drop the survey collection with: Then recreate it to run this example. \nREMOVE DOCUMENTS FROM NESTED ARRAYS \nCreate a new survey collection with documents that are embedded in nested arrays. db.survey.drop()\ndb.survey.insertMany( [   {      _id: 1,      results: [         {            item: \"A\",            score: 5,            answers: [ { q: 1, a: 4 }, { q: 2, a: 6 } ]         },         {            item: \"B\",            score: 8,            answers: [ { q: 1, a: 8 }, { q: 2, a: 9 } ]         }      ]   },   {      _id: 2,      results: [         {            item: \"C\",            score: 8,            answers: [ { q: 1, a: 8 }, { q: 2, a: 7 } ]         },         {            item: \"B\",            score: 4,            answers: [ { q: 1, a: 0 }, { q: 2, a: 8 } ]         }      ]   }] ) \n\n db.survey.updateMany(  { },  {     $pull:        {           results:              {                 answers: { $elemMatch: { q: 2, a: { $gte: 8 } } }              }        }  })  The operation updated the results array in each document it matched. db.collection.updateMany() removed documents from results when an element of the embedded answers array matched the selection conditions in the highlighted line. {  _id: 1,  results: [    {      item: 'A',      score: 5,      answers: [ { q: 1, a: 4 }, { q: 2, a: 6 } ]    }  ]},{  _id: 2,  results: [    {      item: 'C',      score: 8,      answers: [ { q: 1, a: 8 }, { q: 2, a: 7 } ]    }  ]}  \nTIP \nSEE ALSO:  * db.collection.updateMany()  * db.collection.findAndModify() ←  $pop$push → On this page  * Behavior\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/replace/": " Docs Home → MongoDB Manual \nREPLACE EVENT \nOn this page    \n * Summary\n   \n * Description\n * Behavior\n * Document Pre- and Post-Images\n * Examples \nSUMMARY \nreplace \nA replace event occurs when an update operation removes a document from a collection and replaces it with a new document, such as when the replaceOne method is called. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. documentKey\ndocument Document that contains the _id value of the document created or modified by the CRUD operation. For sharded collections, this field also displays the full shard key for the document. The _id field is not repeated if it is already a part of the shard key. fullDocument\ndocument The new document created by the operation. Changed in version 6.0. Starting in MongoDB 6.0, if you set the changeStreamPreAndPostImages option using db.createCollection(), create, or collMod, then the fullDocument field shows the document after it was inserted, replaced, or updated (the document post-image). fullDocument is always included for insert events. fullDocumentBeforeChange\ndocument The document before changes were applied by the operation. That is, the document pre-image. This field is available when you enable the changeStreamPreAndPostImages field for a collection using db.createCollection() method or the create or collMod commands. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.coll\nstring The name of the collection where the event occurred. ns.db\nstring The name of the database where the event occurred. operationType\nstring The type of operation that the change notification reports. Returns a value of replace for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nBEHAVIOR  DOCUMENT PRE- AND POST-IMAGES \nStarting in MongoDB 6.0, you see a fullDocumentBeforeChange document with the fields before the document was changed (or deleted) if you perform these steps:       1. Enable the new changeStreamPreAndPostImages field for a collection using db.createCollection(), create, or collMod.  2. Set fullDocumentBeforeChange to \"required\" or \"whenAvailable\" in db.collection.watch(). Example fullDocumentBeforeChange document in the change stream output: \"fullDocumentBeforeChange\" : {   \"_id\" : ObjectId(\"599af247bb69cd89961c986d\"),   \"userName\" : \"alice123\",   \"name\" : \"Alice Smith\"} For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. Pre- and post-images are not available for a change stream event if the images were:  * Not enabled on the collection at the time of a document update or delete operation.\n\n Additional considerations:  * Enabling pre- and post-images consumes storage space and adds processing time. Only enable pre- and post-images if you need them.  * Limit the change stream event size to less than 16 megabytes. To limit the event size, you can:\n   \n   * Limit the document size to 8 megabytes. You can request pre- and post-images simultaneously in the change stream output if other change stream event fields like updateDescription are not large.\n   \n   * Request only post-images in the change stream output for documents up to 16 megabytes if other change stream event fields like updateDescription are not large.\n   \n   * Request only pre-images in the change stream output for documents up to 16 megabytes if:\n     \n     * document updates affect only a small fraction of the document structure or content, and\n     \n     * do not cause a replace change event. A replace event always includes the post-image.  * To request a pre-image, you set fullDocumentBeforeChange to required or whenAvailable in db.collection.watch(). To request a post-image, you set fullDocument using the same method.  * Pre-images are written to the config.system.preimages collection.\n   \n   * The config.system.preimages collection may become large. To limit the collection size, you can set expireAfterSeconds time for the pre-images as shown earlier.\n   \n   * Pre-images are removed asynchronously by a background process. \nIMPORTANT \nBACKWARD-INCOMPATIBLE FEATURE Starting in MongoDB 6.0, if you are using document pre- and post-images for change streams, you must disable changeStreamPreAndPostImages for each collection using the collMod command before you can downgrade to an earlier MongoDB version. \nTIP \nSEE ALSO:  * For change stream events and output, see Change Events.  * To watch a collection for changes, see db.collection.watch().  * For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. \nEXAMPLES \nThe following example illustrates a replace event: {   \"_id\": { <Resume Token> },   \"operationType\": \"replace\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   },   \"documentKey\": {      \"_id\": ObjectId(\"599af247bb69cd89961c986d\")   },   \"fullDocument\": {      \"_id\": ObjectId(\"599af247bb69cd89961c986d\"),      \"userName\": \"alice123\",      \"name\": \"Alice\"   }} A replace operation uses the update command, and consists of two stages:  * Delete the original document with the documentKey and  * Insert the new document using the same documentKey The fullDocument of a replace event represents the document after the insert of the replacement document. ←  rename EventshardCollection Event → On this page  * Summary\n * Description\n * Behavior\n * Document Pre- and Post-Images\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/createIndexes/": " Docs Home → MongoDB Manual \nCREATEINDEXES EVENT \nOn this page    \n * Summary\n   \n * Description\n * Example \nSUMMARY \ncreateIndexes \nNew in version 6.0. A createIndexes event occurs when an index is created on the collection and the change stream has the showExpandedEvents option set to true. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.db\nstring The name of the database where the event occurred. ns.coll\nstring The name of the collection where the event occurred. operationDescription\ndocument Additional information on the change operation. This document and its subfields only appears when the change stream uses expanded events. New in version 6.0. operationDescription.\nindexes\narray An array of documents listing the indexes that were created by the operation. New in version 6.0. operationType\nstring The type of operation that the change notification reports. Returns a value of createIndexes for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example shows a createIndexes event: {   \"_id\": { <ResumeToken> },   \"operationType\": \"createIndexes\",   \"clusterTime\": Timestamp({ t: 1651257835, i: 1 }),   \"collectionUUID\": UUID(\"06bced37-7cc8-4267-96aa-a58a422153d8\"),   \"wallTime\": ISODate(\"2022-04-29T18:43:55.160Z\"),   \"ns\": {      \"db\": \"test\",      \"coll\": \"authors\"   },   \"operationDescription\": {      \"indexes\": [         { \"v\": 2, \"key\": { \"name\": 1 }, \"name\": \"name_1\" }      ]   }} ←  create Eventdelete Event → On this page  * Summary\n * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/drop/": " Docs Home → MongoDB Manual \nDROP EVENT \nOn this page    \n * Synopsis\n   \n * Description\n * Example \nSYNOPSIS \ndrop \nNew in version 4.0.1. A drop event occurs when a collection is dropped from a database. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.coll\nstring The name of the collection where the event occurred. ns.db\nstring The name of the database where the event occurred. operationType\nstring The type of operation that the change notification reports. Returns a value of drop for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example illustrates a drop event: {   \"_id\": { <Resume Token> },   \"operationType\": \"drop\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   }}  A drop event leads to an invalidate event for change streams opened against its own ns collection. ←  delete EventdropDatabase → On this page  * Synopsis\n * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/operator/update/push/": " Docs Home → MongoDB Manual \n$PUSH \nOn this page    \n * Definition\n   \n * Behavior\n * Modifiers\n * Examples \nDEFINITION \n$push \nThe $push operator appends a specified value to an array. The $push operator has the form: { $push: { <field1>: <value1>, ... } }  To specify a <field> in an embedded document or in an array, use dot notation. \nBEHAVIOR \nStarting in MongoDB 5.0, update operators process document fields with string-based names in lexicographic order. Fields with numeric names are processed in numeric order. See Update Operators Behavior for details. If the field is absent in the document to update, $push adds the array field with the value as its element. If the field is not an array, the operation will fail. If the value is an array, $push appends the whole array as a single element. To add each element of the value separately, use the $each modifier with $push. For an example, see Append a Value to Arrays in Multiple Documents. For a list of modifiers available for $push, see Modifiers. Starting in MongoDB 5.0, mongod no longer raises an error when you use an update operator like $push with an empty operand expression ( { } ). An empty update results in no changes and no oplog entry is created (meaning that the operation is a no-op). \nMODIFIERS \nYou can use the $push operator with the following modifiers: Modifier\nDescription\n$each\nAppends multiple values to the array field.\n$slice\nLimits the number of array elements. Requires the use of the $each modifier.\n$sort\nOrders elements of the array. Requires the use of the $each modifier.\n$position\nSpecifies the location in the array at which to insert the new elements. Requires the use of the $each modifier. Without the $position modifier, the $push appends the elements to the end of the array. When used with modifiers, the $push operator has the form: { $push: { <field1>: { <modifier1>: <value1>, ... }, ... } }  The processing of the $push operation with modifiers occur in the following order, regardless of the order in which the modifiers appear:       1. Update array to add elements in the correct position.  2. Apply sort, if specified.  3. Slice the array, if specified.  4. Store the array. \nEXAMPLES \nCreate the students collection: db.students.insertOne( { _id: 1, scores: [ 44, 78, 38, 80 ] } )  \nAPPEND A VALUE TO AN ARRAY \nThe following example appends 89 to the scores array: db.students.updateOne(   { _id: 1 },   { $push: { scores: 89 } })  Example output: { _id: 1, scores: [ 44, 78, 38, 80, 89 ] }  \nAPPEND A VALUE TO ARRAYS IN MULTIPLE DOCUMENTS \nAdd the following documents to the students collection: db.students.insertMany( [   { _id: 2, scores: [ 45, 78, 38, 80, 89 ] } ,   { _id: 3, scores: [ 46, 78, 38, 80, 89 ] } ,   { _id: 4, scores: [ 47, 78, 38, 80, 89 ] }] )  The following $push operation appends 95 to the scores array in each document: db.students.updateMany(   { },   { $push: { scores: 95 } })  To confirm that each scores array includes 95, run the following operation: db.students.find()  The operation returns the following results: [   { _id: 1, scores: [ 44, 78, 38, 80, 89, 95 ] },   { _id: 2, scores: [ 45, 78, 38, 80, 89, 95 ] },   { _id: 3, scores: [ 46, 78, 38, 80, 89, 95 ] },   { _id: 4, scores: [ 47, 78, 38, 80, 89, 95 ] }] \nAPPEND MULTIPLE VALUES TO AN ARRAY \nUse $push with the $each modifier to append multiple values to the array field. The following example appends each element of [ 90, 92, 85 ] to the scores array for the document where the name field equals joe:\n\n  \nUSE $PUSH OPERATOR WITH MULTIPLE MODIFIERS \nAdd the following document to the students collection: db.students.insertOne(   {      \"_id\" : 5,      \"quizzes\" : [         { \"wk\": 1, \"score\" : 10 },         { \"wk\": 2, \"score\" : 8 },         { \"wk\": 3, \"score\" : 5 },         { \"wk\": 4, \"score\" : 6 }      ]   })  The following $push operation uses:  * the $each modifier to add multiple documents to the quizzes array,  * the $sort modifier to sort all the elements of the modified quizzes array by the score field in descending order, and  * the $slice modifier to keep only the first three sorted elements of the quizzes array. db.students.updateOne(   { _id: 5 },   {     $push: {       quizzes: {          $each: [ { wk: 5, score: 8 }, { wk: 6, score: 7 }, { wk: 7, score: 6 } ],          $sort: { score: -1 },          $slice: 3       }     }   })  After the operation only the three highest scoring quizzes are in the array: {  \"_id\" : 5,  \"quizzes\" : [     { \"wk\" : 1, \"score\" : 10 },     { \"wk\" : 2, \"score\" : 8 },     { \"wk\" : 5, \"score\" : 8 }  ]}  \nTIP \nSEE ALSO:  * db.collection.updateMany()  * db.collection.findAndModify() ←  $pull$pullAll → On this page  * Definition\n * Behavior\n * Modifiers\n * Examples Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/dropIndexes/": " Docs Home → MongoDB Manual \nDROPINDEXES \nOn this page    \n * Synopsis\n   \n * Descriptions\n * Example \nSYNOPSIS \ndropIndexes \nNew in version 6.0. A dropIndexes event occurs when an index is dropped from the collection and the change stream has the showExpandedEvents option set to true. \nDESCRIPTIONS \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.db\nstring The name of the database where the event occurred. ns.coll\nstring The name of the collection where the event occurred. operationDescription\ndocument Additional information on the change operation. This document and its subfields only appears when the change stream uses expanded events. New in version 6.0. operationDescription.\nindexes\narray An array of documents listing the indexes that were dropped by the operation. New in version 6.0. operationType\nstring The type of operation that the change notification reports. Returns a value of dropIndexes for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example shows a dropIndexes event: {   \"_id\": { <ResumeToken> },   \"operationType\": \"dropIndexes\",   \"clusterTime\": <Timestamp>   \"collectionUUID\": <uuid>,   \"wallTime\": <isodate>,   \"ns\": {      \"db\": \"test\",      \"coll\": \"authors\" },   \"operationDescription\": {      \"indexes\": [         { \"v\": 2, \"key\": { \"name\": 1 }, \"name\": \"name_1\" }      ]   }} ←  dropDatabaseinsert → On this page  * Synopsis\n * Descriptions\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/dropDatabase/": " Docs Home → MongoDB Manual \nDROPDATABASE \nOn this page    \n * Synopsis\n   \n * Description\n * Example \nSYNOPSIS \ndropDatabase \nNew in version 4.0.1. A dropDatabase event occurs when a database is dropped. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.db\nstring The name of the database where the event occurred. operationType\nstring The type of operation that the change notification reports. Returns a value of dropDatabase for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example illustrates a dropDatabase event: {   \"_id\": { <Resume Token> },   \"operationType\": \"dropDatabase\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\"   }} A dropDatabase command generates a drop event for each collection in the database before generating a dropDatabase event for the database. A dropDatabase event leads to an invalidate event for change streams opened against its own ns.db database. ←  drop EventdropIndexes → On this page  * Synopsis\n * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/update/": " Docs Home → MongoDB Manual \nUPDATE EVENT \nOn this page    \n * Summary\n   \n * Description\n * Behavior\n * Document Pre- and Post-Images\n * Example \nSUMMARY \nupdate \nAn update event occurs when an operation updates a document in a collection. \nNOTE \nDISAMBIGUATION To learn more about events that occur when collection options are modified, see the modify event. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. documentKey\ndocument Document that contains the _id value of the document created or modified by the CRUD operation. For sharded collections, this field also displays the full shard key for the document. The _id field is not repeated if it is already a part of the shard key. fullDocument\ndocument The document created or modified by a CRUD operation. This field only appears if you configured the change stream with fullDocument set to updateLookup. When you configure the change stream with updateLookup, the field represents the current majority-committed version of the document modified by the update operation. The document may differ from the changes described in updateDescription if any other majority-committed operations have modified the document between the original update operation and the full document lookup. For more information, see Lookup Full Document for Update Operations. Changed in version 6.0. Starting in MongoDB 6.0, if you set the changeStreamPreAndPostImages option using db.createCollection(), create, or collMod, then the fullDocument field shows the document after it was inserted, replaced, or updated (the document post-image). fullDocument is always included for insert events. fullDocumentBeforeChange\ndocument The document before changes were applied by the operation. That is, the document pre-image. This field is available when you enable the changeStreamPreAndPostImages field for a collection using db.createCollection() method or the create or collMod commands. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.coll\nstring The name of the collection where the event occurred. ns.db\nstring The name of the database where the event occurred. operationType\nstring The type of operation that the change notification reports. Returns a value of update for these change events. updateDescription\ndocument A document describing the fields that were updated or removed by the update operation. updateDescription.\nremovedFields\narray An array of fields that were removed by the update operation. updateDescription.\ntruncatedArrays\narray An array of documents which record array truncations performed with pipeline-based updates using one or more of the following stages:      * $addFields  * $set  * $replaceRoot  * $replaceWith \nNOTE If the entire array is replaced, the truncations will be reported under updateDescription.updatedFields. updateDescription.\ntrucatedArrays.\nfield\nstring The name of the truncated field. updateDescription.\ntrucatedArrays.\nnewSize\ninteger The number of elements in the truncated array. updateDescription.\nupdatedFields\ndocument A document whose keys correspond to the fields that were modified by the update operation. The value of each field corresponds to the new value of those fields, rather than the operation that resulted in the new value.\n\n Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nBEHAVIOR  DOCUMENT PRE- AND POST-IMAGES \nStarting in MongoDB 6.0, you see a fullDocumentBeforeChange document with the fields before the document was changed (or deleted) if you perform these steps:  1. Enable the new changeStreamPreAndPostImages field for a collection using db.createCollection(), create, or collMod.  2. Set fullDocumentBeforeChange to \"required\" or \"whenAvailable\" in db.collection.watch(). Example fullDocumentBeforeChange document in the change stream output: \"fullDocumentBeforeChange\" : {   \"_id\" : ObjectId(\"599af247bb69cd89961c986d\"),   \"userName\" : \"alice123\",   \"name\" : \"Alice Smith\"} For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. Pre- and post-images are not available for a change stream event if the images were:  * Not enabled on the collection at the time of a document update or delete operation.  * Removed after the pre- and post-image retention time set in expireAfterSeconds.\n   \n   * The following example sets expireAfterSeconds to 100 seconds:\n     \n     use admindb.runCommand( {   setClusterParameter:      { changeStreamOptions: { preAndPostImages: { expireAfterSeconds: 100 } } }} )\n     \n     \n   \n   * The following example returns the current changeStreamOptions settings, including expireAfterSeconds:\n     \n     db.adminCommand( { getClusterParameter: \"changeStreamOptions\" } )\n     \n     \n   \n   * Setting expireAfterSeconds to off uses the default retention policy: pre- and post-images are retained until the corresponding change stream events are removed from the oplog.\n   \n   * If a change stream event is removed from the oplog, then the corresponding pre- and post-images are also deleted regardless of the expireAfterSeconds pre- and post-image retention time. Additional considerations:  * Enabling pre- and post-images consumes storage space and adds processing time. Only enable pre- and post-images if you need them.  * Limit the change stream event size to less than 16 megabytes. To limit the event size, you can:\n   \n   * Limit the document size to 8 megabytes. You can request pre- and post-images simultaneously in the change stream output if other change stream event fields like updateDescription are not large.\n   \n   * Request only post-images in the change stream output for documents up to 16 megabytes if other change stream event fields like updateDescription are not large.\n   \n   * Request only pre-images in the change stream output for documents up to 16 megabytes if:\n     \n     * document updates affect only a small fraction of the document structure or content, and\n     \n     * do not cause a replace change event. A replace event always includes the post-image.  * To request a pre-image, you set fullDocumentBeforeChange to required or whenAvailable in db.collection.watch(). To request a post-image, you set fullDocument using the same method.  * Pre-images are written to the config.system.preimages collection.\n   \n   * The config.system.preimages collection may become large. To limit the collection size, you can set expireAfterSeconds time for the pre-images as shown earlier.\n   \n   * Pre-images are removed asynchronously by a background process. \nIMPORTANT \nBACKWARD-INCOMPATIBLE FEATURE Starting in MongoDB 6.0, if you are using document pre- and post-images for change streams, you must disable changeStreamPreAndPostImages for each collection using the collMod command before you can downgrade to an earlier MongoDB version. \nTIP \nSEE ALSO:\n\n  * To watch a collection for changes, see db.collection.watch().  * For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. \nEXAMPLE \nThe following example illustrates an update event: {   \"_id\": { <Resume Token> },   \"operationType\": \"update\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   },   \"documentKey\": {      \"_id\": ObjectId(\"58a4eb4a30c75625e00d2820\")   },   \"updateDescription\": {      \"updatedFields\": {         \"email\": \"alice@10gen.com\"      },      \"removedFields\": [\"phoneNumber\"],      \"truncatedArrays\": [ {         \"field\" : \"vacation_time\",         \"newSize\" : 36      } ]   }} The following example illustrates an update event for change streams opened with the fullDocument : updateLookup option: {   \"_id\": { <Resume Token> },   \"operationType\": \"update\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   },   \"documentKey\": {      \"_id\": ObjectId(\"58a4eb4a30c75625e00d2820\")   },   \"updateDescription\": {      \"updatedFields\": {         \"email\": \"alice@10gen.com\"      },      \"removedFields\": [\"phoneNumber\"],      \"truncatedArrays\": [ {         \"field\" : \"vacation_time\",         \"newSize\" : 36      } ]   },   \"fullDocument\": {      \"_id\": ObjectId(\"58a4eb4a30c75625e00d2820\"),      \"name\": \"Alice\",      \"userName\": \"alice123\",      \"email\": \"alice@10gen.com\",      \"team\": \"replication\"   }} The fullDocument document represents the most current majority-committed version of the updated document. The fullDocument document may vary from the document at the time of the update operation depending on the number of interleaving majority-committed operations that occur between the update operation and the document lookup. ←  shardCollection EventTime Series → On this page  * Summary\n * Description\n * Behavior\n * Document Pre- and Post-Images\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/create/": " Docs Home → MongoDB Manual \nCREATE EVENT \nOn this page    \n * Summary\n   \n * Description\n * Example \nSUMMARY \ncreate \nNew in version 6.0. A create event occurs when a collection is created on a watched database and the change stream has the showExpandedEvents option set to true. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.coll\nstring The name of the collection where the event occurred. ns.db\nstring The name of the database where the event occurred. operationDescription\ndocument Additional information on the change operation. This document and its subfields only appears when the change stream uses expanded events. New in version 6.0. operationDescription.\nidIndex\ndocument The default _id index for a new collection. New in version 6.0. operationType\nstring The type of operation that the change notification reports. Returns a value of create for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example shows a create event: {   \"_id\": { <ResumeToken> },   \"operationType\": \"create\",   \"clusterTime\": Timestamp({ t: 1654894547, i: 24 }),   \"collectionUUID\": UUID(\"98046a1a-b649-4e5b-9c75-67594221ce19\"),   \"wallTime\": ISODate(\"2022-06-10T20:55:47.947Z\"),   \"ns\": {      \"db\": \"test\",      \"coll\": \"names\"   },   \"operationDescription\": {      \"idIndex\": { \"v\": 2, \"key\": { _id: 1 }, \"name\": \"_id_\" }   }} ←  Change EventscreateIndexes Event → On this page  * Summary\n * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/modify/": " Docs Home → MongoDB Manual \nMODIFY EVENT \nOn this page    \n * Summary\n   \n * Description\n * Example \nSUMMARY \nmodify \nNew in version 6.0. A modify event occurs when a collection is modified, such as when the collMod command adds or remove options from a collection or view. This event is received only if the change stream has the showExpandedEvents option set to true. \nNOTE \nDISAMBIGUATION To learn more about events that occur when individual documents are updated, see the update event. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.db\nstring The name of the database where the event occurred. ns.coll\nstring The name of the collection where the event occurred. operationDescription\ndocument Additional information on the change operation. This document and its subfields only appears when the change stream uses expanded events. New in version 6.0. operationDescription.\nindex\ndocument The index that was modified. New in version 6.0. operationDescription.\nindexes\narray\nAn array of documents listing the indexes that were changed by the operation.\noperationType\nstring The type of operation that the change notification reports. Returns a value of modify for these change events. stateBeforeChange\ndocument Document shows collection and index options before the operation changed them. stateBeforeChange.\ncollectionOptions\ndocument Document shows collection options before the operation changed them. stateBeforeChange.\nindexOptions\ndocument Document shows index options before the operation changed them. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example shows a modify event:\n\n ←  invalidaterename Event → On this page  * Summary\n * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/rename/": " Docs Home → MongoDB Manual \nRENAME EVENT \nOn this page    \n * Summary\n   \n * Description\n * Behavior\n * Expanded Event Information\n * Example \nSUMMARY \nrename \nNew in version 4.0.1. A rename event occurs when a collection is renamed. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.db\nstring The name of the database where the event occurred. ns.coll\nstring The name of the collection where the event occurred. operationDescription\ndocument Additional information on the change operation. This document and its subfields only appears when the change stream uses expanded events. New in version 6.0. operationDescription.\ndropTarget\nUUID UUID of the collection that was dropped in the rename operation. New in version 6.0. operationDescription.\nto\ndocument The new namespace of the collection after the rename. New in version 6.0. operationDescription.\nto.coll\ndocument The new name of the collection after the rename. New in version 6.0. operationDescription.\nto.db\ndocument The new name of the database after the rename. New in version 6.0. operationType\nstring The type of operation that the change notification reports. Returns a value of rename for these change events. to\ndocument The new namespace of the collection after the rename. to.coll\ndocument The new name of the collection after the rename. to.db\ndocument The new name of the database after the rename. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nBEHAVIOR  EXPANDED EVENT INFORMATION \nChanged in version 6.0. Starting in MongoDB 6.0, when the showExpandedEvents option is set to true for the change stream, the rename event includes an operationDescription document. This document provides a to field showing the changed database and collection and a dropTarget field indicating whether the rename operation removed the collection before the rename. \nEXAMPLE \nThe following example illustrates a rename event: {   \"_id\": { <Resume Token> },   \"operationType\": \"rename\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   },   \"to\": {      \"db\": \"engineering\",      \"coll\": \"people\"   },   \"operationDescription\": {      \"to\": {         \"db\": \"engineering\",         \"coll\": \"people\"      }   }}\n\n ←  modify Eventreplace Event → On this page  * Summary\n * Description\n * Behavior\n * Expanded Event Information\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/insert/": " Docs Home → MongoDB Manual \nINSERT \nOn this page    \n * Summary\n   \n * Description\n * Example \nSUMMARY \ninsert \nAn insert event occurs when an operation adds documents to a collection. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. documentKey\ndocument Document that contains the _id value of the document created or modified by the CRUD operation. For sharded collections, this field also displays the full shard key for the document. The _id field is not repeated if it is already a part of the shard key. fullDocument\ndocument The document created by the operation. Changed in version 6.0. Starting in MongoDB 6.0, if you set the changeStreamPreAndPostImages option using db.createCollection(), create, or collMod, then the fullDocument field shows the document after it was inserted, replaced, or updated (the document post-image). fullDocument is always included for insert events. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.coll\nstring The name of the collection where the event occurred. ns.db\nstring The name of the database where the event occurred. operationType\nstring The type of operation that the change notification reports. Returns a value of insert for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nEXAMPLE \nThe following example illustrates an insert event: {   \"_id\": { <Resume Token> },   \"operationType\": \"insert\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   },   \"documentKey\": {      \"userName\": \"alice123\",      \"_id\": ObjectId(\"599af247bb69cd89961c986d\")   },   \"fullDocument\": {      \"_id\": ObjectId(\"599af247bb69cd89961c986d\"),      \"userName\": \"alice123\",      \"name\": \"Alice\"   }} The documentKey field includes both the _id and the userName field. This indicates that the engineering.users collection is sharded, with a shard key on userName and _id. The fullDocument document represents the version of the document at the time of the insert. ←  dropIndexesinvalidate → On this page  * Summary\n * Description\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/applications/indexes/": " Docs Home → MongoDB Manual \nINDEXING STRATEGIES \nThe best indexes for your application must take a number of factors into account, including the kinds of queries you expect, the ratio of reads to writes, and the amount of free memory on your system. When developing your indexing strategy you should have a deep understanding of your application's queries. Before you build indexes, map out the types of queries you will run so that you can build indexes that reference those fields. Indexes come with a performance cost, but are more than worth the cost for frequent queries on large data sets. Consider the relative frequency of each query in the application and whether the query justifies an index. The best overall strategy for designing indexes is to profile a variety of index configurations with data sets similar to the ones you'll be running in production to see which configurations perform best. Inspect the current indexes created for your collections to ensure they are supporting your current and planned queries. If an index is no longer used, drop the index. Generally, MongoDB only uses one index to fulfill most queries. However, each clause of an $or query may use a different index, and in addition, MongoDB can use an intersection of multiple indexes. The following documents introduce indexing strategies: Use the ESR (Equality, Sort, Range) RuleThe ESR (Equality, Sort, Range) Rule is a guide to creating indexes that support your queries efficiently.Create Indexes to Support Your QueriesAn index supports a query when the index contains all the fields scanned by the query. Creating indexes that support queries results in greatly increased query performance.Use Indexes to Sort Query ResultsTo support efficient queries, use the strategies here when you specify the sequential order and sort order of index fields.Ensure Indexes Fit in RAMWhen your index fits in RAM, the system can avoid reading the index from disk and you get the fastest processing.Create Queries that Ensure SelectivitySelectivity is the ability of a query to narrow results using the index. Selectivity allows MongoDB to use the index for a larger portion of the work associated with fulfilling the query.\n←  Measure Index UseThe ESR (Equality, Sort, Range) Rule → Share Feedback\n",
  "https://www.mongodb.com/docs/manual/reference/change-events/delete/": " Docs Home → MongoDB Manual \nDELETE EVENT \nOn this page    \n * Synopsis\n   \n * Description\n * Behavior\n * Document Pre- and Post-Images\n * Example \nSYNOPSIS \ndelete \nA delete event occurs when operations remove documents from a collection, such as when a user or application executes the delete command. \nDESCRIPTION \nField\nType\nDescription\n_id\nDocument A BSON object which serves as an identifier for the change stream event. This value is used as the resumeToken for the resumeAfter parameter when resuming a change stream. The _id object has the following form: {   \"_data\" : <BinData|hex string>}  The _data type depends on the MongoDB versions and, in some cases, the feature compatibility version (FCV) at the time of the change stream's opening or resumption. See Resume Tokens for the full list of _data types. For an example of resuming a change stream by resumeToken, see Resume a Change Stream. clusterTime\nTimestamp The timestamp from the oplog entry associated with the event. Change stream event notifications associated with a multi-document transaction all have the same clusterTime value: the time when the transaction was committed. On sharded clusters, events with the same clusterTime may not all relate to the same transaction. Some events don't relate to a transaction at all. To identify events for a single transaction, you can use the combination of lsid and txnNumber in the change stream event document. New in version 4.0. collectionUUID\nUUID UUID identifying the collection where the change occurred. New in version 6.0. documentKey\ndocument Document that contains the _id value of the document created or modified by the CRUD operation. For sharded collections, this field also displays the full shard key for the document. The _id field is not repeated if it is already a part of the shard key. lsid\ndocument The identifier for the session associated with the transaction. Only present if the operation is part of a multi-document transaction. New in version 4.0. ns\ndocument The namespace (database and or collection) affected by the event. ns.coll\nstring The name of the collection where the event occurred. ns.db\nstring The name of the database where the event occurred. operationDescription\ndocument Additional information on the change operation. This document and its subfields only appears when the change stream uses expanded events. New in version 6.0. operationType\nstring The type of operation that the change notification reports. Returns a value of delete for these change events. txnNumber\nNumberLong Together with the lsid, a number that helps uniquely identify a transction. Only present if the operation is part of a multi-document transaction. New in version 4.0. wallTime\nISODate The server date and time of the database operation. wallTime differs from clusterTime in that clusterTime is a timestamp taken from the oplog entry associated with the database operation event. New in version 6.0. \nBEHAVIOR  DOCUMENT PRE- AND POST-IMAGES \nStarting in MongoDB 6.0, you see a fullDocumentBeforeChange document with the fields before the document was changed (or deleted) if you perform these steps:       1. Enable the new changeStreamPreAndPostImages field for a collection using db.createCollection(), create, or collMod.  2. Set fullDocumentBeforeChange to \"required\" or \"whenAvailable\" in db.collection.watch(). Example fullDocumentBeforeChange document in the change stream output: \"fullDocumentBeforeChange\" : {   \"_id\" : ObjectId(\"599af247bb69cd89961c986d\"),   \"userName\" : \"alice123\",   \"name\" : \"Alice Smith\"} For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. Pre- and post-images are not available for a change stream event if the images were:  * Not enabled on the collection at the time of a document update or delete operation.\n\n Additional considerations:  * Enabling pre- and post-images consumes storage space and adds processing time. Only enable pre- and post-images if you need them.  * Limit the change stream event size to less than 16 megabytes. To limit the event size, you can:\n   \n   * Limit the document size to 8 megabytes. You can request pre- and post-images simultaneously in the change stream output if other change stream event fields like updateDescription are not large.\n   \n   * Request only post-images in the change stream output for documents up to 16 megabytes if other change stream event fields like updateDescription are not large.\n   \n   * Request only pre-images in the change stream output for documents up to 16 megabytes if:\n     \n     * document updates affect only a small fraction of the document structure or content, and\n     \n     * do not cause a replace change event. A replace event always includes the post-image.  * To request a pre-image, you set fullDocumentBeforeChange to required or whenAvailable in db.collection.watch(). To request a post-image, you set fullDocument using the same method.  * Pre-images are written to the config.system.preimages collection.\n   \n   * The config.system.preimages collection may become large. To limit the collection size, you can set expireAfterSeconds time for the pre-images as shown earlier.\n   \n   * Pre-images are removed asynchronously by a background process. \nIMPORTANT \nBACKWARD-INCOMPATIBLE FEATURE Starting in MongoDB 6.0, if you are using document pre- and post-images for change streams, you must disable changeStreamPreAndPostImages for each collection using the collMod command before you can downgrade to an earlier MongoDB version. \nTIP \nSEE ALSO:  * For change stream events and output, see Change Events.  * To watch a collection for changes, see db.collection.watch().  * For complete examples with the change stream output, see Change Streams with Document Pre- and Post-Images. \nEXAMPLE \nThe following example illustrates a delete event: {   \"_id\": { <Resume Token> },   \"operationType\": \"delete\",   \"clusterTime\": <Timestamp>,   \"wallTime\": <ISODate>,   \"ns\": {      \"db\": \"engineering\",      \"coll\": \"users\"   },   \"documentKey\": {      \"_id\": ObjectId(\"599af247bb69cd89961c986d\")   }} The fullDocument document is omitted as the document no longer exists at the time the change stream cursor sends the delete event to the client. ←  createIndexes Eventdrop Event → On this page  * Synopsis\n * Description\n * Behavior\n * Document Pre- and Post-Images\n * Example Share Feedback\n",
  "https://www.mongodb.com/docs/manual/applications/data-models-relationships/": " Docs Home → MongoDB Manual \nMODEL RELATIONSHIPS BETWEEN DOCUMENTS \nModel One-to-One Relationships with Embedded DocumentsPresents a data model that uses embedded documents to describe one-to-one relationships between connected data.Model One-to-Many Relationships with Embedded DocumentsPresents a data model that uses embedded documents to describe one-to-many relationships between connected data.Model One-to-Many Relationships with Document ReferencesPresents a data model that uses references to describe one-to-many relationships between documents.\n←  Data Model Examples and PatternsModel One-to-One Relationships with Embedded Documents → Share Feedback\n"
}